{"id": "2602.07152", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2602.07152", "abs": "https://arxiv.org/abs/2602.07152", "authors": ["Kristopher W. Reese", "Taylor Kulp-McDowall", "Michael Majurski", "Tim Blattner", "Derek Juba", "Peter Bajcsy", "Antonio Cardone", "Philippe Dessauw", "Alden Dima", "Anthony J. Kearsley", "Melinda Kleczynski", "Joel Vasanth", "Walid Keyrouz", "Chace Ashcraft", "Neil Fendley", "Ted Staley", "Trevor Stout", "Josh Carney", "Greg Canal", "Will Redman", "Aurora Schmidt", "Cameron Hickert", "William Paul", "Jared Markowitz", "Nathan Drenkow", "David Shriver", "Marissa Connor", "Keltin Grimes", "Marco Christiani", "Hayden Moore", "Jordan Widjaja", "Kasimir Gabert", "Uma Balakrishnan", "Satyanadh Gundimada", "John Jacobellis", "Sandya Lakkur", "Vitus Leung", "Jon Roose", "Casey Battaglino", "Farinaz Koushanfar", "Greg Fields", "Xihe Gu", "Yaman Jandali", "Xinqiao Zhang", "Akash Vartak", "Tim Oates", "Ben Erichson", "Michael Mahoney", "Rauf Izmailov", "Xiangyu Zhang", "Guangyu Shen", "Siyuan Cheng", "Shiqing Ma", "XiaoFeng Wang", "Haixu Tang", "Di Tang", "Xiaoyi Chen", "Zihao Wang", "Rui Zhu", "Susmit Jha", "Xiao Lin", "Manoj Acharya", "Wenchao Li", "Chao Chen"], "title": "Trojans in Artificial Intelligence (TrojAI) Final Report", "comment": null, "summary": "The Intelligence Advanced Research Projects Activity (IARPA) launched the TrojAI program to confront an emerging vulnerability in modern artificial intelligence: the threat of AI Trojans. These AI trojans are malicious, hidden backdoors intentionally embedded within an AI model that can cause a system to fail in unexpected ways, or allow a malicious actor to hijack the AI model at will. This multi-year initiative helped to map out the complex nature of the threat, pioneered foundational detection methods, and identified unsolved challenges that require ongoing attention by the burgeoning AI security field. This report synthesizes the program's key findings, including methodologies for detection through weight analysis and trigger inversion, as well as approaches for mitigating Trojan risks in deployed models. Comprehensive test and evaluation results highlight detector performance, sensitivity, and the prevalence of \"natural\" Trojans. The report concludes with lessons learned and recommendations for advancing AI security research.", "AI": {"tldr": "IARPA TrojAI\u9879\u76ee\u9488\u5bf9AI\u6728\u9a6c\u5a01\u80c1\u5c55\u5f00\u7814\u7a76\uff0c\u5f00\u53d1\u68c0\u6d4b\u65b9\u6cd5\u5e76\u8bc6\u522b\u672a\u89e3\u51b3\u6311\u6218\uff0c\u4e3aAI\u5b89\u5168\u9886\u57df\u63d0\u4f9b\u91cd\u8981\u53c2\u8003\u3002", "motivation": "\u5e94\u5bf9\u73b0\u4ee3\u4eba\u5de5\u667a\u80fd\u4e2d\u65b0\u5174\u7684AI\u6728\u9a6c\u6f0f\u6d1e\u5a01\u80c1\uff0c\u8fd9\u4e9b\u6076\u610f\u540e\u95e8\u53ef\u5bfc\u81f4\u7cfb\u7edf\u610f\u5916\u5931\u8d25\u6216\u88ab\u6076\u610f\u884c\u4e3a\u8005\u52ab\u6301\u3002", "method": "\u901a\u8fc7\u6743\u91cd\u5206\u6790\u548c\u89e6\u53d1\u5668\u53cd\u6f14\u7b49\u65b9\u6cd5\u8fdb\u884c\u68c0\u6d4b\uff0c\u5e76\u5f00\u53d1\u90e8\u7f72\u6a21\u578b\u7684\u6728\u9a6c\u98ce\u9669\u7f13\u89e3\u65b9\u6cd5\u3002", "result": "\u68c0\u6d4b\u5668\u6027\u80fd\u3001\u654f\u611f\u5ea6\u8bc4\u4f30\u7ed3\u679c\uff0c\u4ee5\u53ca\"\u81ea\u7136\"\u6728\u9a6c\u7684\u666e\u904d\u6027\u53d1\u73b0\uff0c\u63d0\u4f9b\u4e86\u5168\u9762\u7684\u6d4b\u8bd5\u8bc4\u4f30\u6570\u636e\u3002", "conclusion": "\u603b\u7ed3\u4e86\u9879\u76ee\u7ecf\u9a8c\u6559\u8bad\uff0c\u4e3a\u63a8\u8fdbAI\u5b89\u5168\u7814\u7a76\u63d0\u4f9b\u5efa\u8bae\uff0c\u5f3a\u8c03\u8be5\u9886\u57df\u9700\u8981\u6301\u7eed\u5173\u6ce8\u672a\u89e3\u51b3\u7684\u6311\u6218\u3002"}}
{"id": "2602.07092", "categories": ["cs.MA", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07092", "abs": "https://arxiv.org/abs/2602.07092", "authors": ["Haipeng Jiang", "Kailong Ren", "Zimo Yin", "Zhetao Sun", "Xin Gan", "Guangyi Lv", "Ming He", "Peng Wang", "Congli Yin", "Hong Pan", "Changwen Zhang", "Shan Tong", "Zhengyu Xu", "Zeping Chen", "Yubin Huangfu", "Yanzhi Xu", "Xing Su", "Qin Feng", "Dong An", "Jianping Fan"], "title": "Lemon Agent Technical Report", "comment": null, "summary": "Recent advanced LLM-powered agent systems have exhibited their remarkable capabilities in tackling complex, long-horizon tasks. Nevertheless, they still suffer from inherent limitations in resource efficiency, context management, and multimodal perception. Based on these observations, Lemon Agent is introduced, a multi-agent orchestrator-worker system built on a newly proposed AgentCortex framework, which formalizes the classic Planner-Executor-Memory paradigm through an adaptive task execution mechanism. Our system integrates a hierarchical self-adaptive scheduling mechanism that operates at both the overall orchestrator layer and workers layer. This mechanism can dynamically adjust computational intensity based on task complexity. It enables orchestrator to allocate one or more workers for parallel subtask execution, while workers can further improve operational efficiency by invoking tools concurrently. By virtue of this two-tier architecture, the system achieves synergistic balance between global task coordination and local task execution, thereby optimizing resource utilization and task processing efficiency in complex scenarios. To reduce context redundancy and increase information density during parallel steps, we adopt a three-tier progressive context management strategy. To make fuller use of historical information, we propose a self-evolving memory system, which can extract multi-dimensional valid information from all historical experiences to assist in completing similar tasks. Furthermore, we provide an enhanced MCP toolset. Empirical evaluations on authoritative benchmarks demonstrate that our Lemon Agent can achieve a state-of-the-art 91.36% overall accuracy on GAIA and secures the top position on the xbench-DeepSearch leaderboard with a score of 77+.", "AI": {"tldr": "Lemon Agent\u662f\u4e00\u4e2a\u57fa\u4e8eAgentCortex\u6846\u67b6\u7684\u591a\u667a\u80fd\u4f53\u7f16\u6392-\u5de5\u4f5c\u8005\u7cfb\u7edf\uff0c\u901a\u8fc7\u5206\u5c42\u81ea\u9002\u5e94\u8c03\u5ea6\u673a\u5236\u3001\u4e09\u5c42\u6e10\u8fdb\u5f0f\u4e0a\u4e0b\u6587\u7ba1\u7406\u548c\u81ea\u6f14\u5316\u8bb0\u5fc6\u7cfb\u7edf\uff0c\u5728\u590d\u6742\u4efb\u52a1\u5904\u7406\u4e2d\u5b9e\u73b0\u4e86\u8d44\u6e90\u6548\u7387\u3001\u4e0a\u4e0b\u6587\u7ba1\u7406\u548c\u591a\u6a21\u6001\u611f\u77e5\u7684\u4f18\u5316\uff0c\u5728\u6743\u5a01\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u4f18\u5f02\u8868\u73b0\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8eLLM\u7684\u667a\u80fd\u4f53\u7cfb\u7edf\u5728\u5904\u7406\u590d\u6742\u957f\u65f6\u7a0b\u4efb\u52a1\u65f6\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u4ecd\u5b58\u5728\u8d44\u6e90\u6548\u7387\u3001\u4e0a\u4e0b\u6587\u7ba1\u7406\u548c\u591a\u6a21\u6001\u611f\u77e5\u65b9\u9762\u7684\u56fa\u6709\u5c40\u9650\u6027\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u7814\u7a76\u8005\u63d0\u51fa\u4e86Lemon Agent\u7cfb\u7edf\u3002", "method": "\u57fa\u4e8e\u65b0\u63d0\u51fa\u7684AgentCortex\u6846\u67b6\u6784\u5efa\u591a\u667a\u80fd\u4f53\u7f16\u6392-\u5de5\u4f5c\u8005\u7cfb\u7edf\uff0c\u91c7\u7528\u5206\u5c42\u81ea\u9002\u5e94\u8c03\u5ea6\u673a\u5236\uff08\u7f16\u6392\u5c42\u548c\u5de5\u4f5c\u5c42\uff09\u3001\u4e09\u5c42\u6e10\u8fdb\u5f0f\u4e0a\u4e0b\u6587\u7ba1\u7406\u7b56\u7565\u3001\u81ea\u6f14\u5316\u8bb0\u5fc6\u7cfb\u7edf\uff0c\u4ee5\u53ca\u589e\u5f3a\u7684MCP\u5de5\u5177\u96c6\u3002", "result": "\u5728\u6743\u5a01\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cLemon Agent\u5728GAIA\u4e0a\u5b9e\u73b0\u4e8691.36%\u7684\u6574\u4f53\u51c6\u786e\u7387\uff0c\u5728xbench-DeepSearch\u6392\u884c\u699c\u4e0a\u4ee577+\u5206\u4f4d\u5c45\u699c\u9996\uff0c\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "Lemon Agent\u901a\u8fc7\u5176\u521b\u65b0\u7684\u67b6\u6784\u8bbe\u8ba1\uff0c\u5728\u590d\u6742\u4efb\u52a1\u5904\u7406\u4e2d\u5b9e\u73b0\u4e86\u5168\u5c40\u534f\u8c03\u4e0e\u5c40\u90e8\u6267\u884c\u7684\u534f\u540c\u5e73\u8861\uff0c\u663e\u8457\u4f18\u5316\u4e86\u8d44\u6e90\u5229\u7528\u548c\u4efb\u52a1\u5904\u7406\u6548\u7387\uff0c\u4e3a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u53c2\u8003\u3002"}}
{"id": "2602.06973", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.06973", "abs": "https://arxiv.org/abs/2602.06973", "authors": ["Lucky Susanto", "Musa Izzanardi Wijanarko", "Khumaisa Nur'aini", "Farid Adilazuarda", "Alham Fikri Aji", "Derry Tanti Wijaya"], "title": "Does Visual Rendering Bypass Tokenization? Investigating Script-Tokenizer Misalignment in Pixel-Based Language Models", "comment": "Submitted to ARR January", "summary": "While pixel-based language modeling aims to bypass the sub-word tokenization bottleneck by rendering text as images, recent multimodal variants such as DualGPT reintroduce text tokenizers to improve autoregressive performance. We investigate a fundamental question, does visual rendering truly decouple a model from tokenization constraints? Focusing on four Indonesian low-resource local languages that have their own non-Latin scripts (i.e., Javanese, Balinese, Sundanese, and Lampungnese), we evaluate the impact of script-tokenizer alignment within the DualGPT architecture. Our results show that, despite visual rendering, reintegrating a text tokenizer into the architecture reintroduces the same issue that pixel-based language modeling aims to resolve, which is the tokenizer misalignment problem. Despite having lower OOV and fertility rates, we show that the Llama 2 tokenizer performs significantly worse than a custom tokenizer, with improvements of up to 30.15 chrF++. Our findings serve as a warning for future multimodal variants, as text tokenizers remain a significant barrier to equitable models.", "AI": {"tldr": "\u50cf\u7d20\u8bed\u8a00\u5efa\u6a21\u65e8\u5728\u901a\u8fc7\u5c06\u6587\u672c\u6e32\u67d3\u4e3a\u56fe\u50cf\u6765\u7ed5\u8fc7\u5b50\u8bcd\u5206\u8bcd\u74f6\u9888\uff0c\u4f46DualGPT\u7b49\u591a\u6a21\u6001\u53d8\u4f53\u91cd\u65b0\u5f15\u5165\u6587\u672c\u5206\u8bcd\u5668\u4ee5\u63d0\u5347\u81ea\u56de\u5f52\u6027\u80fd\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u5373\u4f7f\u6709\u89c6\u89c9\u6e32\u67d3\uff0c\u91cd\u65b0\u96c6\u6210\u6587\u672c\u5206\u8bcd\u5668\u4ecd\u4f1a\u91cd\u65b0\u5f15\u5165\u5206\u8bcd\u5668\u5bf9\u9f50\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u5370\u5c3c\u4f4e\u8d44\u6e90\u672c\u5730\u8bed\u8a00\u4e2d\u3002", "motivation": "\u7814\u7a76\u50cf\u7d20\u8bed\u8a00\u5efa\u6a21\u662f\u5426\u771f\u6b63\u89e3\u9664\u4e86\u6a21\u578b\u5bf9\u5206\u8bcd\u5668\u7684\u4f9d\u8d56\uff0c\u7279\u522b\u662f\u5728\u591a\u6a21\u6001\u53d8\u4f53\u5982DualGPT\u91cd\u65b0\u5f15\u5165\u6587\u672c\u5206\u8bcd\u5668\u7684\u60c5\u51b5\u4e0b\u3002\u5173\u6ce8\u5370\u5c3c\u56db\u79cd\u4f7f\u7528\u975e\u62c9\u4e01\u6587\u5b57\u7684\u4f4e\u8d44\u6e90\u672c\u5730\u8bed\u8a00\uff08\u722a\u54c7\u8bed\u3001\u5df4\u5398\u8bed\u3001\u5dfd\u4ed6\u8bed\u3001\u6960\u699c\u8bed\uff09\uff0c\u8bc4\u4f30\u811a\u672c-\u5206\u8bcd\u5668\u5bf9\u9f50\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\u3002", "method": "\u5728DualGPT\u67b6\u6784\u4e2d\u8bc4\u4f30\u811a\u672c-\u5206\u8bcd\u5668\u5bf9\u9f50\u95ee\u9898\u3002\u4f7f\u7528\u56db\u79cd\u5370\u5c3c\u4f4e\u8d44\u6e90\u672c\u5730\u8bed\u8a00\uff08\u722a\u54c7\u8bed\u3001\u5df4\u5398\u8bed\u3001\u5dfd\u4ed6\u8bed\u3001\u6960\u699c\u8bed\uff09\uff0c\u6bd4\u8f83Llama 2\u5206\u8bcd\u5668\u4e0e\u81ea\u5b9a\u4e49\u5206\u8bcd\u5668\u7684\u6027\u80fd\u5dee\u5f02\u3002\u901a\u8fc7OOV\uff08\u8bcd\u6c47\u5916\uff09\u7387\u3001\u751f\u80b2\u7387\uff08fertility rates\uff09\u548cchrF++\u6307\u6807\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u5c3d\u7ba1\u89c6\u89c9\u6e32\u67d3\uff0c\u91cd\u65b0\u96c6\u6210\u6587\u672c\u5206\u8bcd\u5668\u4ecd\u4f1a\u91cd\u65b0\u5f15\u5165\u5206\u8bcd\u5668\u5bf9\u9f50\u95ee\u9898\u3002Llama 2\u5206\u8bcd\u5668\u867d\u7136\u5177\u6709\u8f83\u4f4e\u7684OOV\u548c\u751f\u80b2\u7387\uff0c\u4f46\u6027\u80fd\u663e\u8457\u5dee\u4e8e\u81ea\u5b9a\u4e49\u5206\u8bcd\u5668\uff0cchrF++\u6307\u6807\u63d0\u5347\u9ad8\u8fbe30.15\u3002\u8fd9\u8868\u660e\u6587\u672c\u5206\u8bcd\u5668\u4ecd\u7136\u662f\u5b9e\u73b0\u516c\u5e73\u6a21\u578b\u7684\u91cd\u8981\u969c\u788d\u3002", "conclusion": "\u50cf\u7d20\u8bed\u8a00\u5efa\u6a21\u7684\u591a\u6a21\u6001\u53d8\u4f53\u91cd\u65b0\u5f15\u5165\u6587\u672c\u5206\u8bcd\u5668\u4f1a\u91cd\u65b0\u5f15\u5165\u539f\u672c\u8bd5\u56fe\u89e3\u51b3\u7684\u95ee\u9898\u3002\u6587\u672c\u5206\u8bcd\u5668\u5bf9\u9f50\u95ee\u9898\u4ecd\u7136\u662f\u6a21\u578b\u6027\u80fd\u7684\u5173\u952e\u74f6\u9888\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u4f4e\u8d44\u6e90\u3001\u975e\u62c9\u4e01\u6587\u5b57\u8bed\u8a00\u65f6\u3002\u8fd9\u5bf9\u672a\u6765\u591a\u6a21\u6001\u53d8\u4f53\u8bbe\u8ba1\u63d0\u51fa\u4e86\u8b66\u544a\u3002"}}
{"id": "2602.07006", "categories": ["cs.CV", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.07006", "abs": "https://arxiv.org/abs/2602.07006", "authors": ["Alokesh Manna", "Neil Spencer", "Dipak K. Dey"], "title": "Scalable spatial point process models for forensic footwear analysis", "comment": null, "summary": "Shoe print evidence recovered from crime scenes plays a key role in forensic investigations. By examining shoe prints, investigators can determine details of the footwear worn by suspects. However, establishing that a suspect's shoes match the make and model of a crime scene print may not be sufficient. Typically, thousands of shoes of the same size, make, and model are manufactured, any of which could be responsible for the print. Accordingly, a popular approach used by investigators is to examine the print for signs of ``accidentals,'' i.e., cuts, scrapes, and other features that accumulate on shoe soles after purchase due to wear. While some patterns of accidentals are common on certain types of shoes, others are highly distinctive, potentially distinguishing the suspect's shoe from all others. Quantifying the rarity of a pattern is thus essential to accurately measuring the strength of forensic evidence. In this study, we address this task by developing a hierarchical Bayesian model. Our improvement over existing methods primarily stems from two advancements. First, we frame our approach in terms of a latent Gaussian model, thus enabling inference to be efficiently scaled to large collections of annotated shoe prints via integrated nested Laplace approximations. Second, we incorporate spatially varying coefficients to model the relationship between shoes' tread patterns and accidental locations. We demonstrate these improvements through superior performance on held-out data, which enhances accuracy and reliability in forensic shoe print analysis.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u5206\u5c42\u8d1d\u53f6\u65af\u6a21\u578b\u6765\u91cf\u5316\u978b\u5370\u4e2d\"\u5076\u7136\u7279\u5f81\"\u7684\u7a00\u6709\u6027\uff0c\u901a\u8fc7\u6f5c\u5728\u9ad8\u65af\u6a21\u578b\u548c\u7a7a\u95f4\u53d8\u5316\u7cfb\u6570\u63d0\u9ad8\u4e86\u6cd5\u533b\u978b\u5370\u5206\u6790\u7684\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\u3002", "motivation": "\u72af\u7f6a\u73b0\u573a\u7684\u978b\u5370\u8bc1\u636e\u5728\u6cd5\u533b\u8c03\u67e5\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4ec5\u5339\u914d\u978b\u5b50\u7684\u54c1\u724c\u548c\u578b\u53f7\u901a\u5e38\u4e0d\u591f\uff0c\u56e0\u4e3a\u540c\u6b3e\u978b\u5b50\u53ef\u80fd\u751f\u4ea7\u6570\u5343\u53cc\u3002\u9700\u8981\u91cf\u5316\u978b\u5e95\u78e8\u635f\u5f62\u6210\u7684\"\u5076\u7136\u7279\u5f81\"\uff08\u5982\u5212\u75d5\u3001\u5207\u53e3\uff09\u7684\u7a00\u6709\u6027\uff0c\u4ee5\u51c6\u786e\u8bc4\u4f30\u8bc1\u636e\u5f3a\u5ea6\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u5206\u5c42\u8d1d\u53f6\u65af\u6a21\u578b\uff0c\u91c7\u7528\u6f5c\u5728\u9ad8\u65af\u6a21\u578b\u6846\u67b6\uff0c\u901a\u8fc7\u96c6\u6210\u5d4c\u5957\u62c9\u666e\u62c9\u65af\u8fd1\u4f3c\u5b9e\u73b0\u9ad8\u6548\u63a8\u7406\uff0c\u80fd\u591f\u6269\u5c55\u5230\u5927\u578b\u6ce8\u91ca\u978b\u5370\u6570\u636e\u96c6\u3002\u540c\u65f6\u5f15\u5165\u4e86\u7a7a\u95f4\u53d8\u5316\u7cfb\u6570\u6765\u5efa\u6a21\u978b\u5e95\u82b1\u7eb9\u56fe\u6848\u4e0e\u5076\u7136\u7279\u5f81\u4f4d\u7f6e\u4e4b\u95f4\u7684\u5173\u7cfb\u3002", "result": "\u5728\u4fdd\u7559\u6570\u636e\u4e0a\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\uff0c\u63d0\u9ad8\u4e86\u978b\u5370\u5206\u6790\u7684\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u6709\u663e\u8457\u6539\u8fdb\u3002", "conclusion": "\u63d0\u51fa\u7684\u5206\u5c42\u8d1d\u53f6\u65af\u6a21\u578b\u80fd\u591f\u6709\u6548\u91cf\u5316\u978b\u5370\u4e2d\u5076\u7136\u7279\u5f81\u7684\u7a00\u6709\u6027\uff0c\u4e3a\u6cd5\u533b\u978b\u5370\u5206\u6790\u63d0\u4f9b\u4e86\u66f4\u51c6\u786e\u548c\u53ef\u9760\u7684\u91cf\u5316\u5de5\u5177\uff0c\u6709\u52a9\u4e8e\u589e\u5f3a\u6cd5\u533b\u8bc1\u636e\u7684\u5f3a\u5ea6\u8bc4\u4f30\u3002"}}
{"id": "2602.07008", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.07008", "abs": "https://arxiv.org/abs/2602.07008", "authors": ["Ruoyu Chen", "Shangquan Sun", "Xiaoqing Guo", "Sanyi Zhang", "Kangwei Liu", "Shiming Liu", "Zhangcheng Wang", "Qunli Zhang", "Hua Zhang", "Xiaochun Cao"], "title": "Where Not to Learn: Prior-Aligned Training with Subset-based Attribution Constraints for Reliable Decision-Making", "comment": null, "summary": "Reliable models should not only predict correctly, but also justify decisions with acceptable evidence. Yet conventional supervised learning typically provides only class-level labels, allowing models to achieve high accuracy through shortcut correlations rather than the intended evidence. Human priors can help constrain such behavior, but aligning models to these priors remains challenging because learned representations often diverge from human perception. To address this challenge, we propose an attribution-based human prior alignment method. We encode human priors as input regions that the model is expected to rely on (e.g., bounding boxes), and leverage a highly faithful subset-selection-based attribution approach to expose the model's decision evidence during training. When the attribution region deviates substantially from the prior regions, we penalize reliance on off-prior evidence, encouraging the model to shift its attribution toward the intended regions. This is achieved through a training objective that imposes attribution constraints induced by the human prior. We validate our method on both image classification and click decision tasks in MLLM-based GUI agent models. Across conventional classification and autoregressive generation settings, human prior alignment consistently improves task accuracy while also enhancing the model's decision reasonability.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5f52\u56e0\u7684\u4eba\u7c7b\u5148\u9a8c\u5bf9\u9f50\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ea6\u675f\u6a21\u578b\u51b3\u7b56\u8bc1\u636e\u4e0e\u4eba\u7c7b\u5148\u9a8c\u533a\u57df\u7684\u4e00\u81f4\u6027\uff0c\u63d0\u9ad8\u6a21\u578b\u53ef\u9760\u6027\u548c\u51b3\u7b56\u5408\u7406\u6027", "motivation": "\u4f20\u7edf\u76d1\u7763\u5b66\u4e60\u4ec5\u63d0\u4f9b\u7c7b\u522b\u6807\u7b7e\uff0c\u6a21\u578b\u53ef\u80fd\u901a\u8fc7\u6377\u5f84\u76f8\u5173\u6027\u800c\u975e\u9884\u671f\u8bc1\u636e\u5b9e\u73b0\u9ad8\u51c6\u786e\u7387\uff0c\u9700\u8981\u5c06\u6a21\u578b\u51b3\u7b56\u4e0e\u4eba\u7c7b\u5148\u9a8c\u8bc1\u636e\u5bf9\u9f50\u4ee5\u63d0\u9ad8\u53ef\u9760\u6027", "method": "\u5c06\u4eba\u7c7b\u5148\u9a8c\u7f16\u7801\u4e3a\u6a21\u578b\u5e94\u4f9d\u8d56\u7684\u8f93\u5165\u533a\u57df\uff08\u5982\u8fb9\u754c\u6846\uff09\uff0c\u4f7f\u7528\u9ad8\u4fdd\u771f\u5b50\u96c6\u9009\u62e9\u5f52\u56e0\u65b9\u6cd5\u66b4\u9732\u8bad\u7ec3\u4e2d\u6a21\u578b\u7684\u51b3\u7b56\u8bc1\u636e\uff0c\u5f53\u5f52\u56e0\u533a\u57df\u504f\u79bb\u5148\u9a8c\u533a\u57df\u65f6\u60e9\u7f5a\u5bf9\u975e\u5148\u9a8c\u8bc1\u636e\u7684\u4f9d\u8d56", "result": "\u5728\u56fe\u50cf\u5206\u7c7b\u548c\u70b9\u51fb\u51b3\u7b56\u4efb\u52a1\u4e2d\uff0c\u4eba\u7c7b\u5148\u9a8c\u5bf9\u9f50\u65b9\u6cd5\u5728\u4f20\u7edf\u5206\u7c7b\u548c\u81ea\u56de\u5f52\u751f\u6210\u8bbe\u7f6e\u4e0b\u5747\u63d0\u9ad8\u4e86\u4efb\u52a1\u51c6\u786e\u7387\uff0c\u540c\u65f6\u589e\u5f3a\u4e86\u6a21\u578b\u51b3\u7b56\u7684\u5408\u7406\u6027", "conclusion": "\u57fa\u4e8e\u5f52\u56e0\u7684\u4eba\u7c7b\u5148\u9a8c\u5bf9\u9f50\u65b9\u6cd5\u80fd\u6709\u6548\u7ea6\u675f\u6a21\u578b\u4f9d\u8d56\u9884\u671f\u8bc1\u636e\uff0c\u63d0\u9ad8\u6a21\u578b\u53ef\u9760\u6027\u548c\u51b3\u7b56\u5408\u7406\u6027\uff0c\u5728\u591a\u79cd\u4efb\u52a1\u8bbe\u7f6e\u4e2d\u5747\u6709\u826f\u597d\u8868\u73b0"}}
{"id": "2602.07034", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07034", "abs": "https://arxiv.org/abs/2602.07034", "authors": ["Jinxiu Qu", "Zirui Tang", "Hongzhang Huang", "Boyu Niu", "Wei Zhou", "Jiannan Wang", "Yitong Song", "Guoliang Li", "Xuanhe Zhou", "Fan Wu"], "title": "ST-Raptor: An Agentic System for Semi-Structured Table QA", "comment": null, "summary": "Semi-structured table question answering (QA) is a challenging task that requires (1) precise extraction of cell contents and positions and (2) accurate recovery of key implicit logical structures, hierarchical relationships, and semantic associations encoded in table layouts. In practice, such tables are often interpreted manually by human experts, which is labor-intensive and time-consuming. However, automating this process remains difficult. Existing Text-to-SQL methods typically require converting semi-structured tables into structured formats, inevitably leading to information loss, while approaches like Text-to-Code and multimodal LLM-based QA struggle with complex layouts and often yield inaccurate answers. To address these limitations, we present ST-Raptor, an agentic system for semi-structured table QA. ST-Raptor offers an interactive analysis environment that combines visual editing, tree-based structural modeling, and agent-driven query resolution to support accurate and user-friendly table understanding. Experimental results on both benchmark and real-world datasets demonstrate that ST-Raptor outperforms existing methods in both accuracy and usability. The code is available at https://github.com/weAIDB/ST-Raptor, and a demonstration video is available at https://youtu.be/9GDR-94Cau4.", "AI": {"tldr": "ST-Raptor\u662f\u4e00\u4e2a\u7528\u4e8e\u534a\u7ed3\u6784\u5316\u8868\u683c\u95ee\u7b54\u7684\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u901a\u8fc7\u89c6\u89c9\u7f16\u8f91\u3001\u6811\u72b6\u7ed3\u6784\u5efa\u6a21\u548c\u667a\u80fd\u4f53\u9a71\u52a8\u67e5\u8be2\u89e3\u51b3\uff0c\u5728\u51c6\u786e\u6027\u548c\u53ef\u7528\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u534a\u7ed3\u6784\u5316\u8868\u683c\u95ee\u7b54\u9700\u8981\u7cbe\u786e\u63d0\u53d6\u5355\u5143\u683c\u5185\u5bb9\u548c\u4f4d\u7f6e\uff0c\u5e76\u6062\u590d\u8868\u683c\u5e03\u5c40\u4e2d\u9690\u542b\u7684\u903b\u8f91\u7ed3\u6784\u3001\u5c42\u6b21\u5173\u7cfb\u548c\u8bed\u4e49\u5173\u8054\u3002\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u4fe1\u606f\u4e22\u5931\u3001\u5904\u7406\u590d\u6742\u5e03\u5c40\u56f0\u96be\u7b49\u95ee\u9898\uff0c\u4eba\u5de5\u89e3\u91ca\u53c8\u8017\u65f6\u8017\u529b\u3002", "method": "ST-Raptor\u63d0\u4f9b\u4e86\u4e00\u4e2a\u4ea4\u4e92\u5f0f\u5206\u6790\u73af\u5883\uff0c\u7ed3\u5408\u89c6\u89c9\u7f16\u8f91\u3001\u57fa\u4e8e\u6811\u7684\u7ed3\u6784\u5efa\u6a21\u548c\u667a\u80fd\u4f53\u9a71\u52a8\u7684\u67e5\u8be2\u89e3\u51b3\uff0c\u652f\u6301\u51c6\u786e\u4e14\u7528\u6237\u53cb\u597d\u7684\u8868\u683c\u7406\u89e3\u3002", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u548c\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cST-Raptor\u5728\u51c6\u786e\u6027\u548c\u53ef\u7528\u6027\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "ST-Raptor\u901a\u8fc7\u667a\u80fd\u4f53\u7cfb\u7edf\u6709\u6548\u89e3\u51b3\u4e86\u534a\u7ed3\u6784\u5316\u8868\u683c\u95ee\u7b54\u7684\u6311\u6218\uff0c\u63d0\u4f9b\u4e86\u6bd4\u73b0\u6709\u65b9\u6cd5\u66f4\u51c6\u786e\u3001\u66f4\u6613\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.07072", "categories": ["cs.SE", "cs.MA"], "pdf": "https://arxiv.org/pdf/2602.07072", "abs": "https://arxiv.org/abs/2602.07072", "authors": ["Igor Costa"], "title": "AgentSpawn: Adaptive Multi-Agent Collaboration Through Dynamic Spawning for Long-Horizon Code Generation", "comment": "18 pages, 4 figures, 6 tables", "summary": "Long-horizon code generation requires sustained context and adaptive expertise across domains. Current multi-agent systems use static workflows that cannot adapt when runtime analysis reveals unanticipated complexity. We propose AgentSpawn, an architecture enabling dynamic agent collaboration through: (1) automatic memory transfer during spawning, (2) adaptive spawning policies triggered by runtime complexity metrics, and (3) coherence protocols for concurrent modifications. AgentSpawn addresses five critical gaps in existing research around memory continuity, skill inheritance, task resumption, runtime spawning, and concurrent coherence. Experimental validation demonstrates AgentSpawn achieves 34% higher completion rates than static baselines on benchmarks like SWE-bench while reducing memory overhead by 42% through selective slicing.", "AI": {"tldr": "AgentSpawn\uff1a\u4e00\u79cd\u52a8\u6001\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u67b6\u6784\uff0c\u901a\u8fc7\u81ea\u52a8\u5185\u5b58\u8f6c\u79fb\u3001\u81ea\u9002\u5e94\u751f\u6210\u7b56\u7565\u548c\u4e00\u81f4\u6027\u534f\u8bae\uff0c\u663e\u8457\u63d0\u5347\u957f\u65f6\u7a0b\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u7684\u5b8c\u6210\u7387\u5e76\u964d\u4f4e\u5185\u5b58\u5f00\u9500\u3002", "motivation": "\u73b0\u6709\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u91c7\u7528\u9759\u6001\u5de5\u4f5c\u6d41\uff0c\u65e0\u6cd5\u5728\u8fd0\u884c\u65f6\u5206\u6790\u53d1\u73b0\u672a\u9884\u671f\u590d\u6742\u6027\u65f6\u8fdb\u884c\u81ea\u9002\u5e94\u8c03\u6574\u3002\u957f\u65f6\u7a0b\u4ee3\u7801\u751f\u6210\u9700\u8981\u6301\u7eed\u7684\u4e0a\u4e0b\u6587\u548c\u8de8\u9886\u57df\u7684\u81ea\u9002\u5e94\u4e13\u4e1a\u77e5\u8bc6\uff0c\u5f53\u524d\u7cfb\u7edf\u5b58\u5728\u4e94\u4e2a\u5173\u952e\u5dee\u8ddd\uff1a\u5185\u5b58\u8fde\u7eed\u6027\u3001\u6280\u80fd\u7ee7\u627f\u3001\u4efb\u52a1\u6062\u590d\u3001\u8fd0\u884c\u65f6\u751f\u6210\u548c\u5e76\u53d1\u4e00\u81f4\u6027\u3002", "method": "\u63d0\u51faAgentSpawn\u67b6\u6784\uff0c\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u673a\u5236\uff1a(1) \u751f\u6210\u8fc7\u7a0b\u4e2d\u7684\u81ea\u52a8\u5185\u5b58\u8f6c\u79fb\uff0c(2) \u7531\u8fd0\u884c\u65f6\u590d\u6742\u5ea6\u6307\u6807\u89e6\u53d1\u7684\u81ea\u9002\u5e94\u751f\u6210\u7b56\u7565\uff0c(3) \u5e76\u53d1\u4fee\u6539\u7684\u4e00\u81f4\u6027\u534f\u8bae\u3002\u8be5\u67b6\u6784\u89e3\u51b3\u4e86\u73b0\u6709\u7814\u7a76\u4e2d\u7684\u4e94\u4e2a\u5173\u952e\u5dee\u8ddd\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u663e\u793a\uff0cAgentSpawn\u5728SWE-bench\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6bd4\u9759\u6001\u57fa\u7ebf\u65b9\u6cd5\u5b9e\u73b0\u4e8634%\u66f4\u9ad8\u7684\u5b8c\u6210\u7387\uff0c\u540c\u65f6\u901a\u8fc7\u9009\u62e9\u6027\u5207\u7247\u5c06\u5185\u5b58\u5f00\u9500\u964d\u4f4e\u4e8642%\u3002", "conclusion": "AgentSpawn\u901a\u8fc7\u52a8\u6001\u667a\u80fd\u4f53\u534f\u4f5c\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86\u957f\u65f6\u7a0b\u4ee3\u7801\u751f\u6210\u4e2d\u7684\u81ea\u9002\u5e94\u6027\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4efb\u52a1\u5b8c\u6210\u6548\u7387\u5e76\u964d\u4f4e\u4e86\u8d44\u6e90\u6d88\u8017\uff0c\u4e3a\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u52a8\u6001\u5de5\u4f5c\u6d41\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2602.06976", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.PL"], "pdf": "https://arxiv.org/pdf/2602.06976", "abs": "https://arxiv.org/abs/2602.06976", "authors": ["Chen Shen", "Wei Cheng", "Jingyue Yang", "Huan Zhang", "Yuhan Wu", "Wei Hu"], "title": "Bridging the Knowledge Void: Inference-time Acquisition of Unfamiliar Programming Languages for Coding Tasks", "comment": null, "summary": "The proficiency of Large Language Models (LLMs) in coding tasks is often a reflection of their extensive pre-training corpora, which typically collapses when confronted with previously unfamiliar programming languages. Departing from data-intensive finetuning, we investigate the paradigm of Inference-time Language Acquisition (ILA), where an LLM masters an unfamiliar language through dynamic interaction with limited external resources. In this paper, we propose ILA-agent, a general ILA framework that equips LLMs with a set of behavioral primitives. By modeling essential human-like behaviors as a suite of tools, ILA-agent enables LLMs to incrementally explore, apply, and verify language knowledge through structured interactions with the official documentation and execution environment. To provide a rigorous evaluation in a low-resource setting, we construct Cangjie-bench, a multi-task benchmark based on the novel statically-typed language Cangjie. We instantiate ILA-agent for Cangjie and evaluate its performance across code generation, translation, and program repair tasks. Results using diverse LLMs demonstrate that ILA-agent significantly outperforms retrieval-augmented baselines. Further analysis of agent trajectories characterizes the emergent behavior patterns while highlighting persisting performance gaps.", "AI": {"tldr": "ILA-agent\u6846\u67b6\u8ba9\u5927\u8bed\u8a00\u6a21\u578b\u901a\u8fc7\u6709\u9650\u5916\u90e8\u8d44\u6e90\u52a8\u6001\u4ea4\u4e92\u5b66\u4e60\u65b0\u7f16\u7a0b\u8bed\u8a00\uff0c\u663e\u8457\u4f18\u4e8e\u68c0\u7d22\u589e\u5f3a\u57fa\u7ebf\u65b9\u6cd5", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u7f16\u7a0b\u4efb\u52a1\u4e2d\u7684\u80fd\u529b\u4f9d\u8d56\u4e8e\u5927\u91cf\u9884\u8bad\u7ec3\u6570\u636e\uff0c\u4f46\u5728\u9762\u5bf9\u4e0d\u719f\u6089\u7684\u7f16\u7a0b\u8bed\u8a00\u65f6\u8868\u73b0\u4f1a\u6025\u5267\u4e0b\u964d\u3002\u4f20\u7edf\u7684\u6570\u636e\u5bc6\u96c6\u578b\u5fae\u8c03\u65b9\u6cd5\u6210\u672c\u9ad8\uff0c\u56e0\u6b64\u9700\u8981\u7814\u7a76\u63a8\u7406\u65f6\u8bed\u8a00\u83b7\u53d6\u7684\u65b0\u8303\u5f0f", "method": "\u63d0\u51faILA-agent\u6846\u67b6\uff0c\u5c06\u4eba\u7c7b\u5173\u952e\u884c\u4e3a\u5efa\u6a21\u4e3a\u4e00\u7ec4\u5de5\u5177\uff0c\u4f7fLLM\u80fd\u591f\u901a\u8fc7\u7ed3\u6784\u5316\u4ea4\u4e92\uff08\u8bbf\u95ee\u5b98\u65b9\u6587\u6863\u548c\u6267\u884c\u73af\u5883\uff09\u6765\u589e\u91cf\u63a2\u7d22\u3001\u5e94\u7528\u548c\u9a8c\u8bc1\u8bed\u8a00\u77e5\u8bc6", "result": "\u5728Cangjie-bench\u591a\u4efb\u52a1\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cILA-agent\u5728\u4ee3\u7801\u751f\u6210\u3001\u7ffb\u8bd1\u548c\u7a0b\u5e8f\u4fee\u590d\u4efb\u52a1\u4e0a\u663e\u8457\u4f18\u4e8e\u68c0\u7d22\u589e\u5f3a\u57fa\u7ebf\u65b9\u6cd5\u3002\u8f68\u8ff9\u5206\u6790\u63ed\u793a\u4e86\u6d8c\u73b0\u7684\u884c\u4e3a\u6a21\u5f0f\uff0c\u540c\u65f6\u6307\u51fa\u4e86\u6301\u7eed\u5b58\u5728\u7684\u6027\u80fd\u5dee\u8ddd", "conclusion": "ILA-agent\u4e3aLLM\u5b66\u4e60\u65b0\u7f16\u7a0b\u8bed\u8a00\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u63a8\u7406\u65f6\u83b7\u53d6\u8303\u5f0f\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u4ea4\u4e92\u548c\u5de5\u5177\u5316\u884c\u4e3a\u5efa\u6a21\uff0c\u5728\u4f4e\u8d44\u6e90\u8bbe\u7f6e\u4e0b\u5b9e\u73b0\u4e86\u663e\u8457\u6027\u80fd\u63d0\u5347"}}
{"id": "2602.07011", "categories": ["cs.CV", "cs.AI", "eess.IV"], "pdf": "https://arxiv.org/pdf/2602.07011", "abs": "https://arxiv.org/abs/2602.07011", "authors": ["Zhuonan Wang", "Zhenxuan Fan", "Siwen Tan", "Yu Zhong", "Yuqian Yuan", "Haoyuan Li", "Hao Jiang", "Wenqiao Zhang", "Feifei Shao", "Hongwei Wang", "Jun Xiao"], "title": "MAU-GPT: Enhancing Multi-type Industrial Anomaly Understanding via Anomaly-aware and Generalist Experts Adaptation", "comment": "9 pages, 5 figures", "summary": "As industrial manufacturing scales, automating fine-grained product image analysis has become critical for quality control. However, existing approaches are hindered by limited dataset coverage and poor model generalization across diverse and complex anomaly patterns. To address these challenges, we introduce MAU-Set, a comprehensive dataset for Multi-type industrial Anomaly Understanding. It spans multiple industrial domains and features a hierarchical task structure, ranging from binary classification to complex reasoning. Alongside this dataset, we establish a rigorous evaluation protocol to facilitate fair and comprehensive model assessment. Building upon this foundation, we further present MAU-GPT, a domain-adapted multimodal large model specifically designed for industrial anomaly understanding. It incorporates a novel AMoE-LoRA mechanism that unifies anomaly-aware and generalist experts adaptation, enhancing both understanding and reasoning across diverse defect classes. Extensive experiments show that MAU-GPT consistently outperforms prior state-of-the-art methods across all domains, demonstrating strong potential for scalable and automated industrial inspection.", "AI": {"tldr": "\u63d0\u51fa\u4e86MAU-Set\u5de5\u4e1a\u5f02\u5e38\u7406\u89e3\u6570\u636e\u96c6\u548cMAU-GPT\u591a\u6a21\u6001\u5927\u6a21\u578b\uff0c\u901a\u8fc7AMoE-LoRA\u673a\u5236\u7edf\u4e00\u5f02\u5e38\u611f\u77e5\u548c\u901a\u7528\u4e13\u5bb6\u9002\u914d\uff0c\u5728\u5de5\u4e1a\u5f02\u5e38\u68c0\u6d4b\u4efb\u52a1\u4e0a\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5de5\u4e1a\u5236\u9020\u89c4\u6a21\u5316\u53d1\u5c55\u9700\u8981\u81ea\u52a8\u5316\u7ec6\u7c92\u5ea6\u4ea7\u54c1\u56fe\u50cf\u5206\u6790\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u53d7\u9650\u4e8e\u6570\u636e\u96c6\u8986\u76d6\u4e0d\u8db3\u548c\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u5dee\uff0c\u65e0\u6cd5\u5e94\u5bf9\u591a\u6837\u590d\u6742\u7684\u5f02\u5e38\u6a21\u5f0f\u3002", "method": "1) \u6784\u5efaMAU-Set\u591a\u7c7b\u578b\u5de5\u4e1a\u5f02\u5e38\u7406\u89e3\u6570\u636e\u96c6\uff0c\u6db5\u76d6\u591a\u4e2a\u5de5\u4e1a\u9886\u57df\u548c\u5206\u5c42\u4efb\u52a1\u7ed3\u6784\uff1b2) \u63d0\u51faMAU-GPT\u9886\u57df\u9002\u914d\u591a\u6a21\u6001\u5927\u6a21\u578b\uff0c\u91c7\u7528\u65b0\u9896\u7684AMoE-LoRA\u673a\u5236\u7edf\u4e00\u5f02\u5e38\u611f\u77e5\u548c\u901a\u7528\u4e13\u5bb6\u9002\u914d\u3002", "result": "MAU-GPT\u5728\u6240\u6709\u9886\u57df\u90fd\u6301\u7eed\u4f18\u4e8e\u5148\u524d\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u5c55\u793a\u4e86\u53ef\u6269\u5c55\u81ea\u52a8\u5316\u5de5\u4e1a\u68c0\u6d4b\u7684\u5f3a\u5927\u6f5c\u529b\u3002", "conclusion": "MAU-Set\u6570\u636e\u96c6\u548cMAU-GPT\u6a21\u578b\u4e3a\u89e3\u51b3\u5de5\u4e1a\u5f02\u5e38\u7406\u89e3\u4e2d\u7684\u6570\u636e\u96c6\u8986\u76d6\u548c\u6a21\u578b\u6cdb\u5316\u6311\u6218\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\uff0c\u63a8\u52a8\u4e86\u81ea\u52a8\u5316\u5de5\u4e1a\u68c0\u6d4b\u7684\u53d1\u5c55\u3002"}}
{"id": "2602.07079", "categories": ["cs.SE", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.07079", "abs": "https://arxiv.org/abs/2602.07079", "authors": ["Go Frendi Gunawan", "Mukhlis Amien"], "title": "Comprehensive Evaluation of Large Language Models on Software Engineering Tasks: A Multi-Task Benchmark", "comment": "10 pages, 7 figures. Under review. Code and data will be fully released", "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in software engineering, yet comprehensive benchmarks covering diverse SE activities remain limited. We present a multi-task evaluation of 11 state-of-the-art LLMs across five representative software engineering tasks: bug fixing, feature development, code refactoring, technical copywriting, and research synthesis. Our automated verification framework measures both output quality and completion efficiency. Key findings reveal that (1) models achieving identical perfect scores exhibit 22x variation in completion time, 49x variation in tool efficiency, and 53x variation in estimated cost; (2) tool usage frequency shows no correlation with success (r = 0.077, p = 0.575) - one model used 917 tool calls while another solved the same task with 3 calls; (3) we identify two distinct inefficiency patterns: loop inefficiency and inference inefficiency; and (4) coding tasks achieve 100 percent success while research tasks present greater challenges (90.9 percent). We release all experimental data, verification scripts, and analysis code for full reproducibility.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5bf911\u4e2a\u5148\u8fdb\u5927\u8bed\u8a00\u6a21\u578b\u57285\u4e2a\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u4e0a\u8fdb\u884c\u4e86\u591a\u4efb\u52a1\u8bc4\u4f30\uff0c\u53d1\u73b0\u6a21\u578b\u6027\u80fd\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff1a\u76f8\u540c\u5b8c\u7f8e\u5206\u6570\u7684\u6a21\u578b\u5728\u5b8c\u6210\u65f6\u95f4\u3001\u5de5\u5177\u6548\u7387\u548c\u6210\u672c\u4e0a\u5dee\u5f02\u5de8\u5927\uff0c\u5de5\u5177\u4f7f\u7528\u9891\u7387\u4e0e\u6210\u529f\u7387\u65e0\u5173\uff0c\u5e76\u8bc6\u522b\u51fa\u4e24\u79cd\u4f4e\u6548\u6a21\u5f0f\u3002", "motivation": "\u5c3d\u7ba1\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u5c55\u73b0\u51fa\u5353\u8d8a\u80fd\u529b\uff0c\u4f46\u7f3a\u4e4f\u8986\u76d6\u591a\u6837\u5316\u8f6f\u4ef6\u5de5\u7a0b\u6d3b\u52a8\u7684\u7efc\u5408\u57fa\u51c6\u6d4b\u8bd5\u3002\u73b0\u6709\u7814\u7a76\u901a\u5e38\u805a\u7126\u5355\u4e00\u4efb\u52a1\uff0c\u672a\u80fd\u5168\u9762\u8bc4\u4f30\u6a21\u578b\u5728\u5b9e\u9645\u8f6f\u4ef6\u5de5\u7a0b\u5de5\u4f5c\u6d41\u4e2d\u7684\u8868\u73b0\u3002", "method": "\u7814\u7a76\u91c7\u7528\u591a\u4efb\u52a1\u8bc4\u4f30\u65b9\u6cd5\uff0c\u5bf911\u4e2a\u6700\u5148\u8fdb\u7684\u5927\u8bed\u8a00\u6a21\u578b\u57285\u4e2a\u4ee3\u8868\u6027\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u4e0a\u8fdb\u884c\u6d4b\u8bd5\uff1abug\u4fee\u590d\u3001\u529f\u80fd\u5f00\u53d1\u3001\u4ee3\u7801\u91cd\u6784\u3001\u6280\u672f\u6587\u6863\u7f16\u5199\u548c\u7814\u7a76\u7efc\u8ff0\u3002\u5f00\u53d1\u4e86\u81ea\u52a8\u5316\u9a8c\u8bc1\u6846\u67b6\u6765\u6d4b\u91cf\u8f93\u51fa\u8d28\u91cf\u548c\u5b8c\u6210\u6548\u7387\u3002", "result": "\u5173\u952e\u53d1\u73b0\u5305\u62ec\uff1a1) \u83b7\u5f97\u76f8\u540c\u5b8c\u7f8e\u5206\u6570\u7684\u6a21\u578b\u5728\u5b8c\u6210\u65f6\u95f4\u4e0a\u5b58\u572822\u500d\u5dee\u5f02\uff0c\u5de5\u5177\u6548\u738749\u500d\u5dee\u5f02\uff0c\u4f30\u8ba1\u6210\u672c53\u500d\u5dee\u5f02\uff1b2) \u5de5\u5177\u4f7f\u7528\u9891\u7387\u4e0e\u6210\u529f\u7387\u65e0\u76f8\u5173\u6027(r=0.077)\uff1b3) \u8bc6\u522b\u51fa\u4e24\u79cd\u4f4e\u6548\u6a21\u5f0f\uff1a\u5faa\u73af\u4f4e\u6548\u548c\u63a8\u7406\u4f4e\u6548\uff1b4) \u7f16\u7801\u4efb\u52a1\u8fbe\u5230100%\u6210\u529f\u7387\uff0c\u800c\u7814\u7a76\u4efb\u52a1\u66f4\u5177\u6311\u6218\u6027(90.9%)\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u4f9b\u4e86\u5bf9LLMs\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u8868\u73b0\u7684\u5168\u9762\u8bc4\u4f30\uff0c\u63ed\u793a\u4e86\u6a21\u578b\u6548\u7387\u7684\u663e\u8457\u5dee\u5f02\u548c\u5de5\u5177\u4f7f\u7528\u7684\u65e0\u6548\u6027\uff0c\u5f3a\u8c03\u4e86\u5728\u8bc4\u4f30LLMs\u65f6\u9700\u8981\u8003\u8651\u6548\u7387\u6307\u6807\u7684\u91cd\u8981\u6027\u3002\u6240\u6709\u5b9e\u9a8c\u6570\u636e\u3001\u9a8c\u8bc1\u811a\u672c\u548c\u5206\u6790\u4ee3\u7801\u5747\u5df2\u516c\u5f00\u4ee5\u786e\u4fdd\u53ef\u590d\u73b0\u6027\u3002"}}
{"id": "2602.07240", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2602.07240", "abs": "https://arxiv.org/abs/2602.07240", "authors": ["Eli Propp", "Seyed Majid Zahedi"], "title": "Hydra: Robust Hardware-Assisted Malware Detection", "comment": null, "summary": "Malware detection using Hardware Performance Counters (HPCs) offers a promising, low-overhead approach for monitoring program behavior. However, a fundamental architectural constraint, that only a limited number of hardware events can be monitored concurrently, creates a significant bottleneck, leading to detection blind spots. Prior work has primarily focused on optimizing machine learning models for a single, statically chosen event set, or on ensembling models over the same feature set. We argue that robustness requires diversifying not only the models, but also the underlying feature sets (i.e., the monitored hardware events) in order to capture a broader spectrum of program behavior. This observation motivates the following research question: Can detection performance be improved by trading temporal granularity for broader coverage, via the strategic scheduling of different feature sets over time? To answer this question, we propose Hydra, a novel detection mechanism that partitions execution traces into time slices and learns an effective schedule of feature sets and corresponding classifiers for deployment. By cycling through complementary feature sets, Hydra mitigates the limitations of a fixed monitoring perspective. Our experimental evaluation shows that Hydra significantly outperforms state-of-the-art single-feature-set baselines, achieving a 19.32% improvement in F1 score and a 60.23% reduction in false positive rate. These results underscore the importance of feature-set diversity and establish strategic multi-feature-set scheduling as an effective principle for robust, hardware-assisted malware detection.", "AI": {"tldr": "Hydra\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6076\u610f\u8f6f\u4ef6\u68c0\u6d4b\u673a\u5236\uff0c\u901a\u8fc7\u65f6\u95f4\u5207\u7247\u548c\u7279\u5f81\u96c6\u8c03\u5ea6\u7b56\u7565\uff0c\u5728\u786c\u4ef6\u6027\u80fd\u8ba1\u6570\u5668\u76d1\u63a7\u53d7\u9650\u7684\u60c5\u51b5\u4e0b\uff0c\u7528\u65f6\u95f4\u7c92\u5ea6\u6362\u53d6\u66f4\u5e7f\u6cdb\u7684\u884c\u4e3a\u8986\u76d6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u786c\u4ef6\u6027\u80fd\u8ba1\u6570\u5668\uff08HPCs\uff09\u7528\u4e8e\u6076\u610f\u8f6f\u4ef6\u68c0\u6d4b\u65f6\u5b58\u5728\u4e00\u4e2a\u6839\u672c\u6027\u67b6\u6784\u9650\u5236\uff1a\u53ea\u80fd\u540c\u65f6\u76d1\u63a7\u6709\u9650\u6570\u91cf\u7684\u4e8b\u4ef6\uff0c\u8fd9\u5bfc\u81f4\u68c0\u6d4b\u76f2\u70b9\u3002\u73b0\u6709\u5de5\u4f5c\u4e3b\u8981\u5173\u6ce8\u4e3a\u9759\u6001\u9009\u62e9\u7684\u5355\u4e00\u4e8b\u4ef6\u96c6\u4f18\u5316\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c\u800c\u9c81\u68d2\u6027\u68c0\u6d4b\u9700\u8981\u4e0d\u4ec5\u591a\u6837\u5316\u6a21\u578b\uff0c\u8fd8\u8981\u591a\u6837\u5316\u5e95\u5c42\u7279\u5f81\u96c6\uff08\u5373\u76d1\u63a7\u7684\u786c\u4ef6\u4e8b\u4ef6\uff09\uff0c\u4ee5\u6355\u83b7\u66f4\u5e7f\u6cdb\u7684\u7a0b\u5e8f\u884c\u4e3a\u3002", "method": "\u63d0\u51faHydra\u68c0\u6d4b\u673a\u5236\uff1a\u5c06\u6267\u884c\u8f68\u8ff9\u5212\u5206\u4e3a\u65f6\u95f4\u5207\u7247\uff0c\u5b66\u4e60\u6709\u6548\u7684\u7279\u5f81\u96c6\u548c\u76f8\u5e94\u5206\u7c7b\u5668\u7684\u90e8\u7f72\u8c03\u5ea6\u7b56\u7565\u3002\u901a\u8fc7\u5faa\u73af\u4f7f\u7528\u4e92\u8865\u7684\u7279\u5f81\u96c6\uff0cHydra\u7f13\u89e3\u4e86\u56fa\u5b9a\u76d1\u63a7\u89c6\u89d2\u7684\u9650\u5236\u3002", "result": "\u5b9e\u9a8c\u8bc4\u4f30\u663e\u793a\uff0cHydra\u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u5355\u7279\u5f81\u96c6\u57fa\u7ebf\u65b9\u6cd5\uff0cF1\u5206\u6570\u63d0\u9ad8\u4e8619.32%\uff0c\u8bef\u62a5\u7387\u964d\u4f4e\u4e8660.23%\u3002", "conclusion": "\u7279\u5f81\u96c6\u591a\u6837\u6027\u5bf9\u4e8e\u9c81\u68d2\u68c0\u6d4b\u81f3\u5173\u91cd\u8981\uff0c\u6218\u7565\u6027\u591a\u7279\u5f81\u96c6\u8c03\u5ea6\u662f\u786c\u4ef6\u8f85\u52a9\u6076\u610f\u8f6f\u4ef6\u68c0\u6d4b\u7684\u6709\u6548\u539f\u5219\u3002\u901a\u8fc7\u7528\u65f6\u95f4\u7c92\u5ea6\u6362\u53d6\u66f4\u5e7f\u6cdb\u8986\u76d6\uff0c\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u68c0\u6d4b\u6027\u80fd\u3002"}}
{"id": "2602.08389", "categories": ["cs.MA", "cs.AI", "cs.GT", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.08389", "abs": "https://arxiv.org/abs/2602.08389", "authors": ["Yao-hua Franck Xu", "Tayeb Lemlouma", "Arnaud Braud", "Jean-Marie Bonnin"], "title": "Altruism and Fair Objective in Mixed-Motive Markov games", "comment": null, "summary": "Cooperation is fundamental for society's viability, as it enables the emergence of structure within heterogeneous groups that seek collective well-being. However, individuals are inclined to defect in order to benefit from the group's cooperation without contributing the associated costs, thus leading to unfair situations. In game theory, social dilemmas entail this dichotomy between individual interest and collective outcome. The most dominant approach to multi-agent cooperation is the utilitarian welfare which can produce efficient highly inequitable outcomes. This paper proposes a novel framework to foster fairer cooperation by replacing the standard utilitarian objective with Proportional Fairness. We introduce a fair altruistic utility for each agent, defined on the individual log-payoff space and derive the analytical conditions required to ensure cooperation in classic social dilemmas. We then extend this framework to sequential settings by defining a Fair Markov Game and deriving novel fair Actor-Critic algorithms to learn fair policies. Finally, we evaluate our method in various social dilemma environments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6bd4\u4f8b\u516c\u5e73\u6027\u7684\u65b0\u6846\u67b6\u6765\u4fc3\u8fdb\u591a\u667a\u80fd\u4f53\u5408\u4f5c\u4e2d\u7684\u516c\u5e73\u6027\uff0c\u66ff\u4ee3\u4f20\u7edf\u7684\u529f\u5229\u4e3b\u4e49\u76ee\u6807\uff0c\u5e76\u5728\u7ecf\u5178\u793e\u4f1a\u56f0\u5883\u548c\u5e8f\u5217\u73af\u5883\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u5408\u4f5c\u5bf9\u793e\u4f1a\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4e2a\u4f53\u503e\u5411\u4e8e\u642d\u4fbf\u8f66\u4ee5\u83b7\u53d6\u96c6\u4f53\u5408\u4f5c\u7684\u5229\u76ca\u800c\u4e0d\u627f\u62c5\u6210\u672c\uff0c\u5bfc\u81f4\u4e0d\u516c\u5e73\u3002\u4f20\u7edf\u529f\u5229\u4e3b\u4e49\u65b9\u6cd5\u867d\u7136\u80fd\u4ea7\u751f\u9ad8\u6548\u7ed3\u679c\uff0c\u4f46\u53ef\u80fd\u5bfc\u81f4\u9ad8\u5ea6\u4e0d\u5e73\u7b49\u3002\u9700\u8981\u4e00\u79cd\u65b0\u6846\u67b6\u6765\u4fc3\u8fdb\u66f4\u516c\u5e73\u7684\u5408\u4f5c\u3002", "method": "\u63d0\u51fa\u7528\u6bd4\u4f8b\u516c\u5e73\u6027\u66ff\u4ee3\u6807\u51c6\u529f\u5229\u4e3b\u4e49\u76ee\u6807\uff0c\u4e3a\u6bcf\u4e2a\u667a\u80fd\u4f53\u5b9a\u4e49\u57fa\u4e8e\u4e2a\u4f53\u5bf9\u6570\u6536\u76ca\u7a7a\u95f4\u7684\u516c\u5e73\u5229\u4ed6\u6548\u7528\u51fd\u6570\u3002\u63a8\u5bfc\u4e86\u786e\u4fdd\u7ecf\u5178\u793e\u4f1a\u56f0\u5883\u4e2d\u5408\u4f5c\u7684\u89e3\u6790\u6761\u4ef6\uff0c\u5e76\u5c06\u6846\u67b6\u6269\u5c55\u5230\u5e8f\u5217\u8bbe\u7f6e\uff0c\u5b9a\u4e49\u4e86\u516c\u5e73\u9a6c\u5c14\u53ef\u592b\u535a\u5f08\uff0c\u5e76\u63a8\u5bfc\u4e86\u65b0\u7684\u516c\u5e73Actor-Critic\u7b97\u6cd5\u6765\u5b66\u4e60\u516c\u5e73\u7b56\u7565\u3002", "result": "\u5728\u5404\u79cd\u793e\u4f1a\u56f0\u5883\u73af\u5883\u4e2d\u8bc4\u4f30\u4e86\u8be5\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86\u57fa\u4e8e\u6bd4\u4f8b\u516c\u5e73\u6027\u7684\u6846\u67b6\u80fd\u591f\u4fc3\u8fdb\u66f4\u516c\u5e73\u7684\u5408\u4f5c\u3002", "conclusion": "\u6bd4\u4f8b\u516c\u5e73\u6027\u6846\u67b6\u4e3a\u591a\u667a\u80fd\u4f53\u5408\u4f5c\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u516c\u5e73\u6027\u4fc3\u8fdb\u65b9\u6cd5\uff0c\u80fd\u591f\u5e73\u8861\u6548\u7387\u4e0e\u516c\u5e73\uff0c\u5728\u7ecf\u5178\u548c\u5e8f\u5217\u793e\u4f1a\u56f0\u5883\u4e2d\u90fd\u80fd\u5b9e\u73b0\u66f4\u516c\u5e73\u7684\u5408\u4f5c\u7ed3\u679c\u3002"}}
{"id": "2602.07120", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.07120", "abs": "https://arxiv.org/abs/2602.07120", "authors": ["Jacqueline He", "Jonathan Hayase", "Wen-tau Yih", "Sewoong Oh", "Luke Zettlemoyer", "Pang Wei Koh"], "title": "Anchored Decoding: Provably Reducing Copyright Risk for Any Language Model", "comment": "51 pages, 12 figures, 16 tables. Code is publicly available at https://github.com/jacqueline-he/anchored-decoding", "summary": "Modern language models (LMs) tend to memorize portions of their training data and emit verbatim spans. When the underlying sources are sensitive or copyright-protected, such reproduction raises issues of consent and compensation for creators and compliance risks for developers. We propose Anchored Decoding, a plug-and-play inference-time method for suppressing verbatim copying: it enables decoding from any risky LM trained on mixed-license data by keeping generation in bounded proximity to a permissively trained safe LM. Anchored Decoding adaptively allocates a user-chosen information budget over the generation trajectory and enforces per-step constraints that yield a sequence-level guarantee, enabling a tunable risk-utility trade-off. To make Anchored Decoding practically useful, we introduce a new permissively trained safe model (TinyComma 1.8B), as well as Anchored$_{\\mathrm{Byte}}$ Decoding, a byte-level variant of our method that enables cross-vocabulary fusion via the ByteSampler framework (Hayase et al., 2025). We evaluate our methods across six model pairs on long-form evaluations of copyright risk and utility. Anchored and Anchored$_{\\mathrm{Byte}}$ Decoding define a new Pareto frontier, preserving near-original fluency and factuality while eliminating up to 75% of the measurable copying gap (averaged over six copying metrics) between the risky baseline and a safe reference, at a modest inference overhead.", "AI": {"tldr": "\u63d0\u51faAnchored Decoding\u65b9\u6cd5\uff0c\u901a\u8fc7\u63a8\u7406\u65f6\u7ea6\u675f\u751f\u6210\u6587\u672c\u4e0e\u5b89\u5168\u6a21\u578b\u7684\u63a5\u8fd1\u5ea6\uff0c\u6291\u5236\u8bed\u8a00\u6a21\u578b\u7684\u9010\u5b57\u590d\u5236\u884c\u4e3a\uff0c\u5e73\u8861\u7248\u6743\u98ce\u9669\u4e0e\u6a21\u578b\u6548\u7528\u3002", "motivation": "\u73b0\u4ee3\u8bed\u8a00\u6a21\u578b\u503e\u5411\u4e8e\u8bb0\u5fc6\u8bad\u7ec3\u6570\u636e\u5e76\u9010\u5b57\u8f93\u51fa\uff0c\u5f53\u6570\u636e\u6d89\u53ca\u654f\u611f\u6216\u7248\u6743\u4fdd\u62a4\u5185\u5bb9\u65f6\uff0c\u8fd9\u5f15\u53d1\u4e86\u521b\u4f5c\u8005\u540c\u610f\u4e0e\u8865\u507f\u95ee\u9898\u4ee5\u53ca\u5f00\u53d1\u8005\u7684\u5408\u89c4\u98ce\u9669\u3002", "method": "\u63d0\u51faAnchored Decoding\u65b9\u6cd5\uff1a1\uff09\u5728\u63a8\u7406\u65f6\u901a\u8fc7\u7ea6\u675f\u751f\u6210\u6587\u672c\u4e0e\u8bb8\u53ef\u8bad\u7ec3\u7684\u5b89\u5168\u6a21\u578b\u4fdd\u6301\u6709\u9650\u63a5\u8fd1\u5ea6\uff1b2\uff09\u81ea\u9002\u5e94\u5206\u914d\u7528\u6237\u9009\u62e9\u7684\u4fe1\u606f\u9884\u7b97\uff1b3\uff09\u5f15\u5165\u5b57\u8282\u7ea7\u53d8\u4f53Anchored$_{\\mathrm{Byte}}$ Decoding\u5b9e\u73b0\u8de8\u8bcd\u6c47\u878d\u5408\uff1b4\uff09\u63d0\u4f9b\u65b0\u7684\u5b89\u5168\u6a21\u578bTinyComma 1.8B\u3002", "result": "\u5728\u516d\u4e2a\u6a21\u578b\u5bf9\u7684\u957f\u5f62\u5f0f\u8bc4\u4f30\u4e2d\uff0cAnchored\u548cAnchored$_{\\mathrm{Byte}}$ Decoding\u5b9a\u4e49\u4e86\u65b0\u7684\u5e15\u7d2f\u6258\u524d\u6cbf\uff1a\u4fdd\u6301\u63a5\u8fd1\u539f\u59cb\u6d41\u7545\u6027\u548c\u4e8b\u5b9e\u6027\uff0c\u540c\u65f6\u6d88\u9664\u9ad8\u8fbe75%\u7684\u53ef\u6d4b\u91cf\u590d\u5236\u5dee\u8ddd\uff08\u5e73\u5747\u516d\u4e2a\u590d\u5236\u6307\u6807\uff09\uff0c\u63a8\u7406\u5f00\u9500\u9002\u4e2d\u3002", "conclusion": "Anchored Decoding\u662f\u4e00\u79cd\u5373\u63d2\u5373\u7528\u7684\u63a8\u7406\u65f6\u65b9\u6cd5\uff0c\u80fd\u591f\u6709\u6548\u6291\u5236\u8bed\u8a00\u6a21\u578b\u7684\u9010\u5b57\u590d\u5236\u884c\u4e3a\uff0c\u5728\u7248\u6743\u98ce\u9669\u4e0e\u6a21\u578b\u6548\u7528\u4e4b\u95f4\u5b9e\u73b0\u53ef\u8c03\u5e73\u8861\uff0c\u4e3a\u6df7\u5408\u8bb8\u53ef\u6570\u636e\u8bad\u7ec3\u7684\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u5b9e\u7528\u7684\u5408\u89c4\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.07012", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07012", "abs": "https://arxiv.org/abs/2602.07012", "authors": ["Zhonghua Wang", "Lie Ju", "Sijia Li", "Wei Feng", "Sijin Zhou", "Ming Hu", "Jianhao Xiong", "Xiaoying Tang", "Yifan Peng", "Mingquan Lin", "Yaodong Ding", "Yong Zeng", "Wenbin Wei", "Li Dong", "Zongyuan Ge"], "title": "A General Model for Retinal Segmentation and Quantification", "comment": null, "summary": "Retinal imaging is fast, non-invasive, and widely available, offering quantifiable structural and vascular signals for ophthalmic and systemic health assessment. This accessibility creates an opportunity to study how quantitative retinal phenotypes relate to ocular and systemic diseases. However, such analyses remain difficult at scale due to the limited availability of public multi-label datasets and the lack of a unified segmentation-to-quantification pipeline. We present RetSAM, a general retinal segmentation and quantification framework for fundus imaging. It delivers robust multi-target segmentation and standardized biomarker extraction, supporting downstream ophthalmologic studies and oculomics correlation analyses. Trained on over 200,000 fundus images, RetSAM supports three task categories and segments five anatomical structures, four retinal phenotypic patterns, and more than 20 distinct lesion types. It converts these segmentation results into over 30 standardized biomarkers that capture structural morphology, vascular geometry, and degenerative changes. Trained with a multi-stage strategy using both private and public fundus data, RetSAM achieves superior segmentation performance on 17 public datasets. It improves on prior best methods by 3.9 percentage points in DSC on average, with up to 15 percentage points on challenging multi-task benchmarks, and generalizes well across diverse populations, imaging devices, and clinical settings. The resulting biomarkers enable systematic correlation analyses across major ophthalmic diseases, including diabetic retinopathy, age-related macular degeneration, glaucoma, and pathologic myopia. Together, RetSAM transforms fundus images into standardized, interpretable quantitative phenotypes, enabling large-scale ophthalmic research and translation.", "AI": {"tldr": "RetSAM\u662f\u4e00\u4e2a\u901a\u7528\u7684\u89c6\u7f51\u819c\u5206\u5272\u548c\u91cf\u5316\u6846\u67b6\uff0c\u80fd\u591f\u4ece\u773c\u5e95\u56fe\u50cf\u4e2d\u5206\u5272\u591a\u79cd\u89e3\u5256\u7ed3\u6784\u3001\u8868\u578b\u6a21\u5f0f\u548c\u75c5\u53d8\u7c7b\u578b\uff0c\u5e76\u63d0\u53d630\u591a\u79cd\u6807\u51c6\u5316\u751f\u7269\u6807\u5fd7\u7269\uff0c\u652f\u6301\u5927\u89c4\u6a21\u773c\u79d1\u7814\u7a76\u548c\u7cfb\u7edf\u6027\u75be\u75c5\u5173\u8054\u5206\u6790\u3002", "motivation": "\u89c6\u7f51\u819c\u6210\u50cf\u5feb\u901f\u3001\u65e0\u521b\u4e14\u5e7f\u6cdb\u5e94\u7528\uff0c\u4e3a\u773c\u79d1\u548c\u5168\u8eab\u5065\u5eb7\u8bc4\u4f30\u63d0\u4f9b\u4e86\u53ef\u91cf\u5316\u7684\u7ed3\u6784\u548c\u8840\u7ba1\u4fe1\u53f7\u3002\u7136\u800c\uff0c\u7531\u4e8e\u7f3a\u4e4f\u516c\u5171\u591a\u6807\u7b7e\u6570\u636e\u96c6\u548c\u7edf\u4e00\u7684\u5206\u5272\u5230\u91cf\u5316\u6d41\u7a0b\uff0c\u5927\u89c4\u6a21\u5206\u6790\u4ecd\u7136\u56f0\u96be\u3002", "method": "RetSAM\u662f\u4e00\u4e2a\u901a\u7528\u7684\u89c6\u7f51\u819c\u5206\u5272\u548c\u91cf\u5316\u6846\u67b6\uff0c\u91c7\u7528\u591a\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff0c\u4f7f\u7528\u79c1\u6709\u548c\u516c\u5171\u773c\u5e95\u6570\u636e\uff0c\u8bad\u7ec3\u8d85\u8fc720\u4e07\u5f20\u773c\u5e95\u56fe\u50cf\u3002\u652f\u6301\u4e09\u7c7b\u4efb\u52a1\uff1a\u5206\u5272\u4e94\u79cd\u89e3\u5256\u7ed3\u6784\u3001\u56db\u79cd\u89c6\u7f51\u819c\u8868\u578b\u6a21\u5f0f\u548c20\u591a\u79cd\u4e0d\u540c\u75c5\u53d8\u7c7b\u578b\uff0c\u5e76\u5c06\u5206\u5272\u7ed3\u679c\u8f6c\u5316\u4e3a30\u591a\u79cd\u6807\u51c6\u5316\u751f\u7269\u6807\u5fd7\u7269\u3002", "result": "\u572817\u4e2a\u516c\u5171\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u4f18\u8d8a\u7684\u5206\u5272\u6027\u80fd\uff0c\u5e73\u5747DSC\u6bd4\u5148\u524d\u6700\u4f73\u65b9\u6cd5\u63d0\u9ad83.9\u4e2a\u767e\u5206\u70b9\uff0c\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u591a\u4efb\u52a1\u57fa\u51c6\u4e0a\u63d0\u5347\u9ad8\u8fbe15\u4e2a\u767e\u5206\u70b9\u3002\u5728\u4e0d\u540c\u4eba\u7fa4\u3001\u6210\u50cf\u8bbe\u5907\u548c\u4e34\u5e8a\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "RetSAM\u5c06\u773c\u5e95\u56fe\u50cf\u8f6c\u5316\u4e3a\u6807\u51c6\u5316\u3001\u53ef\u89e3\u91ca\u7684\u5b9a\u91cf\u8868\u578b\uff0c\u652f\u6301\u8de8\u4e3b\u8981\u773c\u79d1\u75be\u75c5\uff08\u5305\u62ec\u7cd6\u5c3f\u75c5\u89c6\u7f51\u819c\u75c5\u53d8\u3001\u5e74\u9f84\u76f8\u5173\u6027\u9ec4\u6591\u53d8\u6027\u3001\u9752\u5149\u773c\u548c\u75c5\u7406\u6027\u8fd1\u89c6\uff09\u7684\u7cfb\u7edf\u6027\u5173\u8054\u5206\u6790\uff0c\u5b9e\u73b0\u4e86\u5927\u89c4\u6a21\u773c\u79d1\u7814\u7a76\u548c\u8f6c\u5316\u5e94\u7528\u3002"}}
{"id": "2602.07249", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.07249", "abs": "https://arxiv.org/abs/2602.07249", "authors": ["Qi Sun", "Ahmed Abdo", "Luis Burbano", "Ziyang Li", "Yaxing Yao", "Alvaro Cardenas", "Yinzhi Cao"], "title": "Beyond Crash: Hijacking Your Autonomous Vehicle for Fun and Profit", "comment": null, "summary": "Autonomous Vehicles (AVs), especially vision-based AVs, are rapidly being deployed without human operators. As AVs operate in safety-critical environments, understanding their robustness in an adversarial environment is an important research problem. Prior physical adversarial attacks on vision-based autonomous vehicles predominantly target immediate safety failures (e.g., a crash, a traffic-rule violation, or a transient lane departure) by inducing a short-lived perception or control error. This paper shows a qualitatively different risk: a long-horizon route integrity compromise, where an attacker gradually steers a victim AV away from its intended route and into an attacker-chosen destination while the victim continues to drive \"normally.\" This will not pose a danger to the victim vehicle itself, but also to potential passengers sitting inside the vehicle.\n  In this paper, we design and implement the first adversarial framework, called JackZebra, that performs route-level hijacking of a vision-based end-to-end driving stack using a physically plausible attacker vehicle with a reconfigurable display mounted on the rear. The central challenge is temporal persistence: adversarial influence must remain effective in changing viewpoints, lighting, weather, traffic, and the victim's continual replanning -- without triggering conspicuous failures. Our key insight is to treat route hijacking as a closed-loop control problem and to convert adversarial patches into steering primitives that can be selected online via an interactive adjustment loop. Our adversarial patches are also carefully designed against worst-case background and sensor variations so that the adversarial impacts on the victim. Our evaluation shows that JackZebra can successfully hijack victim vehicles to deviate from original routes and stop at adversarial destinations with a high success rate.", "AI": {"tldr": "JackZebra\u662f\u9996\u4e2a\u9488\u5bf9\u89c6\u89c9\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u8def\u7ebf\u7ea7\u52ab\u6301\u5bf9\u6297\u6846\u67b6\uff0c\u901a\u8fc7\u53ef\u91cd\u6784\u663e\u793a\u5c4f\u7684\u7269\u7406\u653b\u51fb\u8f66\u8f86\uff0c\u9010\u6b65\u5c06\u53d7\u5bb3\u8f66\u8f86\u5f15\u5bfc\u81f3\u653b\u51fb\u8005\u9009\u62e9\u7684\u76ee\u7684\u5730\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u5728\u5b89\u5168\u5173\u952e\u73af\u5883\u4e2d\u8fd0\u884c\uff0c\u73b0\u6709\u7269\u7406\u5bf9\u6297\u653b\u51fb\u4e3b\u8981\u9488\u5bf9\u5373\u65f6\u5b89\u5168\u6545\u969c\uff08\u5982\u78b0\u649e\u3001\u8fdd\u89c4\uff09\uff0c\u4f46\u672c\u6587\u5173\u6ce8\u4e00\u79cd\u5b9a\u6027\u4e0d\u540c\u7684\u98ce\u9669\uff1a\u957f\u671f\u8def\u7ebf\u5b8c\u6574\u6027\u7834\u574f\uff0c\u653b\u51fb\u8005\u9010\u6b65\u5c06\u53d7\u5bb3\u8f66\u8f86\u5f15\u5bfc\u81f3\u653b\u51fb\u8005\u9009\u62e9\u7684\u76ee\u7684\u5730\uff0c\u540c\u65f6\u8f66\u8f86\u4fdd\u6301\"\u6b63\u5e38\"\u884c\u9a76\u3002", "method": "\u8bbe\u8ba1JackZebra\u5bf9\u6297\u6846\u67b6\uff0c\u4f7f\u7528\u914d\u5907\u53ef\u91cd\u6784\u663e\u793a\u5c4f\u7684\u7269\u7406\u653b\u51fb\u8f66\u8f86\u3002\u6838\u5fc3\u6311\u6218\u662f\u65f6\u95f4\u6301\u4e45\u6027\uff1a\u5bf9\u6297\u5f71\u54cd\u5fc5\u987b\u5728\u89c6\u89d2\u3001\u5149\u7167\u3001\u5929\u6c14\u3001\u4ea4\u901a\u53d8\u5316\u53ca\u53d7\u5bb3\u8005\u6301\u7eed\u91cd\u65b0\u89c4\u5212\u7684\u60c5\u51b5\u4e0b\u4fdd\u6301\u6709\u6548\u3002\u5173\u952e\u89c1\u89e3\u662f\u5c06\u8def\u7ebf\u52ab\u6301\u89c6\u4e3a\u95ed\u73af\u63a7\u5236\u95ee\u9898\uff0c\u5c06\u5bf9\u6297\u8865\u4e01\u8f6c\u6362\u4e3a\u53ef\u901a\u8fc7\u4ea4\u4e92\u8c03\u6574\u5faa\u73af\u5728\u7ebf\u9009\u62e9\u7684\u8f6c\u5411\u539f\u8bed\u3002", "result": "\u8bc4\u4f30\u663e\u793aJackZebra\u80fd\u591f\u6210\u529f\u52ab\u6301\u53d7\u5bb3\u8f66\u8f86\u504f\u79bb\u539f\u59cb\u8def\u7ebf\uff0c\u5e76\u4ee5\u9ad8\u6210\u529f\u7387\u5728\u5bf9\u6297\u76ee\u7684\u5730\u505c\u6b62\u3002", "conclusion": "\u672c\u6587\u5c55\u793a\u4e86\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u9762\u4e34\u7684\u65b0\u578b\u957f\u671f\u8def\u7ebf\u52ab\u6301\u98ce\u9669\uff0c\u63d0\u51fa\u4e86\u9996\u4e2a\u7269\u7406\u53ef\u884c\u7684\u8def\u7ebf\u7ea7\u5bf9\u6297\u653b\u51fb\u6846\u67b6\uff0c\u63ed\u793a\u4e86\u81ea\u52a8\u9a7e\u9a76\u5b89\u5168\u9700\u8981\u5173\u6ce8\u8d85\u8d8a\u5373\u65f6\u5b89\u5168\u6545\u969c\u7684\u957f\u671f\u5b8c\u6574\u6027\u5a01\u80c1\u3002"}}
{"id": "2602.08529", "categories": ["cs.MA"], "pdf": "https://arxiv.org/pdf/2602.08529", "abs": "https://arxiv.org/abs/2602.08529", "authors": ["Ning Lin", "Haolun Li", "Mingshu Liu", "Chengyun Ruan", "Kaibo Huang", "Yukun Wei", "Zhongliang Yang", "Linna Zhou"], "title": "EvoCorps: An Evolutionary Multi-Agent Framework for Depolarizing Online Discourse", "comment": null, "summary": "Polarization in online discourse erodes social trust and accelerates misinformation, yet technical responses remain largely diagnostic and post-hoc. Current governance approaches suffer from inherent latency and static policies, struggling to counter coordinated adversarial amplification that evolves in real-time. We present EvoCorps, an evolutionary multi-agent framework for proactive depolarization. EvoCorps frames discourse governance as a dynamic social game and coordinates roles for monitoring, planning, grounded generation, and multi-identity diffusion. A retrieval-augmented collective cognition core provides factual grounding and action--outcome memory, while closed-loop evolutionary learning adapts strategies as the environment and attackers change. We implement EvoCorps on the MOSAIC social-AI simulation platform for controlled evaluation in a multi-source news stream with adversarial injection and amplification. Across emotional polarization, viewpoint extremity, and argumentative rationality, EvoCorps improves discourse outcomes over an adversarial baseline, pointing to a practical path from detection and post-hoc mitigation to in-process, closed-loop intervention. The code is available at https://github.com/ln2146/EvoCorps.", "AI": {"tldr": "EvoCorps\u662f\u4e00\u4e2a\u8fdb\u5316\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u7528\u4e8e\u4e3b\u52a8\u53bb\u6781\u5316\u5728\u7ebf\u8ba8\u8bba\uff0c\u901a\u8fc7\u52a8\u6001\u793e\u4f1a\u6e38\u620f\u7684\u65b9\u5f0f\u534f\u8c03\u76d1\u63a7\u3001\u89c4\u5212\u3001\u751f\u6210\u548c\u6269\u6563\u89d2\u8272\uff0c\u5b9e\u73b0\u5b9e\u65f6\u5bf9\u6297\u6027\u653e\u5927\u7684\u95ed\u73af\u5e72\u9884\u3002", "motivation": "\u5728\u7ebf\u8ba8\u8bba\u4e2d\u7684\u6781\u5316\u73b0\u8c61\u4fb5\u8680\u793e\u4f1a\u4fe1\u4efb\u5e76\u52a0\u901f\u9519\u8bef\u4fe1\u606f\u4f20\u64ad\uff0c\u5f53\u524d\u7684\u6280\u672f\u54cd\u5e94\u4e3b\u8981\u662f\u8bca\u65ad\u6027\u548c\u4e8b\u540e\u5904\u7406\u3002\u73b0\u6709\u6cbb\u7406\u65b9\u6cd5\u5b58\u5728\u56fa\u6709\u5ef6\u8fdf\u548c\u9759\u6001\u7b56\u7565\uff0c\u96be\u4ee5\u5e94\u5bf9\u5b9e\u65f6\u6f14\u53d8\u7684\u534f\u8c03\u5bf9\u6297\u6027\u653e\u5927\u3002", "method": "\u63d0\u51faEvoCorps\u8fdb\u5316\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u5c06\u8ba8\u8bba\u6cbb\u7406\u89c6\u4e3a\u52a8\u6001\u793e\u4f1a\u6e38\u620f\uff0c\u534f\u8c03\u76d1\u63a7\u3001\u89c4\u5212\u3001\u57fa\u4e8e\u4e8b\u5b9e\u7684\u751f\u6210\u548c\u591a\u8eab\u4efd\u6269\u6563\u89d2\u8272\u3002\u91c7\u7528\u68c0\u7d22\u589e\u5f3a\u7684\u96c6\u4f53\u8ba4\u77e5\u6838\u5fc3\u63d0\u4f9b\u4e8b\u5b9e\u57fa\u7840\u548c\u884c\u52a8-\u7ed3\u679c\u8bb0\u5fc6\uff0c\u901a\u8fc7\u95ed\u73af\u8fdb\u5316\u5b66\u4e60\u5728\u73af\u5883\u548c\u653b\u51fb\u8005\u53d8\u5316\u65f6\u8c03\u6574\u7b56\u7565\u3002", "result": "\u5728MOSAIC\u793e\u4ea4AI\u6a21\u62df\u5e73\u53f0\u4e0a\u8fdb\u884c\u63a7\u5236\u8bc4\u4f30\uff0c\u5728\u591a\u6e90\u65b0\u95fb\u6d41\u4e2d\u52a0\u5165\u5bf9\u6297\u6027\u6ce8\u5165\u548c\u653e\u5927\u7684\u573a\u666f\u4e2d\uff0cEvoCorps\u5728\u60c5\u611f\u6781\u5316\u3001\u89c2\u70b9\u6781\u7aef\u6027\u548c\u8bba\u8bc1\u7406\u6027\u65b9\u9762\u5747\u4f18\u4e8e\u5bf9\u6297\u6027\u57fa\u7ebf\u3002", "conclusion": "EvoCorps\u4e3a\u4ece\u68c0\u6d4b\u548c\u4e8b\u540e\u7f13\u89e3\u8f6c\u5411\u8fc7\u7a0b\u4e2d\u95ed\u73af\u5e72\u9884\u63d0\u4f9b\u4e86\u5b9e\u7528\u8def\u5f84\uff0c\u5c55\u793a\u4e86\u4e3b\u52a8\u53bb\u6781\u5316\u6846\u67b6\u7684\u53ef\u884c\u6027\u3002"}}
{"id": "2602.07013", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.07013", "abs": "https://arxiv.org/abs/2602.07013", "authors": ["Jiaxi Yang", "Shicheng Liu", "Yuchen Yang", "Dongwon Lee"], "title": "Steering to Say No: Configurable Refusal via Activation Steering in Vision Language Models", "comment": null, "summary": "With the rapid advancement of Vision Language Models (VLMs), refusal mechanisms have become a critical component for ensuring responsible and safe model behavior. However, existing refusal strategies are largely \\textit{one-size-fits-all} and fail to adapt to diverse user needs and contextual constraints, leading to either under-refusal or over-refusal. In this work, we firstly explore the challenges mentioned above and develop \\textbf{C}onfigurable \\textbf{R}efusal in \\textbf{VLM}s (\\textbf{CR-VLM}), a robust and efficient approach for {\\em configurable} refusal based on activation steering. CR-VLM consists of three integrated components: (1) extracting a configurable refusal vector via a teacher-forced mechanism to amplify the refusal signal; (2) introducing a gating mechanism that mitigates over-refusal by preserving acceptance for in-scope queries; and (3) designing a counterfactual vision enhancement module that aligns visual representations with refusal requirements. Comprehensive experiments across multiple datasets and various VLMs demonstrate that CR-VLM achieves effective, efficient, and robust configurable refusals, offering a scalable path toward user-adaptive safety alignment in VLMs.", "AI": {"tldr": "CR-VLM\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u914d\u7f6e\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u62d2\u7edd\u673a\u5236\uff0c\u901a\u8fc7\u6fc0\u6d3b\u5f15\u5bfc\u5b9e\u73b0\u7528\u6237\u81ea\u9002\u5e94\u7684\u5b89\u5168\u5bf9\u9f50\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u62d2\u7edd\u7b56\u7565\"\u4e00\u5200\u5207\"\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u62d2\u7edd\u673a\u5236\u5927\u591a\u662f\"\u4e00\u5200\u5207\"\u7684\uff0c\u65e0\u6cd5\u9002\u5e94\u4e0d\u540c\u7528\u6237\u9700\u6c42\u548c\u4e0a\u4e0b\u6587\u7ea6\u675f\uff0c\u5bfc\u81f4\u8981\u4e48\u62d2\u7edd\u4e0d\u8db3\u8981\u4e48\u8fc7\u5ea6\u62d2\u7edd\uff0c\u9700\u8981\u66f4\u7075\u6d3b\u53ef\u914d\u7f6e\u7684\u62d2\u7edd\u7b56\u7565\u3002", "method": "CR-VLM\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a1)\u901a\u8fc7\u6559\u5e08\u5f3a\u5236\u673a\u5236\u63d0\u53d6\u53ef\u914d\u7f6e\u62d2\u7edd\u5411\u91cf\u4ee5\u589e\u5f3a\u62d2\u7edd\u4fe1\u53f7\uff1b2)\u5f15\u5165\u95e8\u63a7\u673a\u5236\u4fdd\u62a4\u8303\u56f4\u5185\u67e5\u8be2\u7684\u63a5\u53d7\u6027\uff0c\u7f13\u89e3\u8fc7\u5ea6\u62d2\u7edd\uff1b3)\u8bbe\u8ba1\u53cd\u4e8b\u5b9e\u89c6\u89c9\u589e\u5f3a\u6a21\u5757\uff0c\u4f7f\u89c6\u89c9\u8868\u793a\u4e0e\u62d2\u7edd\u8981\u6c42\u5bf9\u9f50\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u548c\u5404\u79cd\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e0a\u7684\u7efc\u5408\u5b9e\u9a8c\u8868\u660e\uff0cCR-VLM\u5b9e\u73b0\u4e86\u6709\u6548\u3001\u9ad8\u6548\u4e14\u9c81\u68d2\u7684\u53ef\u914d\u7f6e\u62d2\u7edd\uff0c\u4e3aVLMs\u7684\u7528\u6237\u81ea\u9002\u5e94\u5b89\u5168\u5bf9\u9f50\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u8def\u5f84\u3002", "conclusion": "CR-VLM\u901a\u8fc7\u6fc0\u6d3b\u5f15\u5bfc\u7684\u53ef\u914d\u7f6e\u62d2\u7edd\u673a\u5236\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u62d2\u7edd\u7b56\u7565\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u66f4\u7075\u6d3b\u3001\u7528\u6237\u81ea\u9002\u5e94\u7684\u5b89\u5168\u5bf9\u9f50\u65b9\u6cd5\u3002"}}
{"id": "2602.07083", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07083", "abs": "https://arxiv.org/abs/2602.07083", "authors": ["Yongqing Jiang", "Jianze Wang", "Zhiqi Shen", "Zhenghong Lin", "Jiayuan Wang", "Yijian Yang", "Kaoshan Dai", "Haoran Luo"], "title": "Rethinking Scientific Modeling: Toward Physically Consistent and Simulation-Executable Programmatic Generation", "comment": null, "summary": "Structural modeling is a fundamental component of computational engineering science, in which even minor physical inconsistencies or specification violations may invalidate downstream simulations. The potential of large language models (LLMs) for automatic generation of modeling code has been demonstrated. However, non-executable or physically inconsistent outputs remain prevalent under stringent engineering constraints. A framework for physics-consistent automatic building modeling is therefore proposed, integrating domain knowledge construction, constraint-oriented model alignment, and verification-driven evaluation. CivilInstruct is introduced as a domain-specific dataset that formalizes structural engineering knowledge and constraint reasoning to enable simulation-ready model generation. A two-stage fine-tuning strategy is further employed to enforce constraint satisfaction and application programming interface compliance, substantially reducing hallucinated and non-conforming outputs. MBEval is presented as a verification-driven benchmark that evaluates executability and structural dynamics consistency through closed-loop validation. Experimental results show consistent improvements over baselines across rigorous verification metrics. Our code is available at https://github.com/Jovanqing/AutoBM.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u7269\u7406\u4e00\u81f4\u6027\u81ea\u52a8\u5efa\u7b51\u5efa\u6a21\u6846\u67b6\uff0c\u901a\u8fc7\u9886\u57df\u77e5\u8bc6\u6784\u5efa\u3001\u7ea6\u675f\u5bfc\u5411\u6a21\u578b\u5bf9\u9f50\u548c\u9a8c\u8bc1\u9a71\u52a8\u8bc4\u4f30\uff0c\u663e\u8457\u51cf\u5c11\u5e7b\u89c9\u548c\u975e\u5408\u89c4\u8f93\u51fa", "motivation": "\u7ed3\u6784\u5efa\u6a21\u662f\u8ba1\u7b97\u5de5\u7a0b\u79d1\u5b66\u7684\u57fa\u7840\uff0c\u5373\u4f7f\u5fae\u5c0f\u7684\u7269\u7406\u4e0d\u4e00\u81f4\u6216\u89c4\u8303\u8fdd\u53cd\u90fd\u53ef\u80fd\u4f7f\u4e0b\u6e38\u6a21\u62df\u5931\u6548\u3002\u867d\u7136\u5927\u8bed\u8a00\u6a21\u578b\u5df2\u5c55\u793a\u81ea\u52a8\u751f\u6210\u5efa\u6a21\u4ee3\u7801\u7684\u6f5c\u529b\uff0c\u4f46\u5728\u4e25\u683c\u5de5\u7a0b\u7ea6\u675f\u4e0b\uff0c\u4e0d\u53ef\u6267\u884c\u6216\u7269\u7406\u4e0d\u4e00\u81f4\u7684\u8f93\u51fa\u4ecd\u7136\u666e\u904d\u5b58\u5728", "method": "\u63d0\u51fa\u7269\u7406\u4e00\u81f4\u6027\u81ea\u52a8\u5efa\u7b51\u5efa\u6a21\u6846\u67b6\uff0c\u5305\u62ec\uff1a1\uff09\u5f15\u5165CivilInstruct\u9886\u57df\u7279\u5b9a\u6570\u636e\u96c6\uff0c\u5f62\u5f0f\u5316\u7ed3\u6784\u5de5\u7a0b\u77e5\u8bc6\u548c\u7ea6\u675f\u63a8\u7406\uff1b2\uff09\u91c7\u7528\u4e24\u9636\u6bb5\u5fae\u8c03\u7b56\u7565\u5f3a\u5236\u7ea6\u675f\u6ee1\u8db3\u548cAPI\u5408\u89c4\u6027\uff1b3\uff09\u63d0\u51faMBEval\u9a8c\u8bc1\u9a71\u52a8\u57fa\u51c6\uff0c\u901a\u8fc7\u95ed\u73af\u9a8c\u8bc1\u8bc4\u4f30\u53ef\u6267\u884c\u6027\u548c\u7ed3\u6784\u52a8\u529b\u5b66\u4e00\u81f4\u6027", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u5728\u4e25\u683c\u7684\u9a8c\u8bc1\u6307\u6807\u4e0a\u76f8\u6bd4\u57fa\u7ebf\u6a21\u578b\u6709\u6301\u7eed\u6539\u8fdb\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u5e7b\u89c9\u548c\u975e\u5408\u89c4\u8f93\u51fa", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u6574\u5408\u9886\u57df\u77e5\u8bc6\u6784\u5efa\u3001\u7ea6\u675f\u5bfc\u5411\u6a21\u578b\u5bf9\u9f50\u548c\u9a8c\u8bc1\u9a71\u52a8\u8bc4\u4f30\uff0c\u5b9e\u73b0\u4e86\u7269\u7406\u4e00\u81f4\u7684\u81ea\u52a8\u5efa\u7b51\u5efa\u6a21\uff0c\u4e3a\u5de5\u7a0b\u9886\u57df\u7684\u53ef\u9760\u4ee3\u7801\u751f\u6210\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2602.07014", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07014", "abs": "https://arxiv.org/abs/2602.07014", "authors": ["Qingyu Wu", "Yuxuan Han", "Haijun Li", "Zhao Xu", "Jianshan Zhao", "Xu Jin", "Longyue Wang", "Weihua Luo"], "title": "Vectra: A New Metric, Dataset, and Model for Visual Quality Assessment in E-Commerce In-Image Machine Translation", "comment": null, "summary": "In-Image Machine Translation (IIMT) powers cross-border e-commerce product listings; existing research focuses on machine translation evaluation, while visual rendering quality is critical for user engagement. When facing context-dense product imagery and multimodal defects, current reference-based methods (e.g., SSIM, FID) lack explainability, while model-as-judge approaches lack domain-grounded, fine-grained reward signals. To bridge this gap, we introduce Vectra, to the best of our knowledge, the first reference-free, MLLM-driven visual quality assessment framework for e-commerce IIMT. Vectra comprises three components: (1) Vectra Score, a multidimensional quality metric system that decomposes visual quality into 14 interpretable dimensions, with spatially-aware Defect Area Ratio (DAR) quantification to reduce annotation ambiguity; (2) Vectra Dataset, constructed from 1.1M real-world product images via diversity-aware sampling, comprising a 2K benchmark for system evaluation, 30K reasoning-based annotations for instruction tuning, and 3.5K expert-labeled preferences for alignment and evaluation; and (3) Vectra Model, a 4B-parameter MLLM that generates both quantitative scores and diagnostic reasoning. Experiments demonstrate that Vectra achieves state-of-the-art correlation with human rankings, and our model outperforms leading MLLMs, including GPT-5 and Gemini-3, in scoring performance. The dataset and model will be released upon acceptance.", "AI": {"tldr": "Vectra\u662f\u9996\u4e2a\u9762\u5411\u7535\u5546\u56fe\u50cf\u5185\u673a\u5668\u7ffb\u8bd1\u7684\u65e0\u53c2\u8003\u89c6\u89c9\u8d28\u91cf\u8bc4\u4f30\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u7ef4\u5ea6\u8d28\u91cf\u6307\u6807\u3001\u5927\u89c4\u6a21\u6570\u636e\u96c6\u548c4B\u53c2\u6570MLLM\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u89e3\u91ca\u6027\u548c\u7ec6\u7c92\u5ea6\u5956\u52b1\u4fe1\u53f7\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u56fe\u50cf\u5185\u673a\u5668\u7ffb\u8bd1\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u673a\u5668\u7ffb\u8bd1\u8bc4\u4f30\uff0c\u800c\u89c6\u89c9\u6e32\u67d3\u8d28\u91cf\u5bf9\u7528\u6237\u53c2\u4e0e\u5ea6\u81f3\u5173\u91cd\u8981\u3002\u5f53\u524d\u57fa\u4e8e\u53c2\u8003\u7684\u65b9\u6cd5\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\uff0c\u6a21\u578b\u5373\u8bc4\u5224\u65b9\u6cd5\u7f3a\u4e4f\u9886\u57df\u57fa\u7840\u3001\u7ec6\u7c92\u5ea6\u7684\u5956\u52b1\u4fe1\u53f7\uff0c\u7279\u522b\u662f\u5728\u9762\u5bf9\u4e0a\u4e0b\u6587\u5bc6\u96c6\u7684\u4ea7\u54c1\u56fe\u50cf\u548c\u591a\u6a21\u6001\u7f3a\u9677\u65f6\u3002", "method": "Vectra\u6846\u67b6\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a1) Vectra Score - \u5c06\u89c6\u89c9\u8d28\u91cf\u5206\u89e3\u4e3a14\u4e2a\u53ef\u89e3\u91ca\u7ef4\u5ea6\u7684\u591a\u7ef4\u8d28\u91cf\u6307\u6807\u7cfb\u7edf\uff0c\u91c7\u7528\u7a7a\u95f4\u611f\u77e5\u7684\u7f3a\u9677\u9762\u79ef\u6bd4\u91cf\u5316\u51cf\u5c11\u6807\u6ce8\u6b67\u4e49\uff1b2) Vectra Dataset - \u901a\u8fc7\u591a\u6837\u6027\u611f\u77e5\u91c7\u6837\u4ece110\u4e07\u771f\u5b9e\u4ea7\u54c1\u56fe\u50cf\u6784\u5efa\u7684\u6570\u636e\u96c6\uff0c\u5305\u62ec2K\u57fa\u51c6\u6d4b\u8bd5\u96c6\u300130K\u63a8\u7406\u6807\u6ce8\u548c3.5K\u4e13\u5bb6\u6807\u6ce8\u504f\u597d\uff1b3) Vectra Model - 4B\u53c2\u6570\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u80fd\u751f\u6210\u5b9a\u91cf\u5206\u6570\u548c\u8bca\u65ad\u63a8\u7406\u3002", "result": "\u5b9e\u9a8c\u8868\u660eVectra\u5728\u4eba\u7c7b\u6392\u540d\u76f8\u5173\u6027\u65b9\u9762\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u5176\u6a21\u578b\u5728\u8bc4\u5206\u6027\u80fd\u4e0a\u8d85\u8d8a\u4e86\u5305\u62ecGPT-5\u548cGemini-3\u5728\u5185\u7684\u9886\u5148MLLMs\u3002", "conclusion": "Vectra\u586b\u8865\u4e86\u7535\u5546\u56fe\u50cf\u5185\u673a\u5668\u7ffb\u8bd1\u89c6\u89c9\u8d28\u91cf\u8bc4\u4f30\u7684\u7a7a\u767d\uff0c\u63d0\u4f9b\u4e86\u9996\u4e2a\u65e0\u53c2\u8003\u3001MLLM\u9a71\u52a8\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u901a\u8fc7\u53ef\u89e3\u91ca\u7684\u591a\u7ef4\u5ea6\u6307\u6807\u3001\u5927\u89c4\u6a21\u6570\u636e\u96c6\u548c\u9ad8\u6548\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u89c9\u8d28\u91cf\u8bc4\u4f30\u7684\u51c6\u786e\u6027\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2602.07086", "categories": ["cs.SE", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.07086", "abs": "https://arxiv.org/abs/2602.07086", "authors": ["Michael Marketsm\u00fcller", "Simon Martin", "Tim Schlippe"], "title": "Evaluating Retrieval-Augmented Generation Variants for Natural Language-Based SQL and API Call Generation", "comment": "preprint of conference submission", "summary": "Enterprise systems increasingly require natural language interfaces that can translate user requests into structured operations such as SQL queries and REST API calls. While large language models (LLMs) show promise for code generation [Chen et al., 2021; Huynh and Lin, 2025], their effectiveness in domain-specific enterprise contexts remains underexplored, particularly when both retrieval and modification tasks must be handled jointly. This paper presents a comprehensive evaluation of three retrieval-augmented generation (RAG) variants [Lewis et al., 2021] -- standard RAG, Self-RAG [Asai et al., 2024], and CoRAG [Wang et al., 2025] -- across SQL query generation, REST API call generation, and a combined task requiring dynamic task classification. Using SAP Transactional Banking as a realistic enterprise use case, we construct a novel test dataset covering both modalities and evaluate 18 experimental configurations under database-only, API-only, and hybrid documentation contexts. Results demonstrate that RAG is essential: Without retrieval, exact match accuracy is 0% across all tasks, whereas retrieval yields substantial gains in execution accuracy (up to 79.30%) and component match accuracy (up to 78.86%). Critically, CoRAG proves most robust in hybrid documentation settings, achieving statistically significant improvements in the combined task (10.29% exact match vs. 7.45% for standard RAG), driven primarily by superior SQL generation performance (15.32% vs. 11.56%). Our findings establish retrieval-policy design as a key determinant of production-grade natural language interfaces, showing that iterative query decomposition outperforms both top-k retrieval and binary relevance filtering under documentation heterogeneity.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u8bc4\u4f30\u4e86\u4e09\u79cdRAG\u53d8\u4f53\u5728\u4f01\u4e1a\u7cfb\u7edf\u81ea\u7136\u8bed\u8a00\u63a5\u53e3\u4e2d\u7684\u5e94\u7528\uff0c\u53d1\u73b0\u5728SQL\u67e5\u8be2\u548cREST API\u8c03\u7528\u751f\u6210\u4efb\u52a1\u4e2d\uff0c\u68c0\u7d22\u589e\u5f3a\u81f3\u5173\u91cd\u8981\uff0c\u5176\u4e2dCoRAG\u5728\u6df7\u5408\u6587\u6863\u73af\u5883\u4e0b\u8868\u73b0\u6700\u4f18\u3002", "motivation": "\u4f01\u4e1a\u7cfb\u7edf\u9700\u8981\u81ea\u7136\u8bed\u8a00\u63a5\u53e3\u6765\u5c06\u7528\u6237\u8bf7\u6c42\u8f6c\u6362\u4e3a\u7ed3\u6784\u5316\u64cd\u4f5c\uff08\u5982SQL\u67e5\u8be2\u548cREST API\u8c03\u7528\uff09\u3002\u867d\u7136\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4ee3\u7801\u751f\u6210\u65b9\u9762\u663e\u793a\u51fa\u6f5c\u529b\uff0c\u4f46\u5728\u7279\u5b9a\u9886\u57df\u7684\u4f01\u4e1a\u73af\u5883\u4e2d\uff0c\u7279\u522b\u662f\u9700\u8981\u540c\u65f6\u5904\u7406\u68c0\u7d22\u548c\u4fee\u6539\u4efb\u52a1\u65f6\uff0c\u5176\u6709\u6548\u6027\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002", "method": "\u4f7f\u7528SAP Transactional Banking\u4f5c\u4e3a\u5b9e\u9645\u4f01\u4e1a\u7528\u4f8b\uff0c\u6784\u5efa\u4e86\u6db5\u76d6SQL\u67e5\u8be2\u751f\u6210\u3001REST API\u8c03\u7528\u751f\u6210\u4ee5\u53ca\u9700\u8981\u52a8\u6001\u4efb\u52a1\u5206\u7c7b\u7684\u6df7\u5408\u4efb\u52a1\u7684\u65b0\u6d4b\u8bd5\u6570\u636e\u96c6\u3002\u8bc4\u4f30\u4e86\u4e09\u79cd\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u53d8\u4f53\uff1a\u6807\u51c6RAG\u3001Self-RAG\u548cCoRAG\uff0c\u5728\u6570\u636e\u5e93\u4e13\u7528\u3001API\u4e13\u7528\u548c\u6df7\u5408\u6587\u6863\u4e09\u79cd\u4e0a\u4e0b\u6587\u4e0b\u768418\u79cd\u5b9e\u9a8c\u914d\u7f6e\u3002", "result": "\u7ed3\u679c\u663e\u793a\u68c0\u7d22\u81f3\u5173\u91cd\u8981\uff1a\u65e0\u68c0\u7d22\u65f6\u6240\u6709\u4efb\u52a1\u7684\u7cbe\u786e\u5339\u914d\u51c6\u786e\u7387\u4e3a0%\uff0c\u800c\u68c0\u7d22\u5e26\u6765\u4e86\u6267\u884c\u51c6\u786e\u7387\uff08\u6700\u9ad879.30%\uff09\u548c\u7ec4\u4ef6\u5339\u914d\u51c6\u786e\u7387\uff08\u6700\u9ad878.86%\uff09\u7684\u663e\u8457\u63d0\u5347\u3002CoRAG\u5728\u6df7\u5408\u6587\u6863\u8bbe\u7f6e\u4e2d\u8868\u73b0\u6700\u7a33\u5065\uff0c\u5728\u6df7\u5408\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u7edf\u8ba1\u663e\u8457\u7684\u6539\u8fdb\uff0810.29%\u7cbe\u786e\u5339\u914d vs \u6807\u51c6RAG\u76847.45%\uff09\uff0c\u4e3b\u8981\u5f97\u76ca\u4e8e\u5176\u5353\u8d8a\u7684SQL\u751f\u6210\u6027\u80fd\uff0815.32% vs 11.56%\uff09\u3002", "conclusion": "\u68c0\u7d22\u7b56\u7565\u8bbe\u8ba1\u662f\u751f\u4ea7\u7ea7\u81ea\u7136\u8bed\u8a00\u63a5\u53e3\u7684\u5173\u952e\u51b3\u5b9a\u56e0\u7d20\uff0c\u8fed\u4ee3\u67e5\u8be2\u5206\u89e3\u5728\u6587\u6863\u5f02\u6784\u6027\u4e0b\u4f18\u4e8etop-k\u68c0\u7d22\u548c\u4e8c\u5143\u76f8\u5173\u6027\u8fc7\u6ee4\u3002CoRAG\u5728\u6df7\u5408\u6587\u6863\u73af\u5883\u4e2d\u7684\u4f18\u8d8a\u6027\u80fd\u4e3a\u4f01\u4e1a\u7cfb\u7edf\u81ea\u7136\u8bed\u8a00\u63a5\u53e3\u7684\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u91cd\u8981\u6307\u5bfc\u3002"}}
{"id": "2602.08938", "categories": ["cs.MA"], "pdf": "https://arxiv.org/pdf/2602.08938", "abs": "https://arxiv.org/abs/2602.08938", "authors": ["Tuo Zhang", "Leonardo Stella"], "title": "Teaching an Old Dynamics New Tricks: Regularization-free Last-iterate Convergence in Zero-sum Games via BNN Dynamics", "comment": null, "summary": "Zero-sum games are a fundamental setting for adversarial training and decision-making in multi-agent learning (MAL). Existing methods often ensure convergence to (approximate) Nash equilibria by introducing a form of regularization. Yet, regularization requires additional hyperparameters, which must be carefully tuned--a challenging task when the payoff structure is known, and considerably harder when the structure is unknown or subject to change. Motivated by this problem, we repurpose a classical model in evolutionary game theory, i.e., the Brown-von Neumann-Nash (BNN) dynamics, by leveraging the intrinsic convergence of this dynamics in zero-sum games without regularization, and provide last-iterate convergence guarantees in noisy normal-form games (NFGs). Importantly, to make this approach more applicable, we develop a novel framework with theoretical guarantees that integrates the BNN dynamics in extensive-form games (EFGs) through counterfactual weighting. Furthermore, we implement an algorithm that instantiates our framework with neural function approximation, enabling scalable learning in both NFGs and EFGs. Empirical results show that our method quickly adapts to nonstationarities, outperforming the state-of-the-art regularization-based approach.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u6b63\u5219\u5316\u7684\u5bf9\u6297\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u8fc7\u91cd\u65b0\u5229\u7528\u8fdb\u5316\u535a\u5f08\u8bba\u4e2d\u7684BNN\u52a8\u6001\uff0c\u5728\u96f6\u548c\u6e38\u620f\u4e2d\u5b9e\u73b0\u6700\u540e\u8fed\u4ee3\u6536\u655b\uff0c\u5e76\u6269\u5c55\u5230\u6269\u5c55\u5f0f\u535a\u5f08\u4e2d\u3002", "motivation": "\u73b0\u6709\u5bf9\u6297\u8bad\u7ec3\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u6b63\u5219\u5316\u6765\u786e\u4fdd\u6536\u655b\u5230\u7eb3\u4ec0\u5747\u8861\uff0c\u4f46\u6b63\u5219\u5316\u9700\u8981\u989d\u5916\u7684\u8d85\u53c2\u6570\u8c03\u4f18\uff0c\u8fd9\u5728\u6536\u76ca\u7ed3\u6784\u672a\u77e5\u6216\u53d8\u5316\u65f6\u5c24\u5176\u56f0\u96be\u3002\u672c\u6587\u65e8\u5728\u5f00\u53d1\u65e0\u9700\u6b63\u5219\u5316\u7684\u6536\u655b\u65b9\u6cd5\u3002", "method": "\u91cd\u65b0\u5229\u7528\u8fdb\u5316\u535a\u5f08\u8bba\u4e2d\u7684Brown-von Neumann-Nash (BNN)\u52a8\u6001\uff0c\u5229\u7528\u5176\u5728\u96f6\u548c\u6e38\u620f\u4e2d\u65e0\u9700\u6b63\u5219\u5316\u7684\u5185\u5728\u6536\u655b\u7279\u6027\uff1b\u5f00\u53d1\u65b0\u6846\u67b6\u5c06BNN\u52a8\u6001\u901a\u8fc7\u53cd\u4e8b\u5b9e\u52a0\u6743\u6574\u5408\u5230\u6269\u5c55\u5f0f\u535a\u5f08\u4e2d\uff1b\u5b9e\u73b0\u57fa\u4e8e\u795e\u7ecf\u51fd\u6570\u8fd1\u4f3c\u7684\u7b97\u6cd5\uff0c\u652f\u6301\u53ef\u6269\u5c55\u5b66\u4e60\u3002", "result": "\u5728\u566a\u58f0\u6b63\u89c4\u5f0f\u535a\u5f08\u4e2d\u63d0\u4f9b\u6700\u540e\u8fed\u4ee3\u6536\u655b\u4fdd\u8bc1\uff1b\u5728\u6269\u5c55\u5f0f\u535a\u5f08\u4e2d\u63d0\u4f9b\u7406\u8bba\u4fdd\u8bc1\uff1b\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u80fd\u5feb\u901f\u9002\u5e94\u975e\u5e73\u7a33\u6027\uff0c\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u6b63\u5219\u5316\u65b9\u6cd5\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u57fa\u4e8eBNN\u52a8\u6001\u7684\u65e0\u6b63\u5219\u5316\u65b9\u6cd5\u5728\u591a\u667a\u80fd\u4f53\u5b66\u4e60\u4e2d\u5177\u6709\u4f18\u52bf\uff0c\u7279\u522b\u662f\u5728\u6536\u76ca\u7ed3\u6784\u672a\u77e5\u6216\u53d8\u5316\u7684\u73af\u5883\u4e2d\uff0c\u4e3a\u5bf9\u6297\u8bad\u7ec3\u63d0\u4f9b\u4e86\u66f4\u7b80\u5355\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.07176", "categories": ["cs.CL", "cs.AI", "cs.ET", "cs.HC"], "pdf": "https://arxiv.org/pdf/2602.07176", "abs": "https://arxiv.org/abs/2602.07176", "authors": ["Mohamed El Hajji", "Tarek Ait Baha", "Aicha Dakir", "Hammou Fadili", "Youssef Es-Saady"], "title": "Open TutorAI: An Open-source Platform for Personalized and Immersive Learning with Generative AI", "comment": "19 pages, 15 figures", "summary": "Recent advances in artificial intelligence have created new possibilities for making education more scalable, adaptive, and learner-centered. However, existing educational chatbot systems often lack contextual adaptability, real-time responsiveness, and pedagogical agility. which can limit learner engagement and diminish instructional effectiveness. Thus, there is a growing need for open, integrative platforms that combine AI and immersive technologies to support personalized, meaningful learning experiences. This paper presents Open TutorAI, an open-source educational platform based on LLMs and generative technologies that provides dynamic, personalized tutoring. The system integrates natural language processing with customizable 3D avatars to enable multimodal learner interaction. Through a structured onboarding process, it captures each learner's goals and preferences in order to configure a learner-specific AI assistant. This assistant is accessible via both text-based and avatar-driven interfaces. The platform includes tools for organizing content, providing embedded feedback, and offering dedicated interfaces for learners, educators, and parents. This work focuses on learner-facing components, delivering a tool for adaptive support that responds to individual learner profiles without requiring technical expertise. Its assistant-generation pipeline and avatar integration enhance engagement and emotional presence, creating a more humanized, immersive learning environment. Embedded learning analytics support self-regulated learning by tracking engagement patterns and generating actionable feedback. The result is Open TutorAI, which unites modular architecture, generative AI, and learner analytics within an open-source framework. It contributes to the development of next-generation intelligent tutoring systems.", "AI": {"tldr": "Open TutorAI\u662f\u4e00\u4e2a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u548c\u751f\u6210\u6280\u672f\u7684\u5f00\u6e90\u6559\u80b2\u5e73\u53f0\uff0c\u901a\u8fc7\u52a8\u6001\u4e2a\u6027\u5316\u8f85\u5bfc\u30013D\u865a\u62df\u5316\u8eab\u96c6\u6210\u548c\u5b66\u4e60\u5206\u6790\uff0c\u63d0\u4f9b\u81ea\u9002\u5e94\u3001\u6c89\u6d78\u5f0f\u7684\u5b66\u4e60\u4f53\u9a8c\u3002", "motivation": "\u73b0\u6709\u6559\u80b2\u804a\u5929\u673a\u5668\u4eba\u7cfb\u7edf\u7f3a\u4e4f\u4e0a\u4e0b\u6587\u9002\u5e94\u6027\u3001\u5b9e\u65f6\u54cd\u5e94\u6027\u548c\u6559\u5b66\u7075\u6d3b\u6027\uff0c\u9650\u5236\u4e86\u5b66\u4e60\u53c2\u4e0e\u5ea6\u548c\u6559\u5b66\u6548\u679c\u3002\u9700\u8981\u5f00\u653e\u3001\u96c6\u6210\u7684\u5e73\u53f0\u7ed3\u5408AI\u548c\u6c89\u6d78\u5f0f\u6280\u672f\u6765\u652f\u6301\u4e2a\u6027\u5316\u3001\u6709\u610f\u4e49\u7684\u5b66\u4e60\u4f53\u9a8c\u3002", "method": "\u57fa\u4e8eLLMs\u548c\u751f\u6210\u6280\u672f\u6784\u5efa\u5f00\u6e90\u6559\u80b2\u5e73\u53f0\uff0c\u96c6\u6210\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4e0e\u53ef\u5b9a\u52363D\u865a\u62df\u5316\u8eab\u5b9e\u73b0\u591a\u6a21\u6001\u4ea4\u4e92\u3002\u901a\u8fc7\u7ed3\u6784\u5316\u5165\u95e8\u6d41\u7a0b\u6355\u83b7\u5b66\u4e60\u8005\u76ee\u6807\u548c\u504f\u597d\uff0c\u914d\u7f6e\u4e2a\u6027\u5316AI\u52a9\u624b\u3002\u5e73\u53f0\u5305\u542b\u5185\u5bb9\u7ec4\u7ec7\u3001\u5d4c\u5165\u5f0f\u53cd\u9988\u5de5\u5177\uff0c\u4ee5\u53ca\u9762\u5411\u5b66\u4e60\u8005\u3001\u6559\u80b2\u8005\u548c\u5bb6\u957f\u7684\u4e13\u7528\u754c\u9762\u3002", "result": "\u5f00\u53d1\u4e86Open TutorAI\u5e73\u53f0\uff0c\u5c06\u6a21\u5757\u5316\u67b6\u6784\u3001\u751f\u6210\u5f0fAI\u548c\u5b66\u4e60\u5206\u6790\u7edf\u4e00\u5728\u5f00\u6e90\u6846\u67b6\u4e2d\u3002\u901a\u8fc7\u52a9\u624b\u751f\u6210\u7ba1\u9053\u548c\u865a\u62df\u5316\u8eab\u96c6\u6210\u589e\u5f3a\u53c2\u4e0e\u5ea6\u548c\u60c5\u611f\u5b58\u5728\u611f\uff0c\u5d4c\u5165\u5f0f\u5b66\u4e60\u5206\u6790\u652f\u6301\u81ea\u6211\u8c03\u8282\u5b66\u4e60\uff0c\u8ddf\u8e2a\u53c2\u4e0e\u6a21\u5f0f\u5e76\u751f\u6210\u53ef\u64cd\u4f5c\u7684\u53cd\u9988\u3002", "conclusion": "Open TutorAI\u4e3a\u4e0b\u4e00\u4ee3\u667a\u80fd\u8f85\u5bfc\u7cfb\u7edf\u7684\u53d1\u5c55\u505a\u51fa\u8d21\u732e\uff0c\u901a\u8fc7\u7ed3\u5408\u751f\u6210\u5f0fAI\u3001\u6c89\u6d78\u5f0f\u6280\u672f\u548c\u5b66\u4e60\u5206\u6790\uff0c\u521b\u5efa\u4e86\u66f4\u52a0\u4eba\u6027\u5316\u3001\u6c89\u6d78\u5f0f\u7684\u5b66\u4e60\u73af\u5883\uff0c\u652f\u6301\u4e2a\u6027\u5316\u81ea\u9002\u5e94\u5b66\u4e60\u3002"}}
{"id": "2602.07015", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2602.07015", "abs": "https://arxiv.org/abs/2602.07015", "authors": ["Subreena", "Mohammad Amzad Hossain", "Mirza Raquib", "Saydul Akbar Murad", "Farida Siddiqi Prity", "Muhammad Hanif", "Nick Rahimi"], "title": "Robust and Real-Time Bangladeshi Currency Recognition: A Dual-Stream MobileNet and EfficientNet Approach", "comment": null, "summary": "Accurate currency recognition is essential for assistive technologies, particularly for visually impaired individuals who rely on others to identify banknotes. This dependency puts them at risk of fraud and exploitation. To address these challenges, we first build a new Bangladeshi banknote dataset that includes both controlled and real-world scenarios, ensuring a more comprehensive and diverse representation. Next, to enhance the dataset's robustness, we incorporate four additional datasets, including public benchmarks, to cover various complexities and improve the model's generalization. To overcome the limitations of current recognition models, we propose a novel hybrid CNN architecture that combines MobileNetV3-Large and EfficientNetB0 for efficient feature extraction. This is followed by an effective multilayer perceptron (MLP) classifier to improve performance while keeping computational costs low, making the system suitable for resource-constrained devices. The experimental results show that the proposed model achieves 97.95% accuracy on controlled datasets, 92.84% on complex backgrounds, and 94.98% accuracy when combining all datasets. The model's performance is thoroughly evaluated using five-fold cross-validation and seven metrics: accuracy, precision, recall, F1-score, Cohen's Kappa, MCC, and AUC. Additionally, explainable AI methods like LIME and SHAP are incorporated to enhance transparency and interpretability.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7528\u4e8e\u5b5f\u52a0\u62c9\u56fd\u7eb8\u5e01\u8bc6\u522b\u7684\u6df7\u5408CNN\u67b6\u6784\uff0c\u7ed3\u5408MobileNetV3-Large\u548cEfficientNetB0\u8fdb\u884c\u7279\u5f81\u63d0\u53d6\uff0c\u914d\u5408MLP\u5206\u7c7b\u5668\uff0c\u5728\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u8bc6\u522b\u3002", "motivation": "\u51c6\u786e\u7684\u8d27\u5e01\u8bc6\u522b\u5bf9\u89c6\u969c\u4eba\u58eb\u81f3\u5173\u91cd\u8981\uff0c\u4ed6\u4eec\u4f9d\u8d56\u4ed6\u4eba\u8bc6\u522b\u7eb8\u5e01\uff0c\u8fd9\u4f7f\u4ed6\u4eec\u9762\u4e34\u6b3a\u8bc8\u548c\u5265\u524a\u7684\u98ce\u9669\u3002\u73b0\u6709\u8bc6\u522b\u6a21\u578b\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u8981\u66f4\u9c81\u68d2\u548c\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "1) \u6784\u5efa\u65b0\u7684\u5b5f\u52a0\u62c9\u56fd\u7eb8\u5e01\u6570\u636e\u96c6\uff0c\u5305\u542b\u53d7\u63a7\u548c\u771f\u5b9e\u573a\u666f\uff1b2) \u6574\u5408\u56db\u4e2a\u989d\u5916\u6570\u636e\u96c6\u589e\u5f3a\u9c81\u68d2\u6027\uff1b3) \u63d0\u51fa\u6df7\u5408CNN\u67b6\u6784\uff0c\u7ed3\u5408MobileNetV3-Large\u548cEfficientNetB0\u8fdb\u884c\u7279\u5f81\u63d0\u53d6\uff1b4) \u4f7f\u7528\u591a\u5c42\u611f\u77e5\u673a(MLP)\u5206\u7c7b\u5668\u63d0\u9ad8\u6027\u80fd\uff1b5) \u91c7\u7528\u4e94\u6298\u4ea4\u53c9\u9a8c\u8bc1\u548c\u4e03\u79cd\u8bc4\u4f30\u6307\u6807\uff1b6) \u96c6\u6210LIME\u548cSHAP\u7b49\u53ef\u89e3\u91caAI\u65b9\u6cd5\u3002", "result": "\u6a21\u578b\u5728\u53d7\u63a7\u6570\u636e\u96c6\u4e0a\u8fbe\u523097.95%\u51c6\u786e\u7387\uff0c\u590d\u6742\u80cc\u666f\u4e0a\u8fbe\u523092.84%\u51c6\u786e\u7387\uff0c\u6240\u6709\u6570\u636e\u96c6\u7ec4\u5408\u4e0a\u8fbe\u523094.98%\u51c6\u786e\u7387\u3002\u901a\u8fc7\u4e03\u79cd\u8bc4\u4f30\u6307\u6807\u548c\u53ef\u89e3\u91ca\u6027\u5206\u6790\u9a8c\u8bc1\u4e86\u6a21\u578b\u6027\u80fd\u3002", "conclusion": "\u63d0\u51fa\u7684\u6df7\u5408CNN\u67b6\u6784\u5728\u5b5f\u52a0\u62c9\u56fd\u7eb8\u5e01\u8bc6\u522b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u65e2\u4fdd\u6301\u4e86\u9ad8\u7cbe\u5ea6\u53c8\u9002\u5408\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\uff0c\u540c\u65f6\u901a\u8fc7\u53ef\u89e3\u91caAI\u65b9\u6cd5\u589e\u5f3a\u4e86\u900f\u660e\u5ea6\uff0c\u4e3a\u89c6\u969c\u4eba\u58eb\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u8f85\u52a9\u6280\u672f\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.07187", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07187", "abs": "https://arxiv.org/abs/2602.07187", "authors": ["Hanyu Wang", "Yuanpu Cao", "Lu Lin", "Jinghui Chen"], "title": "PreFlect: From Retrospective to Prospective Reflection in Large Language Model Agents", "comment": null, "summary": "Advanced large language model agents typically adopt self-reflection for improving performance, where agents iteratively analyze past actions to correct errors. However, existing reflective approaches are inherently retrospective: agents act, observe failure, and only then attempt to recover. In this work, we introduce PreFlect, a prospective reflection mechanism that shifts the paradigm from post hoc correction to pre-execution foresight by criticizing and refining agent plans before execution. To support grounded prospective reflection, we distill planning errors from historical agent trajectories, capturing recurring success and failure patterns observed across past executions. Furthermore, we complement prospective reflection with a dynamic re-planning mechanism that provides execution-time plan update in case the original plan encounters unexpected deviation. Evaluations on different benchmarks demonstrate that PreFlect significantly improves overall agent utility on complex real-world tasks, outperforming strong reflection-based baselines and several more complex agent architectures. Code will be updated at https://github.com/wwwhy725/PreFlect.", "AI": {"tldr": "PreFlect\u63d0\u51fa\u524d\u77bb\u6027\u53cd\u601d\u673a\u5236\uff0c\u5728\u8ba1\u5212\u6267\u884c\u524d\u8fdb\u884c\u6279\u8bc4\u548c\u4f18\u5316\uff0c\u76f8\u6bd4\u4f20\u7edf\u540e\u9a8c\u6027\u53cd\u601d\u663e\u8457\u63d0\u5347\u667a\u80fd\u4f53\u6027\u80fd", "motivation": "\u73b0\u6709\u5927\u578b\u8bed\u8a00\u6a21\u578b\u667a\u80fd\u4f53\u7684\u53cd\u601d\u673a\u5236\u672c\u8d28\u4e0a\u662f\u56de\u987e\u6027\u7684\uff1a\u667a\u80fd\u4f53\u5148\u884c\u52a8\uff0c\u89c2\u5bdf\u5230\u5931\u8d25\u540e\u624d\u5c1d\u8bd5\u6062\u590d\u3002\u8fd9\u79cd\u540e\u9a8c\u6027\u65b9\u6cd5\u5b58\u5728\u6548\u7387\u95ee\u9898\uff0c\u9700\u8981\u5728\u6267\u884c\u524d\u5c31\u8fdb\u884c\u524d\u77bb\u6027\u89c4\u5212\u4f18\u5316\u3002", "method": "1. \u524d\u77bb\u6027\u53cd\u601d\u673a\u5236\uff1a\u5728\u6267\u884c\u524d\u5bf9\u667a\u80fd\u4f53\u8ba1\u5212\u8fdb\u884c\u6279\u8bc4\u548c\u4f18\u5316\uff1b2. \u4ece\u5386\u53f2\u8f68\u8ff9\u4e2d\u63d0\u70bc\u89c4\u5212\u9519\u8bef\u6a21\u5f0f\uff1b3. \u52a8\u6001\u91cd\u89c4\u5212\u673a\u5236\uff1a\u5728\u6267\u884c\u8fc7\u7a0b\u4e2d\u9047\u5230\u610f\u5916\u504f\u5dee\u65f6\u5b9e\u65f6\u66f4\u65b0\u8ba1\u5212", "result": "\u5728\u4e0d\u540c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cPreFlect\u663e\u8457\u63d0\u5347\u4e86\u590d\u6742\u73b0\u5b9e\u4efb\u52a1\u4e2d\u7684\u667a\u80fd\u4f53\u6548\u7528\uff0c\u4f18\u4e8e\u57fa\u4e8e\u53cd\u601d\u7684\u57fa\u7ebf\u65b9\u6cd5\u548c\u591a\u4e2a\u66f4\u590d\u6742\u7684\u667a\u80fd\u4f53\u67b6\u6784", "conclusion": "\u524d\u77bb\u6027\u53cd\u601d\u673a\u5236\u6bd4\u4f20\u7edf\u540e\u9a8c\u6027\u53cd\u601d\u66f4\u6709\u6548\uff0c\u901a\u8fc7\u6267\u884c\u524d\u4f18\u5316\u8ba1\u5212\u548c\u6267\u884c\u4e2d\u52a8\u6001\u8c03\u6574\uff0c\u663e\u8457\u63d0\u5347\u667a\u80fd\u4f53\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u6027\u80fd"}}
{"id": "2602.07147", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.07147", "abs": "https://arxiv.org/abs/2602.07147", "authors": ["Marco De Luca", "Michele Perlotto", "Anna Rita Fasolino", "Porfirio Tramontana"], "title": "Architectural Anti-Patterns in Student-Developed Microservice Architectures: An Exploratory Study", "comment": null, "summary": "Teaching microservice architectures is challenging due to distributed complexity and the gap between academia and industry. Understanding the quality issues students introduce in MSAs is essential to improve education. This study analyzes student-developed microservices using an established anti-pattern taxonomy and derives lessons learned with actionable teaching recommendations. We conducted a longitudinal, project-based course (2023-2025) involving 216 Master's students (67 teams) who designed and deployed a realistic, containerized MSA for a gamified testing platform. The final systems revealed 23 out of 58 known MSA anti-patterns, spanning five categories. Security issues were most frequent, highlighting weaknesses in authentication, authorization, and data protection. Team Organization and Service Interaction problems followed, reflecting limited DevOps experience and difficulties in inter-service coordination. Fewer issues appeared in Intra-service Design and Inter-service Decomposition, suggesting students generally defined service boundaries well. Overall, students prioritized feature delivery over robustness and operational discipline. To address this, we recommend enforcing minimal standards (API contracts, gateways), providing labs on resilient communication, integrating security-by-design practices, and offering CI-CD templates. The paper contributes a realistic, full-scale educational experience and a replicable model for teaching industry-aligned microservice architecture.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5206\u6790\u4e86216\u540d\u7855\u58eb\u751f\u5728\u5fae\u670d\u52a1\u67b6\u6784\u8bfe\u7a0b\u4e2d\u5f00\u53d1\u7684\u7cfb\u7edf\uff0c\u53d1\u73b0\u5b66\u751f\u5f15\u5165\u4e8623\u79cd\u5df2\u77e5\u53cd\u6a21\u5f0f\uff0c\u5176\u4e2d\u5b89\u5168\u95ee\u9898\u6700\u4e3a\u7a81\u51fa\uff0c\u53cd\u6620\u4e86\u5b66\u751f\u66f4\u5173\u6ce8\u529f\u80fd\u5b9e\u73b0\u800c\u975e\u7cfb\u7edf\u5065\u58ee\u6027\u3002", "motivation": "\u7531\u4e8e\u5206\u5e03\u5f0f\u7cfb\u7edf\u7684\u590d\u6742\u6027\u548c\u5b66\u672f\u754c\u4e0e\u5de5\u4e1a\u754c\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u6559\u6388\u5fae\u670d\u52a1\u67b6\u6784\u5177\u6709\u6311\u6218\u6027\u3002\u4e86\u89e3\u5b66\u751f\u5728\u5fae\u670d\u52a1\u67b6\u6784\u4e2d\u5f15\u5165\u7684\u8d28\u91cf\u95ee\u9898\u5bf9\u4e8e\u6539\u8fdb\u6559\u80b2\u81f3\u5173\u91cd\u8981\u3002", "method": "\u7814\u7a76\u91c7\u7528\u7eb5\u5411\u3001\u57fa\u4e8e\u9879\u76ee\u7684\u8bfe\u7a0b\u8bbe\u8ba1\uff082023-2025\u5e74\uff09\uff0c\u6d89\u53ca216\u540d\u7855\u58eb\u751f\uff0867\u4e2a\u56e2\u961f\uff09\uff0c\u4ed6\u4eec\u4e3a\u6e38\u620f\u5316\u6d4b\u8bd5\u5e73\u53f0\u8bbe\u8ba1\u5e76\u90e8\u7f72\u4e86\u73b0\u5b9e\u7684\u5bb9\u5668\u5316\u5fae\u670d\u52a1\u67b6\u6784\u3002\u4f7f\u7528\u5df2\u5efa\u7acb\u7684\u53cd\u6a21\u5f0f\u5206\u7c7b\u6cd5\u5206\u6790\u5b66\u751f\u5f00\u53d1\u7684\u7cfb\u7edf\u3002", "result": "\u6700\u7ec8\u7cfb\u7edf\u63ed\u793a\u4e8658\u79cd\u5df2\u77e5\u5fae\u670d\u52a1\u53cd\u6a21\u5f0f\u4e2d\u768423\u79cd\uff0c\u6db5\u76d6\u4e94\u4e2a\u7c7b\u522b\uff1a\u5b89\u5168\u95ee\u9898\u6700\u4e3a\u9891\u7e41\uff08\u8ba4\u8bc1\u3001\u6388\u6743\u3001\u6570\u636e\u4fdd\u62a4\uff09\uff1b\u56e2\u961f\u7ec4\u7ec7\u548c\u670d\u52a1\u4ea4\u4e92\u95ee\u9898\u6b21\u4e4b\uff1b\u670d\u52a1\u5185\u8bbe\u8ba1\u548c\u8de8\u670d\u52a1\u5206\u89e3\u95ee\u9898\u8f83\u5c11\u3002\u5b66\u751f\u666e\u904d\u4f18\u5148\u8003\u8651\u529f\u80fd\u4ea4\u4ed8\u800c\u975e\u5065\u58ee\u6027\u548c\u8fd0\u7ef4\u7eaa\u5f8b\u3002", "conclusion": "\u4e3a\u6539\u8fdb\u5fae\u670d\u52a1\u67b6\u6784\u6559\u80b2\uff0c\u5efa\u8bae\uff1a\u5f3a\u5236\u6267\u884c\u6700\u4f4e\u6807\u51c6\uff08API\u5951\u7ea6\u3001\u7f51\u5173\uff09\u3001\u63d0\u4f9b\u5f39\u6027\u901a\u4fe1\u5b9e\u9a8c\u3001\u96c6\u6210\u5b89\u5168\u8bbe\u8ba1\u5b9e\u8df5\u3001\u63d0\u4f9bCI/CD\u6a21\u677f\u3002\u8be5\u7814\u7a76\u8d21\u732e\u4e86\u73b0\u5b9e\u3001\u5168\u9762\u7684\u6559\u80b2\u7ecf\u9a8c\u548c\u53ef\u590d\u5236\u7684\u884c\u4e1a\u5bf9\u9f50\u5fae\u670d\u52a1\u67b6\u6784\u6559\u5b66\u6a21\u578b\u3002"}}
{"id": "2602.08965", "categories": ["cs.MA", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.08965", "abs": "https://arxiv.org/abs/2602.08965", "authors": ["John Gardiner", "Orlando Romero", "Brendan Tivnan", "Nicol\u00f2 Dal Fabbro", "George J. Pappas"], "title": "Learning to Coordinate via Quantum Entanglement in Multi-Agent Reinforcement Learning", "comment": null, "summary": "The inability to communicate poses a major challenge to coordination in multi-agent reinforcement learning (MARL). Prior work has explored correlating local policies via shared randomness, sometimes in the form of a correlation device, as a mechanism to assist in decentralized decision-making. In contrast, this work introduces the first framework for training MARL agents to exploit shared quantum entanglement as a coordination resource, which permits a larger class of communication-free correlated policies than shared randomness alone. This is motivated by well-known results in quantum physics which posit that, for certain single-round cooperative games with no communication, shared quantum entanglement enables strategies that outperform those that only use shared randomness. In such cases, we say that there is quantum advantage. Our framework is based on a novel differentiable policy parameterization that enables optimization over quantum measurements, together with a novel policy architecture that decomposes joint policies into a quantum coordinator and decentralized local actors. To illustrate the effectiveness of our proposed method, we first show that we can learn, purely from experience, strategies that attain quantum advantage in single-round games that are treated as black box oracles. We then demonstrate how our machinery can learn policies with quantum advantage in an illustrative multi-agent sequential decision-making problem formulated as a decentralized partially observable Markov decision process (Dec-POMDP).", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u9996\u4e2a\u5229\u7528\u5171\u4eab\u91cf\u5b50\u7ea0\u7f20\u4f5c\u4e3a\u534f\u8c03\u8d44\u6e90\u7684MARL\u6846\u67b6\uff0c\u76f8\u6bd4\u4f20\u7edf\u5171\u4eab\u968f\u673a\u6027\uff0c\u91cf\u5b50\u7ea0\u7f20\u5141\u8bb8\u66f4\u4e30\u5bcc\u7684\u65e0\u901a\u4fe1\u5173\u8054\u7b56\u7565\uff0c\u5e76\u5728\u67d0\u4e9b\u5408\u4f5c\u6e38\u620f\u4e2d\u5c55\u73b0\u51fa\u91cf\u5b50\u4f18\u52bf\u3002", "motivation": "\u5728\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u4e2d\uff0c\u65e0\u6cd5\u901a\u4fe1\u662f\u534f\u8c03\u7684\u4e3b\u8981\u6311\u6218\u3002\u4f20\u7edf\u65b9\u6cd5\u4f7f\u7528\u5171\u4eab\u968f\u673a\u6027\uff08\u5982\u76f8\u5173\u8bbe\u5907\uff09\u6765\u5173\u8054\u5c40\u90e8\u7b56\u7565\uff0c\u4f46\u91cf\u5b50\u7269\u7406\u7814\u7a76\u8868\u660e\uff0c\u5728\u67d0\u4e9b\u65e0\u901a\u4fe1\u7684\u5355\u8f6e\u5408\u4f5c\u6e38\u620f\u4e2d\uff0c\u5171\u4eab\u91cf\u5b50\u7ea0\u7f20\u80fd\u5b9e\u73b0\u6bd4\u5171\u4eab\u968f\u673a\u6027\u66f4\u4f18\u7684\u7b56\u7565\uff0c\u5373\u5b58\u5728\u91cf\u5b50\u4f18\u52bf\u3002\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u80fd\u5229\u7528\u91cf\u5b50\u7ea0\u7f20\u7684MARL\u6846\u67b6\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u53ef\u5fae\u5206\u7b56\u7565\u53c2\u6570\u5316\u7684\u6846\u67b6\uff1a1\uff09\u5f15\u5165\u65b0\u9896\u7684\u53ef\u5fae\u5206\u7b56\u7565\u53c2\u6570\u5316\uff0c\u5141\u8bb8\u4f18\u5316\u91cf\u5b50\u6d4b\u91cf\uff1b2\uff09\u8bbe\u8ba1\u65b0\u578b\u7b56\u7565\u67b6\u6784\uff0c\u5c06\u8054\u5408\u7b56\u7565\u5206\u89e3\u4e3a\u91cf\u5b50\u534f\u8c03\u5668\u548c\u5206\u6563\u7684\u5c40\u90e8\u6267\u884c\u5668\uff1b3\uff09\u5c06\u6846\u67b6\u5e94\u7528\u4e8e\u5355\u8f6e\u6e38\u620f\uff08\u4f5c\u4e3a\u9ed1\u76d2oracle\uff09\u548c\u5e8f\u5217\u51b3\u7b56\u95ee\u9898\uff08Dec-POMDP\uff09\u3002", "result": "1\uff09\u5728\u5355\u8f6e\u6e38\u620f\u4e2d\uff0c\u80fd\u591f\u7eaf\u7cb9\u4ece\u7ecf\u9a8c\u4e2d\u5b66\u4e60\u5230\u5b9e\u73b0\u91cf\u5b50\u4f18\u52bf\u7684\u7b56\u7565\uff1b2\uff09\u5728\u4f5c\u4e3aDec-POMDP\u5efa\u6a21\u7684\u591a\u667a\u80fd\u4f53\u5e8f\u5217\u51b3\u7b56\u95ee\u9898\u4e2d\uff0c\u5c55\u793a\u4e86\u5b66\u4e60\u5177\u6709\u91cf\u5b50\u4f18\u52bf\u7b56\u7565\u7684\u80fd\u529b\u3002", "conclusion": "\u8be5\u7814\u7a76\u9996\u6b21\u5c06\u91cf\u5b50\u7ea0\u7f20\u4f5c\u4e3a\u534f\u8c03\u8d44\u6e90\u5f15\u5165MARL\u6846\u67b6\uff0c\u8bc1\u660e\u4e86\u91cf\u5b50\u4f18\u52bf\u5728\u5f3a\u5316\u5b66\u4e60\u73af\u5883\u4e2d\u7684\u53ef\u5b9e\u73b0\u6027\uff0c\u4e3a\u591a\u667a\u80fd\u4f53\u534f\u8c03\u5f00\u8f9f\u4e86\u65b0\u7684\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2602.07181", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.07181", "abs": "https://arxiv.org/abs/2602.07181", "authors": ["Tianyu Zhao", "Siqi Li", "Yasser Shoukry", "Salma Elmalaki"], "title": "Can LLMs Discern the Traits Influencing Your Preferences? Evaluating Personality-Driven Preference Alignment in LLMs", "comment": null, "summary": "User preferences are increasingly used to personalize Large Language Model (LLM) responses, yet how to reliably leverage preference signals for answer generation remains under-explored. In practice, preferences can be noisy, incomplete, or even misleading, which can degrade answer quality when applied naively. Motivated by the observation that stable personality traits shape everyday preferences, we study personality as a principled ''latent'' signal behind preference statements. Through extensive experiments, we find that conditioning on personality-aligned preferences substantially improves personalized question answering: selecting preferences consistent with a user's inferred personality increases answer-choice accuracy from 29.25% to 76%, compared to using randomly selected preferences. Based on these findings, we introduce PACIFIC (Preference Alignment Choices Inference for Five-factor Identity Characterization), a personality-labeled preference dataset containing 1200 preference statements spanning diverse domains (e.g., travel, movies, education), annotated with Big-Five (OCEAN) trait directions. Finally, we propose a framework that enables an LLM model to automatically retrieve personality-aligned preferences and incorporate them during answer generation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faPACIFIC\u6846\u67b6\uff0c\u901a\u8fc7\u4eba\u683c\u7279\u8d28\u4f5c\u4e3a\u6f5c\u5728\u4fe1\u53f7\u6765\u7b5b\u9009\u7528\u6237\u504f\u597d\uff0c\u663e\u8457\u63d0\u5347\u4e2a\u6027\u5316\u95ee\u7b54\u51c6\u786e\u6027\uff08\u4ece29.25%\u63d0\u5347\u81f376%\uff09\uff0c\u5e76\u521b\u5efa\u4e86\u5305\u542b1200\u6761\u504f\u597d\u9648\u8ff0\u7684\u4eba\u683c\u6807\u6ce8\u6570\u636e\u96c6\u3002", "motivation": "\u5f53\u524d\u4e2a\u6027\u5316LLM\u54cd\u5e94\u4e3b\u8981\u4f9d\u8d56\u7528\u6237\u504f\u597d\uff0c\u4f46\u504f\u597d\u4fe1\u53f7\u53ef\u80fd\u5b58\u5728\u566a\u58f0\u3001\u4e0d\u5b8c\u6574\u751a\u81f3\u8bef\u5bfc\u6027\uff0c\u76f4\u63a5\u5e94\u7528\u4f1a\u964d\u4f4e\u56de\u7b54\u8d28\u91cf\u3002\u7814\u7a76\u53d1\u73b0\u7a33\u5b9a\u7684\u4eba\u683c\u7279\u8d28\u662f\u504f\u597d\u80cc\u540e\u7684\u6f5c\u5728\u4fe1\u53f7\uff0c\u56e0\u6b64\u63a2\u7d22\u5982\u4f55\u5229\u7528\u4eba\u683c\u7279\u8d28\u6765\u53ef\u9760\u5730\u5229\u7528\u504f\u597d\u4fe1\u53f7\u3002", "method": "1\uff09\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4eba\u683c\u5bf9\u9f50\u504f\u597d\u5bf9\u4e2a\u6027\u5316\u95ee\u7b54\u7684\u6539\u8fdb\u6548\u679c\uff1b2\uff09\u521b\u5efaPACIFIC\u6570\u636e\u96c6\uff0c\u5305\u542b1200\u6761\u8de8\u9886\u57df\u504f\u597d\u9648\u8ff0\uff0c\u6807\u6ce8\u4e86Big-Five\uff08OCEAN\uff09\u4eba\u683c\u7279\u8d28\u65b9\u5411\uff1b3\uff09\u63d0\u51fa\u6846\u67b6\u4f7fLLM\u80fd\u81ea\u52a8\u68c0\u7d22\u4eba\u683c\u5bf9\u9f50\u504f\u597d\u5e76\u5728\u7b54\u6848\u751f\u6210\u4e2d\u6574\u5408\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u9009\u62e9\u4e0e\u7528\u6237\u63a8\u65ad\u4eba\u683c\u4e00\u81f4\u7684\u504f\u597d\u80fd\u663e\u8457\u63d0\u5347\u7b54\u6848\u9009\u62e9\u51c6\u786e\u6027\uff1a\u4ece\u4f7f\u7528\u968f\u673a\u9009\u62e9\u504f\u597d\u768429.25%\u63d0\u5347\u81f376%\u3002\u521b\u5efa\u4e86\u4eba\u683c\u6807\u6ce8\u7684\u504f\u597d\u6570\u636e\u96c6\uff0c\u5e76\u63d0\u51fa\u4e86\u6709\u6548\u7684\u504f\u597d\u6574\u5408\u6846\u67b6\u3002", "conclusion": "\u4eba\u683c\u7279\u8d28\u53ef\u4f5c\u4e3a\u53ef\u9760\u7b5b\u9009\u7528\u6237\u504f\u597d\u7684\u6f5c\u5728\u4fe1\u53f7\uff0c\u4eba\u683c\u5bf9\u9f50\u504f\u597d\u80fd\u663e\u8457\u6539\u5584\u4e2a\u6027\u5316\u95ee\u7b54\u8d28\u91cf\u3002PACIFIC\u6570\u636e\u96c6\u548c\u63d0\u51fa\u7684\u6846\u67b6\u4e3a\u57fa\u4e8e\u504f\u597d\u7684\u4e2a\u6027\u5316LLM\u54cd\u5e94\u63d0\u4f9b\u4e86\u66f4\u7a33\u5065\u7684\u65b9\u6cd5\u3002"}}
{"id": "2602.07016", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.07016", "abs": "https://arxiv.org/abs/2602.07016", "authors": ["Mohsen Mostafa"], "title": "Gaussian-Constrained LeJEPA Representations for Unsupervised Scene Discovery and Pose Consistency", "comment": "10 pages, 3 figures, https://www.kaggle.com/code/babydriver1233/optimized-pipeline-for-the-image-matching-challeng, https://www.kaggle.com/code/babydriver1233/integrating-lejepa-for-enhanced-image-matching", "summary": "Unsupervised 3D scene reconstruction from unstructured image collections remains a fundamental challenge in computer vision, particularly when images originate from multiple unrelated scenes and contain significant visual ambiguity. The Image Matching Challenge 2025 (IMC2025) highlights these difficulties by requiring both scene discovery and camera pose estimation under real-world conditions, including outliers and mixed content. This paper investigates the application of Gaussian-constrained representations inspired by LeJEPA (Joint Embedding Predictive Architecture) to address these challenges. We present three progressively refined pipelines, culminating in a LeJEPA-inspired approach that enforces isotropic Gaussian constraints on learned image embeddings. Rather than introducing new theoretical guarantees, our work empirically evaluates how these constraints influence clustering consistency and pose estimation robustness in practice. Experimental results on IMC2025 demonstrate that Gaussian-constrained embeddings can improve scene separation and pose plausibility compared to heuristic-driven baselines, particularly in visually ambiguous settings. These findings suggest that theoretically motivated representation constraints offer a promising direction for bridging self-supervised learning principles and practical structure-from-motion pipelines.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5728IMC2025\u6311\u6218\u4e2d\uff0c\u4f7f\u7528\u53d7LeJEPA\u542f\u53d1\u7684\u5404\u5411\u540c\u6027\u9ad8\u65af\u7ea6\u675f\u8868\u793a\u6765\u6539\u8fdb\u65e0\u76d1\u77633D\u573a\u666f\u91cd\u5efa\uff0c\u7279\u522b\u662f\u5728\u591a\u573a\u666f\u3001\u89c6\u89c9\u6a21\u7cca\u6761\u4ef6\u4e0b\u7684\u573a\u666f\u53d1\u73b0\u548c\u76f8\u673a\u59ff\u6001\u4f30\u8ba1\u3002", "motivation": "\u4ece\u975e\u7ed3\u6784\u5316\u56fe\u50cf\u96c6\u5408\u4e2d\u8fdb\u884c\u65e0\u76d1\u77633D\u573a\u666f\u91cd\u5efa\u9762\u4e34\u91cd\u5927\u6311\u6218\uff0c\u7279\u522b\u662f\u5f53\u56fe\u50cf\u6765\u81ea\u591a\u4e2a\u4e0d\u76f8\u5173\u573a\u666f\u4e14\u5b58\u5728\u663e\u8457\u89c6\u89c9\u6a21\u7cca\u65f6\u3002IMC2025\u6311\u6218\u8981\u6c42\u5728\u5b9e\u9645\u6761\u4ef6\u4e0b\uff08\u5305\u542b\u5f02\u5e38\u503c\u548c\u6df7\u5408\u5185\u5bb9\uff09\u540c\u65f6\u8fdb\u884c\u573a\u666f\u53d1\u73b0\u548c\u76f8\u673a\u59ff\u6001\u4f30\u8ba1\uff0c\u8fd9\u51f8\u663e\u4e86\u8fd9\u4e9b\u56f0\u96be\u3002", "method": "\u63d0\u51fa\u4e86\u4e09\u79cd\u9010\u6b65\u6539\u8fdb\u7684\u7ba1\u9053\uff0c\u6700\u7ec8\u91c7\u7528\u53d7LeJEPA\u542f\u53d1\u7684\u5404\u5411\u540c\u6027\u9ad8\u65af\u7ea6\u675f\u65b9\u6cd5\uff0c\u5bf9\u5b66\u4e60\u5230\u7684\u56fe\u50cf\u5d4c\u5165\u65bd\u52a0\u9ad8\u65af\u7ea6\u675f\u3002\u8be5\u65b9\u6cd5\u4e0d\u662f\u5f15\u5165\u65b0\u7684\u7406\u8bba\u4fdd\u8bc1\uff0c\u800c\u662f\u7ecf\u9a8c\u6027\u5730\u8bc4\u4f30\u8fd9\u4e9b\u7ea6\u675f\u5982\u4f55\u5f71\u54cd\u5b9e\u9645\u4e2d\u7684\u805a\u7c7b\u4e00\u81f4\u6027\u548c\u59ff\u6001\u4f30\u8ba1\u9c81\u68d2\u6027\u3002", "result": "\u5728IMC2025\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u542f\u53d1\u5f0f\u57fa\u7ebf\u65b9\u6cd5\u76f8\u6bd4\uff0c\u9ad8\u65af\u7ea6\u675f\u5d4c\u5165\u53ef\u4ee5\u6539\u5584\u573a\u666f\u5206\u79bb\u548c\u59ff\u6001\u5408\u7406\u6027\uff0c\u7279\u522b\u662f\u5728\u89c6\u89c9\u6a21\u7cca\u8bbe\u7f6e\u4e2d\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u8868\u660e\uff0c\u7406\u8bba\u9a71\u52a8\u7684\u8868\u793a\u7ea6\u675f\u4e3a\u6865\u63a5\u81ea\u76d1\u7763\u5b66\u4e60\u539f\u7406\u548c\u5b9e\u9645\u8fd0\u52a8\u7ed3\u6784\u7ba1\u9053\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u524d\u666f\u7684\u65b9\u5411\u3002"}}
{"id": "2602.07238", "categories": ["cs.AI", "cs.LG", "econ.GN"], "pdf": "https://arxiv.org/pdf/2602.07238", "abs": "https://arxiv.org/abs/2602.07238", "authors": ["Matthias Mertens", "Natalia Fischl-Lanzoni", "Neil Thompson"], "title": "Is there \"Secret Sauce'' in Large Language Model Development?", "comment": null, "summary": "Do leading LLM developers possess a proprietary ``secret sauce'', or is LLM performance driven by scaling up compute? Using training and benchmark data for 809 models released between 2022 and 2025, we estimate scaling-law regressions with release-date and developer fixed effects. We find clear evidence of developer-specific efficiency advantages, but their importance depends on where models lie in the performance distribution. At the frontier, 80-90% of performance differences are explained by higher training compute, implying that scale--not proprietary technology--drives frontier advances. Away from the frontier, however, proprietary techniques and shared algorithmic progress substantially reduce the compute required to reach fixed capability thresholds. Some companies can systematically produce smaller models more efficiently. Strikingly, we also find substantial variation of model efficiency within companies; a firm can train two models with more than 40x compute efficiency difference. We also discuss the implications for AI leadership and capability diffusion.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u5728AI\u6a21\u578b\u6027\u80fd\u524d\u6cbf\uff0c80-90%\u7684\u6027\u80fd\u5dee\u5f02\u7531\u8bad\u7ec3\u8ba1\u7b97\u91cf\u89e3\u91ca\uff0c\u800c\u975e\u4e13\u6709\u6280\u672f\uff1b\u4f46\u5728\u975e\u524d\u6cbf\u9886\u57df\uff0c\u4e13\u6709\u6280\u672f\u548c\u5171\u4eab\u7b97\u6cd5\u8fdb\u6b65\u663e\u8457\u964d\u4f4e\u8fbe\u5230\u7279\u5b9a\u80fd\u529b\u6240\u9700\u7684\u8ba1\u7b97\u91cf\u3002", "motivation": "\u63a2\u7a76\u9886\u5148LLM\u5f00\u53d1\u8005\u662f\u5426\u62e5\u6709\u4e13\u6709\"\u79d8\u65b9\"\uff0c\u8fd8\u662fLLM\u6027\u80fd\u4e3b\u8981\u7531\u8ba1\u7b97\u89c4\u6a21\u9a71\u52a8\uff0c\u4ee5\u7406\u89e3AI\u9886\u5bfc\u529b\u548c\u80fd\u529b\u6269\u6563\u7684\u673a\u5236\u3002", "method": "\u4f7f\u75282022-2025\u5e74\u95f4\u53d1\u5e03\u7684809\u4e2a\u6a21\u578b\u7684\u8bad\u7ec3\u548c\u57fa\u51c6\u6d4b\u8bd5\u6570\u636e\uff0c\u901a\u8fc7\u5305\u542b\u53d1\u5e03\u65e5\u671f\u548c\u5f00\u53d1\u8005\u56fa\u5b9a\u6548\u5e94\u7684\u7f29\u653e\u5b9a\u5f8b\u56de\u5f52\u8fdb\u884c\u5206\u6790\u3002", "result": "1) \u5b58\u5728\u5f00\u53d1\u8005\u7279\u5b9a\u7684\u6548\u7387\u4f18\u52bf\uff0c\u4f46\u5176\u91cd\u8981\u6027\u53d6\u51b3\u4e8e\u6a21\u578b\u5728\u6027\u80fd\u5206\u5e03\u4e2d\u7684\u4f4d\u7f6e\uff1b2) \u5728\u6027\u80fd\u524d\u6cbf\uff0c80-90%\u7684\u6027\u80fd\u5dee\u5f02\u7531\u66f4\u9ad8\u8bad\u7ec3\u8ba1\u7b97\u91cf\u89e3\u91ca\uff1b3) \u5728\u975e\u524d\u6cbf\u9886\u57df\uff0c\u4e13\u6709\u6280\u672f\u548c\u5171\u4eab\u7b97\u6cd5\u8fdb\u6b65\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u9700\u6c42\uff1b4) \u516c\u53f8\u5185\u90e8\u6a21\u578b\u6548\u7387\u5b58\u5728\u5de8\u5927\u5dee\u5f02\uff08\u53ef\u8fbe40\u500d\u4ee5\u4e0a\uff09\u3002", "conclusion": "\u524d\u6cbfAI\u8fdb\u6b65\u4e3b\u8981\u7531\u8ba1\u7b97\u89c4\u6a21\u9a71\u52a8\u800c\u975e\u4e13\u6709\u6280\u672f\uff0c\u4f46\u4e13\u6709\u6280\u672f\u5728\u975e\u524d\u6cbf\u9886\u57df\u4ecd\u80fd\u663e\u8457\u63d0\u9ad8\u6548\u7387\uff1b\u516c\u53f8\u5185\u90e8\u6548\u7387\u5dee\u5f02\u5de8\u5927\uff0c\u8fd9\u5bf9AI\u9886\u5bfc\u529b\u548c\u80fd\u529b\u6269\u6563\u5177\u6709\u91cd\u8981\u542f\u793a\u3002"}}
{"id": "2602.07190", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07190", "abs": "https://arxiv.org/abs/2602.07190", "authors": ["Anagha Kulkarni", "Parin Rajesh Jhaveri", "Prasha Shrestha", "Yu Tong Han", "Reza Amini", "Behrouz Madahian"], "title": "Long-Context Long-Form Question Answering for Legal Domain", "comment": "EACL 2026", "summary": "Legal documents have complex document layouts involving multiple nested sections, lengthy footnotes and further use specialized linguistic devices like intricate syntax and domain-specific vocabulary to ensure precision and authority. These inherent characteristics of legal documents make question answering challenging, and particularly so when the answer to the question spans several pages (i.e. requires long-context) and is required to be comprehensive (i.e. a long-form answer). In this paper, we address the challenges of long-context question answering in context of long-form answers given the idiosyncrasies of legal documents. We propose a question answering system that can (a) deconstruct domain-specific vocabulary for better retrieval from source documents, (b) parse complex document layouts while isolating sections and footnotes and linking them appropriately, (c) generate comprehensive answers using precise domain-specific vocabulary. We also introduce a coverage metric that classifies the performance into recall-based coverage categories allowing human users to evaluate the recall with ease. We curate a QA dataset by leveraging the expertise of professionals from fields such as law and corporate tax. Through comprehensive experiments and ablation studies, we demonstrate the usability and merit of the proposed system.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u6cd5\u5f8b\u6587\u6863\u7684\u957f\u4e0a\u4e0b\u6587\u95ee\u7b54\u7cfb\u7edf\uff0c\u80fd\u591f\u5904\u7406\u590d\u6742\u6587\u6863\u5e03\u5c40\u3001\u4e13\u4e1a\u8bcd\u6c47\uff0c\u5e76\u751f\u6210\u5168\u9762\u7684\u957f\u683c\u5f0f\u7b54\u6848\u3002", "motivation": "\u6cd5\u5f8b\u6587\u6863\u5177\u6709\u590d\u6742\u7684\u6587\u6863\u5e03\u5c40\uff08\u591a\u7ea7\u5d4c\u5957\u7ae0\u8282\u3001\u5197\u957f\u811a\u6ce8\uff09\u548c\u7279\u6b8a\u7684\u8bed\u8a00\u7279\u5f81\uff08\u590d\u6742\u53e5\u6cd5\u3001\u9886\u57df\u7279\u5b9a\u8bcd\u6c47\uff09\uff0c\u8fd9\u4f7f\u5f97\u95ee\u7b54\u4efb\u52a1\u5177\u6709\u6311\u6218\u6027\uff0c\u7279\u522b\u662f\u5f53\u7b54\u6848\u9700\u8981\u8de8\u8d8a\u591a\u9875\uff08\u957f\u4e0a\u4e0b\u6587\uff09\u4e14\u8981\u6c42\u5168\u9762\u6027\uff08\u957f\u683c\u5f0f\u7b54\u6848\uff09\u65f6\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u95ee\u7b54\u7cfb\u7edf\uff0c\u80fd\u591f\uff1a(a) \u89e3\u6784\u9886\u57df\u7279\u5b9a\u8bcd\u6c47\u4ee5\u6539\u8fdb\u4ece\u6e90\u6587\u6863\u7684\u68c0\u7d22\uff1b(b) \u89e3\u6790\u590d\u6742\u6587\u6863\u5e03\u5c40\uff0c\u540c\u65f6\u9694\u79bb\u7ae0\u8282\u548c\u811a\u6ce8\u5e76\u9002\u5f53\u94fe\u63a5\u5b83\u4eec\uff1b(c) \u4f7f\u7528\u7cbe\u786e\u7684\u9886\u57df\u7279\u5b9a\u8bcd\u6c47\u751f\u6210\u5168\u9762\u7b54\u6848\u3002\u8fd8\u5f15\u5165\u4e86\u4e00\u4e2a\u8986\u76d6\u7387\u6307\u6807\uff0c\u5c06\u6027\u80fd\u5206\u7c7b\u4e3a\u57fa\u4e8e\u53ec\u56de\u7684\u8986\u76d6\u7c7b\u522b\uff0c\u4fbf\u4e8e\u4eba\u5de5\u8bc4\u4f30\u53ec\u56de\u7387\u3002", "result": "\u901a\u8fc7\u5229\u7528\u6cd5\u5f8b\u548c\u516c\u53f8\u7a0e\u52a1\u7b49\u9886\u57df\u4e13\u4e1a\u4eba\u58eb\u7684\u4e13\u4e1a\u77e5\u8bc6\uff0c\u7b56\u5212\u4e86\u4e00\u4e2aQA\u6570\u636e\u96c6\u3002\u901a\u8fc7\u5168\u9762\u7684\u5b9e\u9a8c\u548c\u6d88\u878d\u7814\u7a76\uff0c\u8bc1\u660e\u4e86\u6240\u63d0\u51fa\u7cfb\u7edf\u7684\u53ef\u7528\u6027\u548c\u4f18\u70b9\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u6709\u6548\u89e3\u51b3\u4e86\u6cd5\u5f8b\u6587\u6863\u957f\u4e0a\u4e0b\u6587\u95ee\u7b54\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u9700\u8981\u5168\u9762\u957f\u683c\u5f0f\u7b54\u6848\u7684\u573a\u666f\u4e0b\uff0c\u901a\u8fc7\u4e13\u4e1a\u8bcd\u6c47\u5904\u7406\u3001\u590d\u6742\u5e03\u5c40\u89e3\u6790\u548c\u5168\u9762\u7684\u7b54\u6848\u751f\u6210\uff0c\u4e3a\u6cd5\u5f8b\u6587\u6863\u95ee\u7b54\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.07017", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07017", "abs": "https://arxiv.org/abs/2602.07017", "authors": ["Thuraya Alzubaidi", "Sana Ammar", "Maryam Alsharqi", "Islem Rekik", "Muzammil Behzad"], "title": "XAI-CLIP: ROI-Guided Perturbation Framework for Explainable Medical Image Segmentation in Multimodal Vision-Language Models", "comment": null, "summary": "Medical image segmentation is a critical component of clinical workflows, enabling accurate diagnosis, treatment planning, and disease monitoring. However, despite the superior performance of transformer-based models over convolutional architectures, their limited interpretability remains a major obstacle to clinical trust and deployment. Existing explainable artificial intelligence (XAI) techniques, including gradient-based saliency methods and perturbation-based approaches, are often computationally expensive, require numerous forward passes, and frequently produce noisy or anatomically irrelevant explanations. To address these limitations, we propose XAI-CLIP, an ROI-guided perturbation framework that leverages multimodal vision-language model embeddings to localize clinically meaningful anatomical regions and guide the explanation process. By integrating language-informed region localization with medical image segmentation and applying targeted, region-aware perturbations, the proposed method generates clearer, boundary-aware saliency maps while substantially reducing computational overhead. Experiments conducted on the FLARE22 and CHAOS datasets demonstrate that XAI-CLIP achieves up to a 60\\% reduction in runtime, a 44.6\\% improvement in dice score, and a 96.7\\% increase in Intersection-over-Union for occlusion-based explanations compared to conventional perturbation methods. Qualitative results further confirm cleaner and more anatomically consistent attribution maps with fewer artifacts, highlighting that the incorporation of multimodal vision-language representations into perturbation-based XAI frameworks significantly enhances both interpretability and efficiency, thereby enabling transparent and clinically deployable medical image segmentation systems.", "AI": {"tldr": "XAI-CLIP\uff1a\u4e00\u79cd\u57fa\u4e8e\u591a\u6a21\u6001\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684ROI\u5f15\u5bfc\u6270\u52a8\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u66f4\u6e05\u6670\u3001\u8fb9\u754c\u611f\u77e5\u7684\u663e\u8457\u6027\u56fe\uff0c\u663e\u8457\u63d0\u5347\u533b\u5b66\u56fe\u50cf\u5206\u5272\u7684\u53ef\u89e3\u91ca\u6027\u548c\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u5c3d\u7ba1\u57fa\u4e8etransformer\u7684\u533b\u5b66\u56fe\u50cf\u5206\u5272\u6a21\u578b\u6027\u80fd\u4f18\u8d8a\uff0c\u4f46\u5176\u6709\u9650\u7684\u53ef\u89e3\u91ca\u6027\u963b\u788d\u4e86\u4e34\u5e8a\u4fe1\u4efb\u548c\u90e8\u7f72\u3002\u73b0\u6709\u7684\u53ef\u89e3\u91ca\u4eba\u5de5\u667a\u80fd\u6280\u672f\u8ba1\u7b97\u6210\u672c\u9ad8\u3001\u9700\u8981\u591a\u6b21\u524d\u5411\u4f20\u64ad\uff0c\u4e14\u5e38\u4ea7\u751f\u566a\u58f0\u5927\u6216\u89e3\u5256\u5b66\u65e0\u5173\u7684\u89e3\u91ca\u3002", "method": "\u63d0\u51faXAI-CLIP\u6846\u67b6\uff0c\u5229\u7528\u591a\u6a21\u6001\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5d4c\u5165\u5b9a\u4f4d\u4e34\u5e8a\u76f8\u5173\u7684\u89e3\u5256\u533a\u57df\uff0c\u901a\u8fc7\u8bed\u8a00\u5f15\u5bfc\u7684\u533a\u57df\u5b9a\u4f4d\u4e0e\u533b\u5b66\u56fe\u50cf\u5206\u5272\u7ed3\u5408\uff0c\u5e94\u7528\u6709\u9488\u5bf9\u6027\u7684\u533a\u57df\u611f\u77e5\u6270\u52a8\uff0c\u751f\u6210\u8fb9\u754c\u611f\u77e5\u7684\u663e\u8457\u6027\u56fe\u3002", "result": "\u5728FLARE22\u548cCHAOS\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff1a\u8fd0\u884c\u65f6\u51cf\u5c1160%\uff0cDice\u5206\u6570\u63d0\u534744.6%\uff0c\u57fa\u4e8e\u906e\u6321\u7684\u89e3\u91ca\u7684IoU\u63d0\u9ad896.7%\uff0c\u751f\u6210\u66f4\u5e72\u51c0\u3001\u89e3\u5256\u5b66\u4e00\u81f4\u7684\u5f52\u56e0\u56fe\uff0c\u51cf\u5c11\u4f2a\u5f71\u3002", "conclusion": "\u5c06\u591a\u6a21\u6001\u89c6\u89c9\u8bed\u8a00\u8868\u793a\u6574\u5408\u5230\u57fa\u4e8e\u6270\u52a8\u7684XAI\u6846\u67b6\u4e2d\uff0c\u663e\u8457\u63d0\u5347\u4e86\u53ef\u89e3\u91ca\u6027\u548c\u6548\u7387\uff0c\u4e3a\u5b9e\u73b0\u900f\u660e\u4e14\u53ef\u4e34\u5e8a\u90e8\u7f72\u7684\u533b\u5b66\u56fe\u50cf\u5206\u5272\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u6548\u9014\u5f84\u3002"}}
{"id": "2602.07253", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.07253", "abs": "https://arxiv.org/abs/2602.07253", "authors": ["Litian Liu", "Reza Pourreza", "Yubing Jian", "Yao Qin", "Roland Memisevic"], "title": "From Out-of-Distribution Detection to Hallucination Detection: A Geometric View", "comment": null, "summary": "Detecting hallucinations in large language models is a critical open problem with significant implications for safety and reliability. While existing hallucination detection methods achieve strong performance in question-answering tasks, they remain less effective on tasks requiring reasoning. In this work, we revisit hallucination detection through the lens of out-of-distribution (OOD) detection, a well-studied problem in areas like computer vision. Treating next-token prediction in language models as a classification task allows us to apply OOD techniques, provided appropriate modifications are made to account for the structural differences in large language models. We show that OOD-based approaches yield training-free, single-sample-based detectors, achieving strong accuracy in hallucination detection for reasoning tasks. Overall, our work suggests that reframing hallucination detection as OOD detection provides a promising and scalable pathway toward language model safety.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5c06\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5e7b\u89c9\u68c0\u6d4b\u91cd\u65b0\u5b9a\u4e49\u4e3a\u5206\u5e03\u5916\u68c0\u6d4b\u95ee\u9898\uff0c\u63d0\u51fa\u65e0\u9700\u8bad\u7ec3\u3001\u57fa\u4e8e\u5355\u6837\u672c\u7684\u68c0\u6d4b\u65b9\u6cd5\uff0c\u5728\u63a8\u7406\u4efb\u52a1\u4e2d\u5b9e\u73b0\u5f3a\u51c6\u786e\u6027\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5e7b\u89c9\u68c0\u6d4b\u662f\u4e00\u4e2a\u5173\u952e\u5f00\u653e\u95ee\u9898\uff0c\u5bf9\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\u6709\u91cd\u8981\u5f71\u54cd\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u95ee\u7b54\u4efb\u52a1\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u9700\u8981\u63a8\u7406\u7684\u4efb\u52a1\u4e2d\u6548\u679c\u8f83\u5dee\u3002\u7814\u7a76\u8005\u5e0c\u671b\u901a\u8fc7\u5206\u5e03\u5916\u68c0\u6d4b\u7684\u89c6\u89d2\u91cd\u65b0\u5ba1\u89c6\u5e7b\u89c9\u68c0\u6d4b\u95ee\u9898\u3002", "method": "\u5c06\u8bed\u8a00\u6a21\u578b\u7684\u4e0b\u4e00\u8bcd\u9884\u6d4b\u89c6\u4e3a\u5206\u7c7b\u4efb\u52a1\uff0c\u5e94\u7528\u5206\u5e03\u5916\u68c0\u6d4b\u6280\u672f\uff0c\u5e76\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\u7684\u7ed3\u6784\u5dee\u5f02\u8fdb\u884c\u9002\u5f53\u4fee\u6539\u3002\u8be5\u65b9\u6cd5\u65e0\u9700\u8bad\u7ec3\uff0c\u4ec5\u9700\u5355\u6837\u672c\u5373\u53ef\u8fdb\u884c\u68c0\u6d4b\u3002", "result": "\u57fa\u4e8e\u5206\u5e03\u5916\u68c0\u6d4b\u7684\u65b9\u6cd5\u5728\u63a8\u7406\u4efb\u52a1\u7684\u5e7b\u89c9\u68c0\u6d4b\u4e2d\u5b9e\u73b0\u4e86\u5f3a\u51c6\u786e\u6027\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u5c06\u5e7b\u89c9\u68c0\u6d4b\u91cd\u65b0\u5b9a\u4e49\u4e3a\u5206\u5e03\u5916\u68c0\u6d4b\u95ee\u9898\uff0c\u4e3a\u5927\u8bed\u8a00\u6a21\u578b\u5b89\u5168\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u524d\u666f\u4e14\u53ef\u6269\u5c55\u7684\u9014\u5f84\u3002"}}
{"id": "2602.07422", "categories": ["cs.CR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.07422", "abs": "https://arxiv.org/abs/2602.07422", "authors": ["Tianyi Wu", "Mingzhe Du", "Yue Liu", "Chengran Yang", "Terry Yue Zhuo", "Jiaheng Zhang", "See-Kiong Ng"], "title": "Secure Code Generation via Online Reinforcement Learning with Vulnerability Reward Model", "comment": null, "summary": "Large language models (LLMs) are increasingly used in software development, yet their tendency to generate insecure code remains a major barrier to real-world deployment. Existing secure code alignment methods often suffer from a functionality--security paradox, improving security at the cost of substantial utility degradation. We propose SecCoderX, an online reinforcement learning framework for functionality-preserving secure code generation. SecCoderX first bridges vulnerability detection and secure code generation by repurposing mature detection resources in two ways: (i) synthesizing diverse, reality-grounded vulnerability-inducing coding tasks for online RL rollouts, and (ii) training a reasoning-based vulnerability reward model that provides scalable and reliable security supervision. Together, these components are unified in an online RL loop to align code LLMs to generate secure and functional code. Extensive experiments demonstrate that SecCoderX achieves state-of-the-art performance, improving Effective Safety Rate (ESR) by approximately 10% over unaligned models, whereas prior methods often degrade ESR by 14-54%. We release our code, dataset and model checkpoints at https://github.com/AndrewWTY/SecCoderX.", "AI": {"tldr": "SecCoderX\u662f\u4e00\u4e2a\u57fa\u4e8e\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u529f\u80fd\u4fdd\u6301\u7684\u5b89\u5168\u4ee3\u7801\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u5b89\u5168\u6027\u548c\u529f\u80fd\u6027\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u5e94\u7528\u65e5\u76ca\u5e7f\u6cdb\uff0c\u4f46\u5176\u751f\u6210\u4e0d\u5b89\u5168\u4ee3\u7801\u7684\u503e\u5411\u963b\u788d\u4e86\u5b9e\u9645\u90e8\u7f72\u3002\u73b0\u6709\u5b89\u5168\u4ee3\u7801\u5bf9\u9f50\u65b9\u6cd5\u5b58\u5728\u529f\u80fd-\u5b89\u5168\u6096\u8bba\uff0c\u5373\u63d0\u9ad8\u5b89\u5168\u6027\u5f80\u5f80\u4ee5\u663e\u8457\u964d\u4f4e\u529f\u80fd\u6027\u4e3a\u4ee3\u4ef7\u3002", "method": "SecCoderX\u91c7\u7528\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u79cd\u65b9\u5f0f\u6865\u63a5\u6f0f\u6d1e\u68c0\u6d4b\u548c\u5b89\u5168\u4ee3\u7801\u751f\u6210\uff1a1\uff09\u5408\u6210\u591a\u6837\u5316\u3001\u57fa\u4e8e\u73b0\u5b9e\u7684\u6f0f\u6d1e\u8bf1\u5bfc\u7f16\u7801\u4efb\u52a1\u7528\u4e8e\u5728\u7ebfRL\u5c55\u5f00\uff1b2\uff09\u8bad\u7ec3\u57fa\u4e8e\u63a8\u7406\u7684\u6f0f\u6d1e\u5956\u52b1\u6a21\u578b\uff0c\u63d0\u4f9b\u53ef\u6269\u5c55\u4e14\u53ef\u9760\u7684\u5b89\u5168\u76d1\u7763\u3002\u8fd9\u4e9b\u7ec4\u4ef6\u5728\u5728\u7ebfRL\u5faa\u73af\u4e2d\u7edf\u4e00\uff0c\u7528\u4e8e\u5bf9\u9f50\u4ee3\u7801LLMs\u751f\u6210\u5b89\u5168\u4e14\u529f\u80fd\u6027\u7684\u4ee3\u7801\u3002", "result": "\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660eSecCoderX\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5c06\u6709\u6548\u5b89\u5168\u7387\uff08ESR\uff09\u6bd4\u672a\u5bf9\u9f50\u6a21\u578b\u63d0\u9ad8\u4e86\u7ea610%\uff0c\u800c\u5148\u524d\u65b9\u6cd5\u5f80\u5f80\u4f7fESR\u964d\u4f4e14-54%\u3002", "conclusion": "SecCoderX\u6210\u529f\u89e3\u51b3\u4e86\u529f\u80fd-\u5b89\u5168\u6096\u8bba\uff0c\u4e3a\u5b89\u5168\u4ee3\u7801\u751f\u6210\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4ee3\u7801LLMs\u7684\u5b89\u5168\u6027\u548c\u529f\u80fd\u6027\u5e73\u8861\u3002"}}
{"id": "2602.07211", "categories": ["cs.CL", "cs.SD"], "pdf": "https://arxiv.org/pdf/2602.07211", "abs": "https://arxiv.org/abs/2602.07211", "authors": ["Ju Lin", "Jing Pan", "Ruizhi Li", "Ming Sun", "Yuzong Liu", "Alaa Hassan", "Jing Zheng", "Florian Metze"], "title": "Equipping LLM with Directional Multi-Talker Speech Understanding Capabilities", "comment": null, "summary": "Recent studies have demonstrated that prompting large language models (LLM) with audio encodings enables effective speech understanding capabilities. However, most speech LLMs are trained on single-channel, single-talker data, which makes it challenging to directly apply them to multi-talker and multi-channel speech understanding task. In this work, we present a comprehensive investigation on how to enable directional multi-talker speech understanding capabilities for LLMs, specifically in smart glasses usecase. We propose two novel approaches to integrate directivity into LLMs: (1) a cascaded system that leverages a source separation front-end module, and (2) an end-to-end system that utilizes serialized output training. All of the approaches utilize a multi-microphone array embedded in smart glasses to optimize directivity interpretation and processing in a streaming manner. Experimental results demonstrate the efficacy of our proposed methods in endowing LLMs with directional speech understanding capabilities, achieving strong performance in both speech recognition and speech translation tasks.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u7d22\u5982\u4f55\u8ba9\u5927\u8bed\u8a00\u6a21\u578b\u5177\u5907\u5b9a\u5411\u591a\u8bf4\u8bdd\u8005\u8bed\u97f3\u7406\u89e3\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u667a\u80fd\u773c\u955c\u5e94\u7528\u573a\u666f\u4e2d\uff0c\u63d0\u51fa\u4e86\u7ea7\u8054\u7cfb\u7edf\u548c\u7aef\u5230\u7aef\u7cfb\u7edf\u4e24\u79cd\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u8bed\u97f3\u5927\u8bed\u8a00\u6a21\u578b\u4e3b\u8981\u57fa\u4e8e\u5355\u901a\u9053\u3001\u5355\u8bf4\u8bdd\u8005\u6570\u636e\u8bad\u7ec3\uff0c\u96be\u4ee5\u76f4\u63a5\u5e94\u7528\u4e8e\u591a\u8bf4\u8bdd\u8005\u548c\u591a\u901a\u9053\u7684\u8bed\u97f3\u7406\u89e3\u4efb\u52a1\uff0c\u7279\u522b\u662f\u5728\u667a\u80fd\u773c\u955c\u7b49\u9700\u8981\u5b9a\u5411\u8bed\u97f3\u7406\u89e3\u7684\u5b9e\u9645\u5e94\u7528\u573a\u666f\u4e2d\u3002", "method": "\u63d0\u51fa\u4e86\u4e24\u79cd\u65b0\u9896\u65b9\u6cd5\uff1a1\uff09\u7ea7\u8054\u7cfb\u7edf\uff0c\u5229\u7528\u6e90\u5206\u79bb\u524d\u7aef\u6a21\u5757\uff1b2\uff09\u7aef\u5230\u7aef\u7cfb\u7edf\uff0c\u91c7\u7528\u5e8f\u5217\u5316\u8f93\u51fa\u8bad\u7ec3\u3002\u4e24\u79cd\u65b9\u6cd5\u90fd\u5229\u7528\u667a\u80fd\u773c\u955c\u4e2d\u7684\u591a\u9ea6\u514b\u98ce\u9635\u5217\uff0c\u4ee5\u6d41\u5f0f\u65b9\u5f0f\u4f18\u5316\u5b9a\u5411\u89e3\u91ca\u548c\u5904\u7406\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u65b9\u6cd5\u80fd\u6709\u6548\u8d4b\u4e88\u5927\u8bed\u8a00\u6a21\u578b\u5b9a\u5411\u8bed\u97f3\u7406\u89e3\u80fd\u529b\uff0c\u5728\u8bed\u97f3\u8bc6\u522b\u548c\u8bed\u97f3\u7ffb\u8bd1\u4efb\u52a1\u4e2d\u90fd\u53d6\u5f97\u4e86\u5f3a\u52b2\u6027\u80fd\u3002", "conclusion": "\u8be5\u7814\u7a76\u6210\u529f\u5b9e\u73b0\u4e86\u8ba9\u5927\u8bed\u8a00\u6a21\u578b\u5177\u5907\u5b9a\u5411\u591a\u8bf4\u8bdd\u8005\u8bed\u97f3\u7406\u89e3\u80fd\u529b\uff0c\u4e3a\u667a\u80fd\u773c\u955c\u7b49\u5b9e\u9645\u5e94\u7528\u573a\u666f\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.07259", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07259", "abs": "https://arxiv.org/abs/2602.07259", "authors": ["Cheol Woo Kim", "Davin Choo", "Tzeh Yuan Neoh", "Milind Tambe"], "title": "Incentive-Aware AI Safety via Strategic Resource Allocation: A Stackelberg Security Games Perspective", "comment": null, "summary": "As AI systems grow more capable and autonomous, ensuring their safety and reliability requires not only model-level alignment but also strategic oversight of the humans and institutions involved in their development and deployment. Existing safety frameworks largely treat alignment as a static optimization problem (e.g., tuning models to desired behavior) while overlooking the dynamic, adversarial incentives that shape how data are collected, how models are evaluated, and how they are ultimately deployed. We propose a new perspective on AI safety grounded in Stackelberg Security Games (SSGs): a class of game-theoretic models designed for adversarial resource allocation under uncertainty. By viewing AI oversight as a strategic interaction between defenders (auditors, evaluators, and deployers) and attackers (malicious actors, misaligned contributors, or worst-case failure modes), SSGs provide a unifying framework for reasoning about incentive design, limited oversight capacity, and adversarial uncertainty across the AI lifecycle. We illustrate how this framework can inform (1) training-time auditing against data/feedback poisoning, (2) pre-deployment evaluation under constrained reviewer resources, and (3) robust multi-model deployment in adversarial environments. This synthesis bridges algorithmic alignment and institutional oversight design, highlighting how game-theoretic deterrence can make AI oversight proactive, risk-aware, and resilient to manipulation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u5c06AI\u5b89\u5168\u89c6\u4e3aStackelberg\u5b89\u5168\u535a\u5f08\u95ee\u9898\uff0c\u5f3a\u8c03\u9700\u8981\u4ece\u9759\u6001\u4f18\u5316\u8f6c\u5411\u52a8\u6001\u6218\u7565\u76d1\u7763\uff0c\u8003\u8651\u5f00\u53d1\u90e8\u7f72\u4e2d\u7684\u5bf9\u6297\u6027\u6fc0\u52b1\u95ee\u9898\u3002", "motivation": "\u73b0\u6709AI\u5b89\u5168\u6846\u67b6\u4e3b\u8981\u5c06\u5bf9\u9f50\u89c6\u4e3a\u9759\u6001\u4f18\u5316\u95ee\u9898\uff0c\u5ffd\u7565\u4e86\u6570\u636e\u6536\u96c6\u3001\u6a21\u578b\u8bc4\u4f30\u548c\u90e8\u7f72\u8fc7\u7a0b\u4e2d\u7684\u52a8\u6001\u5bf9\u6297\u6027\u6fc0\u52b1\u3002\u968f\u7740AI\u7cfb\u7edf\u80fd\u529b\u589e\u5f3a\uff0c\u9700\u8981\u4e0d\u4ec5\u5173\u6ce8\u6a21\u578b\u5c42\u9762\u7684\u5bf9\u9f50\uff0c\u8fd8\u8981\u5bf9\u53c2\u4e0e\u5f00\u53d1\u548c\u90e8\u7f72\u7684\u4eba\u7c7b\u548c\u673a\u6784\u8fdb\u884c\u6218\u7565\u76d1\u7763\u3002", "method": "\u63d0\u51fa\u57fa\u4e8eStackelberg\u5b89\u5168\u535a\u5f08\u7684\u65b0\u89c6\u89d2\uff0c\u5c06AI\u76d1\u7763\u89c6\u4e3a\u9632\u5fa1\u8005\uff08\u5ba1\u8ba1\u5458\u3001\u8bc4\u4f30\u8005\u3001\u90e8\u7f72\u8005\uff09\u4e0e\u653b\u51fb\u8005\uff08\u6076\u610f\u884c\u4e3a\u8005\u3001\u672a\u5bf9\u9f50\u8d21\u732e\u8005\u3001\u6700\u574f\u60c5\u51b5\u6545\u969c\u6a21\u5f0f\uff09\u4e4b\u95f4\u7684\u6218\u7565\u4e92\u52a8\u3002\u8be5\u6846\u67b6\u4e3aAI\u751f\u547d\u5468\u671f\u4e2d\u7684\u6fc0\u52b1\u8bbe\u8ba1\u3001\u6709\u9650\u76d1\u7763\u80fd\u529b\u548c\u5bf9\u6297\u6027\u4e0d\u786e\u5b9a\u6027\u63d0\u4f9b\u7edf\u4e00\u5206\u6790\u5de5\u5177\u3002", "result": "\u8be5\u6846\u67b6\u53ef\u5e94\u7528\u4e8e\uff1a(1) \u8bad\u7ec3\u65f6\u5ba1\u8ba1\u6570\u636e/\u53cd\u9988\u6295\u6bd2\uff0c(2) \u6709\u9650\u8bc4\u5ba1\u8d44\u6e90\u4e0b\u7684\u9884\u90e8\u7f72\u8bc4\u4f30\uff0c(3) \u5bf9\u6297\u73af\u5883\u4e2d\u7684\u9c81\u68d2\u591a\u6a21\u578b\u90e8\u7f72\u3002\u5c06\u7b97\u6cd5\u5bf9\u9f50\u4e0e\u673a\u6784\u76d1\u7763\u8bbe\u8ba1\u76f8\u7ed3\u5408\uff0c\u4f7fAI\u76d1\u7763\u66f4\u5177\u524d\u77bb\u6027\u3001\u98ce\u9669\u610f\u8bc6\u548c\u6297\u64cd\u7eb5\u80fd\u529b\u3002", "conclusion": "Stackelberg\u5b89\u5168\u535a\u5f08\u4e3aAI\u5b89\u5168\u63d0\u4f9b\u4e86\u7edf\u4e00\u6846\u67b6\uff0c\u5f3a\u8c03\u901a\u8fc7\u535a\u5f08\u8bba\u5a01\u6151\u4f7fAI\u76d1\u7763\u66f4\u52a0\u4e3b\u52a8\u3001\u98ce\u9669\u611f\u77e5\u548c\u6297\u64cd\u7eb5\uff0c\u586b\u8865\u4e86\u7b97\u6cd5\u5bf9\u9f50\u4e0e\u673a\u6784\u76d1\u7763\u8bbe\u8ba1\u4e4b\u95f4\u7684\u7a7a\u767d\u3002"}}
{"id": "2602.07412", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.07412", "abs": "https://arxiv.org/abs/2602.07412", "authors": ["Raula Gaikovina Kula", "Christoph Treude", "Xing Hu", "Sebastian Baltes", "Earl T. Barr", "Kelly Blincoe", "Fabio Calefato", "Junjie Chen", "Marc Cheong", "Youmei Fan", "Daniel M. German", "Marco Gerosa", "Jin L. C. Guo", "Shinpei Hayashi", "Robert Hirschfeld", "Reid Holmes", "Yintong Huo", "Takashi Kobayashi", "Michele Lanza", "Zhongxin Liu", "Olivier Nourry", "Nicole Novielli", "Denys Poshyvanyk", "Shinobu Saito", "Kazumasa Shimari", "Igor Steinmacher", "Mairieli Wessel", "Markus Wagner", "Annie Vella", "Laurie Williams", "Xin Xia"], "title": "Forecasting Developer Environments with GenAI: A Research Perspective", "comment": "IDE Workshop", "summary": "Generative Artificial Intelligence (GenAI) models are achieving remarkable performance in various tasks, including code generation, testing, code review, and program repair. The ability to increase the level of abstraction away from writing code has the potential to change the Human-AI interaction within the integrated development environment (IDE). To explore the impact of GenAI on IDEs, 33 experts from the Software Engineering, Artificial Intelligence, and Human-Computer Interaction domains gathered to discuss challenges and opportunities at Shonan Meeting 222, a four-day intensive research meeting. Four themes emerged as areas of interest for researchers and practitioners.", "AI": {"tldr": "\u4e13\u5bb6\u4f1a\u8bae\u63a2\u8ba8\u751f\u6210\u5f0fAI\u5bf9IDE\u7684\u5f71\u54cd\uff0c\u8bc6\u522b\u4e86\u56db\u4e2a\u5173\u952e\u7814\u7a76\u4e3b\u9898", "motivation": "\u63a2\u7d22\u751f\u6210\u5f0fAI\u5728\u4ee3\u7801\u751f\u6210\u3001\u6d4b\u8bd5\u3001\u4ee3\u7801\u5ba1\u67e5\u548c\u7a0b\u5e8f\u4fee\u590d\u7b49\u65b9\u9762\u7684\u5353\u8d8a\u8868\u73b0\u5982\u4f55\u6539\u53d8IDE\u4e2d\u7684\u4eba\u673a\u4ea4\u4e92\uff0c\u7406\u89e3\u5176\u5bf9\u8f6f\u4ef6\u5f00\u53d1\u73af\u5883\u7684\u5f71\u54cd", "method": "\u7ec4\u7ec7\u4e3a\u671f\u56db\u5929\u7684Shonan Meeting 222\u7814\u7a76\u4f1a\u8bae\uff0c\u6c47\u96c633\u4f4d\u6765\u81ea\u8f6f\u4ef6\u5de5\u7a0b\u3001\u4eba\u5de5\u667a\u80fd\u548c\u4eba\u673a\u4ea4\u4e92\u9886\u57df\u7684\u4e13\u5bb6\uff0c\u901a\u8fc7\u8ba8\u8bba\u8bc6\u522b\u6311\u6218\u548c\u673a\u9047", "result": "\u4f1a\u8bae\u786e\u5b9a\u4e86\u56db\u4e2a\u4e3b\u8981\u7814\u7a76\u4e3b\u9898\uff0c\u4e3a\u7814\u7a76\u4eba\u5458\u548c\u4ece\u4e1a\u8005\u63d0\u4f9b\u4e86\u91cd\u70b9\u5173\u6ce8\u9886\u57df", "conclusion": "\u751f\u6210\u5f0fAI\u6709\u6f5c\u529b\u663e\u8457\u6539\u53d8IDE\u4e2d\u7684\u4eba\u673a\u4ea4\u4e92\uff0c\u9700\u8981\u8de8\u5b66\u79d1\u5408\u4f5c\u6765\u63a2\u7d22\u5176\u5f71\u54cd\u5e76\u89e3\u51b3\u76f8\u5173\u6311\u6218"}}
{"id": "2602.07319", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07319", "abs": "https://arxiv.org/abs/2602.07319", "authors": ["Savan Doshi"], "title": "Beyond Accuracy: Risk-Sensitive Evaluation of Hallucinated Medical Advice", "comment": null, "summary": "Large language models are increasingly being used in patient-facing medical question answering, where hallucinated outputs can vary widely in potential harm. However, existing hallucination standards and evaluation metrics focus primarily on factual correctness, treating all errors as equally severe. This obscures clinically relevant failure modes, particularly when models generate unsupported but actionable medical language. We propose a risk-sensitive evaluation framework that quantifies hallucinations through the presence of risk-bearing language, including treatment directives, contraindications, urgency cues, and mentions of high-risk medications. Rather than assessing clinical correctness, our approach evaluates the potential impact of hallucinated content if acted upon. We further combine risk scoring with a relevance measure to identify high-risk, low-grounding failures. We apply this framework to three instruction-tuned language models using controlled patient-facing prompts designed as safety stress tests. Our results show that models with similar surface-level behavior exhibit substantially different risk profiles and that standard evaluation metrics fail to capture these distinctions. These findings highlight the importance of incorporating risk sensitivity into hallucination evaluation and suggest that evaluation validity is critically dependent on task and prompt design.", "AI": {"tldr": "\u63d0\u51fa\u98ce\u9669\u654f\u611f\u7684\u5e7b\u89c9\u8bc4\u4f30\u6846\u67b6\uff0c\u901a\u8fc7\u98ce\u9669\u6027\u8bed\u8a00\u91cf\u5316\u533b\u7597\u95ee\u7b54\u4e2d\u7684\u5e7b\u89c9\u98ce\u9669\uff0c\u800c\u975e\u4ec5\u5173\u6ce8\u4e8b\u5b9e\u6b63\u786e\u6027", "motivation": "\u73b0\u6709\u5e7b\u89c9\u8bc4\u4f30\u6807\u51c6\u4e3b\u8981\u5173\u6ce8\u4e8b\u5b9e\u6b63\u786e\u6027\uff0c\u5c06\u6240\u6709\u9519\u8bef\u89c6\u4e3a\u540c\u7b49\u4e25\u91cd\uff0c\u8fd9\u63a9\u76d6\u4e86\u4e34\u5e8a\u76f8\u5173\u7684\u5931\u8d25\u6a21\u5f0f\uff0c\u7279\u522b\u662f\u5f53\u6a21\u578b\u751f\u6210\u65e0\u652f\u6301\u4f46\u53ef\u64cd\u4f5c\u7684\u533b\u7597\u8bed\u8a00\u65f6", "method": "\u63d0\u51fa\u98ce\u9669\u654f\u611f\u8bc4\u4f30\u6846\u67b6\uff0c\u901a\u8fc7\u68c0\u6d4b\u98ce\u9669\u6027\u8bed\u8a00\uff08\u6cbb\u7597\u6307\u4ee4\u3001\u7981\u5fcc\u75c7\u3001\u7d27\u6025\u63d0\u793a\u3001\u9ad8\u98ce\u9669\u836f\u7269\u63d0\u53ca\uff09\u6765\u91cf\u5316\u5e7b\u89c9\uff0c\u7ed3\u5408\u98ce\u9669\u8bc4\u5206\u4e0e\u76f8\u5173\u6027\u5ea6\u91cf\u8bc6\u522b\u9ad8\u98ce\u9669\u3001\u4f4e\u57fa\u7840\u6027\u5931\u8d25", "result": "\u5e94\u7528\u8be5\u6846\u67b6\u5bf9\u4e09\u4e2a\u6307\u4ee4\u8c03\u4f18\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u6d4b\u8bd5\uff0c\u7ed3\u679c\u663e\u793a\u5177\u6709\u76f8\u4f3c\u8868\u9762\u884c\u4e3a\u7684\u6a21\u578b\u8868\u73b0\u51fa\u663e\u8457\u4e0d\u540c\u7684\u98ce\u9669\u7279\u5f81\uff0c\u6807\u51c6\u8bc4\u4f30\u6307\u6807\u65e0\u6cd5\u6355\u6349\u8fd9\u4e9b\u5dee\u5f02", "conclusion": "\u5c06\u98ce\u9669\u654f\u611f\u6027\u7eb3\u5165\u5e7b\u89c9\u8bc4\u4f30\u81f3\u5173\u91cd\u8981\uff0c\u8bc4\u4f30\u6709\u6548\u6027\u4e25\u91cd\u4f9d\u8d56\u4e8e\u4efb\u52a1\u548c\u63d0\u793a\u8bbe\u8ba1"}}
{"id": "2602.07025", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07025", "abs": "https://arxiv.org/abs/2602.07025", "authors": ["Daniele Savietto", "Declan Campbell", "Andr\u00e9 Panisson", "Marco Nurisso", "Giovanni Petri", "Jonathan D. Cohen", "Alan Perotti"], "title": "The Geometry of Representational Failures in Vision Language Models", "comment": null, "summary": "Vision-Language Models (VLMs) exhibit puzzling failures in multi-object visual tasks, such as hallucinating non-existent elements or failing to identify the most similar objects among distractions. While these errors mirror human cognitive constraints, such as the \"Binding Problem\", the internal mechanisms driving them in artificial systems remain poorly understood. Here, we propose a mechanistic insight by analyzing the representational geometry of open-weight VLMs (Qwen, InternVL, Gemma), comparing methodologies to distill \"concept vectors\" - latent directions encoding visual concepts. We validate our concept vectors via steering interventions that reliably manipulate model behavior in both simplified and naturalistic vision tasks (e.g., forcing the model to perceive a red flower as blue). We observe that the geometric overlap between these vectors strongly correlates with specific error patterns, offering a grounded quantitative framework to understand how internal representations shape model behavior and drive visual failures.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5206\u6790\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u591a\u7269\u4f53\u89c6\u89c9\u4efb\u52a1\u4e2d\u7684\u5931\u8d25\u673a\u5236\uff0c\u901a\u8fc7\u6982\u5ff5\u5411\u91cf\u51e0\u4f55\u91cd\u53e0\u89e3\u91ca\u6a21\u578b\u9519\u8bef\u6a21\u5f0f", "motivation": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u591a\u7269\u4f53\u89c6\u89c9\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u4ee4\u4eba\u56f0\u60d1\u7684\u5931\u8d25\uff0c\u5982\u5e7b\u89c9\u4e0d\u5b58\u5728\u7684\u5143\u7d20\u6216\u65e0\u6cd5\u5728\u5e72\u6270\u7269\u4e2d\u8bc6\u522b\u6700\u76f8\u4f3c\u7684\u5bf9\u8c61\u3002\u8fd9\u4e9b\u9519\u8bef\u53cd\u6620\u4e86\u4eba\u7c7b\u8ba4\u77e5\u9650\u5236\uff08\u5982\"\u7ed1\u5b9a\u95ee\u9898\"\uff09\uff0c\u4f46\u4eba\u5de5\u7cfb\u7edf\u4e2d\u7684\u5185\u90e8\u673a\u5236\u4ecd\u4e0d\u6e05\u695a\u3002", "method": "\u901a\u8fc7\u5206\u6790\u5f00\u653e\u6743\u91cdVLMs\uff08Qwen\u3001InternVL\u3001Gemma\uff09\u7684\u8868\u5f81\u51e0\u4f55\uff0c\u6bd4\u8f83\u63d0\u53d6\"\u6982\u5ff5\u5411\u91cf\"\u7684\u65b9\u6cd5\u2014\u2014\u7f16\u7801\u89c6\u89c9\u6982\u5ff5\u7684\u6f5c\u5728\u65b9\u5411\u3002\u901a\u8fc7\u8f6c\u5411\u5e72\u9884\u9a8c\u8bc1\u6982\u5ff5\u5411\u91cf\uff0c\u5728\u7b80\u5316\u548c\u81ea\u7136\u4e3b\u4e49\u89c6\u89c9\u4efb\u52a1\u4e2d\u53ef\u9760\u5730\u64cd\u7eb5\u6a21\u578b\u884c\u4e3a\u3002", "result": "\u89c2\u5bdf\u5230\u8fd9\u4e9b\u5411\u91cf\u4e4b\u95f4\u7684\u51e0\u4f55\u91cd\u53e0\u4e0e\u7279\u5b9a\u9519\u8bef\u6a21\u5f0f\u5f3a\u76f8\u5173\uff0c\u4e3a\u7406\u89e3\u5185\u90e8\u8868\u5f81\u5982\u4f55\u5851\u9020\u6a21\u578b\u884c\u4e3a\u548c\u9a71\u52a8\u89c6\u89c9\u5931\u8d25\u63d0\u4f9b\u4e86\u57fa\u4e8e\u91cf\u5316\u7684\u6846\u67b6\u3002", "conclusion": "\u6982\u5ff5\u5411\u91cf\u7684\u51e0\u4f55\u91cd\u53e0\u4e3a\u7406\u89e3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u591a\u7269\u4f53\u4efb\u52a1\u4e2d\u7684\u5931\u8d25\u673a\u5236\u63d0\u4f9b\u4e86\u673a\u5236\u6027\u6d1e\u5bdf\uff0c\u5efa\u7acb\u4e86\u5185\u90e8\u8868\u5f81\u4e0e\u884c\u4e3a\u9519\u8bef\u4e4b\u95f4\u7684\u5b9a\u91cf\u8054\u7cfb\u6846\u67b6\u3002"}}
{"id": "2602.07267", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.07267", "abs": "https://arxiv.org/abs/2602.07267", "authors": ["Fengyuan Liu", "Jay Gala", "Nilaksh", "Dzmitry Bahdanau", "Siva Reddy", "Hugo Larochelle"], "title": "BRIDGE: Predicting Human Task Completion Time From Model Performance", "comment": null, "summary": "Evaluating the real-world capabilities of AI systems requires grounding benchmark performance in human-interpretable measures of task difficulty. Existing approaches that rely on direct human task completion time annotations are costly, noisy, and difficult to scale across benchmarks. In this work, we propose BRIDGE, a unified psychometric framework that learns the latent difficulty scale from model responses and anchors it to human task completion time. Using a two-parameter logistic Item Response Theory model, we jointly estimate latent task difficulty and model capability from model performance data across multiple benchmarks. We demonstrate that latent task difficulty varies linearly with the logarithm of human completion time, allowing human task completion time to be inferred for new benchmarks from model performance alone. Leveraging this alignment, we forecast frontier model capabilities in terms of human task length and independently reproduce METR's exponential scaling results, with the 50% solvable task horizon doubling approximately every 6 months.", "AI": {"tldr": "BRIDGE\u6846\u67b6\u901a\u8fc7\u6a21\u578b\u54cd\u5e94\u5b66\u4e60\u4efb\u52a1\u96be\u5ea6\uff0c\u5e76\u5c06\u5176\u4e0e\u4eba\u7c7b\u5b8c\u6210\u65f6\u95f4\u5bf9\u9f50\uff0c\u5b9e\u73b0\u4ec5\u4ece\u6a21\u578b\u6027\u80fd\u63a8\u65ad\u4eba\u7c7b\u4efb\u52a1\u5b8c\u6210\u65f6\u95f4", "motivation": "\u73b0\u6709\u57fa\u4e8e\u4eba\u7c7b\u4efb\u52a1\u5b8c\u6210\u65f6\u95f4\u7684\u57fa\u51c6\u8bc4\u4f30\u65b9\u6cd5\u6210\u672c\u9ad8\u3001\u566a\u58f0\u5927\u3001\u96be\u4ee5\u6269\u5c55\uff0c\u9700\u8981\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u65b9\u6cd5\u6765\u5c06\u6a21\u578b\u6027\u80fd\u4e0e\u4eba\u7c7b\u53ef\u89e3\u91ca\u7684\u4efb\u52a1\u96be\u5ea6\u8054\u7cfb\u8d77\u6765", "method": "\u4f7f\u7528\u53cc\u53c2\u6570\u903b\u8f91\u9879\u76ee\u53cd\u5e94\u7406\u8bba\u6a21\u578b\uff0c\u4ece\u591a\u4e2a\u57fa\u51c6\u7684\u6a21\u578b\u6027\u80fd\u6570\u636e\u4e2d\u8054\u5408\u4f30\u8ba1\u6f5c\u5728\u4efb\u52a1\u96be\u5ea6\u548c\u6a21\u578b\u80fd\u529b\uff0c\u53d1\u73b0\u6f5c\u5728\u4efb\u52a1\u96be\u5ea6\u4e0e\u4eba\u7c7b\u5b8c\u6210\u65f6\u95f4\u7684\u5bf9\u6570\u5448\u7ebf\u6027\u5173\u7cfb", "result": "\u6f5c\u5728\u4efb\u52a1\u96be\u5ea6\u4e0e\u4eba\u7c7b\u5b8c\u6210\u65f6\u95f4\u7684\u5bf9\u6570\u5448\u7ebf\u6027\u5173\u7cfb\uff0c\u53ef\u4ee5\u4ece\u6a21\u578b\u6027\u80fd\u5355\u72ec\u63a8\u65ad\u65b0\u57fa\u51c6\u7684\u4eba\u7c7b\u4efb\u52a1\u5b8c\u6210\u65f6\u95f4\uff1b\u524d\u6cbf\u6a21\u578b\u80fd\u529b\u9884\u6d4b\u663e\u793a\uff0c50%\u53ef\u89e3\u51b3\u4efb\u52a1\u8303\u56f4\u5927\u7ea6\u6bcf6\u4e2a\u6708\u7ffb\u500d", "conclusion": "BRIDGE\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u5fc3\u7406\u6d4b\u91cf\u6846\u67b6\uff0c\u80fd\u591f\u4ece\u6a21\u578b\u54cd\u5e94\u4e2d\u5b66\u4e60\u4efb\u52a1\u96be\u5ea6\u5e76\u5c06\u5176\u951a\u5b9a\u5230\u4eba\u7c7b\u4efb\u52a1\u5b8c\u6210\u65f6\u95f4\uff0c\u4e3aAI\u7cfb\u7edf\u771f\u5b9e\u4e16\u754c\u80fd\u529b\u8bc4\u4f30\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u65b9\u6cd5"}}
{"id": "2602.07408", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2602.07408", "abs": "https://arxiv.org/abs/2602.07408", "authors": ["Hyomin Kim", "Sang-Yeon Hwang", "Jaechang Lim", "Yinhua Piao", "Yunhak Oh", "Woo Youn Kim", "Chanyoung Park", "Sungsoo Ahn", "Junhyeok Jeon"], "title": "Progressive Multi-Agent Reasoning for Biological Perturbation Prediction", "comment": "17 pages, 4 figures, 9 tables", "summary": "Predicting gene regulation responses to biological perturbations requires reasoning about underlying biological causalities. While large language models (LLMs) show promise for such tasks, they are often overwhelmed by the entangled nature of high-dimensional perturbation results. Moreover, recent works have primarily focused on genetic perturbations in single-cell experiments, leaving bulk-cell chemical perturbations, which is central to drug discovery, largely unexplored. Motivated by this, we present LINCSQA, a novel benchmark for predicting target gene regulation under complex chemical perturbations in bulk-cell environments. We further propose PBio-Agent, a multi-agent framework that integrates difficulty-aware task sequencing with iterative knowledge refinement. Our key insight is that genes affected by the same perturbation share causal structure, allowing confidently predicted genes to contextualize more challenging cases. The framework employs specialized agents enriched with biological knowledge graphs, while a synthesis agent integrates outputs and specialized judges ensure logical coherence. PBio-Agent outperforms existing baselines on both LINCSQA and PerturbQA, enabling even smaller models to predict and explain complex biological processes without additional training.", "AI": {"tldr": "LINCSQA\u662f\u7528\u4e8e\u9884\u6d4b\u6279\u91cf\u7ec6\u80de\u5316\u5b66\u6270\u52a8\u4e0b\u9776\u57fa\u56e0\u8c03\u63a7\u7684\u65b0\u57fa\u51c6\uff0cPBio-Agent\u662f\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u96be\u5ea6\u611f\u77e5\u4efb\u52a1\u6392\u5e8f\u548c\u8fed\u4ee3\u77e5\u8bc6\u7cbe\u70bc\u6765\u63d0\u5347\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u5355\u7ec6\u80de\u9057\u4f20\u6270\u52a8\uff0c\u800c\u836f\u7269\u53d1\u73b0\u6838\u5fc3\u7684\u6279\u91cf\u7ec6\u80de\u5316\u5b66\u6270\u52a8\u7814\u7a76\u4e0d\u8db3\u3002\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u9ad8\u7ef4\u6270\u52a8\u7ed3\u679c\u65f6\u5bb9\u6613\u88ab\u7ea0\u7f20\u7684\u751f\u7269\u5b66\u5173\u7cfb\u6df9\u6ca1\uff0c\u9700\u8981\u65b0\u7684\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e00\u6311\u6218\u3002", "method": "\u63d0\u51faPBio-Agent\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u5305\u542b\u96be\u5ea6\u611f\u77e5\u4efb\u52a1\u6392\u5e8f\u548c\u8fed\u4ee3\u77e5\u8bc6\u7cbe\u70bc\u3002\u5173\u952e\u6d1e\u89c1\u662f\uff1a\u53d7\u76f8\u540c\u6270\u52a8\u5f71\u54cd\u7684\u57fa\u56e0\u5171\u4eab\u56e0\u679c\u7ed3\u6784\uff0c\u4f7f\u7f6e\u4fe1\u9884\u6d4b\u7684\u57fa\u56e0\u80fd\u4e3a\u66f4\u56f0\u96be\u6848\u4f8b\u63d0\u4f9b\u4e0a\u4e0b\u6587\u3002\u6846\u67b6\u4f7f\u7528\u751f\u7269\u77e5\u8bc6\u56fe\u8c31\u589e\u5f3a\u7684\u4e13\u95e8\u667a\u80fd\u4f53\uff0c\u5408\u6210\u667a\u80fd\u4f53\u6574\u5408\u8f93\u51fa\uff0c\u4e13\u95e8\u5224\u65ad\u5668\u786e\u4fdd\u903b\u8f91\u4e00\u81f4\u6027\u3002", "result": "PBio-Agent\u5728LINCSQA\u548cPerturbQA\u57fa\u51c6\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5373\u4f7f\u8f83\u5c0f\u7684\u6a21\u578b\u4e5f\u80fd\u5728\u6ca1\u6709\u989d\u5916\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\u9884\u6d4b\u548c\u89e3\u91ca\u590d\u6742\u7684\u751f\u7269\u8fc7\u7a0b\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u586b\u8865\u4e86\u6279\u91cf\u7ec6\u80de\u5316\u5b66\u6270\u52a8\u9884\u6d4b\u7684\u7a7a\u767d\uff0c\u63d0\u51fa\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\u901a\u8fc7\u5229\u7528\u57fa\u56e0\u95f4\u7684\u56e0\u679c\u7ed3\u6784\u5171\u4eab\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u9ad8\u7ef4\u6270\u52a8\u7ed3\u679c\u7684\u590d\u6742\u6027\u6311\u6218\uff0c\u4e3a\u836f\u7269\u53d1\u73b0\u63d0\u4f9b\u4e86\u6709\u529b\u5de5\u5177\u3002"}}
{"id": "2602.07338", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07338", "abs": "https://arxiv.org/abs/2602.07338", "authors": ["Geng Liu", "Fei Zhu", "Rong Feng", "Changyi Ma", "Shiqi Wang", "Gaofeng Meng"], "title": "Intent Mismatch Causes LLMs to Get Lost in Multi-Turn Conversation", "comment": null, "summary": "Multi-turn conversation has emerged as a predominant interaction paradigm for Large Language Models (LLMs). Users often employ follow-up questions to refine their intent, expecting LLMs to adapt dynamically. However, recent research reveals that LLMs suffer a substantial performance drop in multi-turn settings compared to single-turn interactions with fully specified instructions, a phenomenon termed ``Lost in Conversation'' (LiC). While this prior work attributes LiC to model unreliability, we argue that the root cause lies in an intent alignment gap rather than intrinsic capability deficits. In this paper, we first demonstrate that LiC is not a failure of model capability but rather a breakdown in interaction between users and LLMs. We theoretically show that scaling model size or improving training alone cannot resolve this gap, as it arises from structural ambiguity in conversational context rather than representational limitations. To address this, we propose to decouple intent understanding from task execution through a Mediator-Assistant architecture. By utilizing an experience-driven Mediator to explicate user inputs into explicit, well-structured instructions based on historical interaction patterns, our approach effectively bridges the gap between vague user intent and model interpretation. Experimental results demonstrate that this method significantly mitigates performance degradation in multi-turn conversations across diverse LLMs.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\"\u5bf9\u8bdd\u8ff7\u5931\"\u73b0\u8c61\u7684\u6839\u672c\u539f\u56e0\u4e0d\u662f\u6a21\u578b\u80fd\u529b\u7f3a\u9677\uff0c\u800c\u662f\u7528\u6237\u610f\u56fe\u4e0e\u6a21\u578b\u7406\u89e3\u4e4b\u95f4\u7684\u5bf9\u9f50\u5dee\u8ddd\uff0c\u5e76\u63d0\u51fa\u4e86\u57fa\u4e8e\u7ecf\u9a8c\u9a71\u52a8\u8c03\u89e3\u5668\u7684\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u591a\u8f6e\u5bf9\u8bdd\u5df2\u6210\u4e3aLLMs\u7684\u4e3b\u8981\u4ea4\u4e92\u8303\u5f0f\uff0c\u4f46\u7814\u7a76\u53d1\u73b0LLMs\u5728\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u7684\u8868\u73b0\u663e\u8457\u4f4e\u4e8e\u5355\u8f6e\u5b8c\u6574\u6307\u4ee4\uff0c\u8fd9\u79cd\u73b0\u8c61\u88ab\u79f0\u4e3a\"\u5bf9\u8bdd\u8ff7\u5931\"\u3002\u73b0\u6709\u7814\u7a76\u5c06\u5176\u5f52\u56e0\u4e8e\u6a21\u578b\u4e0d\u53ef\u9760\u6027\uff0c\u4f46\u672c\u6587\u8ba4\u4e3a\u6839\u672c\u539f\u56e0\u5728\u4e8e\u610f\u56fe\u5bf9\u9f50\u5dee\u8ddd\u800c\u975e\u5185\u5728\u80fd\u529b\u7f3a\u9677\u3002", "method": "\u63d0\u51fa\u8c03\u89e3\u5668-\u52a9\u624b\u67b6\u6784\uff0c\u901a\u8fc7\u7ecf\u9a8c\u9a71\u52a8\u7684\u8c03\u89e3\u5668\u5c06\u6a21\u7cca\u7684\u7528\u6237\u8f93\u5165\u8f6c\u5316\u4e3a\u660e\u786e\u3001\u7ed3\u6784\u5316\u7684\u6307\u4ee4\u3002\u8c03\u89e3\u5668\u57fa\u4e8e\u5386\u53f2\u4ea4\u4e92\u6a21\u5f0f\u6765\u7406\u89e3\u7528\u6237\u610f\u56fe\uff0c\u5c06\u610f\u56fe\u7406\u89e3\u4e0e\u4efb\u52a1\u6267\u884c\u89e3\u8026\uff0c\u4ece\u800c\u5f25\u5408\u7528\u6237\u610f\u56fe\u4e0e\u6a21\u578b\u89e3\u91ca\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u663e\u8457\u51cf\u8f7b\u591a\u79cdLLMs\u5728\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u7684\u6027\u80fd\u4e0b\u964d\u95ee\u9898\uff0c\u9a8c\u8bc1\u4e86\u8c03\u89e3\u5668-\u52a9\u624b\u67b6\u6784\u7684\u6709\u6548\u6027\u3002", "conclusion": "\"\u5bf9\u8bdd\u8ff7\u5931\"\u73b0\u8c61\u6e90\u4e8e\u4ea4\u4e92\u8fc7\u7a0b\u4e2d\u7684\u610f\u56fe\u5bf9\u9f50\u5dee\u8ddd\u800c\u975e\u6a21\u578b\u80fd\u529b\u9650\u5236\u3002\u901a\u8fc7\u8c03\u89e3\u5668\u67b6\u6784\u5c06\u610f\u56fe\u7406\u89e3\u4e0e\u4efb\u52a1\u6267\u884c\u89e3\u8026\uff0c\u53ef\u4ee5\u6709\u6548\u89e3\u51b3\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u7684\u6027\u80fd\u4e0b\u964d\u95ee\u9898\uff0c\u8fd9\u4e3a\u6539\u8fdbLLMs\u7684\u5bf9\u8bdd\u4ea4\u4e92\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u5411\u3002"}}
{"id": "2602.07026", "categories": ["cs.CV", "cs.AI", "cs.MM"], "pdf": "https://arxiv.org/pdf/2602.07026", "abs": "https://arxiv.org/abs/2602.07026", "authors": ["Xiaomin Yu", "Yi Xin", "Wenjie Zhang", "Chonghan Liu", "Hanzhen Zhao", "Xiaoxing Hu", "Xinlei Yu", "Ziyue Qiao", "Hao Tang", "Xue Yang", "Xiaobin Hu", "Chengwei Qin", "Hui Xiong", "Yu Qiao", "Shuicheng Yan"], "title": "Modality Gap-Driven Subspace Alignment Training Paradigm For Multimodal Large Language Models", "comment": null, "summary": "Despite the success of multimodal contrastive learning in aligning visual and linguistic representations, a persistent geometric anomaly, the Modality Gap, remains: embeddings of distinct modalities expressing identical semantics occupy systematically offset regions. Prior approaches to bridge this gap are largely limited by oversimplified isotropic assumptions, hindering their application in large-scale scenarios. In this paper, we address these limitations by precisely characterizing the geometric shape of the modality gap and leveraging it for efficient model scaling. First, we propose the Fixed-frame Modality Gap Theory, which decomposes the modality gap within a frozen reference frame into stable biases and anisotropic residuals. Guided by this precise modeling, we introduce ReAlign, a training-free modality alignment strategy. Utilizing statistics from massive unpaired data, ReAlign aligns text representation into the image representation distribution via a three-step process comprising Anchor, Trace, and Centroid Alignment, thereby explicitly rectifying geometric misalignment. Building on ReAlign, we propose ReVision, a scalable training paradigm for Multimodal Large Language Models (MLLMs). ReVision integrates ReAlign into the pretraining stage, enabling the model to learn the distribution of visual representations from unpaired text before visual instruction tuning, without the need for large-scale, high-quality image-text pairs. Our framework demonstrates that statistically aligned unpaired data can effectively substitute for expensive image-text pairs, offering a robust path for the efficient scaling of MLLMs.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51faReVision\u6846\u67b6\uff0c\u901a\u8fc7ReAlign\u8bad\u7ec3\u514d\u8d39\u6a21\u6001\u5bf9\u9f50\u7b56\u7565\uff0c\u5229\u7528\u672a\u914d\u5bf9\u6570\u636e\u89e3\u51b3\u591a\u6a21\u6001\u5bf9\u6bd4\u5b66\u4e60\u4e2d\u7684\u6a21\u6001\u95f4\u9699\u95ee\u9898\uff0c\u4e3a\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u9ad8\u6548\u6269\u5c55\u8def\u5f84\u3002", "motivation": "\u591a\u6a21\u6001\u5bf9\u6bd4\u5b66\u4e60\u5728\u89c6\u89c9\u548c\u8bed\u8a00\u8868\u793a\u5bf9\u9f50\u65b9\u9762\u53d6\u5f97\u6210\u529f\uff0c\u4f46\u5b58\u5728\u6a21\u6001\u95f4\u9699\u95ee\u9898\uff1a\u8868\u8fbe\u76f8\u540c\u8bed\u4e49\u7684\u4e0d\u540c\u6a21\u6001\u5d4c\u5165\u5360\u636e\u7cfb\u7edf\u504f\u79fb\u533a\u57df\u3002\u5148\u524d\u65b9\u6cd5\u53d7\u9650\u4e8e\u8fc7\u5ea6\u7b80\u5316\u7684\u5404\u5411\u540c\u6027\u5047\u8bbe\uff0c\u96be\u4ee5\u5e94\u7528\u4e8e\u5927\u89c4\u6a21\u573a\u666f\u3002", "method": "1. \u63d0\u51fa\u56fa\u5b9a\u6846\u67b6\u6a21\u6001\u95f4\u9699\u7406\u8bba\uff0c\u5c06\u6a21\u6001\u95f4\u9699\u5206\u89e3\u4e3a\u7a33\u5b9a\u504f\u5dee\u548c\u5404\u5411\u5f02\u6027\u6b8b\u5dee\uff1b2. \u63d0\u51faReAlign\u8bad\u7ec3\u514d\u8d39\u6a21\u6001\u5bf9\u9f50\u7b56\u7565\uff0c\u5229\u7528\u5927\u89c4\u6a21\u672a\u914d\u5bf9\u6570\u636e\u7edf\u8ba1\u4fe1\u606f\uff0c\u901a\u8fc7\u951a\u70b9\u3001\u8ffd\u8e2a\u548c\u8d28\u5fc3\u5bf9\u9f50\u4e09\u6b65\u5c06\u6587\u672c\u8868\u793a\u5bf9\u9f50\u5230\u56fe\u50cf\u8868\u793a\u5206\u5e03\uff1b3. \u63d0\u51faReVision\u53ef\u6269\u5c55\u8bad\u7ec3\u8303\u5f0f\uff0c\u5c06ReAlign\u96c6\u6210\u5230\u9884\u8bad\u7ec3\u9636\u6bb5\uff0c\u8ba9\u6a21\u578b\u5728\u89c6\u89c9\u6307\u4ee4\u8c03\u4f18\u524d\u4ece\u672a\u914d\u5bf9\u6587\u672c\u4e2d\u5b66\u4e60\u89c6\u89c9\u8868\u793a\u5206\u5e03\u3002", "result": "\u8be5\u6846\u67b6\u8bc1\u660e\u7edf\u8ba1\u5bf9\u9f50\u7684\u672a\u914d\u5bf9\u6570\u636e\u53ef\u4ee5\u6709\u6548\u66ff\u4ee3\u6602\u8d35\u7684\u56fe\u50cf-\u6587\u672c\u5bf9\uff0c\u4e3a\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u9ad8\u6548\u6269\u5c55\u63d0\u4f9b\u4e86\u7a33\u5065\u8def\u5f84\u3002", "conclusion": "\u901a\u8fc7\u7cbe\u786e\u5efa\u6a21\u6a21\u6001\u95f4\u9699\u51e0\u4f55\u5f62\u72b6\u5e76\u5229\u7528\u672a\u914d\u5bf9\u6570\u636e\u7edf\u8ba1\u4fe1\u606f\uff0cReVision\u6846\u67b6\u89e3\u51b3\u4e86\u591a\u6a21\u6001\u8868\u793a\u5bf9\u9f50\u4e2d\u7684\u51e0\u4f55\u5931\u51c6\u95ee\u9898\uff0c\u4e3a\u5927\u89c4\u6a21\u591a\u6a21\u6001\u6a21\u578b\u8bad\u7ec3\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.07274", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07274", "abs": "https://arxiv.org/abs/2602.07274", "authors": ["Kaijie Zhu", "Yuzhou Nie", "Yijiang Li", "Yiming Huang", "Jialian Wu", "Jiang Liu", "Ximeng Sun", "Zhenfei Yin", "Lun Wang", "Zicheng Liu", "Emad Barsoum", "William Yang Wang", "Wenbo Guo"], "title": "TermiGen: High-Fidelity Environment and Robust Trajectory Synthesis for Terminal Agents", "comment": null, "summary": "Executing complex terminal tasks remains a significant challenge for open-weight LLMs, constrained by two fundamental limitations. First, high-fidelity, executable training environments are scarce: environments synthesized from real-world repositories are not diverse and scalable, while trajectories synthesized by LLMs suffer from hallucinations. Second, standard instruction tuning uses expert trajectories that rarely exhibit simple mistakes common to smaller models. This creates a distributional mismatch, leaving student models ill-equipped to recover from their own runtime failures. To bridge these gaps, we introduce TermiGen, an end-to-end pipeline for synthesizing verifiable environments and resilient expert trajectories. Termi-Gen first generates functionally valid tasks and Docker containers via an iterative multi-agent refinement loop. Subsequently, we employ a Generator-Critic protocol that actively injects errors during trajectory collection, synthesizing data rich in error-correction cycles. Fine-tuned on this TermiGen-generated dataset, our TermiGen-Qwen2.5-Coder-32B achieves a 31.3% pass rate on TerminalBench. This establishes a new open-weights state-of-the-art, outperforming existing baselines and notably surpassing capable proprietary models such as o4-mini. Dataset is avaiable at https://github.com/ucsb-mlsec/terminal-bench-env.", "AI": {"tldr": "TermiGen\u662f\u4e00\u4e2a\u7aef\u5230\u7aef\u7ba1\u9053\uff0c\u7528\u4e8e\u5408\u6210\u53ef\u9a8c\u8bc1\u7684\u7ec8\u7aef\u73af\u5883\u548c\u5f39\u6027\u4e13\u5bb6\u8f68\u8ff9\uff0c\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u8fed\u4ee3\u751f\u6210\u6709\u6548\u4efb\u52a1\u548cDocker\u5bb9\u5668\uff0c\u5e76\u4f7f\u7528\u751f\u6210\u5668-\u6279\u8bc4\u5668\u534f\u8bae\u6ce8\u5165\u9519\u8bef\u6765\u8bad\u7ec3\u6a21\u578b\uff0c\u6700\u7ec8\u5728TerminalBench\u4e0a\u8fbe\u523031.3%\u7684\u901a\u8fc7\u7387\uff0c\u521b\u9020\u4e86\u65b0\u7684\u5f00\u6e90\u6a21\u578b\u6700\u4f73\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u5f00\u6e90\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6267\u884c\u590d\u6742\u7ec8\u7aef\u4efb\u52a1\u65f6\u9762\u4e34\u4e24\u5927\u9650\u5236\uff1a1\uff09\u7f3a\u4e4f\u9ad8\u4fdd\u771f\u3001\u53ef\u6267\u884c\u7684\u8bad\u7ec3\u73af\u5883\uff0c\u73b0\u6709\u73af\u5883\u8981\u4e48\u4e0d\u591f\u591a\u6837\u5316\u548c\u53ef\u6269\u5c55\uff0c\u8981\u4e48\u5b58\u5728\u5e7b\u89c9\u95ee\u9898\uff1b2\uff09\u6807\u51c6\u6307\u4ee4\u8c03\u4f18\u4f7f\u7528\u7684\u4e13\u5bb6\u8f68\u8ff9\u5f88\u5c11\u5305\u542b\u5c0f\u6a21\u578b\u5e38\u89c1\u7684\u7b80\u5355\u9519\u8bef\uff0c\u5bfc\u81f4\u5b66\u751f\u6a21\u578b\u65e0\u6cd5\u6709\u6548\u4ece\u81ea\u8eab\u8fd0\u884c\u65f6\u9519\u8bef\u4e2d\u6062\u590d\u3002", "method": "TermiGen\u91c7\u7528\u7aef\u5230\u7aef\u7ba1\u9053\uff1a\u9996\u5148\u901a\u8fc7\u8fed\u4ee3\u591a\u667a\u80fd\u4f53\u7cbe\u70bc\u5faa\u73af\u751f\u6210\u529f\u80fd\u6709\u6548\u7684\u4efb\u52a1\u548cDocker\u5bb9\u5668\uff1b\u7136\u540e\u4f7f\u7528\u751f\u6210\u5668-\u6279\u8bc4\u5668\u534f\u8bae\uff0c\u5728\u8f68\u8ff9\u6536\u96c6\u8fc7\u7a0b\u4e2d\u4e3b\u52a8\u6ce8\u5165\u9519\u8bef\uff0c\u5408\u6210\u5bcc\u542b\u9519\u8bef\u7ea0\u6b63\u5faa\u73af\u7684\u6570\u636e\u3002\u6700\u540e\u4f7f\u7528\u8fd9\u4e9b\u6570\u636e\u5bf9\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\u3002", "result": "\u4f7f\u7528TermiGen\u751f\u6210\u7684\u6570\u636e\u96c6\u5fae\u8c03\u7684TermiGen-Qwen2.5-Coder-32B\u6a21\u578b\u5728TerminalBench\u4e0a\u8fbe\u5230\u4e8631.3%\u7684\u901a\u8fc7\u7387\uff0c\u521b\u9020\u4e86\u65b0\u7684\u5f00\u6e90\u6a21\u578b\u6700\u4f73\u6027\u80fd\uff0c\u751a\u81f3\u8d85\u8fc7\u4e86o4-mini\u7b49\u4e13\u6709\u6a21\u578b\u3002", "conclusion": "TermiGen\u901a\u8fc7\u5408\u6210\u53ef\u9a8c\u8bc1\u73af\u5883\u548c\u5f39\u6027\u4e13\u5bb6\u8f68\u8ff9\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5f00\u6e90\u5927\u8bed\u8a00\u6a21\u578b\u5728\u7ec8\u7aef\u4efb\u52a1\u8bad\u7ec3\u4e2d\u7684\u73af\u5883\u7a00\u7f3a\u548c\u5206\u5e03\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5728\u590d\u6742\u7ec8\u7aef\u4efb\u52a1\u4e0a\u7684\u6267\u884c\u80fd\u529b\u3002"}}
{"id": "2602.07561", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.07561", "abs": "https://arxiv.org/abs/2602.07561", "authors": ["Quanjun Zhang", "Ye Shang", "Haichuan Hu", "Chunrong Fang", "Zhenyu Chen", "Liang Xiao"], "title": "ComPass: Contrastive Learning for Automated Patch Correctness Assessment in Program Repair", "comment": "30 pages, 3 figures", "summary": "Automated program repair (APR) attempts to reduce manual debugging efforts and plays a vital role in software maintenance. Despite remarkable progress, APR is still limited in generating overfitting patches, i.e., patches passing available test suites but incorrect. This issue, known as patch overfitting, has become a key concern in the APR community, with numerous approaches proposed to address it. Very recent work proposes a pre-trained language model (PLM)-based automated patch correctness assessment (APCA) approach, indicating the potential of such PLMs in reasoning about patch correctness. Despite being promising, it is still far from perfect due to various limitations, such as the training paradigm and training dataset. In this paper, we present ComPass, a PLM-based APCA approach that leverages contrastive learning and data augmentation to address the technical limitations of prior work. Our work is inspired by the opportunity to integrate contrastive learning with recent PLMs in the field of patch correctness assessment, where large-scale labeled patches are difficult to obtain. ComPass utilizes code transformation rules to generate semantic-preserving code snippets for both unlabeled pre-training corpus and labeled fine-tuning patches. ComPass then pre-trains PLMs with contrastive learning, which captures code features with the same semantics but different structures. ComPass finally integrates representation embeddings of patch code snippets and fine-tunes PLMs with a binary classifier jointly to assess patch code correctness. Experimental results on 2274 real-world patches from Defects4J demonstrate that ComPass achieves an accuracy of 88.35%, significantly outperforming state-of-the-art baseline APPT.", "AI": {"tldr": "ComPass\uff1a\u57fa\u4e8e\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u548c\u5bf9\u6bd4\u5b66\u4e60\u7684\u81ea\u52a8\u8865\u4e01\u6b63\u786e\u6027\u8bc4\u4f30\u65b9\u6cd5\uff0c\u901a\u8fc7\u4ee3\u7801\u8f6c\u6362\u89c4\u5219\u751f\u6210\u8bed\u4e49\u4fdd\u7559\u7684\u4ee3\u7801\u7247\u6bb5\uff0c\u7ed3\u5408\u5bf9\u6bd4\u5b66\u4e60\u548c\u6570\u636e\u589e\u5f3a\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8865\u4e01\u6b63\u786e\u6027\u8bc4\u4f30\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u5f53\u524d\u81ea\u52a8\u7a0b\u5e8f\u4fee\u590d\uff08APR\uff09\u5b58\u5728\u8865\u4e01\u8fc7\u62df\u5408\u95ee\u9898\uff0c\u5373\u8865\u4e01\u80fd\u901a\u8fc7\u73b0\u6709\u6d4b\u8bd5\u5957\u4ef6\u4f46\u5b9e\u9645\u4e0d\u6b63\u786e\u3002\u867d\u7136\u5df2\u6709\u57fa\u4e8e\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\uff08PLM\uff09\u7684\u81ea\u52a8\u8865\u4e01\u6b63\u786e\u6027\u8bc4\u4f30\uff08APCA\uff09\u65b9\u6cd5\uff0c\u4f46\u7531\u4e8e\u8bad\u7ec3\u8303\u5f0f\u548c\u6570\u636e\u96c6\u7684\u9650\u5236\uff0c\u6548\u679c\u4ecd\u4e0d\u7406\u60f3\u3002\u9700\u8981\u4e00\u79cd\u80fd\u66f4\u597d\u5229\u7528\u6709\u9650\u6807\u6ce8\u6570\u636e\u7684\u65b9\u6cd5\u6765\u63d0\u5347\u8865\u4e01\u6b63\u786e\u6027\u8bc4\u4f30\u7684\u51c6\u786e\u6027\u3002", "method": "ComPass\u91c7\u7528\u5bf9\u6bd4\u5b66\u4e60\u548c\u6570\u636e\u589e\u5f3a\u6280\u672f\uff1a1\uff09\u4f7f\u7528\u4ee3\u7801\u8f6c\u6362\u89c4\u5219\u4e3a\u65e0\u6807\u6ce8\u9884\u8bad\u7ec3\u8bed\u6599\u548c\u6807\u6ce8\u5fae\u8c03\u8865\u4e01\u751f\u6210\u8bed\u4e49\u4fdd\u7559\u7684\u4ee3\u7801\u7247\u6bb5\uff1b2\uff09\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u9884\u8bad\u7ec3PLMs\uff0c\u6355\u6349\u76f8\u540c\u8bed\u4e49\u4f46\u4e0d\u540c\u7ed3\u6784\u7684\u4ee3\u7801\u7279\u5f81\uff1b3\uff09\u96c6\u6210\u8865\u4e01\u4ee3\u7801\u7247\u6bb5\u7684\u8868\u793a\u5d4c\u5165\uff0c\u5e76\u4f7f\u7528\u4e8c\u5143\u5206\u7c7b\u5668\u8054\u5408\u5fae\u8c03PLMs\u6765\u8bc4\u4f30\u8865\u4e01\u6b63\u786e\u6027\u3002", "result": "\u5728Defects4J\u76842274\u4e2a\u771f\u5b9e\u4e16\u754c\u8865\u4e01\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cComPass\u8fbe\u5230\u4e8688.35%\u7684\u51c6\u786e\u7387\uff0c\u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5APPT\u3002", "conclusion": "ComPass\u901a\u8fc7\u7ed3\u5408\u5bf9\u6bd4\u5b66\u4e60\u548c\u6570\u636e\u589e\u5f3a\u6280\u672f\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709PLM-based APCA\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5728\u81ea\u52a8\u8865\u4e01\u6b63\u786e\u6027\u8bc4\u4f30\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u6539\u8fdb\uff0c\u4e3a\u51cf\u5c11\u81ea\u52a8\u7a0b\u5e8f\u4fee\u590d\u4e2d\u7684\u8865\u4e01\u8fc7\u62df\u5408\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.07572", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2602.07572", "abs": "https://arxiv.org/abs/2602.07572", "authors": ["Yanna Jiang", "Haiyu Deng", "Qin Wang", "Guangsheng Yu", "Xu Wang", "Yilin Sai", "Shiping Chen", "Wei Ni", "Ren Ping Liu"], "title": "SoK: Credential-Based Trust Management in Decentralized Ledger Systems", "comment": "Appear at Trustcom'25 (DOI: 10.1109/Trustcom66490.2025.00197)", "summary": "Trust management systems (TMS) are crucial for managing trust in distributed environments. The rise of decentralized systems and blockchain has sparked interest in credential-based decentralized trust management systems (DTMS). This paper bridges the gap between theory and practice through a systematic review of credential-based DTMS. We analyze existing DTMS solutions through multiple dimensions, including their architectural designs, credential mechanisms, and trust evaluation models. Our survey provides a detailed taxonomy of credential-based DTMS approaches and establishes comprehensive evaluation criteria for assessing DTMS implementations. Through extensive analysis of current systems and implementations, we identify critical challenges and promising research directions in the field. Our examination offers valuable insights for researchers and practitioners working on DTMS, particularly in areas such as access control, reputation systems, and blockchain-based trust frameworks.", "AI": {"tldr": "\u5bf9\u57fa\u4e8e\u51ed\u8bc1\u7684\u53bb\u4e2d\u5fc3\u5316\u4fe1\u4efb\u7ba1\u7406\u7cfb\u7edf\u8fdb\u884c\u7cfb\u7edf\u6027\u7efc\u8ff0\uff0c\u5206\u6790\u67b6\u6784\u8bbe\u8ba1\u3001\u51ed\u8bc1\u673a\u5236\u548c\u4fe1\u4efb\u8bc4\u4f30\u6a21\u578b\uff0c\u5efa\u7acb\u5206\u7c7b\u4f53\u7cfb\u548c\u8bc4\u4f30\u6807\u51c6", "motivation": "\u968f\u7740\u53bb\u4e2d\u5fc3\u5316\u7cfb\u7edf\u548c\u533a\u5757\u94fe\u7684\u5174\u8d77\uff0c\u57fa\u4e8e\u51ed\u8bc1\u7684\u53bb\u4e2d\u5fc3\u5316\u4fe1\u4efb\u7ba1\u7406\u7cfb\u7edf\u53d8\u5f97\u8d8a\u6765\u8d8a\u91cd\u8981\uff0c\u4f46\u7406\u8bba\u4e0e\u5b9e\u8df5\u4e4b\u95f4\u5b58\u5728\u5dee\u8ddd\uff0c\u9700\u8981\u8fdb\u884c\u7cfb\u7edf\u6027\u5206\u6790", "method": "\u901a\u8fc7\u7cfb\u7edf\u6027\u6587\u732e\u7efc\u8ff0\u65b9\u6cd5\uff0c\u4ece\u591a\u4e2a\u7ef4\u5ea6\u5206\u6790\u73b0\u6709\u7684DTMS\u89e3\u51b3\u65b9\u6848\uff0c\u5305\u62ec\u67b6\u6784\u8bbe\u8ba1\u3001\u51ed\u8bc1\u673a\u5236\u548c\u4fe1\u4efb\u8bc4\u4f30\u6a21\u578b\uff0c\u5efa\u7acb\u8be6\u7ec6\u7684\u5206\u7c7b\u4f53\u7cfb\u548c\u5168\u9762\u7684\u8bc4\u4f30\u6807\u51c6", "result": "\u63d0\u4f9b\u4e86\u57fa\u4e8e\u51ed\u8bc1\u7684DTMS\u65b9\u6cd5\u7684\u8be6\u7ec6\u5206\u7c7b\uff0c\u5efa\u7acb\u4e86\u8bc4\u4f30DTMS\u5b9e\u73b0\u7684\u7efc\u5408\u6807\u51c6\uff0c\u8bc6\u522b\u4e86\u8be5\u9886\u57df\u7684\u5173\u952e\u6311\u6218\u548c\u6709\u524d\u666f\u7684\u7814\u7a76\u65b9\u5411", "conclusion": "\u8be5\u7814\u7a76\u4e3aDTMS\u9886\u57df\u7684\u7814\u7a76\u8005\u548c\u5b9e\u8df5\u8005\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u89c1\u89e3\uff0c\u7279\u522b\u662f\u5728\u8bbf\u95ee\u63a7\u5236\u3001\u58f0\u8a89\u7cfb\u7edf\u548c\u57fa\u4e8e\u533a\u5757\u94fe\u7684\u4fe1\u4efb\u6846\u67b6\u7b49\u9886\u57df\uff0c\u6709\u52a9\u4e8e\u5f25\u5408\u7406\u8bba\u4e0e\u5b9e\u8df5\u4e4b\u95f4\u7684\u5dee\u8ddd"}}
{"id": "2602.07361", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2602.07361", "abs": "https://arxiv.org/abs/2602.07361", "authors": ["Long S. T. Nguyen", "Quan M. Bui", "Tin T. Ngo", "Quynh T. N. Vo", "Dung N. H. Le", "Tho T. Quan"], "title": "ViHERMES: A Graph-Grounded Multihop Question Answering Benchmark and System for Vietnamese Healthcare Regulations", "comment": "Accepted at ACIIDS 2026", "summary": "Question Answering (QA) over regulatory documents is inherently challenging due to the need for multihop reasoning across legally interdependent texts, a requirement that is particularly pronounced in the healthcare domain where regulations are hierarchically structured and frequently revised through amendments and cross-references. Despite recent progress in retrieval-augmented and graph-based QA methods, systematic evaluation in this setting remains limited, especially for low-resource languages such as Vietnamese, due to the lack of benchmark datasets that explicitly support multihop reasoning over healthcare regulations. In this work, we introduce the Vietnamese Healthcare Regulations-Multihop Reasoning Dataset (ViHERMES), a benchmark designed for multihop QA over Vietnamese healthcare regulatory documents. ViHERMES consists of high-quality question-answer pairs that require reasoning across multiple regulations and capture diverse dependency patterns, including amendment tracing, cross-document comparison, and procedural synthesis. To construct the dataset, we propose a controlled multihop QA generation pipeline based on semantic clustering and graph-inspired data mining, followed by large language model-based generation with structured evidence and reasoning annotations. We further present a graph-aware retrieval framework that models formal legal relations at the level of legal units and supports principled context expansion for legally valid and coherent answers. Experimental results demonstrate that ViHERMES provides a challenging benchmark for evaluating multihop regulatory QA systems and that the proposed graph-aware approach consistently outperforms strong retrieval-based baselines. The ViHERMES dataset and system implementation are publicly available at https://github.com/ura-hcmut/ViHERMES.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86ViHERMES\u6570\u636e\u96c6\uff0c\u8fd9\u662f\u4e00\u4e2a\u4e13\u95e8\u7528\u4e8e\u8d8a\u5357\u8bed\u533b\u7597\u6cd5\u89c4\u591a\u8df3\u95ee\u7b54\u7684\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u65e8\u5728\u89e3\u51b3\u533b\u7597\u6cd5\u89c4\u95ee\u7b54\u4e2d\u8de8\u6587\u6863\u63a8\u7406\u7684\u6311\u6218\u3002", "motivation": "\u533b\u7597\u6cd5\u89c4\u6587\u6863\u5177\u6709\u5c42\u7ea7\u7ed3\u6784\u548c\u9891\u7e41\u4fee\u8ba2\u7684\u7279\u70b9\uff0c\u9700\u8981\u8fdb\u884c\u591a\u8df3\u63a8\u7406\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\uff08\u5982\u8d8a\u5357\u8bed\uff09\u4e2d\u7f3a\u4e4f\u7cfb\u7edf\u8bc4\u4f30\uff0c\u7f3a\u5c11\u660e\u786e\u652f\u6301\u533b\u7597\u6cd5\u89c4\u591a\u8df3\u63a8\u7406\u7684\u57fa\u51c6\u6570\u636e\u96c6\u3002", "method": "\u63d0\u51fa\u4e86\u57fa\u4e8e\u8bed\u4e49\u805a\u7c7b\u548c\u56fe\u542f\u53d1\u6570\u636e\u6316\u6398\u7684\u53d7\u63a7\u591a\u8df3\u95ee\u7b54\u751f\u6210\u6d41\u7a0b\uff0c\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u5e26\u6709\u7ed3\u6784\u5316\u8bc1\u636e\u548c\u63a8\u7406\u6807\u6ce8\u7684\u95ee\u9898-\u7b54\u6848\u5bf9\u3002\u540c\u65f6\u63d0\u51fa\u4e86\u56fe\u611f\u77e5\u68c0\u7d22\u6846\u67b6\uff0c\u5728\u6cd5\u89c4\u5355\u5143\u5c42\u9762\u5efa\u6a21\u6b63\u5f0f\u6cd5\u5f8b\u5173\u7cfb\uff0c\u652f\u6301\u539f\u5219\u6027\u7684\u4e0a\u4e0b\u6587\u6269\u5c55\u3002", "result": "ViHERMES\u4e3a\u8bc4\u4f30\u591a\u8df3\u6cd5\u89c4\u95ee\u7b54\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5177\u6709\u6311\u6218\u6027\u7684\u57fa\u51c6\uff0c\u63d0\u51fa\u7684\u56fe\u611f\u77e5\u65b9\u6cd5\u5728\u5b9e\u9a8c\u4e2d\u4e00\u81f4\u4f18\u4e8e\u5f3a\u68c0\u7d22\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "ViHERMES\u586b\u8865\u4e86\u8d8a\u5357\u8bed\u533b\u7597\u6cd5\u89c4\u591a\u8df3\u63a8\u7406\u57fa\u51c6\u6570\u636e\u96c6\u7684\u7a7a\u767d\uff0c\u63d0\u51fa\u7684\u56fe\u611f\u77e5\u68c0\u7d22\u6846\u67b6\u80fd\u6709\u6548\u652f\u6301\u6cd5\u89c4\u6587\u6863\u7684\u591a\u8df3\u95ee\u7b54\u4efb\u52a1\uff0c\u76f8\u5173\u6570\u636e\u96c6\u548c\u7cfb\u7edf\u5b9e\u73b0\u5df2\u516c\u5f00\u63d0\u4f9b\u3002"}}
{"id": "2602.07027", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.07027", "abs": "https://arxiv.org/abs/2602.07027", "authors": ["Sanggeon Yun", "Ryozo Masukawa", "SungHeon Jeong", "Wenjun Huang", "Hanning Chen", "Mohsen Imani"], "title": "Fair Context Learning for Evidence-Balanced Test-Time Adaptation in Vision-Language Models", "comment": null, "summary": "Vision-Language Models (VLMs) such as CLIP enable strong zero-shot recognition but suffer substantial degradation under distribution shifts. Test-Time Adaptation (TTA) aims to improve robustness using only unlabeled test samples, yet most prompt-based TTA methods rely on entropy minimization -- an approach that can amplify spurious correlations and induce overconfident errors when classes share visual features. We propose Fair Context Learning (FCL), an episodic TTA framework that avoids entropy minimization by explicitly addressing shared-evidence bias. Motivated by our additive evidence decomposition assumption, FCL decouples adaptation into (i) augmentation-based exploration to identify plausible class candidates, and (ii) fairness-driven calibration that adapts text contexts to equalize sensitivity to common visual evidence. This fairness constraint mitigates partial feature obsession and enables effective calibration of text embeddings without relying on entropy reduction. Through extensive evaluation, we empirically validate our theoretical motivation and show that FCL achieves competitive adaptation performance relative to state-of-the-art TTA methods across diverse domain-shift and fine-grained benchmarks.", "AI": {"tldr": "FCL\u662f\u4e00\u79cd\u65e0\u9700\u71b5\u6700\u5c0f\u5316\u7684\u6d4b\u8bd5\u65f6\u9002\u5e94\u6846\u67b6\uff0c\u901a\u8fc7\u516c\u5e73\u6027\u7ea6\u675f\u89e3\u51b3\u5171\u4eab\u8bc1\u636e\u504f\u5dee\u95ee\u9898\uff0c\u63d0\u5347CLIP\u7b49\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u5206\u5e03\u504f\u79fb\u4e0b\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u63d0\u793a\u7684\u6d4b\u8bd5\u65f6\u9002\u5e94\u65b9\u6cd5\u4f9d\u8d56\u71b5\u6700\u5c0f\u5316\uff0c\u4f46\u5728\u7c7b\u522b\u5171\u4eab\u89c6\u89c9\u7279\u5f81\u65f6\u4f1a\u653e\u5927\u865a\u5047\u76f8\u5173\u6027\u5e76\u5bfc\u81f4\u8fc7\u5ea6\u81ea\u4fe1\u7684\u9519\u8bef\uff0c\u9700\u8981\u907f\u514d\u71b5\u6700\u5c0f\u5316\u6765\u89e3\u51b3\u5171\u4eab\u8bc1\u636e\u504f\u5dee\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u516c\u5e73\u4e0a\u4e0b\u6587\u5b66\u4e60(FCL)\u6846\u67b6\uff0c\u57fa\u4e8e\u52a0\u6027\u8bc1\u636e\u5206\u89e3\u5047\u8bbe\uff0c\u5c06\u9002\u5e94\u8fc7\u7a0b\u89e3\u8026\u4e3a\uff1a(1)\u57fa\u4e8e\u589e\u5f3a\u7684\u63a2\u7d22\u8bc6\u522b\u53ef\u4fe1\u7c7b\u522b\u5019\u9009\uff1b(2)\u516c\u5e73\u9a71\u52a8\u7684\u6821\u51c6\uff0c\u901a\u8fc7\u8c03\u6574\u6587\u672c\u4e0a\u4e0b\u6587\u4f7f\u6a21\u578b\u5bf9\u5e38\u89c1\u89c6\u89c9\u8bc1\u636e\u7684\u654f\u611f\u6027\u5747\u7b49\u5316\u3002", "result": "FCL\u5728\u591a\u79cd\u9886\u57df\u504f\u79fb\u548c\u7ec6\u7c92\u5ea6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u4e0e\u6700\u5148\u8fdb\u6d4b\u8bd5\u65f6\u9002\u5e94\u65b9\u6cd5\u76f8\u7ade\u4e89\u7684\u6027\u80fd\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7406\u8bba\u52a8\u673a\u3002", "conclusion": "\u901a\u8fc7\u907f\u514d\u71b5\u6700\u5c0f\u5316\u5e76\u5f15\u5165\u516c\u5e73\u6027\u7ea6\u675f\uff0cFCL\u80fd\u591f\u6709\u6548\u7f13\u89e3\u90e8\u5206\u7279\u5f81\u4f9d\u8d56\u95ee\u9898\uff0c\u5728\u4e0d\u4f9d\u8d56\u71b5\u51cf\u5c11\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u5bf9\u6587\u672c\u5d4c\u5165\u7684\u6709\u6548\u6821\u51c6\uff0c\u63d0\u5347\u6a21\u578b\u5728\u5206\u5e03\u504f\u79fb\u4e0b\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2602.07276", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.07276", "abs": "https://arxiv.org/abs/2602.07276", "authors": ["Pengrui Han", "Xueqiang Xu", "Keyang Xuan", "Peiyang Song", "Siru Ouyang", "Runchu Tian", "Yuqing Jiang", "Cheng Qian", "Pengcheng Jiang", "Jiashuo Sun", "Junxia Cui", "Ming Zhong", "Ge Liu", "Jiawei Han", "Jiaxuan You"], "title": "Steer2Adapt: Dynamically Composing Steering Vectors Elicits Efficient Adaptation of LLMs", "comment": null, "summary": "Activation steering has emerged as a promising approach for efficiently adapting large language models (LLMs) to downstream behaviors. However, most existing steering methods rely on a single static direction per task or concept, making them inflexible under task variation and inadequate for complex tasks that require multiple coordinated capabilities. To address this limitation, we propose STEER2ADAPT, a lightweight framework that adapts LLMs by composing steering vectors rather than learning new ones from scratch. In many domains (e.g., reasoning or safety), tasks share a small set of underlying concept dimensions. STEER2ADAPT captures these dimensions as a reusable, low-dimensional semantic prior subspace, and adapts to new tasks by dynamically discovering a linear combination of basis vectors from only a handful of examples. Experiments across 9 tasks and 3 models in both reasoning and safety domains demonstrate the effectiveness of STEER2ADAPT, achieving an average improvement of 8.2%. Extensive analyses further show that STEER2ADAPT is a data-efficient, stable, and transparent inference-time adaptation method for LLMs.", "AI": {"tldr": "STEER2ADAPT\uff1a\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u6846\u67b6\uff0c\u901a\u8fc7\u7ec4\u5408\u800c\u975e\u4ece\u5934\u5b66\u4e60\u65b0\u7684\u5f15\u5bfc\u5411\u91cf\u6765\u9002\u914d\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u5229\u7528\u53ef\u91cd\u7528\u7684\u4f4e\u7ef4\u8bed\u4e49\u5148\u9a8c\u5b50\u7a7a\u95f4\uff0c\u4ec5\u9700\u5c11\u91cf\u793a\u4f8b\u5373\u53ef\u52a8\u6001\u53d1\u73b0\u57fa\u5411\u91cf\u7684\u7ebf\u6027\u7ec4\u5408\u3002", "motivation": "\u73b0\u6709\u6fc0\u6d3b\u5f15\u5bfc\u65b9\u6cd5\u5927\u591a\u4f9d\u8d56\u6bcf\u4e2a\u4efb\u52a1\u6216\u6982\u5ff5\u7684\u5355\u4e00\u9759\u6001\u65b9\u5411\uff0c\u5728\u4efb\u52a1\u53d8\u5316\u65f6\u4e0d\u591f\u7075\u6d3b\uff0c\u4e14\u65e0\u6cd5\u5904\u7406\u9700\u8981\u591a\u4e2a\u534f\u8c03\u80fd\u529b\u7684\u590d\u6742\u4efb\u52a1\u3002", "method": "\u63d0\u51faSTEER2ADAPT\u6846\u67b6\uff0c\u5c06\u4efb\u52a1\u5171\u4eab\u7684\u5e95\u5c42\u6982\u5ff5\u7ef4\u5ea6\u6355\u83b7\u4e3a\u53ef\u91cd\u7528\u7684\u4f4e\u7ef4\u8bed\u4e49\u5148\u9a8c\u5b50\u7a7a\u95f4\uff0c\u901a\u8fc7\u5c11\u91cf\u793a\u4f8b\u52a8\u6001\u53d1\u73b0\u57fa\u5411\u91cf\u7684\u7ebf\u6027\u7ec4\u5408\u6765\u9002\u914d\u65b0\u4efb\u52a1\u3002", "result": "\u57289\u4e2a\u4efb\u52a1\u548c3\u4e2a\u6a21\u578b\u7684\u63a8\u7406\u548c\u5b89\u5168\u9886\u57df\u5b9e\u9a8c\u4e2d\uff0c\u5e73\u5747\u63d0\u53478.2%\uff0c\u8bc1\u660e\u8be5\u65b9\u6cd5\u5177\u6709\u6570\u636e\u6548\u7387\u9ad8\u3001\u7a33\u5b9a\u6027\u597d\u548c\u900f\u660e\u5ea6\u9ad8\u7684\u7279\u70b9\u3002", "conclusion": "STEER2ADAPT\u662f\u4e00\u79cd\u6709\u6548\u7684\u6570\u636e\u9ad8\u6548\u3001\u7a33\u5b9a\u4e14\u900f\u660e\u7684\u63a8\u7406\u65f6\u9002\u914d\u65b9\u6cd5\uff0c\u80fd\u591f\u901a\u8fc7\u7ec4\u5408\u73b0\u6709\u5f15\u5bfc\u5411\u91cf\u6765\u7075\u6d3b\u9002\u5e94\u590d\u6742\u4efb\u52a1\u3002"}}
{"id": "2602.07569", "categories": ["cs.SE", "cs.SI"], "pdf": "https://arxiv.org/pdf/2602.07569", "abs": "https://arxiv.org/abs/2602.07569", "authors": ["Eduardo C. Peixoto", "Hector Oliveira", "Geber L. Ramalho", "Cesar Fran\u00e7a"], "title": "Clarifying Core Dimensions in Digital Maturity Models: An Integrative Approach", "comment": "34 pages, 8 figures, 10 tables, 2 appendices", "summary": "Digital Transformation (DT) initiatives frequently face high failure rates, and while Digital Maturity Models (DMMs) offer potential solutions, they have notable shortcomings. Specifically, there is significant disparity in the dimensions considered relevant, a lack of clarity in their definitions, and uncertainty regarding their components. This study aims to provide a clearer understanding of DMMs by proposing integrative definitions of the most frequently used dimensions. Using a Systematic Mapping approach, including automatic search and snowballing techniques, we analyzed 76 DMMs to answer two Research Questions: (RQ1) What are the most frequent dimensions in DMMs? and (RQ2) How are these dimensions described, including their components? We reconcile varying interpretations of the ten most frequent dimensions -- Organization, Strategy, Technology, Culture, Process, Operations, People, Management, Customer, and Data -- and propose integrative definitions for each. Compared to previous analyses, this study provides a broader and more recent perspective on Digital Maturity Models.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u7cfb\u7edf\u6620\u5c04\u65b9\u6cd5\u5206\u6790\u4e8676\u4e2a\u6570\u5b57\u6210\u719f\u5ea6\u6a21\u578b\uff0c\u8bc6\u522b\u51fa\u6700\u5e38\u7528\u768410\u4e2a\u7ef4\u5ea6\uff08\u7ec4\u7ec7\u3001\u6218\u7565\u3001\u6280\u672f\u3001\u6587\u5316\u3001\u6d41\u7a0b\u3001\u8fd0\u8425\u3001\u4eba\u5458\u3001\u7ba1\u7406\u3001\u5ba2\u6237\u548c\u6570\u636e\uff09\uff0c\u5e76\u4e3a\u6bcf\u4e2a\u7ef4\u5ea6\u63d0\u51fa\u4e86\u6574\u5408\u6027\u5b9a\u4e49\uff0c\u4ee5\u89e3\u51b3\u73b0\u6709\u6a21\u578b\u7ef4\u5ea6\u5b9a\u4e49\u4e0d\u6e05\u6670\u3001\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\u3002", "motivation": "\u6570\u5b57\u5316\u8f6c\u578b\u9879\u76ee\u5931\u8d25\u7387\u9ad8\uff0c\u6570\u5b57\u6210\u719f\u5ea6\u6a21\u578b\u867d\u6709\u6f5c\u529b\u4f46\u5b58\u5728\u660e\u663e\u7f3a\u9677\uff1a\u7ef4\u5ea6\u9009\u62e9\u5dee\u5f02\u5927\u3001\u5b9a\u4e49\u4e0d\u6e05\u6670\u3001\u7ec4\u6210\u90e8\u5206\u4e0d\u660e\u786e\u3002\u9700\u8981\u5bf9\u8fd9\u4e9b\u6a21\u578b\u63d0\u4f9b\u66f4\u6e05\u6670\u7684\u7406\u89e3\u3002", "method": "\u91c7\u7528\u7cfb\u7edf\u6620\u5c04\u65b9\u6cd5\uff0c\u5305\u62ec\u81ea\u52a8\u641c\u7d22\u548c\u6eda\u96ea\u7403\u6280\u672f\uff0c\u5206\u6790\u4e8676\u4e2a\u6570\u5b57\u6210\u719f\u5ea6\u6a21\u578b\uff0c\u56de\u7b54\u4e24\u4e2a\u7814\u7a76\u95ee\u9898\uff1a1) DMM\u4e2d\u6700\u5e38\u7528\u7684\u7ef4\u5ea6\u662f\u4ec0\u4e48\uff1f2) \u8fd9\u4e9b\u7ef4\u5ea6\u5982\u4f55\u63cf\u8ff0\uff0c\u5305\u62ec\u5176\u7ec4\u6210\u90e8\u5206\uff1f", "result": "\u8bc6\u522b\u51fa10\u4e2a\u6700\u5e38\u7528\u7684\u7ef4\u5ea6\uff1a\u7ec4\u7ec7\u3001\u6218\u7565\u3001\u6280\u672f\u3001\u6587\u5316\u3001\u6d41\u7a0b\u3001\u8fd0\u8425\u3001\u4eba\u5458\u3001\u7ba1\u7406\u3001\u5ba2\u6237\u548c\u6570\u636e\u3002\u5bf9\u8fd9\u4e9b\u7ef4\u5ea6\u7684\u4e0d\u540c\u89e3\u91ca\u8fdb\u884c\u4e86\u8c03\u548c\uff0c\u5e76\u4e3a\u6bcf\u4e2a\u7ef4\u5ea6\u63d0\u51fa\u4e86\u6574\u5408\u6027\u5b9a\u4e49\u3002", "conclusion": "\u76f8\u6bd4\u4e4b\u524d\u7684\u5206\u6790\uff0c\u672c\u7814\u7a76\u4e3a\u6570\u5b57\u6210\u719f\u5ea6\u6a21\u578b\u63d0\u4f9b\u4e86\u66f4\u5e7f\u6cdb\u548c\u66f4\u65b0\u7684\u89c6\u89d2\uff0c\u901a\u8fc7\u660e\u786e\u7684\u6574\u5408\u6027\u5b9a\u4e49\u5e2e\u52a9\u89e3\u51b3\u73b0\u6709\u6a21\u578b\u7ef4\u5ea6\u5b9a\u4e49\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\uff0c\u4e3a\u6570\u5b57\u5316\u8f6c\u578b\u5b9e\u8df5\u63d0\u4f9b\u66f4\u6e05\u6670\u7684\u6307\u5bfc\u6846\u67b6\u3002"}}
{"id": "2602.07652", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07652", "abs": "https://arxiv.org/abs/2602.07652", "authors": ["Sai Puppala", "Ismail Hossain", "Md Jahangir Alam", "Yoonpyo Lee", "Jay Yoo", "Tanzim Ahad", "Syed Bahauddin Alam", "Sajedul Talukder"], "title": "Agent-Fence: Mapping Security Vulnerabilities Across Deep Research Agents", "comment": null, "summary": "Large language models are increasingly deployed as *deep agents* that plan, maintain persistent state, and invoke external tools, shifting safety failures from unsafe text to unsafe *trajectories*. We introduce **AgentFence**, an architecture-centric security evaluation that defines 14 trust-boundary attack classes spanning planning, memory, retrieval, tool use, and delegation, and detects failures via *trace-auditable conversation breaks* (unauthorized or unsafe tool use, wrong-principal actions, state/objective integrity violations, and attack-linked deviations). Holding the base model fixed, we evaluate eight agent archetypes under persistent multi-turn interaction and observe substantial architectural variation in mean security break rate (MSBR), ranging from $0.29 \\pm 0.04$ (LangGraph) to $0.51 \\pm 0.07$ (AutoGPT). The highest-risk classes are operational: Denial-of-Wallet ($0.62 \\pm 0.08$), Authorization Confusion ($0.54 \\pm 0.10$), Retrieval Poisoning ($0.47 \\pm 0.09$), and Planning Manipulation ($0.44 \\pm 0.11$), while prompt-centric classes remain below $0.20$ under standard settings. Breaks are dominated by boundary violations (SIV 31%, WPA 27%, UTI+UTA 24%, ATD 18%), and authorization confusion correlates with objective and tool hijacking ($\u03c1\\approx 0.63$ and $\u03c1\\approx 0.58$). AgentFence reframes agent security around what matters operationally: whether an agent stays within its goal and authority envelope over time.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faAgentFence\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30LLM\u667a\u80fd\u4f53\u5728\u89c4\u5212\u3001\u8bb0\u5fc6\u3001\u5de5\u5177\u8c03\u7528\u7b49\u4efb\u52a1\u4e2d\u7684\u5b89\u5168\u8fb9\u754c\u653b\u51fb\uff0c\u53d1\u73b0\u4e0d\u540c\u67b6\u6784\u7684\u667a\u80fd\u4f53\u5b89\u5168\u6f0f\u6d1e\u7387\u5dee\u5f02\u663e\u8457\uff080.29-0.51\uff09\uff0c\u4e3b\u8981\u98ce\u9669\u6765\u81ea\u64cd\u4f5c\u5c42\u9762\u7684\u653b\u51fb\u7c7b\u578b\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u88ab\u90e8\u7f72\u4e3a\u5177\u6709\u89c4\u5212\u3001\u72b6\u6001\u7ef4\u62a4\u548c\u5de5\u5177\u8c03\u7528\u80fd\u529b\u7684\u6df1\u5ea6\u667a\u80fd\u4f53\uff0c\u5b89\u5168\u5931\u6548\u4ece\u6587\u672c\u4e0d\u5b89\u5168\u8f6c\u5411\u8f68\u8ff9\u4e0d\u5b89\u5168\uff0c\u9700\u8981\u7cfb\u7edf\u8bc4\u4f30\u667a\u80fd\u4f53\u67b6\u6784\u7684\u5b89\u5168\u8fb9\u754c\u3002", "method": "\u63d0\u51faAgentFence\u67b6\u6784\u4e2d\u5fc3\u5b89\u5168\u8bc4\u4f30\u6846\u67b6\uff0c\u5b9a\u4e4914\u79cd\u4fe1\u4efb\u8fb9\u754c\u653b\u51fb\u7c7b\u522b\uff0c\u901a\u8fc7\u8f68\u8ff9\u53ef\u5ba1\u8ba1\u7684\u5bf9\u8bdd\u4e2d\u65ad\u68c0\u6d4b\u5b89\u5168\u5931\u6548\uff0c\u5728\u4fdd\u6301\u57fa\u7840\u6a21\u578b\u4e0d\u53d8\u7684\u60c5\u51b5\u4e0b\u8bc4\u4f30\u516b\u79cd\u667a\u80fd\u4f53\u67b6\u6784\u3002", "result": "\u4e0d\u540c\u667a\u80fd\u4f53\u67b6\u6784\u7684\u5e73\u5747\u5b89\u5168\u4e2d\u65ad\u7387\u5dee\u5f02\u663e\u8457\uff080.29-0.51\uff09\uff0c\u6700\u9ad8\u98ce\u9669\u7c7b\u522b\u4e3a\u94b1\u5305\u62d2\u7edd\u653b\u51fb\u3001\u6388\u6743\u6df7\u6dc6\u3001\u68c0\u7d22\u6bd2\u5316\u548c\u89c4\u5212\u64cd\u7eb5\uff0c\u5b89\u5168\u4e2d\u65ad\u4e3b\u8981\u7531\u8fb9\u754c\u8fdd\u89c4\u4e3b\u5bfc\u3002", "conclusion": "AgentFence\u5c06\u667a\u80fd\u4f53\u5b89\u5168\u91cd\u65b0\u5b9a\u4e49\u4e3a\u64cd\u4f5c\u5c42\u9762\u7684\u95ee\u9898\uff1a\u667a\u80fd\u4f53\u662f\u5426\u80fd\u5728\u957f\u671f\u8fd0\u884c\u4e2d\u4fdd\u6301\u5728\u76ee\u6807\u548c\u6743\u9650\u8303\u56f4\u5185\uff0c\u63ed\u793a\u4e86\u67b6\u6784\u8bbe\u8ba1\u5bf9\u5b89\u5168\u6027\u7684\u91cd\u8981\u5f71\u54cd\u3002"}}
{"id": "2602.08104", "categories": ["cs.AI", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2602.08104", "abs": "https://arxiv.org/abs/2602.08104", "authors": ["Risal Shahriar Shefin", "Debashis Gupta", "Thai Le", "Sarra Alqahtani"], "title": "Interpretable Failure Analysis in Multi-Agent Reinforcement Learning Systems", "comment": null, "summary": "Multi-Agent Reinforcement Learning (MARL) is increasingly deployed in safety-critical domains, yet methods for interpretable failure detection and attribution remain underdeveloped. We introduce a two-stage gradient-based framework that provides interpretable diagnostics for three critical failure analysis tasks: (1) detecting the true initial failure source (Patient-0); (2) validating why non-attacked agents may be flagged first due to domino effects; and (3) tracing how failures propagate through learned coordination pathways. Stage 1 performs interpretable per-agent failure detection via Taylor-remainder analysis of policy-gradient costs, declaring an initial Patient-0 candidate at the first threshold crossing. Stage 2 provides validation through geometric analysis of critic derivatives-first-order sensitivity and directional second-order curvature aggregated over causal windows to construct interpretable contagion graphs. This approach explains \"downstream-first\" detection anomalies by revealing pathways that amplify upstream deviations. Evaluated across 500 episodes in Simple Spread (3 and 5 agents) and 100 episodes in StarCraft II using MADDPG and HATRPO, our method achieves 88.2-99.4% Patient-0 detection accuracy while providing interpretable geometric evidence for detection decisions. By moving beyond black-box detection to interpretable gradient-level forensics, this framework offers practical tools for diagnosing cascading failures in safety-critical MARL systems.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u68af\u5ea6\u7684\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u7528\u4e8e\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u53ef\u89e3\u91ca\u6545\u969c\u68c0\u6d4b\u4e0e\u6eaf\u6e90\uff0c\u80fd\u8bc6\u522b\u521d\u59cb\u6545\u969c\u6e90\u3001\u9a8c\u8bc1\u591a\u7c73\u8bfa\u6548\u5e94\u3001\u8ffd\u8e2a\u6545\u969c\u4f20\u64ad\u8def\u5f84\u3002", "motivation": "\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u5728\u5b89\u5168\u5173\u952e\u9886\u57df\u5e94\u7528\u65e5\u76ca\u5e7f\u6cdb\uff0c\u4f46\u7f3a\u4e4f\u53ef\u89e3\u91ca\u7684\u6545\u969c\u68c0\u6d4b\u4e0e\u5f52\u56e0\u65b9\u6cd5\u3002\u73b0\u6709\u65b9\u6cd5\u591a\u4e3a\u9ed1\u76d2\u68c0\u6d4b\uff0c\u96be\u4ee5\u7406\u89e3\u6545\u969c\u4f20\u64ad\u673a\u5236\u548c\u68c0\u6d4b\u5f02\u5e38\u3002", "method": "\u4e24\u9636\u6bb5\u68af\u5ea6\u6846\u67b6\uff1a\u7b2c\u4e00\u9636\u6bb5\u901a\u8fc7\u7b56\u7565\u68af\u5ea6\u6210\u672c\u7684\u6cf0\u52d2\u4f59\u9879\u5206\u6790\u8fdb\u884c\u53ef\u89e3\u91ca\u7684\u667a\u80fd\u4f53\u7ea7\u6545\u969c\u68c0\u6d4b\uff0c\u786e\u5b9a\u521d\u59cb\u6545\u969c\u5019\u9009\uff1b\u7b2c\u4e8c\u9636\u6bb5\u901a\u8fc7\u8bc4\u8bba\u5bb6\u5bfc\u6570\u7684\u51e0\u4f55\u5206\u6790\uff08\u4e00\u9636\u654f\u611f\u6027\u548c\u4e8c\u9636\u66f2\u7387\uff09\u6784\u5efa\u53ef\u89e3\u91ca\u7684\u4f20\u67d3\u56fe\uff0c\u9a8c\u8bc1\u6545\u969c\u4f20\u64ad\u8def\u5f84\u3002", "result": "\u5728Simple Spread\uff083\u548c5\u667a\u80fd\u4f53\uff09\u548cStarCraft II\u73af\u5883\u4e2d\u8bc4\u4f30\uff0c\u4f7f\u7528MADDPG\u548cHATRPO\u7b97\u6cd5\uff0c\u5b9e\u73b0\u4e8688.2-99.4%\u7684\u521d\u59cb\u6545\u969c\u6e90\u68c0\u6d4b\u51c6\u786e\u7387\uff0c\u5e76\u63d0\u4f9b\u68c0\u6d4b\u51b3\u7b56\u7684\u51e0\u4f55\u8bc1\u636e\u3002", "conclusion": "\u8be5\u6846\u67b6\u8d85\u8d8a\u4e86\u9ed1\u76d2\u68c0\u6d4b\uff0c\u63d0\u4f9b\u4e86\u68af\u5ea6\u5c42\u9762\u7684\u53ef\u89e3\u91ca\u6cd5\u533b\u5206\u6790\u5de5\u5177\uff0c\u4e3a\u5b89\u5168\u5173\u952e\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u7684\u7ea7\u8054\u6545\u969c\u8bca\u65ad\u63d0\u4f9b\u4e86\u5b9e\u7528\u65b9\u6cd5\u3002"}}
{"id": "2602.07374", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07374", "abs": "https://arxiv.org/abs/2602.07374", "authors": ["Nisharg Nargund", "Priyesh Shukla"], "title": "TernaryLM: Memory-Efficient Language Modeling via Native 1-Bit Quantization with Adaptive Layer-wise Scaling", "comment": null, "summary": "Large language models (LLMs) achieve remarkable performance but demand substantial computational resources, limiting deployment on edge devices and resource-constrained environments. We present TernaryLM, a 132M parameter transformer architecture that employs native 1-bit ternary quantization {-1, 0, +1} during training, achieving significant memory reduction without sacrificing language modeling capability. Unlike post-training quantization approaches that quantize pre-trained full-precision models, TernaryLM learns quantization-aware representations from scratch using straight-through estimators and adaptive per-layer scaling factors. Our experiments demonstrate: (1) validation perplexity of 58.42 on TinyStories; (2) downstream transfer with 82.47 percent F1 on MRPC paraphrase detection; (3) 2.4x memory reduction (498MB vs 1197MB) with comparable inference latency; and (4) stable training dynamics across diverse corpora. We provide layer-wise quantization analysis showing that middle transformer layers exhibit highest compatibility with extreme quantization, informing future non-uniform precision strategies. Our results suggest that native 1-bit training is a promising direction for efficient neural language models. Code is available at https://github.com/1nisharg/TernaryLM-Memory-Efficient-Language-Modeling.", "AI": {"tldr": "TernaryLM\u662f\u4e00\u79cd132M\u53c2\u6570\u7684Transformer\u6a21\u578b\uff0c\u91c7\u7528\u539f\u751f1\u4f4d\u4e09\u5143\u91cf\u5316{-1, 0, +1}\u8bad\u7ec3\uff0c\u663e\u8457\u51cf\u5c11\u5185\u5b58\u5360\u7528\u800c\u4e0d\u727a\u7272\u8bed\u8a00\u5efa\u6a21\u80fd\u529b\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u9700\u8981\u5927\u91cf\u8ba1\u7b97\u8d44\u6e90\uff0c\u9650\u5236\u4e86\u5728\u8fb9\u7f18\u8bbe\u5907\u548c\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u7684\u90e8\u7f72\u3002\u9700\u8981\u5f00\u53d1\u66f4\u9ad8\u6548\u7684\u6a21\u578b\u6765\u964d\u4f4e\u5185\u5b58\u9700\u6c42\u3002", "method": "\u91c7\u7528\u539f\u751f1\u4f4d\u4e09\u5143\u91cf\u5316{-1, 0, +1}\u8bad\u7ec3\uff0c\u4f7f\u7528\u76f4\u901a\u4f30\u8ba1\u5668\u548c\u81ea\u9002\u5e94\u9010\u5c42\u7f29\u653e\u56e0\u5b50\u4ece\u5934\u5b66\u4e60\u91cf\u5316\u611f\u77e5\u8868\u793a\uff0c\u800c\u4e0d\u662f\u5bf9\u9884\u8bad\u7ec3\u6a21\u578b\u8fdb\u884c\u540e\u8bad\u7ec3\u91cf\u5316\u3002", "result": "1) TinyStories\u9a8c\u8bc1\u56f0\u60d1\u5ea658.42\uff1b2) MRPC\u590d\u8ff0\u68c0\u6d4bF1\u5206\u657082.47%\uff1b3) \u5185\u5b58\u51cf\u5c112.4\u500d(498MB vs 1197MB)\uff0c\u63a8\u7406\u5ef6\u8fdf\u76f8\u5f53\uff1b4) \u5728\u4e0d\u540c\u8bed\u6599\u5e93\u4e0a\u8bad\u7ec3\u7a33\u5b9a\uff1b5) \u4e2d\u95f4Transformer\u5c42\u4e0e\u6781\u7aef\u91cf\u5316\u517c\u5bb9\u6027\u6700\u9ad8\u3002", "conclusion": "\u539f\u751f1\u4f4d\u8bad\u7ec3\u662f\u9ad8\u6548\u795e\u7ecf\u8bed\u8a00\u6a21\u578b\u7684\u6709\u524d\u666f\u65b9\u5411\uff0c\u4e3a\u672a\u6765\u975e\u5747\u5300\u7cbe\u5ea6\u7b56\u7565\u63d0\u4f9b\u6307\u5bfc\u3002"}}
{"id": "2602.07308", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07308", "abs": "https://arxiv.org/abs/2602.07308", "authors": ["Sutapa Dey Tithi", "Nazia Alam", "Tahreem Yasir", "Yang Shi", "Xiaoyi Tian", "Min Chi", "Tiffany Barnes"], "title": "Adaptive Scaffolding for Cognitive Engagement in an Intelligent Tutoring System", "comment": null, "summary": "The ICAP framework defines four cognitive engagement levels: Passive, Active, Constructive, and Interactive, where increased cognitive engagement can yield improved learning. However, personalizing learning activities that elicit the optimal level of cognitive engagement remains a key challenge in intelligent tutoring systems (ITS). In this work, we develop and evaluate a system that adaptively scaffolds cognitive engagement by dynamically selecting worked examples in two different ICAP modes: (active) Guided examples and (constructive) Buggy examples. We compare Bayesian Knowledge Tracing (BKT) and Deep Reinforcement Learning (DRL) as adaptive methods against a non-adaptive baseline method for selecting example type in a logic ITS. Our experiment with 113 students demonstrates that both adaptive policies significantly improved student performance on test problems. BKT yielded the largest improvement in posttest scores for low prior knowledge students, helping them catch up with their high prior knowledge peers, whereas DRL yielded significantly higher posttest scores among high prior knowledge students. This paper contributes new insights into the complex interactions of cognitive engagement and adaptivity and their results on learning outcomes.", "AI": {"tldr": "\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u4e2a\u81ea\u9002\u5e94\u7cfb\u7edf\uff0c\u901a\u8fc7\u52a8\u6001\u9009\u62e9\u4e24\u79cdICAP\u6a21\u5f0f\u7684\u4f8b\u9898\uff08\u5f15\u5bfc\u5f0f\u4f8b\u9898\u548c\u9519\u8bef\u5f0f\u4f8b\u9898\uff09\u6765\u4f18\u5316\u8ba4\u77e5\u53c2\u4e0e\u5ea6\uff0c\u6bd4\u8f83\u4e86BKT\u548cDRL\u4e24\u79cd\u81ea\u9002\u5e94\u65b9\u6cd5\u4e0e\u57fa\u7ebf\u65b9\u6cd5\u5728\u903b\u8f91\u667a\u80fd\u8f85\u5bfc\u7cfb\u7edf\u4e2d\u7684\u6548\u679c\u3002", "motivation": "ICAP\u6846\u67b6\u5b9a\u4e49\u4e86\u56db\u79cd\u8ba4\u77e5\u53c2\u4e0e\u6c34\u5e73\uff0c\u66f4\u9ad8\u7684\u8ba4\u77e5\u53c2\u4e0e\u80fd\u5e26\u6765\u66f4\u597d\u7684\u5b66\u4e60\u6548\u679c\uff0c\u4f46\u5728\u667a\u80fd\u8f85\u5bfc\u7cfb\u7edf\u4e2d\u4e2a\u6027\u5316\u8bbe\u8ba1\u80fd\u6fc0\u53d1\u6700\u4f73\u8ba4\u77e5\u53c2\u4e0e\u5ea6\u7684\u5b66\u4e60\u6d3b\u52a8\u4ecd\u7136\u662f\u4e00\u4e2a\u5173\u952e\u6311\u6218\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u81ea\u9002\u5e94\u7cfb\u7edf\uff0c\u901a\u8fc7\u52a8\u6001\u9009\u62e9\u4e24\u79cdICAP\u6a21\u5f0f\u7684\u4f8b\u9898\u6765\u642d\u5efa\u8ba4\u77e5\u53c2\u4e0e\u5ea6\uff1a\u4e3b\u52a8\u6a21\u5f0f\u7684\u5f15\u5bfc\u5f0f\u4f8b\u9898\u548c\u5efa\u6784\u6a21\u5f0f\u7684\u9519\u8bef\u5f0f\u4f8b\u9898\u3002\u6bd4\u8f83\u4e86\u8d1d\u53f6\u65af\u77e5\u8bc6\u8ffd\u8e2a\uff08BKT\uff09\u548c\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff08DRL\uff09\u4f5c\u4e3a\u81ea\u9002\u5e94\u65b9\u6cd5\uff0c\u4e0e\u4e00\u4e2a\u975e\u81ea\u9002\u5e94\u57fa\u7ebf\u65b9\u6cd5\u5728\u903b\u8f91\u667a\u80fd\u8f85\u5bfc\u7cfb\u7edf\u4e2d\u9009\u62e9\u4f8b\u9898\u7c7b\u578b\u7684\u6548\u679c\u3002", "result": "\u5728113\u540d\u5b66\u751f\u53c2\u4e0e\u7684\u5b9e\u9a8c\u4e2d\uff0c\u4e24\u79cd\u81ea\u9002\u5e94\u7b56\u7565\u90fd\u663e\u8457\u63d0\u9ad8\u4e86\u5b66\u751f\u5728\u6d4b\u8bd5\u95ee\u9898\u4e0a\u7684\u8868\u73b0\u3002BKT\u5bf9\u4f4e\u5148\u9a8c\u77e5\u8bc6\u5b66\u751f\u7684\u540e\u6d4b\u6210\u7ee9\u63d0\u5347\u6700\u5927\uff0c\u5e2e\u52a9\u4ed6\u4eec\u8d76\u4e0a\u4e86\u9ad8\u5148\u9a8c\u77e5\u8bc6\u540c\u5b66\u7684\u6c34\u5e73\uff1b\u800cDRL\u5728\u9ad8\u5148\u9a8c\u77e5\u8bc6\u5b66\u751f\u4e2d\u4ea7\u751f\u4e86\u663e\u8457\u66f4\u9ad8\u7684\u540e\u6d4b\u6210\u7ee9\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u8ba4\u77e5\u53c2\u4e0e\u5ea6\u548c\u81ea\u9002\u5e94\u6027\u7684\u590d\u6742\u4ea4\u4e92\u53ca\u5176\u5bf9\u5b66\u4e60\u6210\u679c\u7684\u5f71\u54cd\u63d0\u4f9b\u4e86\u65b0\u7684\u89c1\u89e3\uff0c\u5c55\u793a\u4e86\u4e0d\u540c\u81ea\u9002\u5e94\u65b9\u6cd5\u5bf9\u4e0d\u540c\u77e5\u8bc6\u6c34\u5e73\u5b66\u751f\u7684\u5dee\u5f02\u5316\u6548\u679c\u3002"}}
{"id": "2602.07589", "categories": ["cs.SE", "cs.CY", "cs.ET", "quant-ph"], "pdf": "https://arxiv.org/pdf/2602.07589", "abs": "https://arxiv.org/abs/2602.07589", "authors": ["Andriy Miranskyy"], "title": "A Course on the Introduction to Quantum Software Engineering: Experience Report", "comment": null, "summary": "Quantum computing is increasingly practiced through programming, yet most educational offerings emphasize algorithmic or framework-level use rather than software engineering concerns such as testing, abstraction, tooling, and lifecycle management.\n  This paper reports on the design and first offering of a cross-listed undergraduate--graduate course that frames quantum computing through a software engineering lens, focusing on early-stage competence relevant to software engineering practice. The course integrates foundational quantum concepts with software engineering perspectives, emphasizing executable artifacts, empirical reasoning, and trade-offs arising from probabilistic behaviour, noise, and evolving toolchains. Evidence is drawn from instructor observations, student feedback, surveys, and analysis of student work.\n  Despite minimal prior exposure to quantum computing, students were able to engage productively with quantum software engineering topics once a foundational understanding of quantum information and quantum algorithms, expressed through executable artifacts, was established. This experience report contributes a modular course design, a scalable assessment model for mixed academic levels, and transferable lessons for software engineering educators developing quantum computing curricula.", "AI": {"tldr": "\u8be5\u8bba\u6587\u62a5\u544a\u4e86\u4e00\u95e8\u5c06\u91cf\u5b50\u8ba1\u7b97\u4e0e\u8f6f\u4ef6\u5de5\u7a0b\u7ed3\u5408\u7684\u4ea4\u53c9\u8bfe\u7a0b\u8bbe\u8ba1\uff0c\u5f3a\u8c03\u901a\u8fc7\u8f6f\u4ef6\u5de5\u7a0b\u89c6\u89d2\u6559\u6388\u91cf\u5b50\u8ba1\u7b97\uff0c\u5173\u6ce8\u6d4b\u8bd5\u3001\u62bd\u8c61\u3001\u5de5\u5177\u548c\u751f\u547d\u5468\u671f\u7ba1\u7406\u7b49\u5b9e\u8df5\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u91cf\u5b50\u8ba1\u7b97\u6559\u80b2\u4e3b\u8981\u5173\u6ce8\u7b97\u6cd5\u548c\u6846\u67b6\u5c42\u9762\uff0c\u7f3a\u4e4f\u5bf9\u8f6f\u4ef6\u5de5\u7a0b\u5b9e\u8df5\uff08\u5982\u6d4b\u8bd5\u3001\u62bd\u8c61\u3001\u5de5\u5177\u548c\u751f\u547d\u5468\u671f\u7ba1\u7406\uff09\u7684\u5173\u6ce8\u3002\u91cf\u5b50\u8ba1\u7b97\u65e5\u76ca\u901a\u8fc7\u7f16\u7a0b\u5b9e\u8df5\uff0c\u9700\u8981\u57f9\u517b\u5177\u5907\u8f6f\u4ef6\u5de5\u7a0b\u80fd\u529b\u7684\u91cf\u5b50\u5f00\u53d1\u8005\u3002", "method": "\u8bbe\u8ba1\u5e76\u5b9e\u65bd\u4e86\u4e00\u95e8\u9762\u5411\u672c\u79d1\u751f\u548c\u7814\u7a76\u751f\u7684\u4ea4\u53c9\u8bfe\u7a0b\uff0c\u5c06\u57fa\u7840\u91cf\u5b50\u6982\u5ff5\u4e0e\u8f6f\u4ef6\u5de5\u7a0b\u89c6\u89d2\u7ed3\u5408\uff0c\u5f3a\u8c03\u53ef\u6267\u884c\u5de5\u4ef6\u3001\u7ecf\u9a8c\u63a8\u7406\uff0c\u4ee5\u53ca\u5904\u7406\u6982\u7387\u884c\u4e3a\u3001\u566a\u58f0\u548c\u4e0d\u65ad\u6f14\u5316\u7684\u5de5\u5177\u94fe\u5e26\u6765\u7684\u6743\u8861\u3002", "result": "\u5c3d\u7ba1\u5b66\u751f\u5148\u524d\u5bf9\u91cf\u5b50\u8ba1\u7b97\u63a5\u89e6\u6709\u9650\uff0c\u4f46\u4e00\u65e6\u5efa\u7acb\u4e86\u901a\u8fc7\u53ef\u6267\u884c\u5de5\u4ef6\u8868\u8fbe\u7684\u5bf9\u91cf\u5b50\u4fe1\u606f\u548c\u91cf\u5b50\u7b97\u6cd5\u7684\u57fa\u7840\u7406\u89e3\uff0c\u4ed6\u4eec\u5c31\u80fd\u6709\u6548\u53c2\u4e0e\u91cf\u5b50\u8f6f\u4ef6\u5de5\u7a0b\u4e3b\u9898\u3002\u8bfe\u7a0b\u8bbe\u8ba1\u6a21\u5757\u5316\uff0c\u8bc4\u4f30\u6a21\u578b\u53ef\u6269\u5c55\uff0c\u9002\u5408\u4e0d\u540c\u5b66\u672f\u6c34\u5e73\u3002", "conclusion": "\u8be5\u8bfe\u7a0b\u8bbe\u8ba1\u4e3a\u8f6f\u4ef6\u5de5\u7a0b\u6559\u80b2\u8005\u5f00\u53d1\u91cf\u5b50\u8ba1\u7b97\u8bfe\u7a0b\u63d0\u4f9b\u4e86\u53ef\u8f6c\u79fb\u7684\u7ecf\u9a8c\uff0c\u8bc1\u660e\u4e86\u901a\u8fc7\u8f6f\u4ef6\u5de5\u7a0b\u89c6\u89d2\u6559\u6388\u91cf\u5b50\u8ba1\u7b97\u7684\u53ef\u884c\u6027\uff0c\u5f3a\u8c03\u53ef\u6267\u884c\u5de5\u4ef6\u548c\u7ecf\u9a8c\u63a8\u7406\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2602.07656", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2602.07656", "abs": "https://arxiv.org/abs/2602.07656", "authors": ["Abhishek Kumar Mishra", "Swadeep", "Guevara Noubir", "Mathieu Cunche"], "title": "AirCatch: Effectively tracing advanced tag-based trackers", "comment": null, "summary": "Tag-based tracking ecosystems help users locate lost items, but can be leveraged for unwanted tracking and stalking. Existing protocol-driven defenses and prior academic solutions largely assume stable identifiers or predictable beaconing. However, identifier-based defenses fundamentally break down against advanced rogue trackers that aggressively rotate identifiers. We present AirCatch, a passive detection system that exploits a physical-layer constraint: while logical identifiers can change arbitrarily fast, the transmitter's analog imprint remains stable and reappears as a compact and persistently occupied region in Carrier Frequency Offset (CFO) feature space. AirCatch advances the state of the art along three axes: (i) a novel, modulation-aware CFO fingerprint that augments packet-level CFO with content-independent CFO components that amplify device distinctiveness; (ii) a new tracking detection algorithm based on high core density and persistence that is robust to contamination and evasion through per-identifier segmentation; and (iii) an ultra-low-cost receiver, an approximately 10 dollar BLE SDR named BlePhasyr, built from commodity components, that makes RF fingerprinting based detection practical in resource-constrained deployments. We evaluate AirCatch across Apple, Google, Tile, and Samsung tag families in multi-hour captures, systematically stress-test evasion using a scenario generator over a grid of transmission and rotation periods, and validate in diverse real-world mobility traces including home and office commutes, public transport, car travel, and airport journeys while sweeping background tag density. Across these stress tests, AirCatch achieves no false positives and early detection over a wide range of adversarial configurations and environments, degrading gracefully only in extreme low-rate regimes that also reduce attacker utility.", "AI": {"tldr": "AirCatch\u662f\u4e00\u4e2a\u88ab\u52a8\u68c0\u6d4b\u7cfb\u7edf\uff0c\u5229\u7528\u7269\u7406\u5c42\u7279\u5f81\u68c0\u6d4b\u6076\u610f\u84dd\u7259\u8ddf\u8e2a\u5668\uff0c\u5373\u4f7f\u653b\u51fb\u8005\u5feb\u901f\u8f6e\u6362\u6807\u8bc6\u7b26\u4e5f\u80fd\u6709\u6548\u8bc6\u522b\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u534f\u8bae\u7684\u9632\u5fa1\u65b9\u6848\u548c\u5b66\u672f\u89e3\u51b3\u65b9\u6848\u4e3b\u8981\u5047\u8bbe\u6807\u8bc6\u7b26\u7a33\u5b9a\u6216\u4fe1\u6807\u53ef\u9884\u6d4b\uff0c\u4f46\u5f53\u653b\u51fb\u8005\u4f7f\u7528\u9ad8\u7ea7\u6076\u610f\u8ddf\u8e2a\u5668\u9891\u7e41\u8f6e\u6362\u6807\u8bc6\u7b26\u65f6\uff0c\u8fd9\u4e9b\u57fa\u4e8e\u6807\u8bc6\u7b26\u7684\u9632\u5fa1\u673a\u5236\u4f1a\u5931\u6548\u3002", "method": "AirCatch\u91c7\u7528\u4e09\u4e2a\u521b\u65b0\uff1a1) \u8c03\u5236\u611f\u77e5\u7684\u8f7d\u6ce2\u9891\u7387\u504f\u79fb\u6307\u7eb9\uff0c\u589e\u5f3a\u8bbe\u5907\u533a\u5206\u5ea6\uff1b2) \u57fa\u4e8e\u9ad8\u6838\u5fc3\u5bc6\u5ea6\u548c\u6301\u4e45\u6027\u7684\u8ddf\u8e2a\u68c0\u6d4b\u7b97\u6cd5\uff0c\u901a\u8fc7\u6309\u6807\u8bc6\u7b26\u5206\u6bb5\u6765\u62b5\u6297\u6c61\u67d3\u548c\u89c4\u907f\uff1b3) \u8d85\u4f4e\u6210\u672c\u63a5\u6536\u5668BlePhasyr\uff0c\u4f7f\u7528\u7ea610\u7f8e\u5143\u7684BLE SDR\u786c\u4ef6\u3002", "result": "\u5728Apple\u3001Google\u3001Tile\u548c\u4e09\u661f\u6807\u7b7e\u7cfb\u5217\u7684\u591a\u5c0f\u65f6\u6355\u83b7\u6d4b\u8bd5\u4e2d\uff0cAirCatch\u5b9e\u73b0\u4e86\u96f6\u8bef\u62a5\uff0c\u5e76\u5728\u5e7f\u6cdb\u7684\u5bf9\u6297\u914d\u7f6e\u548c\u73af\u5883\u4e2d\u5b9e\u73b0\u65e9\u671f\u68c0\u6d4b\uff0c\u4ec5\u5728\u6781\u7aef\u4f4e\u901f\u7387\u60c5\u51b5\u4e0b\u6027\u80fd\u4e0b\u964d\u3002", "conclusion": "AirCatch\u901a\u8fc7\u5229\u7528\u7269\u7406\u5c42\u7ea6\u675f\uff08\u8f7d\u6ce2\u9891\u7387\u504f\u79fb\u7279\u5f81\uff09\u6709\u6548\u68c0\u6d4b\u6076\u610f\u8ddf\u8e2a\u5668\uff0c\u5373\u4f7f\u653b\u51fb\u8005\u5feb\u901f\u8f6e\u6362\u903b\u8f91\u6807\u8bc6\u7b26\uff0c\u5176\u6a21\u62df\u7279\u5f81\u4ecd\u4fdd\u6301\u7a33\u5b9a\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u90e8\u7f72\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684RF\u6307\u7eb9\u68c0\u6d4b\u65b9\u6848\u3002"}}
{"id": "2602.08254", "categories": ["cs.AI", "cs.IR", "cs.MA"], "pdf": "https://arxiv.org/pdf/2602.08254", "abs": "https://arxiv.org/abs/2602.08254", "authors": ["Arman Aghaee", "Sepehr Asgarian", "Jouhyun Jeon"], "title": "SynthAgent: A Multi-Agent LLM Framework for Realistic Patient Simulation -- A Case Study in Obesity with Mental Health Comorbidities", "comment": "Presented in AAAI 2026 Singapore at the workshop of Health Intelligence", "summary": "Simulating high-fidelity patients offers a powerful avenue for studying complex diseases while addressing the challenges of fragmented, biased, and privacy-restricted real-world data. In this study, we introduce SynthAgent, a novel Multi-Agent System (MAS) framework designed to model obesity patients with comorbid mental disorders, including depression, anxiety, social phobia, and binge eating disorder. SynthAgent integrates clinical and medical evidence from claims data, population surveys, and patient-centered literature to construct personalized virtual patients enriched with personality traits that influence adherence, emotion regulation, and lifestyle behaviors. Through autonomous agent interactions, the system simulates disease progression, treatment response, and life management across diverse psychosocial contexts. Evaluation of more than 100 generated patients demonstrated that GPT-5 and Claude 4.5 Sonnet achieved the highest fidelity as the core engine in the proposed MAS framework, outperforming Gemini 2.5 Pro and DeepSeek-R1. SynthAgent thus provides a scalable and privacy-preserving framework for exploring patient journeys, behavioral dynamics, and decision-making processes in both medical and psychological domains.", "AI": {"tldr": "SynthAgent\u662f\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u6846\u67b6\uff0c\u7528\u4e8e\u6a21\u62df\u80a5\u80d6\u75c7\u5408\u5e76\u7cbe\u795e\u969c\u788d\u60a3\u8005\uff0c\u901a\u8fc7\u6574\u5408\u4e34\u5e8a\u6570\u636e\u6784\u5efa\u4e2a\u6027\u5316\u865a\u62df\u60a3\u8005\uff0c\u6a21\u62df\u75be\u75c5\u8fdb\u5c55\u548c\u6cbb\u7597\u53cd\u5e94\u3002", "motivation": "\u89e3\u51b3\u771f\u5b9e\u4e16\u754c\u6570\u636e\u788e\u7247\u5316\u3001\u504f\u89c1\u6027\u548c\u9690\u79c1\u9650\u5236\u7684\u95ee\u9898\uff0c\u4e3a\u7814\u7a76\u590d\u6742\u75be\u75c5\u63d0\u4f9b\u9ad8\u4fdd\u771f\u60a3\u8005\u6a21\u62df\u7684\u9014\u5f84\u3002", "method": "\u5f00\u53d1SynthAgent\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u6846\u67b6\uff0c\u6574\u5408\u533b\u4fdd\u7d22\u8d54\u6570\u636e\u3001\u4eba\u53e3\u8c03\u67e5\u548c\u60a3\u8005\u4e2d\u5fc3\u6587\u732e\uff0c\u6784\u5efa\u5177\u6709\u4eba\u683c\u7279\u8d28\u7684\u4e2a\u6027\u5316\u865a\u62df\u60a3\u8005\uff0c\u901a\u8fc7\u81ea\u4e3b\u667a\u80fd\u4f53\u4ea4\u4e92\u6a21\u62df\u75be\u75c5\u8fdb\u5c55\u3001\u6cbb\u7597\u53cd\u5e94\u548c\u751f\u6d3b\u7ba1\u7406\u3002", "result": "\u8bc4\u4f30100\u591a\u4e2a\u751f\u6210\u7684\u865a\u62df\u60a3\u8005\u663e\u793a\uff0cGPT-5\u548cClaude 4.5 Sonnet\u4f5c\u4e3a\u6838\u5fc3\u5f15\u64ce\u8868\u73b0\u6700\u4f73\uff0c\u4fdd\u771f\u5ea6\u6700\u9ad8\uff0c\u4f18\u4e8eGemini 2.5 Pro\u548cDeepSeek-R1\u3002", "conclusion": "SynthAgent\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u4e14\u4fdd\u62a4\u9690\u79c1\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u63a2\u7d22\u533b\u5b66\u548c\u5fc3\u7406\u9886\u57df\u7684\u60a3\u8005\u65c5\u7a0b\u3001\u884c\u4e3a\u52a8\u6001\u548c\u51b3\u7b56\u8fc7\u7a0b\u3002"}}
{"id": "2602.07375", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.07375", "abs": "https://arxiv.org/abs/2602.07375", "authors": ["Peiqi Yu", "Jinhao Wang", "Xinyi Sui", "Nam Ling", "Wei Wang", "Wei Jiang"], "title": "Efficient Post-Training Pruning of Large Language Models with Statistical Correction", "comment": "11 pages, 2 figures, 5 tables", "summary": "Post-training pruning is an effective approach for reducing the size and inference cost of large language models (LLMs), but existing methods often face a trade-off between pruning quality and computational efficiency. Heuristic pruning methods are efficient but sensitive to activation outliers, while reconstruction-based approaches improve fidelity at the cost of heavy computation. In this work, we propose a lightweight post-training pruning framework based on first-order statistical properties of model weights and activations. During pruning, channel-wise statistics are used to calibrate magnitude-based importance scores, reducing bias from activation-dominated channels. After pruning, we apply an analytic energy compensation to correct distributional distortions caused by weight removal. Both steps operate without retraining, gradients, or second-order information. Experiments across multiple LLM families, sparsity patterns, and evaluation tasks show that the proposed approach improves pruning performance while maintaining computational cost comparable to heuristic methods. The results suggest that simple statistical corrections can be effective for post-training pruning of LLMs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e00\u9636\u7edf\u8ba1\u7279\u6027\u7684\u8f7b\u91cf\u7ea7\u540e\u8bad\u7ec3\u526a\u679d\u6846\u67b6\uff0c\u901a\u8fc7\u7edf\u8ba1\u6821\u51c6\u548c\u80fd\u91cf\u8865\u507f\u6765\u63d0\u5347LLM\u526a\u679d\u8d28\u91cf\uff0c\u540c\u65f6\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u540e\u8bad\u7ec3\u526a\u679d\u65b9\u6cd5\u5728\u526a\u679d\u8d28\u91cf\u548c\u8ba1\u7b97\u6548\u7387\u4e4b\u95f4\u5b58\u5728\u6743\u8861\uff1a\u542f\u53d1\u5f0f\u65b9\u6cd5\u6548\u7387\u9ad8\u4f46\u5bf9\u6fc0\u6d3b\u5f02\u5e38\u503c\u654f\u611f\uff0c\u91cd\u6784\u65b9\u6cd5\u4fdd\u771f\u5ea6\u597d\u4f46\u8ba1\u7b97\u6210\u672c\u9ad8\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u6a21\u578b\u6743\u91cd\u548c\u6fc0\u6d3b\u4e00\u9636\u7edf\u8ba1\u7279\u6027\u7684\u8f7b\u91cf\u7ea7\u6846\u67b6\uff1a1) \u526a\u679d\u65f6\u4f7f\u7528\u901a\u9053\u7ea7\u7edf\u8ba1\u6821\u51c6\u57fa\u4e8e\u5e45\u503c\u7684\u91cd\u8981\u6027\u5206\u6570\uff0c\u51cf\u5c11\u6fc0\u6d3b\u4e3b\u5bfc\u901a\u9053\u7684\u504f\u5dee\uff1b2) \u526a\u679d\u540e\u5e94\u7528\u89e3\u6790\u80fd\u91cf\u8865\u507f\u6765\u7ea0\u6b63\u6743\u91cd\u79fb\u9664\u5f15\u8d77\u7684\u5206\u5e03\u5931\u771f\u3002\u6574\u4e2a\u8fc7\u7a0b\u65e0\u9700\u91cd\u8bad\u7ec3\u3001\u68af\u5ea6\u6216\u4e8c\u9636\u4fe1\u606f\u3002", "result": "\u5728\u591a\u4e2aLLM\u5bb6\u65cf\u3001\u7a00\u758f\u6a21\u5f0f\u548c\u8bc4\u4f30\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u63d0\u5347\u4e86\u526a\u679d\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u4e0e\u542f\u53d1\u5f0f\u65b9\u6cd5\u76f8\u5f53\u7684\u8ba1\u7b97\u6210\u672c\u3002", "conclusion": "\u7b80\u5355\u7684\u7edf\u8ba1\u6821\u6b63\u5bf9\u4e8eLLM\u7684\u540e\u8bad\u7ec3\u526a\u679d\u662f\u6709\u6548\u7684\uff0c\u80fd\u591f\u5728\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u7684\u540c\u65f6\u63d0\u5347\u526a\u679d\u8d28\u91cf\u3002"}}
{"id": "2602.07038", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.07038", "abs": "https://arxiv.org/abs/2602.07038", "authors": ["Yifan Ji", "Zhipeng Xu", "Zhenghao Liu", "Zulong Chen", "Qian Zhang", "Zhibo Yang", "Junyang Lin", "Yu Gu", "Ge Yu", "Maosong Sun"], "title": "UNIKIE-BENCH: Benchmarking Large Multimodal Models for Key Information Extraction in Visual Documents", "comment": null, "summary": "Key Information Extraction (KIE) from real-world documents remains challenging due to substantial variations in layout structures, visual quality, and task-specific information requirements. Recent Large Multimodal Models (LMMs) have shown promising potential for performing end-to-end KIE directly from document images. To enable a comprehensive and systematic evaluation across realistic and diverse application scenarios, we introduce UNIKIE-BENCH, a unified benchmark designed to rigorously evaluate the KIE capabilities of LMMs. UNIKIE-BENCH consists of two complementary tracks: a constrained-category KIE track with scenario-predefined schemas that reflect practical application needs, and an open-category KIE track that extracts any key information that is explicitly present in the document. Experiments on 15 state-of-the-art LMMs reveal substantial performance degradation under diverse schema definitions, long-tail key fields, and complex layouts, along with pronounced performance disparities across different document types and scenarios. These findings underscore persistent challenges in grounding accuracy and layout-aware reasoning for LMM-based KIE. All codes and datasets are available at https://github.com/NEUIR/UNIKIE-BENCH.", "AI": {"tldr": "UNIKIE-BENCH\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u5728\u6587\u6863\u5173\u952e\u4fe1\u606f\u63d0\u53d6\u4efb\u52a1\u4e2d\u6027\u80fd\u7684\u7edf\u4e00\u57fa\u51c6\uff0c\u5305\u542b\u7ea6\u675f\u7c7b\u522b\u548c\u5f00\u653e\u7c7b\u522b\u4e24\u4e2a\u4e92\u8865\u7684\u8bc4\u6d4b\u8f68\u9053\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u6587\u6863\u7684\u5173\u952e\u4fe1\u606f\u63d0\u53d6\u9762\u4e34\u5e03\u5c40\u7ed3\u6784\u591a\u6837\u3001\u89c6\u89c9\u8d28\u91cf\u5dee\u5f02\u548c\u4efb\u52a1\u7279\u5b9a\u9700\u6c42\u7b49\u6311\u6218\uff0c\u9700\u8981\u7cfb\u7edf\u8bc4\u4f30\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u5728\u6b64\u4efb\u52a1\u4e0a\u7684\u80fd\u529b\u3002", "method": "\u6784\u5efaUNIKIE-BENCH\u57fa\u51c6\uff0c\u5305\u542b\u4e24\u4e2a\u4e92\u8865\u8f68\u9053\uff1a\u7ea6\u675f\u7c7b\u522bKIE\u8f68\u9053\uff08\u57fa\u4e8e\u573a\u666f\u9884\u5b9a\u4e49\u6a21\u5f0f\uff09\u548c\u5f00\u653e\u7c7b\u522bKIE\u8f68\u9053\uff08\u63d0\u53d6\u6587\u6863\u4e2d\u4efb\u4f55\u663e\u5f0f\u5b58\u5728\u7684\u5173\u952e\u4fe1\u606f\uff09\u3002\u5bf915\u4e2a\u6700\u5148\u8fdb\u7684\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u8fdb\u884c\u5b9e\u9a8c\u8bc4\u4f30\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u6a21\u578b\u5728\u4e0d\u540c\u6a21\u5f0f\u5b9a\u4e49\u3001\u957f\u5c3e\u5173\u952e\u5b57\u6bb5\u548c\u590d\u6742\u5e03\u5c40\u4e0b\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff0c\u4e0d\u540c\u6587\u6863\u7c7b\u578b\u548c\u573a\u666f\u95f4\u5b58\u5728\u660e\u663e\u6027\u80fd\u5dee\u5f02\uff0c\u7a81\u663e\u4e86\u57fa\u4e8eLMM\u7684KIE\u5728\u57fa\u7840\u51c6\u786e\u6027\u548c\u5e03\u5c40\u611f\u77e5\u63a8\u7406\u65b9\u9762\u7684\u6301\u7eed\u6311\u6218\u3002", "conclusion": "UNIKIE-BENCH\u4e3a\u7cfb\u7edf\u8bc4\u4f30\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u7684\u5173\u952e\u4fe1\u606f\u63d0\u53d6\u80fd\u529b\u63d0\u4f9b\u4e86\u7edf\u4e00\u57fa\u51c6\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u5728\u5b9e\u9645\u5e94\u7528\u573a\u666f\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u7279\u522b\u662f\u5728\u6a21\u5f0f\u9002\u5e94\u6027\u3001\u957f\u5c3e\u5b57\u6bb5\u5904\u7406\u548c\u590d\u6742\u5e03\u5c40\u7406\u89e3\u65b9\u9762\u4ecd\u9700\u6539\u8fdb\u3002"}}
{"id": "2602.07339", "categories": ["cs.AI", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.07339", "abs": "https://arxiv.org/abs/2602.07339", "authors": ["Ruturaj Reddy", "Hrishav Bakul Barua", "Junn Yong Loo", "Thanh Thi Nguyen", "Ganesh Krishnasamy"], "title": "RAPiD: Real-time Deterministic Trajectory Planning via Diffusion Behavior Priors for Safe and Efficient Autonomous Driving", "comment": null, "summary": "Diffusion-based trajectory planners have demonstrated strong capability for modeling the multimodal nature of human driving behavior, but their reliance on iterative stochastic sampling poses critical challenges for real-time, safety-critical deployment. In this work, we present RAPiD, a deterministic policy extraction framework that distills a pretrained diffusion-based planner into an efficient policy while eliminating diffusion sampling. Using score-regularized policy optimization, we leverage the score function of a pre-trained diffusion planner as a behavior prior to regularize policy learning. To promote safety and passenger comfort, the policy is optimized using a critic trained to imitate a predictive driver controller, providing dense, safety-focused supervision beyond conventional imitation learning. Evaluations demonstrate that RAPiD achieves competitive performance on closed-loop nuPlan scenarios with an 8x speedup over diffusion baselines, while achieving state-of-the-art generalization among learning-based planners on the interPlan benchmark. The official website of this work is: https://github.com/ruturajreddy/RAPiD.", "AI": {"tldr": "RAPiD\u662f\u4e00\u4e2a\u786e\u5b9a\u6027\u7b56\u7565\u63d0\u53d6\u6846\u67b6\uff0c\u5c06\u9884\u8bad\u7ec3\u7684\u6269\u6563\u8f68\u8ff9\u89c4\u5212\u5668\u84b8\u998f\u4e3a\u9ad8\u6548\u7b56\u7565\uff0c\u6d88\u9664\u6269\u6563\u91c7\u6837\u8fc7\u7a0b\uff0c\u5b9e\u73b08\u500d\u52a0\u901f\u548c\u7ade\u4e89\u6027\u6027\u80fd\u3002", "motivation": "\u6269\u6563\u8f68\u8ff9\u89c4\u5212\u5668\u80fd\u5f88\u597d\u5efa\u6a21\u4eba\u7c7b\u9a7e\u9a76\u7684\u591a\u6a21\u6001\u884c\u4e3a\uff0c\u4f46\u5176\u4f9d\u8d56\u8fed\u4ee3\u968f\u673a\u91c7\u6837\u5bfc\u81f4\u5b9e\u65f6\u6027\u548c\u5b89\u5168\u6027\u6311\u6218\uff0c\u96be\u4ee5\u5728\u5b89\u5168\u5173\u952e\u573a\u666f\u90e8\u7f72\u3002", "method": "\u4f7f\u7528\u5206\u6570\u6b63\u5219\u5316\u7b56\u7565\u4f18\u5316\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u6269\u6563\u89c4\u5212\u5668\u7684\u8bc4\u5206\u51fd\u6570\u4f5c\u4e3a\u884c\u4e3a\u5148\u9a8c\u6765\u6b63\u5219\u5316\u7b56\u7565\u5b66\u4e60\uff1b\u901a\u8fc7\u6a21\u4eff\u9884\u6d4b\u6027\u9a7e\u9a76\u5458\u63a7\u5236\u5668\u7684\u8bc4\u8bba\u5bb6\u63d0\u4f9b\u5bc6\u96c6\u7684\u5b89\u5168\u76d1\u7763\u3002", "result": "\u5728nuPlan\u95ed\u73af\u573a\u666f\u4e2d\u5b9e\u73b0\u7ade\u4e89\u6027\u6027\u80fd\uff0c\u76f8\u6bd4\u6269\u6563\u57fa\u7ebf\u52a0\u901f8\u500d\uff1b\u5728interPlan\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u5b66\u4e60\u578b\u89c4\u5212\u5668\u7684\u6700\u5148\u8fdb\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "RAPiD\u6210\u529f\u5c06\u6269\u6563\u89c4\u5212\u5668\u84b8\u998f\u4e3a\u9ad8\u6548\u786e\u5b9a\u6027\u7b56\u7565\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u8ba1\u7b97\u6548\u7387\uff0c\u4e3a\u5b9e\u65f6\u5b89\u5168\u5173\u952e\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2602.07609", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07609", "abs": "https://arxiv.org/abs/2602.07609", "authors": ["Ruoyu Su", "Alexander Bakhtin", "Noman Ahmad", "Matteo Esposito", "Valentina Lenarduzzi", "Davide Taibi"], "title": "Evaluating Large Language Models for Detecting Architectural Decision Violations", "comment": null, "summary": "Architectural Decision Records (ADRs) play a central role in maintaining software architecture quality, yet many decision violations go unnoticed because projects lack both systematic documentation and automated detection mechanisms. Recent advances in Large Language Models (LLMs) open up new possibilities for automating architectural reasoning at scale. We investigated how effectively LLMs can identify decision violations in open-source systems by examining their agreement, accuracy, and inherent limitations. Our study analyzed 980 ADRs across 109 GitHub repositories using a multi-model pipeline in which one LLM primary screens potential decision violations, and three additional LLMs independently validate the reasoning. We assessed agreement, accuracy, precision, and recall, and complemented the quantitative findings with expert evaluation. The models achieved substantial agreement and strong accuracy for explicit, code-inferable decisions. Accuracy falls short for implicit or deployment-oriented decisions that depend on deployment configuration or organizational knowledge. Therefore, LLMs can meaningfully support validation of architectural decision compliance; however, they are not yet replacing human expertise for decisions not focused on code.", "AI": {"tldr": "LLMs\u80fd\u591f\u6709\u6548\u68c0\u6d4b\u4ee3\u7801\u76f8\u5173\u7684\u67b6\u6784\u51b3\u7b56\u8fdd\u89c4\uff0c\u4f46\u5bf9\u4e8e\u4f9d\u8d56\u90e8\u7f72\u914d\u7f6e\u6216\u7ec4\u7ec7\u77e5\u8bc6\u7684\u9690\u5f0f\u51b3\u7b56\u6548\u679c\u6709\u9650\uff0c\u5c1a\u4e0d\u80fd\u5b8c\u5168\u66ff\u4ee3\u4eba\u5de5\u9a8c\u8bc1\u3002", "motivation": "\u8f6f\u4ef6\u67b6\u6784\u51b3\u7b56\u8bb0\u5f55\uff08ADRs\uff09\u5bf9\u7ef4\u62a4\u67b6\u6784\u8d28\u91cf\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u8bb8\u591a\u51b3\u7b56\u8fdd\u89c4\u672a\u88ab\u53d1\u73b0\uff0c\u56e0\u4e3a\u9879\u76ee\u7f3a\u4e4f\u7cfb\u7edf\u5316\u6587\u6863\u548c\u81ea\u52a8\u5316\u68c0\u6d4b\u673a\u5236\u3002LLMs\u7684\u53d1\u5c55\u4e3a\u5927\u89c4\u6a21\u81ea\u52a8\u5316\u67b6\u6784\u63a8\u7406\u63d0\u4f9b\u4e86\u65b0\u53ef\u80fd\u3002", "method": "\u7814\u7a76\u5206\u6790\u4e86109\u4e2aGitHub\u4ed3\u5e93\u4e2d\u7684980\u4e2aADRs\uff0c\u91c7\u7528\u591a\u6a21\u578b\u6d41\u6c34\u7ebf\u65b9\u6cd5\uff1a\u4e00\u4e2aLLM\u4e3b\u6a21\u578b\u7b5b\u9009\u6f5c\u5728\u51b3\u7b56\u8fdd\u89c4\uff0c\u4e09\u4e2a\u989d\u5916LLM\u72ec\u7acb\u9a8c\u8bc1\u63a8\u7406\u8fc7\u7a0b\u3002\u8bc4\u4f30\u4e86\u6a21\u578b\u95f4\u4e00\u81f4\u6027\u3001\u51c6\u786e\u6027\u3001\u7cbe\u786e\u7387\u548c\u53ec\u56de\u7387\uff0c\u5e76\u8f85\u4ee5\u4e13\u5bb6\u8bc4\u4f30\u3002", "result": "LLMs\u5728\u663e\u5f0f\u3001\u53ef\u4ee3\u7801\u63a8\u65ad\u7684\u51b3\u7b56\u4e0a\u8868\u73b0\u51fa\u663e\u8457\u4e00\u81f4\u6027\u548c\u5f3a\u51c6\u786e\u6027\uff1b\u4f46\u5bf9\u4e8e\u4f9d\u8d56\u90e8\u7f72\u914d\u7f6e\u6216\u7ec4\u7ec7\u77e5\u8bc6\u7684\u9690\u5f0f\u6216\u90e8\u7f72\u5bfc\u5411\u51b3\u7b56\uff0c\u51c6\u786e\u6027\u4e0d\u8db3\u3002", "conclusion": "LLMs\u80fd\u591f\u6709\u610f\u4e49\u5730\u652f\u6301\u67b6\u6784\u51b3\u7b56\u5408\u89c4\u6027\u9a8c\u8bc1\uff0c\u4f46\u5bf9\u4e8e\u975e\u4ee3\u7801\u76f8\u5173\u7684\u51b3\u7b56\uff0c\u5c1a\u4e0d\u80fd\u66ff\u4ee3\u4eba\u7c7b\u4e13\u4e1a\u77e5\u8bc6\u3002"}}
{"id": "2602.07376", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.07376", "abs": "https://arxiv.org/abs/2602.07376", "authors": ["Usman Naseem", "Gautam Siddharth Kashyap", "Sushant Kumar Ray", "Rafiq Ali", "Ebad Shabbir", "Abdullah Mohammad"], "title": "Do Large Language Models Reflect Demographic Pluralism in Safety?", "comment": "Accepted at EACL Findings 2026", "summary": "Large Language Model (LLM) safety is inherently pluralistic, reflecting variations in moral norms, cultural expectations, and demographic contexts. Yet, existing alignment datasets such as ANTHROPIC-HH and DICES rely on demographically narrow annotator pools, overlooking variation in safety perception across communities. Demo-SafetyBench addresses this gap by modeling demographic pluralism directly at the prompt level, decoupling value framing from responses. In Stage I, prompts from DICES are reclassified into 14 safety domains (adapted from BEAVERTAILS) using Mistral 7B-Instruct-v0.3, retaining demographic metadata and expanding low-resource domains via Llama-3.1-8B-Instruct with SimHash-based deduplication, yielding 43,050 samples. In Stage II, pluralistic sensitivity is evaluated using LLMs-as-Raters-Gemma-7B, GPT-4o, and LLaMA-2-7B-under zero-shot inference. Balanced thresholds (delta = 0.5, tau = 10) achieve high reliability (ICC = 0.87) and low demographic sensitivity (DS = 0.12), confirming that pluralistic safety evaluation can be both scalable and demographically robust.", "AI": {"tldr": "Demo-SafetyBench\u662f\u4e00\u4e2a\u89e3\u51b3LLM\u5b89\u5168\u8bc4\u4f30\u4e2d\u4eba\u53e3\u7edf\u8ba1\u5b66\u591a\u6837\u6027\u4e0d\u8db3\u7684\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u91cd\u65b0\u5206\u7c7bDICES\u6570\u636e\u96c6\u523014\u4e2a\u5b89\u5168\u9886\u57df\uff0c\u5e76\u91c7\u7528\u5e73\u8861\u9608\u503c\u5b9e\u73b0\u9ad8\u53ef\u9760\u6027\u548c\u4f4e\u4eba\u53e3\u7edf\u8ba1\u5b66\u654f\u611f\u6027\u3002", "motivation": "\u73b0\u6709LLM\u5b89\u5168\u5bf9\u9f50\u6570\u636e\u96c6\uff08\u5982ANTHROPIC-HH\u548cDICES\uff09\u4f7f\u7528\u4eba\u53e3\u7edf\u8ba1\u5b66\u4e0a\u72ed\u7a84\u7684\u6807\u6ce8\u8005\u7fa4\u4f53\uff0c\u5ffd\u7565\u4e86\u4e0d\u540c\u793e\u533a\u95f4\u5b89\u5168\u611f\u77e5\u7684\u5dee\u5f02\uff0c\u65e0\u6cd5\u53cd\u6620\u5b89\u5168\u8bc4\u4f30\u7684\u591a\u5143\u6027\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u65b9\u6cd5\uff1a\u7b2c\u4e00\u9636\u6bb5\u5c06DICES\u63d0\u793a\u91cd\u65b0\u5206\u7c7b\u523014\u4e2a\u5b89\u5168\u9886\u57df\uff08\u57fa\u4e8eBEAVERTAILS\uff09\uff0c\u4fdd\u7559\u4eba\u53e3\u7edf\u8ba1\u5b66\u5143\u6570\u636e\uff0c\u4f7f\u7528Mistral 7B-Instruct-v0.3\u548cLlama-3.1-8B-Instruct\u8fdb\u884c\u6269\u5c55\uff0c\u901a\u8fc7SimHash\u53bb\u91cd\u5f97\u523043,050\u4e2a\u6837\u672c\uff1b\u7b2c\u4e8c\u9636\u6bb5\u4f7f\u7528LLMs-as-Raters\uff08Gemma-7B\u3001GPT-4o\u3001LLaMA-2-7B\uff09\u8fdb\u884c\u96f6\u6837\u672c\u63a8\u7406\u8bc4\u4f30\uff0c\u91c7\u7528\u5e73\u8861\u9608\u503c\uff08delta=0.5\uff0ctau=10\uff09\u3002", "result": "\u5b9e\u73b0\u4e86\u9ad8\u53ef\u9760\u6027\uff08ICC=0.87\uff09\u548c\u4f4e\u4eba\u53e3\u7edf\u8ba1\u5b66\u654f\u611f\u6027\uff08DS=0.12\uff09\uff0c\u8bc1\u660e\u591a\u5143\u5b89\u5168\u8bc4\u4f30\u53ef\u4ee5\u540c\u65f6\u5177\u5907\u53ef\u6269\u5c55\u6027\u548c\u4eba\u53e3\u7edf\u8ba1\u5b66\u9c81\u68d2\u6027\u3002", "conclusion": "Demo-SafetyBench\u901a\u8fc7\u5728\u63d0\u793a\u5c42\u9762\u76f4\u63a5\u5efa\u6a21\u4eba\u53e3\u7edf\u8ba1\u5b66\u591a\u5143\u4e3b\u4e49\uff0c\u5c06\u4ef7\u503c\u6846\u67b6\u4e0e\u54cd\u5e94\u89e3\u8026\uff0c\u4e3aLLM\u5b89\u5168\u8bc4\u4f30\u63d0\u4f9b\u4e86\u4e00\u4e2a\u65e2\u8003\u8651\u591a\u5143\u6027\u53c8\u4fdd\u6301\u6280\u672f\u53ef\u884c\u6027\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.07041", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.07041", "abs": "https://arxiv.org/abs/2602.07041", "authors": ["Leeje Jang", "Yao-Yi Chiang", "Angela M. Hastings", "Patimaporn Pungchanchaikul", "Martha B. Lucas", "Emily C. Schultz", "Jeffrey P. Louie", "Mohamed Estai", "Wen-Chen Wang", "Ryan H. L. Ip", "Boyen Huang"], "title": "OMNI-Dent: Towards an Accessible and Explainable AI Framework for Automated Dental Diagnosis", "comment": null, "summary": "Accurate dental diagnosis is essential for oral healthcare, yet many individuals lack access to timely professional evaluation. Existing AI-based methods primarily treat diagnosis as a visual pattern recognition task and do not reflect the structured clinical reasoning used by dental professionals. These approaches also require large amounts of expert-annotated data and often struggle to generalize across diverse real-world imaging conditions. To address these limitations, we present OMNI-Dent, a data-efficient and explainable diagnostic framework that incorporates clinical reasoning principles into a Vision-Language Model (VLM)-based pipeline. The framework operates on multi-view smartphone photographs,embeds diagnostic heuristics from dental experts, and guides a general-purpose VLM to perform tooth-level evaluation without dental-specific fine-tuning of the VLM. By utilizing the VLM's existing visual-linguistic capabilities, OMNI-Dent aims to support diagnostic assessment in settings where curated clinical imaging is unavailable. Designed as an early-stage assistive tool, OMNI-Dent helps users identify potential abnormalities and determine when professional evaluation may be needed, offering a practical option for individuals with limited access to in-person care.", "AI": {"tldr": "OMNI-Dent\u662f\u4e00\u4e2a\u6570\u636e\u9ad8\u6548\u4e14\u53ef\u89e3\u91ca\u7684\u7259\u79d1\u8bca\u65ad\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u4e34\u5e8a\u63a8\u7406\u539f\u5219\u878d\u5165\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u5229\u7528\u591a\u89c6\u89d2\u667a\u80fd\u624b\u673a\u7167\u7247\u8fdb\u884c\u7259\u9f7f\u7ea7\u8bc4\u4f30\uff0c\u65e0\u9700\u7259\u79d1\u7279\u5b9a\u5fae\u8c03\u3002", "motivation": "\u5f53\u524dAI\u7259\u79d1\u8bca\u65ad\u65b9\u6cd5\u4e3b\u8981\u5c06\u8bca\u65ad\u89c6\u4e3a\u89c6\u89c9\u6a21\u5f0f\u8bc6\u522b\u4efb\u52a1\uff0c\u672a\u80fd\u53cd\u6620\u7259\u79d1\u4e13\u4e1a\u4eba\u58eb\u7684\u7ed3\u6784\u5316\u4e34\u5e8a\u63a8\u7406\u3002\u8fd9\u4e9b\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u4e13\u5bb6\u6807\u6ce8\u6570\u636e\uff0c\u4e14\u96be\u4ee5\u5728\u4e0d\u540c\u771f\u5b9e\u4e16\u754c\u6210\u50cf\u6761\u4ef6\u4e0b\u6cdb\u5316\u3002\u8bb8\u591a\u4eba\u7f3a\u4e4f\u53ca\u65f6\u7684\u4e13\u4e1a\u8bc4\u4f30\u673a\u4f1a\u3002", "method": "\u63d0\u51faOMNI-Dent\u6846\u67b6\uff0c\u5c06\u4e34\u5e8a\u63a8\u7406\u539f\u5219\u878d\u5165\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u6d41\u7a0b\u3002\u6846\u67b6\u57fa\u4e8e\u591a\u89c6\u89d2\u667a\u80fd\u624b\u673a\u7167\u7247\uff0c\u5d4c\u5165\u7259\u79d1\u4e13\u5bb6\u7684\u8bca\u65ad\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u5f15\u5bfc\u901a\u7528VLM\u8fdb\u884c\u7259\u9f7f\u7ea7\u8bc4\u4f30\uff0c\u65e0\u9700\u5bf9VLM\u8fdb\u884c\u7259\u79d1\u7279\u5b9a\u5fae\u8c03\u3002", "result": "\u8be5\u6846\u67b6\u65e8\u5728\u652f\u6301\u5728\u7f3a\u4e4f\u4e34\u5e8a\u5f71\u50cf\u6570\u636e\u7684\u573a\u666f\u4e0b\u8fdb\u884c\u8bca\u65ad\u8bc4\u4f30\u3002\u4f5c\u4e3a\u65e9\u671f\u8f85\u52a9\u5de5\u5177\uff0c\u5e2e\u52a9\u7528\u6237\u8bc6\u522b\u6f5c\u5728\u5f02\u5e38\u5e76\u786e\u5b9a\u4f55\u65f6\u9700\u8981\u4e13\u4e1a\u8bc4\u4f30\uff0c\u4e3a\u7f3a\u4e4f\u73b0\u573a\u62a4\u7406\u673a\u4f1a\u7684\u4e2a\u4eba\u63d0\u4f9b\u5b9e\u7528\u9009\u62e9\u3002", "conclusion": "OMNI-Dent\u901a\u8fc7\u7ed3\u5408\u4e34\u5e8a\u63a8\u7406\u539f\u5219\u548c\u901a\u7528VLM\u80fd\u529b\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u6570\u636e\u9ad8\u6548\u3001\u53ef\u89e3\u91ca\u7684\u7259\u79d1\u8bca\u65ad\u65b9\u6cd5\uff0c\u7279\u522b\u9002\u7528\u4e8e\u8d44\u6e90\u6709\u9650\u7684\u73af\u5883\uff0c\u80fd\u591f\u5e2e\u52a9\u6539\u5584\u53e3\u8154\u5065\u5eb7\u670d\u52a1\u7684\u53ef\u53ca\u6027\u3002"}}
{"id": "2602.07641", "categories": ["cs.SE", "cs.HC"], "pdf": "https://arxiv.org/pdf/2602.07641", "abs": "https://arxiv.org/abs/2602.07641", "authors": ["Marc Bara"], "title": "HAIF: A Human-AI Integration Framework for Hybrid Team Operations", "comment": "22 pages, 4 figures, 5 tables, 2 appendices", "summary": "The rapid deployment of generative AI, copilots, and agentic systems in knowledge work has created an operational gap: no existing framework addresses how to organize daily work in teams where AI agents perform substantive, delegated tasks alongside humans. Agile, DevOps, MLOps, and AI governance frameworks each cover adjacent concerns but none models the hybrid team as a coherent delivery unit. This paper proposes the Human-AI Integration Framework (HAIF): a protocol-based, scalable operational system built around four core principles, a formal delegation decision model, tiered autonomy with quantifiable transition criteria, and feedback mechanisms designed to integrate into existing Agile and Kanban workflows without requiring additional roles for small teams. The framework is developed following a Design Science Research methodology. HAIF explicitly addresses the central adoption paradox: the more capable AI becomes, the harder it is to justify the oversight the framework demands-and yet the greater the consequences of not providing it. The paper includes domain-specific validation checklists, adaptation guidance for non-software environments, and an examination of the framework's structural limitations-including the increasingly common pattern of continuous human-AI co-production that challenges the discrete delegation model. The framework is tool-agnostic and designed for iterative adoption. Empirical validation is identified as future work.", "AI": {"tldr": "\u63d0\u51faHAIF\u6846\u67b6\u89e3\u51b3\u4eba\u673a\u6df7\u5408\u56e2\u961f\u5de5\u4f5c\u7ec4\u7ec7\u95ee\u9898\uff0c\u5305\u542b\u56db\u5927\u6838\u5fc3\u539f\u5219\u3001\u6b63\u5f0f\u59d4\u6258\u51b3\u7b56\u6a21\u578b\u3001\u5206\u7ea7\u81ea\u6cbb\u548c\u53cd\u9988\u673a\u5236\uff0c\u53ef\u96c6\u6210\u5230\u73b0\u6709\u654f\u6377\u5de5\u4f5c\u6d41\u4e2d", "motivation": "\u751f\u6210\u5f0fAI\u3001\u526f\u9a7e\u9a76\u548c\u667a\u80fd\u4f53\u7cfb\u7edf\u5728\u77e5\u8bc6\u5de5\u4f5c\u4e2d\u5feb\u901f\u90e8\u7f72\uff0c\u4f46\u7f3a\u4e4f\u6846\u67b6\u6765\u7ec4\u7ec7\u4eba\u673a\u6df7\u5408\u56e2\u961f\u7684\u65e5\u5e38\u5de5\u4f5c\u3002\u73b0\u6709\u654f\u6377\u3001DevOps\u3001MLOps\u548cAI\u6cbb\u7406\u6846\u67b6\u90fd\u65e0\u6cd5\u5c06\u6df7\u5408\u56e2\u961f\u4f5c\u4e3a\u7edf\u4e00\u7684\u4ea4\u4ed8\u5355\u5143\u6765\u5efa\u6a21", "method": "\u91c7\u7528\u8bbe\u8ba1\u79d1\u5b66\u7814\u7a76\u65b9\u6cd5\uff0c\u5f00\u53d1\u57fa\u4e8e\u534f\u8bae\u3001\u53ef\u6269\u5c55\u7684HAIF\u6846\u67b6\uff0c\u5305\u542b\u56db\u5927\u6838\u5fc3\u539f\u5219\u3001\u6b63\u5f0f\u59d4\u6258\u51b3\u7b56\u6a21\u578b\u3001\u5206\u7ea7\u81ea\u6cbb\u4e0e\u91cf\u5316\u8fc7\u6e21\u6807\u51c6\u3001\u53cd\u9988\u673a\u5236\uff0c\u53ef\u96c6\u6210\u5230\u73b0\u6709\u654f\u6377\u548c\u770b\u677f\u5de5\u4f5c\u6d41\u4e2d", "result": "\u63d0\u51fa\u5b8c\u6574\u7684HAIF\u6846\u67b6\uff0c\u5305\u542b\u9886\u57df\u7279\u5b9a\u9a8c\u8bc1\u6e05\u5355\u3001\u975e\u8f6f\u4ef6\u73af\u5883\u9002\u5e94\u6307\u5357\uff0c\u5206\u6790\u6846\u67b6\u7ed3\u6784\u9650\u5236\uff08\u5982\u8fde\u7eed\u4eba\u673a\u534f\u540c\u751f\u4ea7\u6311\u6218\u79bb\u6563\u59d4\u6258\u6a21\u578b\uff09\uff0c\u6846\u67b6\u5de5\u5177\u65e0\u5173\u4e14\u652f\u6301\u8fed\u4ee3\u91c7\u7528", "conclusion": "HAIF\u6846\u67b6\u89e3\u51b3\u4e86AI\u80fd\u529b\u8d8a\u5f3a\u8d8a\u96be\u8bc1\u660e\u76d1\u7763\u5fc5\u8981\u6027\u7684\u91c7\u7528\u6096\u8bba\uff0c\u4f46\u7f3a\u4e4f\u76d1\u7763\u7684\u540e\u679c\u4e5f\u8d8a\u4e25\u91cd\u3002\u6846\u67b6\u4e3a\u672a\u6765\u5b9e\u8bc1\u9a8c\u8bc1\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u586b\u8865\u4e86\u4eba\u673a\u6df7\u5408\u56e2\u961f\u64cd\u4f5c\u6846\u67b6\u7684\u7a7a\u767d"}}
{"id": "2602.07722", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2602.07722", "abs": "https://arxiv.org/abs/2602.07722", "authors": ["Sharif Noor Zisad", "Ragib Hasan"], "title": "IPBAC: Interaction Provenance-Based Access Control for Secure and Privacy-Aware Systems", "comment": "This article is accepted and presented in IEEE Consumer Communications & Networking Conference (CCNC 2026) as a poster", "summary": "Traditional access control systems, including RBAC, face significant limitations such as inflexible role definitions, difficulty handling dynamic scenarios, and lack of detailed accountability and traceability. To this end, we introduce the Interaction Provenance-based Access Control (IPBAC) model. In this paper, we explore the integration of interaction provenance with access control to overcome these limitations. Interaction provenance refers to the detailed recording of actions and interactions within a system, capturing comprehensive metadata such as the identity of the actor, the time of an action, and the context. IPBAC ensures stronger protection against unauthorized access, enhances traceability for auditing and compliance, and supports adaptive security policies. This provenance-based access control not only strengthens security, but also provides a robust framework for auditing and compliance.", "AI": {"tldr": "IPBAC\u6a21\u578b\u901a\u8fc7\u96c6\u6210\u4ea4\u4e92\u6eaf\u6e90\u4e0e\u8bbf\u95ee\u63a7\u5236\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfRBAC\u7cfb\u7edf\u89d2\u8272\u5b9a\u4e49\u50f5\u5316\u3001\u96be\u4ee5\u5904\u7406\u52a8\u6001\u573a\u666f\u3001\u7f3a\u4e4f\u8be6\u7ec6\u95ee\u8d23\u548c\u53ef\u8ffd\u6eaf\u6027\u7b49\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u8bbf\u95ee\u63a7\u5236\u7cfb\u7edf\uff08\u5305\u62ecRBAC\uff09\u9762\u4e34\u89d2\u8272\u5b9a\u4e49\u50f5\u5316\u3001\u96be\u4ee5\u5904\u7406\u52a8\u6001\u573a\u666f\u3001\u7f3a\u4e4f\u8be6\u7ec6\u95ee\u8d23\u548c\u53ef\u8ffd\u6eaf\u6027\u7b49\u663e\u8457\u9650\u5236\uff0c\u9700\u8981\u66f4\u7075\u6d3b\u548c\u53ef\u8ffd\u6eaf\u7684\u8bbf\u95ee\u63a7\u5236\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u4ea4\u4e92\u6eaf\u6e90\u7684\u8bbf\u95ee\u63a7\u5236\uff08IPBAC\uff09\u6a21\u578b\uff0c\u5c06\u4ea4\u4e92\u6eaf\u6e90\u4e0e\u8bbf\u95ee\u63a7\u5236\u96c6\u6210\u3002\u4ea4\u4e92\u6eaf\u6e90\u8be6\u7ec6\u8bb0\u5f55\u7cfb\u7edf\u5185\u7684\u64cd\u4f5c\u548c\u4ea4\u4e92\uff0c\u6355\u83b7\u5305\u62ec\u6267\u884c\u8005\u8eab\u4efd\u3001\u64cd\u4f5c\u65f6\u95f4\u3001\u4e0a\u4e0b\u6587\u7b49\u5168\u9762\u7684\u5143\u6570\u636e\u3002", "result": "IPBAC\u786e\u4fdd\u66f4\u5f3a\u7684\u672a\u6388\u6743\u8bbf\u95ee\u9632\u62a4\uff0c\u589e\u5f3a\u5ba1\u8ba1\u548c\u5408\u89c4\u7684\u53ef\u8ffd\u6eaf\u6027\uff0c\u652f\u6301\u81ea\u9002\u5e94\u5b89\u5168\u7b56\u7565\u3002\u8fd9\u79cd\u57fa\u4e8e\u6eaf\u6e90\u7684\u8bbf\u95ee\u63a7\u5236\u4e0d\u4ec5\u589e\u5f3a\u4e86\u5b89\u5168\u6027\uff0c\u8fd8\u4e3a\u5ba1\u8ba1\u548c\u5408\u89c4\u63d0\u4f9b\u4e86\u7a33\u5065\u6846\u67b6\u3002", "conclusion": "\u57fa\u4e8e\u4ea4\u4e92\u6eaf\u6e90\u7684\u8bbf\u95ee\u63a7\u5236\u6a21\u578b\u80fd\u591f\u6709\u6548\u514b\u670d\u4f20\u7edf\u8bbf\u95ee\u63a7\u5236\u7cfb\u7edf\u7684\u5c40\u9650\u6027\uff0c\u63d0\u4f9b\u66f4\u5f3a\u7684\u5b89\u5168\u6027\u3001\u53ef\u8ffd\u6eaf\u6027\u548c\u9002\u5e94\u6027\uff0c\u4e3a\u7cfb\u7edf\u5b89\u5168\u548c\u5408\u89c4\u7ba1\u7406\u63d0\u4f9b\u4e86\u521b\u65b0\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.07381", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.07381", "abs": "https://arxiv.org/abs/2602.07381", "authors": ["Gautam Siddharth Kashyap", "Mark Dras", "Usman Naseem"], "title": "When the Model Said 'No Comment', We Knew Helpfulness Was Dead, Honesty Was Alive, and Safety Was Terrified", "comment": "Accepted at EACL Mains 2026", "summary": "Large Language Models (LLMs) need to be in accordance with human values-being helpful, harmless, and honest (HHH)-is important for safe deployment. Existing works use Supervised Fine-Tuning (SFT) and Mixture-of-Experts (MoE) to align LLMs. However, these works face challenges in multi-objective settings, such as SFT leading to interference between conflicting objectives, while MoEs suffer from miscalibrated routing. We term this failure mode Axis Collapse, marked by (1) disjoint feature spaces causing catastrophic forgetting, and (2) unreliable inference from misrouted experts. To resolve this, we propose AlignX, a two-stage framework. Stage 1 uses prompt-injected fine-tuning to extract axis-specific task features, mitigating catastrophic forgetting. Stage 2 deploys a MoCaE module that calibrates expert routing using fractal and natural geometry, improving inference reliability. AlignX achieves significant gains on Alpaca (Helpfulness), BeaverTails (Harmlessness), and TruthfulQA (Honesty), with +171.5% win rate, +110.1% in truthfulness-informativeness, and 4.3% fewer safety violations. It also reduces latency and memory usage by over 35% compared to prior MoEs. Results across four LLMs validate its generalizability.", "AI": {"tldr": "AlignX\u662f\u4e00\u4e2a\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u901a\u8fc7\u63d0\u793a\u6ce8\u5165\u5fae\u8c03\u548c\u51e0\u4f55\u6821\u51c6\u7684MoE\u6765\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u5728\u591a\u76ee\u6807\u5bf9\u9f50\u4e2d\u7684\u8f74\u5d29\u6e83\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u5e2e\u52a9\u6027\u3001\u65e0\u5bb3\u6027\u548c\u8bda\u5b9e\u6027\uff0c\u540c\u65f6\u964d\u4f4e\u5ef6\u8fdf\u548c\u5185\u5b58\u4f7f\u7528\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u8bed\u8a00\u6a21\u578b\u5bf9\u9f50\u65b9\u6cd5\uff08SFT\u548cMoE\uff09\u5728\u591a\u76ee\u6807\u8bbe\u7f6e\u4e2d\u9762\u4e34\u6311\u6218\uff1aSFT\u5bfc\u81f4\u51b2\u7a81\u76ee\u6807\u4e4b\u95f4\u7684\u5e72\u6270\uff0cMoE\u5b58\u5728\u8def\u7531\u6821\u51c6\u95ee\u9898\u3002\u8fd9\u79cd\u5931\u8d25\u6a21\u5f0f\u88ab\u79f0\u4e3a\"\u8f74\u5d29\u6e83\"\uff0c\u8868\u73b0\u4e3a\uff081\uff09\u7279\u5f81\u7a7a\u95f4\u5206\u79bb\u5bfc\u81f4\u707e\u96be\u6027\u9057\u5fd8\uff0c\uff082\uff09\u4e13\u5bb6\u8bef\u8def\u7531\u5bfc\u81f4\u4e0d\u53ef\u9760\u63a8\u7406\u3002", "method": "AlignX\u91c7\u7528\u4e24\u9636\u6bb5\u6846\u67b6\uff1a\u7b2c\u4e00\u9636\u6bb5\u4f7f\u7528\u63d0\u793a\u6ce8\u5165\u5fae\u8c03\u63d0\u53d6\u8f74\u7279\u5b9a\u4efb\u52a1\u7279\u5f81\uff0c\u7f13\u89e3\u707e\u96be\u6027\u9057\u5fd8\uff1b\u7b2c\u4e8c\u9636\u6bb5\u90e8\u7f72MoCaE\u6a21\u5757\uff0c\u5229\u7528\u5206\u5f62\u548c\u81ea\u7136\u51e0\u4f55\u6821\u51c6\u4e13\u5bb6\u8def\u7531\uff0c\u63d0\u9ad8\u63a8\u7406\u53ef\u9760\u6027\u3002", "result": "\u5728Alpaca\uff08\u5e2e\u52a9\u6027\uff09\u3001BeaverTails\uff08\u65e0\u5bb3\u6027\uff09\u548cTruthfulQA\uff08\u8bda\u5b9e\u6027\uff09\u4e0a\u53d6\u5f97\u663e\u8457\u63d0\u5347\uff1a\u80dc\u7387+171.5%\uff0c\u771f\u5b9e\u6027-\u4fe1\u606f\u6027+110.1%\uff0c\u5b89\u5168\u8fdd\u89c4\u51cf\u5c114.3%\u3002\u76f8\u6bd4\u5148\u524dMoE\u65b9\u6cd5\uff0c\u5ef6\u8fdf\u548c\u5185\u5b58\u4f7f\u7528\u964d\u4f4e\u8d85\u8fc735%\u3002\u5728\u56db\u4e2aLLM\u4e0a\u7684\u7ed3\u679c\u9a8c\u8bc1\u4e86\u5176\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "AlignX\u6709\u6548\u89e3\u51b3\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u591a\u76ee\u6807\u5bf9\u9f50\u4e2d\u7684\u8f74\u5d29\u6e83\u95ee\u9898\uff0c\u901a\u8fc7\u7279\u5f81\u63d0\u53d6\u548c\u51e0\u4f55\u6821\u51c6\u7684\u8def\u7531\u673a\u5236\uff0c\u5728\u4fdd\u6301\u6a21\u578b\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u6548\u7387\uff0c\u4e3a\u5b89\u5168\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2602.07042", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.07042", "abs": "https://arxiv.org/abs/2602.07042", "authors": ["Magesh Rajasekaran", "Md Saiful Islam Sajol", "Frej Berglind", "Supratik Mukhopadhyay", "Kamalika Das"], "title": "COMBOOD: A Semiparametric Approach for Detecting Out-of-distribution Data for Image Classification", "comment": "Copyright by SIAM. Unauthorized reproduction of this article is prohibited First Published in Proceedings of the 2024 SIAM International Conference on Data Mining (SDM24), published by the Society for Industrial and Applied Mathematics (SIAM)", "summary": "Identifying out-of-distribution (OOD) data at inference time is crucial for many machine learning applications, especially for automation. We present a novel unsupervised semi-parametric framework COMBOOD for OOD detection with respect to image recognition. Our framework combines signals from two distance metrics, nearest-neighbor and Mahalanobis, to derive a confidence score for an inference point to be out-of-distribution. The former provides a non-parametric approach to OOD detection. The latter provides a parametric, simple, yet effective method for detecting OOD data points, especially, in the far OOD scenario, where the inference point is far apart from the training data set in the embedding space. However, its performance is not satisfactory in the near OOD scenarios that arise in practical situations. Our COMBOOD framework combines the two signals in a semi-parametric setting to provide a confidence score that is accurate both for the near-OOD and far-OOD scenarios. We show experimental results with the COMBOOD framework for different types of feature extraction strategies. We demonstrate experimentally that COMBOOD outperforms state-of-the-art OOD detection methods on the OpenOOD (both version 1 and most recent version 1.5) benchmark datasets (for both far-OOD and near-OOD) as well as on the documents dataset in terms of accuracy. On a majority of the benchmark datasets, the improvements in accuracy resulting from the COMBOOD framework are statistically significant. COMBOOD scales linearly with the size of the embedding space, making it ideal for many real-life applications.", "AI": {"tldr": "COMBOOD\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u65e0\u76d1\u7763\u534a\u53c2\u6570\u6846\u67b6\uff0c\u7528\u4e8e\u56fe\u50cf\u8bc6\u522b\u7684OOD\u68c0\u6d4b\uff0c\u901a\u8fc7\u7ed3\u5408\u6700\u8fd1\u90bb\u548c\u9a6c\u6c0f\u8ddd\u79bb\u4e24\u79cd\u5ea6\u91cf\u4fe1\u53f7\uff0c\u5728\u8fd1OOD\u548c\u8fdcOOD\u573a\u666f\u4e0b\u90fd\u80fd\u63d0\u4f9b\u51c6\u786e\u7684\u7f6e\u4fe1\u5ea6\u8bc4\u5206\u3002", "motivation": "\u5728\u673a\u5668\u5b66\u4e60\u5e94\u7528\u4e2d\uff0c\u63a8\u7406\u65f6\u8bc6\u522b\u5206\u5e03\u5916\u6570\u636e\u5bf9\u81ea\u52a8\u5316\u5e94\u7528\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u8fd1OOD\u573a\u666f\uff08\u5b9e\u9645\u5e94\u7528\u4e2d\u5e38\u89c1\uff09\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u5904\u7406\u8fdcOOD\u53c8\u80fd\u5904\u7406\u8fd1OOD\u7684\u7edf\u4e00\u6846\u67b6\u3002", "method": "COMBOOD\u6846\u67b6\u7ed3\u5408\u4e86\u4e24\u79cd\u8ddd\u79bb\u5ea6\u91cf\u4fe1\u53f7\uff1a\u6700\u8fd1\u90bb\uff08\u975e\u53c2\u6570\u65b9\u6cd5\uff09\u548c\u9a6c\u6c0f\u8ddd\u79bb\uff08\u53c2\u6570\u65b9\u6cd5\uff09\u3002\u6700\u8fd1\u90bb\u65b9\u6cd5\u63d0\u4f9b\u975e\u53c2\u6570\u7684OOD\u68c0\u6d4b\uff0c\u9a6c\u6c0f\u8ddd\u79bb\u65b9\u6cd5\u5728\u8fdcOOD\u573a\u666f\u4e2d\u7279\u522b\u6709\u6548\u3002\u8be5\u6846\u67b6\u5728\u534a\u53c2\u6570\u8bbe\u7f6e\u4e0b\u5c06\u4e24\u79cd\u4fe1\u53f7\u7ed3\u5408\uff0c\u4e3a\u63a8\u7406\u70b9\u751f\u6210\u7f6e\u4fe1\u5ea6\u8bc4\u5206\u3002", "result": "COMBOOD\u5728OpenOOD\u57fa\u51c6\u6570\u636e\u96c6\uff08\u7248\u672c1\u548c1.5\uff09\u548c\u6587\u6863\u6570\u636e\u96c6\u4e0a\u5747\u4f18\u4e8e\u6700\u5148\u8fdb\u7684OOD\u68c0\u6d4b\u65b9\u6cd5\uff0c\u5728\u8fdcOOD\u548c\u8fd1OOD\u573a\u666f\u4e0b\u90fd\u8868\u73b0\u51fa\u8272\u3002\u5728\u5927\u591a\u6570\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0cCOMBOOD\u5e26\u6765\u7684\u51c6\u786e\u7387\u63d0\u5347\u5177\u6709\u7edf\u8ba1\u663e\u8457\u6027\u3002\u6846\u67b6\u7684\u590d\u6742\u5ea6\u4e0e\u5d4c\u5165\u7a7a\u95f4\u5927\u5c0f\u5448\u7ebf\u6027\u5173\u7cfb\uff0c\u9002\u5408\u5b9e\u9645\u5e94\u7528\u3002", "conclusion": "COMBOOD\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u6700\u8fd1\u90bb\u548c\u9a6c\u6c0f\u8ddd\u79bb\u4e24\u79cd\u8ddd\u79bb\u5ea6\u91cf\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u534a\u53c2\u6570OOD\u68c0\u6d4b\u65b9\u6cd5\uff0c\u5728\u8fd1OOD\u548c\u8fdcOOD\u573a\u666f\u4e0b\u90fd\u80fd\u53d6\u5f97\u4f18\u5f02\u6027\u80fd\uff0c\u4e14\u5177\u6709\u7ebf\u6027\u53ef\u6269\u5c55\u6027\uff0c\u9002\u5408\u5b9e\u9645\u5e94\u7528\u90e8\u7f72\u3002"}}
{"id": "2602.07359", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07359", "abs": "https://arxiv.org/abs/2602.07359", "authors": ["Xiaoqiang Lin", "Jun Hao Liew", "Silvio Savarese", "Junnan Li"], "title": "W&D:Scaling Parallel Tool Calling for Efficient Deep Research Agents", "comment": null, "summary": "Deep research agents have emerged as powerful tools for automating complex intellectual tasks through multi-step reasoning and web-based information seeking. While recent efforts have successfully enhanced these agents by scaling depth through increasing the number of sequential thinking and tool calls, the potential of scaling width via parallel tool calling remains largely unexplored. In this work, we propose the Wide and Deep research agent, a framework designed to investigate the behavior and performance of agents when scaling not only depth but also width via parallel tool calling. Unlike existing approaches that rely on complex multi-agent orchestration to parallelize workloads, our method leverages intrinsic parallel tool calling to facilitate effective coordination within a single reasoning step. We demonstrate that scaling width significantly improves performance on deep research benchmarks while reducing the number of turns required to obtain correct answers. Furthermore, we analyze the factors driving these improvements through case studies and explore various tool call schedulers to optimize parallel tool calling strategy. Our findings suggest that optimizing the trade-off between width and depth is a critical pathway toward high-efficiency deep research agents. Notably, without context management or other tricks, we obtain 62.2% accuracy with GPT-5-Medium on BrowseComp, surpassing the original 54.9% reported by GPT-5-High.", "AI": {"tldr": "\u63d0\u51faWide and Deep\u7814\u7a76\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u5e76\u884c\u5de5\u5177\u8c03\u7528\u5b9e\u73b0\u5bbd\u5ea6\u6269\u5c55\uff0c\u5728\u6df1\u5ea6\u7814\u7a76\u57fa\u51c6\u4e0a\u663e\u8457\u63d0\u5347\u6027\u80fd\u5e76\u51cf\u5c11\u6240\u9700\u6b65\u9aa4\u3002", "motivation": "\u5f53\u524d\u6df1\u5ea6\u7814\u7a76\u667a\u80fd\u4f53\u4e3b\u8981\u901a\u8fc7\u589e\u52a0\u987a\u5e8f\u601d\u8003\u548c\u5de5\u5177\u8c03\u7528\u7684\u6df1\u5ea6\u6765\u63d0\u5347\u6027\u80fd\uff0c\u4f46\u901a\u8fc7\u5e76\u884c\u5de5\u5177\u8c03\u7528\u5b9e\u73b0\u5bbd\u5ea6\u6269\u5c55\u7684\u6f5c\u529b\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "\u63d0\u51faWide and Deep\u7814\u7a76\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u5229\u7528\u5185\u5728\u5e76\u884c\u5de5\u5177\u8c03\u7528\u5728\u5355\u4e2a\u63a8\u7406\u6b65\u9aa4\u5185\u5b9e\u73b0\u6709\u6548\u534f\u8c03\uff0c\u907f\u514d\u590d\u6742\u7684\u591a\u667a\u80fd\u4f53\u7f16\u6392\uff0c\u5e76\u63a2\u7d22\u5404\u79cd\u5de5\u5177\u8c03\u7528\u8c03\u5ea6\u5668\u6765\u4f18\u5316\u5e76\u884c\u7b56\u7565\u3002", "result": "\u5bbd\u5ea6\u6269\u5c55\u663e\u8457\u63d0\u5347\u6df1\u5ea6\u7814\u7a76\u57fa\u51c6\u6027\u80fd\uff0c\u51cf\u5c11\u83b7\u5f97\u6b63\u786e\u7b54\u6848\u6240\u9700\u7684\u8f6e\u6b21\uff1b\u5728BrowseComp\u57fa\u51c6\u4e0a\uff0cGPT-5-Medium\u8fbe\u523062.2%\u51c6\u786e\u7387\uff0c\u8d85\u8fc7GPT-5-High\u62a5\u544a\u768454.9%\u3002", "conclusion": "\u4f18\u5316\u5bbd\u5ea6\u4e0e\u6df1\u5ea6\u4e4b\u95f4\u7684\u6743\u8861\u662f\u5b9e\u73b0\u9ad8\u6548\u6df1\u5ea6\u7814\u7a76\u667a\u80fd\u4f53\u7684\u5173\u952e\u9014\u5f84\uff0c\u5e76\u884c\u5de5\u5177\u8c03\u7528\u80fd\u663e\u8457\u63d0\u5347\u6027\u80fd\u800c\u4e0d\u9700\u8981\u590d\u6742\u7684\u4e0a\u4e0b\u6587\u7ba1\u7406\u6216\u5176\u4ed6\u6280\u5de7\u3002"}}
{"id": "2602.07672", "categories": ["cs.SE", "cs.AI", "cs.LG", "cs.PL", "cs.SC"], "pdf": "https://arxiv.org/pdf/2602.07672", "abs": "https://arxiv.org/abs/2602.07672", "authors": ["Babak Rahmani"], "title": "Debugging code world models", "comment": "8 pages, 4 figures, under review in conference", "summary": "Code World Models (CWMs) are language models trained to simulate program execution by predicting explicit runtime state after every executed command. This execution-based world modeling enables internal verification within the model, offering an alternative to natural language chain-of-thought reasoning. However, the sources of errors and the nature of CWMs' limitations remain poorly understood. We study CWMs from two complementary perspectives: local semantic execution and long-horizon state tracking. On real-code benchmarks, we identify two dominant failure regimes. First, dense runtime state reveals produce token-intensive execution traces, leading to token-budget exhaustion on programs with long execution histories. Second, failures disproportionately concentrate in string-valued state, which we attribute to limitations of subword tokenization rather than program structure. To study long-horizon behavior, we use a controlled permutation-tracking benchmark that isolates state propagation under action execution. We show that long-horizon degradation is driven primarily by incorrect action generation: when actions are replaced with ground-truth commands, a Transformer-based CWM propagates state accurately over long horizons, despite known limitations of Transformers in long-horizon state tracking. These findings suggest directions for more efficient supervision and state representations in CWMs that are better aligned with program execution and data types.", "AI": {"tldr": "\u4ee3\u7801\u4e16\u754c\u6a21\u578b\uff08CWMs\uff09\u901a\u8fc7\u9884\u6d4b\u7a0b\u5e8f\u6267\u884c\u540e\u7684\u8fd0\u884c\u65f6\u72b6\u6001\u6765\u6a21\u62df\u7a0b\u5e8f\u6267\u884c\uff0c\u4f46\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u5931\u8d25\u6a21\u5f0f\uff1a\u957f\u6267\u884c\u5386\u53f2\u5bfc\u81f4\u7684\u4ee4\u724c\u9884\u7b97\u8017\u5c3d\u548c\u5b57\u7b26\u4e32\u503c\u72b6\u6001\u5904\u7406\u56f0\u96be\u3002", "motivation": "\u7814\u7a76\u4ee3\u7801\u4e16\u754c\u6a21\u578b\u7684\u9519\u8bef\u6765\u6e90\u548c\u5c40\u9650\u6027\uff0c\u7406\u89e3\u5176\u5728\u5c40\u90e8\u8bed\u4e49\u6267\u884c\u548c\u957f\u65f6\u72b6\u6001\u8ddf\u8e2a\u65b9\u9762\u7684\u8868\u73b0\uff0c\u4e3a\u6539\u8fdb\u6a21\u578b\u63d0\u4f9b\u65b9\u5411\u3002", "method": "\u4ece\u4e24\u4e2a\u4e92\u8865\u89d2\u5ea6\u7814\u7a76CWMs\uff1a\u5c40\u90e8\u8bed\u4e49\u6267\u884c\u548c\u957f\u65f6\u72b6\u6001\u8ddf\u8e2a\u3002\u4f7f\u7528\u771f\u5b9e\u4ee3\u7801\u57fa\u51c6\u6d4b\u8bd5\u8bc6\u522b\u5931\u8d25\u6a21\u5f0f\uff0c\u5e76\u901a\u8fc7\u53d7\u63a7\u7684\u6392\u5217\u8ddf\u8e2a\u57fa\u51c6\u6d4b\u8bd5\u9694\u79bb\u72b6\u6001\u4f20\u64ad\u884c\u4e3a\u3002", "result": "\u53d1\u73b0\u4e24\u4e2a\u4e3b\u8981\u5931\u8d25\u6a21\u5f0f\uff1a1\uff09\u5bc6\u96c6\u8fd0\u884c\u65f6\u72b6\u6001\u4ea7\u751f\u5bfc\u81f4\u957f\u6267\u884c\u5386\u53f2\u7684\u7a0b\u5e8f\u4ee4\u724c\u9884\u7b97\u8017\u5c3d\uff1b2\uff09\u5b57\u7b26\u4e32\u503c\u72b6\u6001\u5904\u7406\u5931\u8d25\uff0c\u4e3b\u8981\u6e90\u4e8e\u5b50\u8bcd\u5206\u8bcd\u9650\u5236\u800c\u975e\u7a0b\u5e8f\u7ed3\u6784\u3002\u957f\u65f6\u72b6\u6001\u9000\u5316\u4e3b\u8981\u7531\u9519\u8bef\u52a8\u4f5c\u751f\u6210\u9a71\u52a8\uff0c\u5f53\u4f7f\u7528\u771f\u5b9e\u547d\u4ee4\u65f6\uff0cTransformer-based CWM\u80fd\u5728\u957f\u65f6\u8303\u56f4\u5185\u51c6\u786e\u4f20\u64ad\u72b6\u6001\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u6307\u51fa\u4e86\u6539\u8fdbCWMs\u7684\u65b9\u5411\uff1a\u9700\u8981\u66f4\u9ad8\u6548\u7684\u76d1\u7763\u548c\u4e0e\u7a0b\u5e8f\u6267\u884c\u53ca\u6570\u636e\u7c7b\u578b\u66f4\u597d\u5bf9\u9f50\u7684\u72b6\u6001\u8868\u793a\u65b9\u6cd5\u3002"}}
{"id": "2602.07725", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2602.07725", "abs": "https://arxiv.org/abs/2602.07725", "authors": ["Yaoqi Yang", "Yong Chen", "Jiacheng Wang", "Geng Sun", "Dusit Niyato", "Zhu Han"], "title": "Leveraging the Power of Ensemble Learning for Secure Low Altitude Economy", "comment": "7 pages, 2 figures", "summary": "Low Altitude Economy (LAE) holds immense promise for enhancing societal well-being and driving economic growth. However, this burgeoning field is vulnerable to security threats, particularly malicious aircraft intrusion attacks. To address the above concerns, intrusion detection systems (IDS) can be used to defend against malicious aircraft intrusions in LAE. Whereas, due to the heterogeneous data, dynamic environment, and resource-constrained devices within LAE, current IDS face challenges in detection accuracy, adaptability, and resource utilization ratio. In this regard, due to the inherent ability to combine the strengths of multiple models, ensemble learning can realize more robust and diverse anomaly detection further enhance IDS accuracy, thereby improving robustness and efficiency of the secure LAE. Unlike single-model approaches, ensemble learning can leverage the collective knowledge of its constituent models to effectively defend the malicious aircraft intrusion attacks. Specifically, this paper investigates ensemble learning for secure LAE, covering research focuses, solutions, and a case study. We first establish the rationale for ensemble learning and then review research areas and potential solutions, demonstrating the necessities and benefits of applying ensemble learning to secure LAE. Subsequently, we propose a framework of ensemble learning-enabled malicious aircrafts tracking in the secure LAE, where its feasibility and effectiveness are evaluated by the designed case study. Finally, we conclude by outlining promising future research directions for further advancing the ensemble learning-enabled secure LAE.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5728\u4f4e\u7a7a\u7ecf\u6d4e(LAE)\u5b89\u5168\u4e2d\u5e94\u7528\u96c6\u6210\u5b66\u4e60\u6765\u9632\u5fa1\u6076\u610f\u98de\u673a\u5165\u4fb5\u653b\u51fb\uff0c\u901a\u8fc7\u7ed3\u5408\u591a\u4e2a\u6a21\u578b\u7684\u4f18\u52bf\u63d0\u9ad8\u5165\u4fb5\u68c0\u6d4b\u7cfb\u7edf\u7684\u51c6\u786e\u6027\u3001\u9002\u5e94\u6027\u548c\u8d44\u6e90\u5229\u7528\u7387\u3002", "motivation": "\u4f4e\u7a7a\u7ecf\u6d4e\u5177\u6709\u5de8\u5927\u53d1\u5c55\u6f5c\u529b\uff0c\u4f46\u9762\u4e34\u6076\u610f\u98de\u673a\u5165\u4fb5\u7684\u5b89\u5168\u5a01\u80c1\u3002\u5f53\u524d\u5165\u4fb5\u68c0\u6d4b\u7cfb\u7edf\u5728LAE\u7684\u5f02\u6784\u6570\u636e\u3001\u52a8\u6001\u73af\u5883\u548c\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u6761\u4ef6\u4e0b\uff0c\u5b58\u5728\u68c0\u6d4b\u51c6\u786e\u6027\u3001\u9002\u5e94\u6027\u548c\u8d44\u6e90\u5229\u7528\u7387\u65b9\u9762\u7684\u6311\u6218\u3002", "method": "\u91c7\u7528\u96c6\u6210\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u591a\u4e2a\u6a21\u578b\u7684\u96c6\u4f53\u77e5\u8bc6\u6765\u589e\u5f3a\u5f02\u5e38\u68c0\u6d4b\u80fd\u529b\u3002\u5efa\u7acb\u4e86\u96c6\u6210\u5b66\u4e60\u5728\u5b89\u5168LAE\u4e2d\u7684\u5e94\u7528\u6846\u67b6\uff0c\u5305\u62ec\u7814\u7a76\u91cd\u70b9\u3001\u89e3\u51b3\u65b9\u6848\u548c\u6848\u4f8b\u7814\u7a76\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u96c6\u6210\u5b66\u4e60\u652f\u6301\u7684\u6076\u610f\u98de\u673a\u8ffd\u8e2a\u6846\u67b6\u3002", "result": "\u901a\u8fc7\u8bbe\u8ba1\u7684\u6848\u4f8b\u7814\u7a76\u8bc4\u4f30\u4e86\u96c6\u6210\u5b66\u4e60\u6846\u67b6\u7684\u53ef\u884c\u6027\u548c\u6709\u6548\u6027\uff0c\u8bc1\u660e\u4e86\u96c6\u6210\u5b66\u4e60\u80fd\u591f\u63d0\u9ad8LAE\u5b89\u5168\u7cfb\u7edf\u7684\u9c81\u68d2\u6027\u548c\u6548\u7387\uff0c\u589e\u5f3a\u5bf9\u6076\u610f\u98de\u673a\u5165\u4fb5\u653b\u51fb\u7684\u9632\u5fa1\u80fd\u529b\u3002", "conclusion": "\u96c6\u6210\u5b66\u4e60\u4e3a\u5b89\u5168\u4f4e\u7a7a\u7ecf\u6d4e\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u514b\u670d\u5f53\u524d\u5165\u4fb5\u68c0\u6d4b\u7cfb\u7edf\u7684\u5c40\u9650\u6027\u3002\u672a\u6765\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u4ee5\u63a8\u8fdb\u96c6\u6210\u5b66\u4e60\u5728\u5b89\u5168LAE\u4e2d\u7684\u5e94\u7528\u53d1\u5c55\u3002"}}
{"id": "2602.07382", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07382", "abs": "https://arxiv.org/abs/2602.07382", "authors": ["Debtanu Datta", "Rajdeep Mukherjee", "Adrijit Goswami", "Saptarshi Ghosh"], "title": "Advantages of Domain Knowledge Injection for Legal Document Summarization: A Case Study on Summarizing Indian Court Judgments in English and Hindi", "comment": "19 pages, 5 figures, 8 tables", "summary": "Summarizing Indian legal court judgments is a complex task not only due to the intricate language and unstructured nature of the legal texts, but also since a large section of the Indian population does not understand the complex English in which legal text is written, thus requiring summaries in Indian languages. In this study, we aim to improve the summarization of Indian legal text to generate summaries in both English and Hindi (the most widely spoken Indian language), by injecting domain knowledge into diverse summarization models. We propose a framework to enhance extractive neural summarization models by incorporating domain-specific pre-trained encoders tailored for legal texts. Further, we explore the injection of legal domain knowledge into generative models (including Large Language Models) through continual pre-training on large legal corpora in English and Hindi. Our proposed approaches achieve statistically significant improvements in both English-to-English and English-to-Hindi Indian legal document summarization, as measured by standard evaluation metrics, factual consistency metrics, and legal domain-specific metrics. Furthermore, these improvements are validated through domain experts, demonstrating the effectiveness of our approaches.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u5370\u5ea6\u6cd5\u5f8b\u6587\u672c\u6458\u8981\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u6ce8\u5165\u9886\u57df\u77e5\u8bc6\u5230\u4e0d\u540c\u7684\u6458\u8981\u6a21\u578b\u4e2d\uff0c\u751f\u6210\u82f1\u8bed\u548c\u5370\u5730\u8bed\u7684\u6cd5\u5f8b\u5224\u51b3\u6458\u8981\u3002", "motivation": "\u5370\u5ea6\u6cd5\u5f8b\u5224\u51b3\u6458\u8981\u9762\u4e34\u590d\u6742\u8bed\u8a00\u548c\u975e\u7ed3\u6784\u5316\u6587\u672c\u7684\u6311\u6218\uff0c\u4e14\u5927\u90e8\u5206\u5370\u5ea6\u4eba\u53e3\u4e0d\u7406\u89e3\u6cd5\u5f8b\u6587\u672c\u4f7f\u7528\u7684\u590d\u6742\u82f1\u8bed\uff0c\u9700\u8981\u5370\u5ea6\u8bed\u8a00\u7684\u6458\u8981\u3002", "method": "1. \u63d0\u51fa\u6846\u67b6\u589e\u5f3a\u62bd\u53d6\u5f0f\u795e\u7ecf\u6458\u8981\u6a21\u578b\uff0c\u901a\u8fc7\u878d\u5165\u9488\u5bf9\u6cd5\u5f8b\u6587\u672c\u7684\u9886\u57df\u7279\u5b9a\u9884\u8bad\u7ec3\u7f16\u7801\u5668\uff1b2. \u63a2\u7d22\u901a\u8fc7\u6301\u7eed\u9884\u8bad\u7ec3\u5728\u5927\u578b\u82f1\u8bed\u548c\u5370\u5730\u8bed\u6cd5\u5f8b\u8bed\u6599\u5e93\u4e0a\uff0c\u5c06\u6cd5\u5f8b\u9886\u57df\u77e5\u8bc6\u6ce8\u5165\u751f\u6210\u6a21\u578b\uff08\u5305\u62ec\u5927\u8bed\u8a00\u6a21\u578b\uff09\u3002", "result": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u82f1\u8bed\u5230\u82f1\u8bed\u548c\u82f1\u8bed\u5230\u5370\u5730\u8bed\u7684\u5370\u5ea6\u6cd5\u5f8b\u6587\u6863\u6458\u8981\u4e2d\uff0c\u5728\u6807\u51c6\u8bc4\u4f30\u6307\u6807\u3001\u4e8b\u5b9e\u4e00\u81f4\u6027\u6307\u6807\u548c\u6cd5\u5f8b\u9886\u57df\u7279\u5b9a\u6307\u6807\u4e0a\u90fd\u53d6\u5f97\u4e86\u7edf\u8ba1\u663e\u8457\u7684\u6539\u8fdb\uff0c\u5e76\u901a\u8fc7\u9886\u57df\u4e13\u5bb6\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\u3002", "conclusion": "\u901a\u8fc7\u6ce8\u5165\u6cd5\u5f8b\u9886\u57df\u77e5\u8bc6\u5230\u6458\u8981\u6a21\u578b\u4e2d\uff0c\u53ef\u4ee5\u6709\u6548\u6539\u8fdb\u5370\u5ea6\u6cd5\u5f8b\u6587\u672c\u7684\u8de8\u8bed\u8a00\u6458\u8981\u8d28\u91cf\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u82f1\u8bed\u548c\u5370\u5730\u8bed\u7684\u6cd5\u5f8b\u6587\u6863\u6458\u8981\u4efb\u52a1\u3002"}}
{"id": "2602.07044", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07044", "abs": "https://arxiv.org/abs/2602.07044", "authors": ["Tianyi Qu", "Songxiao Yang", "Haolin Wang", "Huadong Song", "Xiaoting Guo", "Wenguang Hu", "Guanlin Liu", "Honghe Chen", "Yafei Ou"], "title": "PipeMFL-240K: A Large-scale Dataset and Benchmark for Object Detection in Pipeline Magnetic Flux Leakage Imaging", "comment": "A dataset contains 240,320 pipeline MFL pseudo-color images and 191,530 bounding-box annotations, collected from 11 pipelines spanning approximately 1,480 km", "summary": "Pipeline integrity is critical to industrial safety and environmental protection, with Magnetic Flux Leakage (MFL) detection being a primary non-destructive testing technology. Despite the promise of deep learning for automating MFL interpretation, progress toward reliable models has been constrained by the absence of a large-scale public dataset and benchmark, making fair comparison and reproducible evaluation difficult. We introduce \\textbf{PipeMFL-240K}, a large-scale, meticulously annotated dataset and benchmark for complex object detection in pipeline MFL pseudo-color images. PipeMFL-240K reflects real-world inspection complexity and poses several unique challenges: (i) an extremely long-tailed distribution over \\textbf{12} categories, (ii) a high prevalence of tiny objects that often comprise only a handful of pixels, and (iii) substantial intra-class variability. The dataset contains \\textbf{240,320} images and \\textbf{191,530} high-quality bounding-box annotations, collected from 11 pipelines spanning approximately \\textbf{1,480} km. Extensive experiments are conducted with state-of-the-art object detectors to establish baselines. Results show that modern detectors still struggle with the intrinsic properties of MFL data, highlighting considerable headroom for improvement, while PipeMFL-240K provides a reliable and challenging testbed to drive future research. As the first public dataset and the first benchmark of this scale and scope for pipeline MFL inspection, it provides a critical foundation for efficient pipeline diagnostics as well as maintenance planning and is expected to accelerate algorithmic innovation and reproducible research in MFL-based pipeline integrity assessment.", "AI": {"tldr": "PipeMFL-240K\uff1a\u9996\u4e2a\u5927\u89c4\u6a21\u516c\u5f00\u7684\u7ba1\u9053\u6f0f\u78c1\u68c0\u6d4b\u6570\u636e\u96c6\u4e0e\u57fa\u51c6\uff0c\u5305\u542b24\u4e07\u5f20\u56fe\u50cf\u548c19\u4e07\u6807\u6ce8\uff0c\u9488\u5bf912\u7c7b\u7f3a\u9677\u7684\u957f\u5c3e\u5206\u5e03\u3001\u5fae\u5c0f\u76ee\u6807\u68c0\u6d4b\u7b49\u6311\u6218\uff0c\u4e3a\u81ea\u52a8\u5316\u7ba1\u9053\u5b8c\u6574\u6027\u8bc4\u4f30\u63d0\u4f9b\u6807\u51c6\u5316\u6d4b\u8bd5\u5e73\u53f0\u3002", "motivation": "\u5f53\u524d\u6df1\u5ea6\u5b66\u4e60\u5728\u7ba1\u9053\u6f0f\u78c1\u68c0\u6d4b\u81ea\u52a8\u5316\u5e94\u7528\u4e2d\u9762\u4e34\u7f3a\u4e4f\u5927\u89c4\u6a21\u516c\u5f00\u6570\u636e\u96c6\u548c\u57fa\u51c6\u7684\u95ee\u9898\uff0c\u5bfc\u81f4\u6a21\u578b\u96be\u4ee5\u516c\u5e73\u6bd4\u8f83\u548c\u53ef\u91cd\u590d\u8bc4\u4f30\uff0c\u963b\u788d\u4e86\u8be5\u9886\u57df\u7684\u7814\u7a76\u8fdb\u5c55\u3002", "method": "\u6784\u5efa\u4e86PipeMFL-240K\u6570\u636e\u96c6\uff0c\u5305\u542b240,320\u5f20\u56fe\u50cf\u548c191,530\u4e2a\u9ad8\u8d28\u91cf\u8fb9\u754c\u6846\u6807\u6ce8\uff0c\u6570\u636e\u6765\u81ea11\u6761\u603b\u957f\u7ea61,480\u516c\u91cc\u7684\u7ba1\u9053\uff0c\u6db5\u76d612\u4e2a\u7f3a\u9677\u7c7b\u522b\uff0c\u5177\u6709\u957f\u5c3e\u5206\u5e03\u3001\u5fae\u5c0f\u76ee\u6807\u548c\u9ad8\u7c7b\u5185\u53d8\u5f02\u7b49\u6311\u6218\u7279\u6027\u3002", "result": "\u4f7f\u7528\u6700\u5148\u8fdb\u7684\u76ee\u6807\u68c0\u6d4b\u5668\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7ed3\u679c\u663e\u793a\u73b0\u6709\u6a21\u578b\u4ecd\u96be\u4ee5\u6709\u6548\u5904\u7406\u6f0f\u78c1\u6570\u636e\u7684\u56fa\u6709\u7279\u6027\uff0c\u8868\u660e\u8be5\u9886\u57df\u4ecd\u6709\u5f88\u5927\u6539\u8fdb\u7a7a\u95f4\uff0c\u800cPipeMFL-240K\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u53ef\u9760\u4e14\u5177\u6709\u6311\u6218\u6027\u7684\u6d4b\u8bd5\u5e73\u53f0\u3002", "conclusion": "PipeMFL-240K\u4f5c\u4e3a\u9996\u4e2a\u5927\u89c4\u6a21\u516c\u5f00\u7684\u7ba1\u9053\u6f0f\u78c1\u68c0\u6d4b\u6570\u636e\u96c6\u548c\u57fa\u51c6\uff0c\u4e3a\u9ad8\u6548\u7ba1\u9053\u8bca\u65ad\u3001\u7ef4\u62a4\u89c4\u5212\u63d0\u4f9b\u4e86\u5173\u952e\u57fa\u7840\uff0c\u6709\u671b\u52a0\u901f\u6f0f\u78c1\u68c0\u6d4b\u9886\u57df\u7684\u7b97\u6cd5\u521b\u65b0\u548c\u53ef\u91cd\u590d\u7814\u7a76\u3002"}}
{"id": "2602.07698", "categories": ["cs.SE", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.07698", "abs": "https://arxiv.org/abs/2602.07698", "authors": ["Adam Sorrenti", "Andriy Miranskyy"], "title": "On Sequence-to-Sequence Models for Automated Log Parsing", "comment": null, "summary": "Log parsing is a critical standard operating procedure in software systems, enabling monitoring, anomaly detection, and failure diagnosis. However, automated log parsing remains challenging due to heterogeneous log formats, distribution shifts between training and deployment data, and the brittleness of rule-based approaches. This study aims to systematically evaluate how sequence modelling architecture, representation choice, sequence length, and training data availability influence automated log parsing performance and computational cost. We conduct a controlled empirical study comparing four sequence modelling architectures: Transformer, Mamba state-space, monodirectional LSTM, and bidirectional LSTM models. In total, 396 models are trained across multiple dataset configurations and evaluated using relative Levenshtein edit distance with statistical significance testing. Transformer achieves the lowest mean relative edit distance (0.111), followed by Mamba (0.145), mono-LSTM (0.186), and bi-LSTM (0.265), where lower values are better. Mamba provides competitive accuracy with substantially lower computational cost. Character-level tokenization generally improves performance, sequence length has negligible practical impact on Transformer accuracy, and both Mamba and Transformer demonstrate stronger sample efficiency than recurrent models. Overall, Transformers reduce parsing error by 23.4%, while Mamba is a strong alternative under data or compute constraints. These results also clarify the roles of representation choice, sequence length, and sample efficiency, providing practical guidance for researchers and practitioners.", "AI": {"tldr": "\u8be5\u7814\u7a76\u7cfb\u7edf\u8bc4\u4f30\u4e86Transformer\u3001Mamba\u3001LSTM\u7b49\u5e8f\u5217\u5efa\u6a21\u67b6\u6784\u5728\u65e5\u5fd7\u89e3\u6790\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u4e0e\u8ba1\u7b97\u6210\u672c\uff0c\u53d1\u73b0Transformer\u51c6\u786e\u7387\u6700\u9ad8\uff0cMamba\u5728\u8ba1\u7b97\u6548\u7387\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u81ea\u52a8\u5316\u65e5\u5fd7\u89e3\u6790\u9762\u4e34\u5f02\u6784\u65e5\u5fd7\u683c\u5f0f\u3001\u8bad\u7ec3\u4e0e\u90e8\u7f72\u6570\u636e\u5206\u5e03\u504f\u79fb\u3001\u57fa\u4e8e\u89c4\u5219\u65b9\u6cd5\u8106\u5f31\u6027\u7b49\u6311\u6218\uff0c\u9700\u8981\u7cfb\u7edf\u8bc4\u4f30\u4e0d\u540c\u5e8f\u5217\u5efa\u6a21\u67b6\u6784\u7684\u6027\u80fd\u548c\u8ba1\u7b97\u6210\u672c\u3002", "method": "\u901a\u8fc7\u63a7\u5236\u5b9e\u9a8c\u6bd4\u8f83\u56db\u79cd\u5e8f\u5217\u5efa\u6a21\u67b6\u6784\uff1aTransformer\u3001Mamba\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u3001\u5355\u5411LSTM\u548c\u53cc\u5411LSTM\uff0c\u5171\u8bad\u7ec3396\u4e2a\u6a21\u578b\uff0c\u4f7f\u7528\u76f8\u5bf9Levenshtein\u7f16\u8f91\u8ddd\u79bb\u8fdb\u884c\u8bc4\u4f30\u5e76\u8fdb\u884c\u7edf\u8ba1\u663e\u8457\u6027\u68c0\u9a8c\u3002", "result": "Transformer\u83b7\u5f97\u6700\u4f4e\u5e73\u5747\u76f8\u5bf9\u7f16\u8f91\u8ddd\u79bb(0.111)\uff0c\u5176\u6b21\u662fMamba(0.145)\u3001\u5355\u5411LSTM(0.186)\u548c\u53cc\u5411LSTM(0.265)\u3002Mamba\u5728\u4fdd\u6301\u7ade\u4e89\u529b\u7684\u51c6\u786e\u7387\u540c\u65f6\u8ba1\u7b97\u6210\u672c\u663e\u8457\u66f4\u4f4e\u3002\u5b57\u7b26\u7ea7\u5206\u8bcd\u901a\u5e38\u80fd\u63d0\u5347\u6027\u80fd\uff0c\u5e8f\u5217\u957f\u5ea6\u5bf9Transformer\u51c6\u786e\u7387\u5f71\u54cd\u53ef\u5ffd\u7565\uff0cMamba\u548cTransformer\u6bd4\u5faa\u73af\u6a21\u578b\u5177\u6709\u66f4\u5f3a\u7684\u6837\u672c\u6548\u7387\u3002", "conclusion": "Transformer\u80fd\u5c06\u89e3\u6790\u9519\u8bef\u51cf\u5c1123.4%\uff0c\u800cMamba\u5728\u6570\u636e\u6216\u8ba1\u7b97\u8d44\u6e90\u53d7\u9650\u65f6\u662f\u5f3a\u6709\u529b\u7684\u66ff\u4ee3\u65b9\u6848\u3002\u7814\u7a76\u7ed3\u679c\u4e3a\u7814\u7a76\u4eba\u5458\u548c\u5b9e\u8df5\u8005\u63d0\u4f9b\u4e86\u5173\u4e8e\u8868\u793a\u9009\u62e9\u3001\u5e8f\u5217\u957f\u5ea6\u548c\u6837\u672c\u6548\u7387\u7684\u5b9e\u7528\u6307\u5bfc\u3002"}}
{"id": "2602.07878", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07878", "abs": "https://arxiv.org/abs/2602.07878", "authors": ["Tianyi Wang", "Huawei Fan", "Yuanchao Shu", "Peng Cheng", "Cong Wang"], "title": "Rethinking Latency Denial-of-Service: Attacking the LLM Serving Framework, Not the Model", "comment": null, "summary": "Large Language Models face an emerging and critical threat known as latency attacks. Because LLM inference is inherently expensive, even modest slowdowns can translate into substantial operating costs and severe availability risks. Recently, a growing body of research has focused on algorithmic complexity attacks by crafting inputs to trigger worst-case output lengths. However, we report a counter-intuitive finding that these algorithmic latency attacks are largely ineffective against modern LLM serving systems. We reveal that system-level optimization such as continuous batching provides a logical isolation to mitigate contagious latency impact on co-located users. To this end, in this paper, we shift the focus from the algorithm to the system layer, and introduce a new Fill and Squeeze attack strategy targeting the state transition of the scheduler. \"Fill\" first exhausts the global KV cache to induce Head-of-Line blocking, while \"Squeeze\" forces the system into repetitive preemption. By manipulating output lengths using methods from simple plain-text prompts to more complex prompt engineering, and leveraging side-channel probing of memory status, we demonstrate that the attack can be orchestrated in a black-box setting with much less cost. Extensive evaluations indicate by up to 20-280x average slowdown on Time to First Token and 1.5-4x average slowdown on Time Per Output Token compared to existing attacks with 30-40% lower attack cost.", "AI": {"tldr": "\u672c\u6587\u63ed\u793a\u4f20\u7edf\u7b97\u6cd5\u590d\u6742\u5ea6\u653b\u51fb\u5bf9\u73b0\u4ee3LLM\u670d\u52a1\u7cfb\u7edf\u65e0\u6548\uff0c\u63d0\u51fa\u9488\u5bf9\u8c03\u5ea6\u5668\u72b6\u6001\u8f6c\u6362\u7684\"\u586b\u5145\u4e0e\u6324\u538b\"\u653b\u51fb\u7b56\u7565\uff0c\u901a\u8fc7\u64cd\u7eb5KV\u7f13\u5b58\u548c\u5f3a\u5236\u62a2\u5360\u5b9e\u73b0\u9ad8\u6548\u7684\u9ed1\u76d2\u653b\u51fb\u3002", "motivation": "LLM\u63a8\u7406\u6210\u672c\u9ad8\u6602\uff0c\u8f7b\u5fae\u5ef6\u8fdf\u5c31\u4f1a\u5bfc\u81f4\u5de8\u5927\u8fd0\u8425\u6210\u672c\u548c\u53ef\u7528\u6027\u98ce\u9669\u3002\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u901a\u8fc7\u7b97\u6cd5\u590d\u6742\u5ea6\u653b\u51fb\u89e6\u53d1\u6700\u574f\u8f93\u51fa\u957f\u5ea6\uff0c\u4f46\u8fd9\u4e9b\u653b\u51fb\u5bf9\u73b0\u4ee3LLM\u670d\u52a1\u7cfb\u7edf\u6548\u679c\u6709\u9650\uff0c\u9700\u8981\u63a2\u7d22\u7cfb\u7edf\u5c42\u9762\u7684\u653b\u51fb\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\"\u586b\u5145\u4e0e\u6324\u538b\"\u653b\u51fb\u7b56\u7565\uff1a1) \"\u586b\u5145\"\u9636\u6bb5\u8017\u5c3d\u5168\u5c40KV\u7f13\u5b58\uff0c\u8bf1\u5bfc\u961f\u5934\u963b\u585e\uff1b2) \"\u6324\u538b\"\u9636\u6bb5\u5f3a\u5236\u7cfb\u7edf\u8fdb\u5165\u91cd\u590d\u62a2\u5360\u72b6\u6001\u3002\u653b\u51fb\u4f7f\u7528\u4ece\u7b80\u5355\u6587\u672c\u63d0\u793a\u5230\u590d\u6742\u63d0\u793a\u5de5\u7a0b\u7684\u591a\u79cd\u65b9\u6cd5\u64cd\u7eb5\u8f93\u51fa\u957f\u5ea6\uff0c\u5e76\u5229\u7528\u5185\u5b58\u72b6\u6001\u4fa7\u4fe1\u9053\u63a2\u6d4b\uff0c\u53ef\u5728\u9ed1\u76d2\u8bbe\u7f6e\u4e2d\u4ee5\u8f83\u4f4e\u6210\u672c\u5b9e\u65bd\u3002", "result": "\u5e7f\u6cdb\u8bc4\u4f30\u663e\u793a\uff0c\u4e0e\u73b0\u6709\u653b\u51fb\u76f8\u6bd4\uff0c\u8be5\u653b\u51fb\u5728\u9996\u4ee4\u724c\u65f6\u95f4\u4e0a\u5b9e\u73b020-280\u500d\u5e73\u5747\u51cf\u901f\uff0c\u5728\u6bcf\u8f93\u51fa\u4ee4\u724c\u65f6\u95f4\u4e0a\u5b9e\u73b01.5-4\u500d\u5e73\u5747\u51cf\u901f\uff0c\u540c\u65f6\u653b\u51fb\u6210\u672c\u964d\u4f4e30-40%\u3002", "conclusion": "\u7cfb\u7edf\u7ea7\u4f18\u5316\u5982\u8fde\u7eed\u6279\u5904\u7406\u867d\u7136\u80fd\u7f13\u89e3\u7b97\u6cd5\u653b\u51fb\u7684\u4f20\u67d3\u6027\u5f71\u54cd\uff0c\u4f46\u8c03\u5ea6\u5668\u72b6\u6001\u8f6c\u6362\u6210\u4e3a\u65b0\u7684\u653b\u51fb\u9762\u3002\u63d0\u51fa\u7684\"\u586b\u5145\u4e0e\u6324\u538b\"\u653b\u51fb\u7b56\u7565\u80fd\u6709\u6548\u9488\u5bf9\u73b0\u4ee3LLM\u670d\u52a1\u7cfb\u7edf\uff0c\u63ed\u793a\u4e86\u7cfb\u7edf\u5c42\u9762\u7684\u5b89\u5168\u6f0f\u6d1e\uff0c\u9700\u8981\u65b0\u7684\u9632\u5fa1\u673a\u5236\u6765\u5e94\u5bf9\u8fd9\u7c7b\u653b\u51fb\u3002"}}
{"id": "2602.07447", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.07447", "abs": "https://arxiv.org/abs/2602.07447", "authors": ["Liviu P Dinu", "Ana Sabina Uban", "Bogdan Iordache", "Anca Dinu", "Simona Georgescu"], "title": "Measuring cross-language intelligibility between Romance languages with computational tools", "comment": "16 pages, 7 figures, 2 tables", "summary": "We present an analysis of mutual intelligibility in related languages applied for languages in the Romance family. We introduce a novel computational metric for estimating intelligibility based on lexical similarity using surface and semantic similarity of related words, and use it to measure mutual intelligibility for the five main Romance languages (French, Italian, Portuguese, Spanish, and Romanian), and compare results using both the orthographic and phonetic forms of words as well as different parallel corpora and vectorial models of word meaning representation. The obtained intelligibility scores confirm intuitions related to intelligibility asymmetry across languages and significantly correlate with results of cloze tests in human experiments.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8bcd\u6c47\u76f8\u4f3c\u5ea6\u7684\u8ba1\u7b97\u6307\u6807\u6765\u8bc4\u4f30\u7f57\u66fc\u8bed\u65cf\u8bed\u8a00\u95f4\u7684\u76f8\u4e92\u53ef\u61c2\u5ea6\uff0c\u901a\u8fc7\u8868\u9762\u548c\u8bed\u4e49\u76f8\u4f3c\u6027\u5206\u6790\uff0c\u9a8c\u8bc1\u4e86\u8be5\u6307\u6807\u4e0e\u4eba\u7c7b\u5b9e\u9a8c\u7ed3\u679c\u7684\u663e\u8457\u76f8\u5173\u6027\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u5f00\u53d1\u4e00\u79cd\u8ba1\u7b97\u65b9\u6cd5\u6765\u91cf\u5316\u76f8\u5173\u8bed\u8a00\u95f4\u7684\u76f8\u4e92\u53ef\u61c2\u5ea6\uff0c\u7279\u522b\u662f\u9488\u5bf9\u7f57\u66fc\u8bed\u65cf\u8bed\u8a00\uff0c\u4ee5\u9a8c\u8bc1\u8ba1\u7b97\u6307\u6807\u4e0e\u4eba\u7c7b\u5b9e\u9645\u7406\u89e3\u80fd\u529b\u7684\u4e00\u81f4\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8bcd\u6c47\u76f8\u4f3c\u5ea6\u7684\u8ba1\u7b97\u6307\u6807\uff0c\u7ed3\u5408\u8868\u9762\u76f8\u4f3c\u6027\u548c\u8bed\u4e49\u76f8\u4f3c\u6027\uff0c\u4f7f\u7528\u6b63\u5b57\u5f62\u5f0f\u548c\u8bed\u97f3\u5f62\u5f0f\uff0c\u5e76\u6bd4\u8f83\u4e0d\u540c\u5e73\u884c\u8bed\u6599\u5e93\u548c\u8bcd\u5411\u91cf\u8868\u793a\u6a21\u578b\u3002", "result": "\u8ba1\u7b97\u5f97\u5230\u7684\u53ef\u61c2\u5ea6\u5206\u6570\u8bc1\u5b9e\u4e86\u8bed\u8a00\u95f4\u53ef\u61c2\u5ea6\u4e0d\u5bf9\u79f0\u7684\u76f4\u89c9\uff0c\u5e76\u4e14\u4e0e\u4eba\u7c7b\u5b8c\u5f62\u586b\u7a7a\u6d4b\u8bd5\u7ed3\u679c\u663e\u8457\u76f8\u5173\uff0c\u9a8c\u8bc1\u4e86\u8be5\u8ba1\u7b97\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u57fa\u4e8e\u8bcd\u6c47\u76f8\u4f3c\u5ea6\u7684\u8ba1\u7b97\u6307\u6807\u80fd\u591f\u6709\u6548\u8bc4\u4f30\u7f57\u66fc\u8bed\u65cf\u8bed\u8a00\u95f4\u7684\u76f8\u4e92\u53ef\u61c2\u5ea6\uff0c\u4e3a\u8bed\u8a00\u7406\u89e3\u548c\u8bed\u8a00\u6559\u5b66\u63d0\u4f9b\u4e86\u65b0\u7684\u91cf\u5316\u5de5\u5177\u3002"}}
{"id": "2602.07045", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07045", "abs": "https://arxiv.org/abs/2602.07045", "authors": ["Zhiming Luo", "Di Wang", "Haonan Guo", "Jing Zhang", "Bo Du"], "title": "VLRS-Bench: A Vision-Language Reasoning Benchmark for Remote Sensing", "comment": null, "summary": "Recent advancements in Multimodal Large Language Models (MLLMs) have enabled complex reasoning. However, existing remote sensing (RS) benchmarks remain heavily biased toward perception tasks, such as object recognition and scene classification. This limitation hinders the development of MLLMs for cognitively demanding RS applications. To address this, , we propose a Vision Language ReaSoning Benchmark (VLRS-Bench), which is the first benchmark exclusively dedicated to complex RS reasoning. Structured across the three core dimensions of Cognition, Decision, and Prediction, VLRS-Bench comprises 2,000 question-answer pairs with an average length of 71 words, spanning 14 tasks and up to eight temporal phases. VLRS-Bench is constructed via a specialized pipeline that integrates RS-specific priors and expert knowledge to ensure geospatial realism and reasoning complexity. Experimental results reveal significant bottlenecks in existing state-of-the-art MLLMs, providing critical insights for advancing multimodal reasoning within the remote sensing community.", "AI": {"tldr": "VLRS-Bench\u662f\u9996\u4e2a\u4e13\u95e8\u9488\u5bf9\u9065\u611f\u590d\u6742\u63a8\u7406\u4efb\u52a1\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b2000\u4e2a\u95ee\u7b54\u5bf9\uff0c\u6db5\u76d6\u8ba4\u77e5\u3001\u51b3\u7b56\u548c\u9884\u6d4b\u4e09\u4e2a\u7ef4\u5ea6\uff0c\u65e8\u5728\u63a8\u52a8\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u9065\u611f\u9886\u57df\u7684\u8ba4\u77e5\u80fd\u529b\u53d1\u5c55\u3002", "motivation": "\u73b0\u6709\u9065\u611f\u57fa\u51c6\u6d4b\u8bd5\u4e3b\u8981\u504f\u5411\u611f\u77e5\u4efb\u52a1\uff08\u5982\u76ee\u6807\u8bc6\u522b\u548c\u573a\u666f\u5206\u7c7b\uff09\uff0c\u8fd9\u9650\u5236\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8ba4\u77e5\u9700\u6c42\u9ad8\u7684\u9065\u611f\u5e94\u7528\u4e2d\u7684\u53d1\u5c55\uff0c\u9700\u8981\u4e13\u95e8\u7684\u590d\u6742\u63a8\u7406\u57fa\u51c6\u6765\u63a8\u52a8\u8be5\u9886\u57df\u8fdb\u6b65\u3002", "method": "\u63d0\u51fa\u4e86VLRS-Bench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b2000\u4e2a\u95ee\u7b54\u5bf9\uff0c\u5e73\u5747\u957f\u5ea671\u8bcd\uff0c\u6db5\u76d614\u4e2a\u4efb\u52a1\u548c\u6700\u591a8\u4e2a\u65f6\u95f4\u9636\u6bb5\u3002\u91c7\u7528\u4e13\u95e8\u6784\u5efa\u6d41\u7a0b\uff0c\u6574\u5408\u9065\u611f\u7279\u5b9a\u5148\u9a8c\u77e5\u8bc6\u548c\u4e13\u5bb6\u77e5\u8bc6\uff0c\u786e\u4fdd\u5730\u7406\u7a7a\u95f4\u771f\u5b9e\u6027\u548c\u63a8\u7406\u590d\u6742\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u73b0\u6709\u6700\u5148\u8fdb\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u9065\u611f\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e0a\u5b58\u5728\u663e\u8457\u74f6\u9888\uff0c\u4e3a\u9065\u611f\u793e\u533a\u63a8\u8fdb\u591a\u6a21\u6001\u63a8\u7406\u63d0\u4f9b\u4e86\u5173\u952e\u89c1\u89e3\u3002", "conclusion": "VLRS-Bench\u586b\u8865\u4e86\u9065\u611f\u9886\u57df\u590d\u6742\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u7684\u7a7a\u767d\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u672a\u6765\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u9065\u611f\u8ba4\u77e5\u5e94\u7528\u4e2d\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003\u6846\u67b6\u3002"}}
{"id": "2602.07399", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.07399", "abs": "https://arxiv.org/abs/2602.07399", "authors": ["Changhua Xu", "Jie Lu", "Junyu Xuan", "En Yu"], "title": "VGAS: Value-Guided Action-Chunk Selection for Few-Shot Vision-Language-Action Adaptation", "comment": "Preprint", "summary": "Vision--Language--Action (VLA) models bridge multimodal reasoning with physical control, but adapting them to new tasks with scarce demonstrations remains unreliable. While fine-tuned VLA policies often produce semantically plausible trajectories, failures often arise from unresolved geometric ambiguities, where near-miss action candidates lead to divergent execution outcomes under limited supervision. We study few-shot VLA adaptation from a \\emph{generation--selection} perspective and propose a novel framework \\textbf{VGAS} (\\textbf{V}alue-\\textbf{G}uided \\textbf{A}ction-chunk \\textbf{S}election). It performs inference-time best-of-$N$ selection to identify action chunks that are both semantically faithful and geometrically precise. Specifically, \\textbf{VGAS} employs a finetuned VLA as a high-recall proposal generator and introduces the \\textrm{Q-Chunk-Former}, a geometrically grounded Transformer critic to resolve fine-grained geometric ambiguities. In addition, we propose \\textit{Explicit Geometric Regularization} (\\texttt{EGR}), which explicitly shapes a discriminative value landscape to preserve action ranking resolution among near-miss candidates while mitigating value instability under scarce supervision. Experiments and theoretical analysis demonstrate that \\textbf{VGAS} consistently improves success rates and robustness under limited demonstrations and distribution shifts. Our code is available at https://github.com/Jyugo-15/VGAS.", "AI": {"tldr": "VGAS\u6846\u67b6\u901a\u8fc7\u751f\u6210-\u9009\u62e9\u8303\u5f0f\u89e3\u51b3VLA\u6a21\u578b\u5728\u5c11\u6837\u672c\u9002\u5e94\u4e2d\u7684\u51e0\u4f55\u6a21\u7cca\u95ee\u9898\uff0c\u4f7f\u7528\u4ef7\u503c\u5f15\u5bfc\u7684\u52a8\u4f5c\u5757\u9009\u62e9\u6765\u63d0\u5347\u8f68\u8ff9\u7684\u51e0\u4f55\u7cbe\u5ea6\u548c\u6210\u529f\u7387\u3002", "motivation": "\u73b0\u6709VLA\u6a21\u578b\u5728\u5c11\u6837\u672c\u9002\u5e94\u4e2d\u9762\u4e34\u51e0\u4f55\u6a21\u7cca\u95ee\u9898\uff0c\u5373\u4f7f\u751f\u6210\u8bed\u4e49\u5408\u7406\u7684\u8f68\u8ff9\uff0c\u7531\u4e8e\u51e0\u4f55\u4e0d\u786e\u5b9a\u6027\u4ecd\u4f1a\u5bfc\u81f4\u6267\u884c\u5931\u8d25\uff0c\u9700\u8981\u89e3\u51b3\u6709\u9650\u76d1\u7763\u4e0b\u7684\u51e0\u4f55\u7cbe\u5ea6\u95ee\u9898\u3002", "method": "\u63d0\u51faVGAS\u6846\u67b6\uff1a1) \u4f7f\u7528\u5fae\u8c03VLA\u4f5c\u4e3a\u9ad8\u53ec\u56de\u7387\u63d0\u8bae\u751f\u6210\u5668\uff1b2) \u5f15\u5165Q-Chunk-Former\u4f5c\u4e3a\u51e0\u4f55\u57fa\u7840Transformer\u6279\u8bc4\u5668\uff1b3) \u63d0\u51fa\u663e\u5f0f\u51e0\u4f55\u6b63\u5219\u5316(EGR)\u6765\u4fdd\u6301\u52a8\u4f5c\u6392\u5e8f\u5206\u8fa8\u7387\u5e76\u7f13\u89e3\u4ef7\u503c\u4e0d\u7a33\u5b9a\u6027\u3002", "result": "\u5b9e\u9a8c\u548c\u7406\u8bba\u5206\u6790\u8868\u660e\uff0cVGAS\u5728\u6709\u9650\u6f14\u793a\u548c\u5206\u5e03\u504f\u79fb\u4e0b\u80fd\u6301\u7eed\u63d0\u5347\u6210\u529f\u7387\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "VGAS\u901a\u8fc7\u751f\u6210-\u9009\u62e9\u8303\u5f0f\u6709\u6548\u89e3\u51b3\u4e86VLA\u6a21\u578b\u5c11\u6837\u672c\u9002\u5e94\u4e2d\u7684\u51e0\u4f55\u6a21\u7cca\u95ee\u9898\uff0c\u4e3a\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u7cfb\u7edf\u7684\u53ef\u9760\u9002\u5e94\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2602.07783", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07783", "abs": "https://arxiv.org/abs/2602.07783", "authors": ["Zejun Zhang", "Yixin Gan", "Zhenchang Xing", "Tian Zhang", "Yi Li", "Xiwei Xu", "Qinghua Lu", "Liming Zhu"], "title": "Still Manual? Automated Linter Configuration via DSL-Based LLM Compilation of Coding Standards", "comment": "Accepted By FSE2026", "summary": "Coding standards are essential for maintaining consistent and high-quality code across teams and projects. Linters help developers enforce these standards by detecting code violations. However, manual linter configuration is complex and expertise-intensive, and the diversity and evolution of programming languages, coding standards, and linters lead to repetitive and maintenance-intensive configuration work. To reduce manual effort, we propose LintCFG, a domain-specific language (DSL)-driven, LLM-based compilation approach to automate linter configuration generation for coding standards, independent of programming languages, coding standards, and linters. Inspired by compiler design, we first design a DSL to express coding rules in a tool-agnostic, structured, readable, and precise manner. Then, we build linter configurations into DSL configuration instructions. For a given natural language coding standard, the compilation process parses it into DSL coding standards, matches them with the DSL configuration instructions to set configuration names, option names and values, verifies consistency between the standards and configurations, and finally generates linter-specific configurations. Experiments with Checkstyle for Java coding standard show that our approach achieves over 90% precision and recall in DSL representation, with accuracy, precision, recall, and F1-scores close to 70% (with some exceeding 70%) in fine-grained linter configuration generation. Notably, our approach outperforms baselines by over 100% in precision. A user study further shows that our approach improves developers' efficiency in configuring linters for coding standards. Finally, we demonstrate the generality of the approach by generating ESLint configurations for JavaScript coding standards, showcasing its broad applicability across other programming languages, coding standards, and linters.", "AI": {"tldr": "LintCFG\uff1a\u57fa\u4e8eLLM\u7684DSL\u9a71\u52a8\u7f16\u8bd1\u65b9\u6cd5\uff0c\u81ea\u52a8\u5316\u751f\u6210\u4ee3\u7801\u68c0\u67e5\u5de5\u5177\u914d\u7f6e\uff0c\u652f\u6301\u8de8\u7f16\u7a0b\u8bed\u8a00\u3001\u7f16\u7801\u6807\u51c6\u548c\u68c0\u67e5\u5de5\u5177", "motivation": "\u624b\u52a8\u914d\u7f6e\u4ee3\u7801\u68c0\u67e5\u5de5\u5177\u590d\u6742\u4e14\u9700\u8981\u4e13\u4e1a\u77e5\u8bc6\uff0c\u7f16\u7a0b\u8bed\u8a00\u3001\u7f16\u7801\u6807\u51c6\u548c\u68c0\u67e5\u5de5\u5177\u7684\u591a\u6837\u6027\u53ca\u6f14\u53d8\u5bfc\u81f4\u91cd\u590d\u4e14\u7ef4\u62a4\u5bc6\u96c6\u7684\u914d\u7f6e\u5de5\u4f5c", "method": "\u8bbe\u8ba1\u9886\u57df\u7279\u5b9a\u8bed\u8a00\uff08DSL\uff09\u4ee5\u5de5\u5177\u65e0\u5173\u7684\u65b9\u5f0f\u8868\u8fbe\u7f16\u7801\u89c4\u5219\uff0c\u5c06\u68c0\u67e5\u5de5\u5177\u914d\u7f6e\u6784\u5efa\u4e3aDSL\u914d\u7f6e\u6307\u4ee4\uff0c\u901a\u8fc7\u7f16\u8bd1\u8fc7\u7a0b\u5c06\u81ea\u7136\u8bed\u8a00\u7f16\u7801\u6807\u51c6\u89e3\u6790\u4e3aDSL\u7f16\u7801\u6807\u51c6\uff0c\u5339\u914d\u914d\u7f6e\u6307\u4ee4\u5e76\u9a8c\u8bc1\u4e00\u81f4\u6027\uff0c\u6700\u7ec8\u751f\u6210\u7279\u5b9a\u68c0\u67e5\u5de5\u5177\u7684\u914d\u7f6e", "result": "\u5728Java\u7f16\u7801\u6807\u51c6\u7684Checkstyle\u5b9e\u9a8c\u4e2d\uff0cDSL\u8868\u793a\u7cbe\u5ea6\u548c\u53ec\u56de\u7387\u8d85\u8fc790%\uff0c\u7ec6\u7c92\u5ea6\u68c0\u67e5\u5de5\u5177\u914d\u7f6e\u751f\u6210\u7684\u51c6\u786e\u7387\u3001\u7cbe\u5ea6\u3001\u53ec\u56de\u7387\u548cF1\u5206\u6570\u63a5\u8fd170%\uff08\u90e8\u5206\u8d85\u8fc770%\uff09\uff0c\u7cbe\u5ea6\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u63d0\u5347\u8d85\u8fc7100%\u3002\u7528\u6237\u7814\u7a76\u8868\u660e\u63d0\u9ad8\u4e86\u5f00\u53d1\u4eba\u5458\u914d\u7f6e\u68c0\u67e5\u5de5\u5177\u7684\u6548\u7387\u548c\u65b9\u6cd5\u7684\u901a\u7528\u6027", "conclusion": "LintCFG\u65b9\u6cd5\u80fd\u6709\u6548\u81ea\u52a8\u5316\u4ee3\u7801\u68c0\u67e5\u5de5\u5177\u914d\u7f6e\uff0c\u652f\u6301\u8de8\u7f16\u7a0b\u8bed\u8a00\u3001\u7f16\u7801\u6807\u51c6\u548c\u68c0\u67e5\u5de5\u5177\uff0c\u663e\u8457\u63d0\u9ad8\u914d\u7f6e\u6548\u7387\u548c\u51c6\u786e\u6027"}}
{"id": "2602.07451", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.07451", "abs": "https://arxiv.org/abs/2602.07451", "authors": ["Huiling Zhen", "Weizhe Lin", "Renxi Liu", "Kai Han", "Yiming Li", "Yuchuan Tian", "Hanting Chen", "Xiaoguang Li", "Xiaosong Li", "Chen Chen", "Xianzhi Yu", "Mingxuan Yuan", "Youliang Yan", "Peifeng Qin", "Jun Wang", "Yu Wang", "Dacheng Tao", "Yunhe Wang"], "title": "DLLM Agent: See Farther, Run Faster", "comment": null, "summary": "Diffusion large language models (DLLMs) have emerged as an alternative to autoregressive (AR) decoding with appealing efficiency and modeling properties, yet their implications for agentic multi-step decision making remain underexplored. We ask a concrete question: when the generation paradigm is changed but the agent framework and supervision are held fixed, do diffusion backbones induce systematically different planning and tool-use behaviors, and do these differences translate into end-to-end efficiency gains? We study this in a controlled setting by instantiating DLLM and AR backbones within the same agent workflow (DeepDiver) and performing matched agent-oriented fine-tuning on the same trajectory data, yielding diffusion-backed DLLM Agents and directly comparable AR agents. Across benchmarks and case studies, we find that, at comparable accuracy, DLLM Agents are on average over 30% faster end to end than AR agents, with some cases exceeding 8x speedup. Conditioned on correct task completion, DLLM Agents also require fewer interaction rounds and tool invocations, consistent with higher planner hit rates that converge earlier to a correct action path with less backtracking. We further identify two practical considerations for deploying diffusion backbones in tool-using agents. First, naive DLLM policies are more prone to structured tool-call failures, necessitating stronger tool-call-specific training to emit valid schemas and arguments. Second, for multi-turn inputs interleaving context and action spans, diffusion-style span corruption requires aligned attention masking to avoid spurious context-action information flow; without such alignment, performance degrades. Finally, we analyze attention dynamics across workflow stages and observe paradigm-specific coordination patterns, suggesting stronger global planning signals in diffusion-backed agents.", "AI": {"tldr": "\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b\u5728\u667a\u80fd\u4f53\u51b3\u7b56\u4e2d\u76f8\u6bd4\u81ea\u56de\u5f52\u6a21\u578b\u80fd\u5b9e\u73b030%\u4ee5\u4e0a\u7684\u7aef\u5230\u7aef\u52a0\u901f\uff0c\u51cf\u5c11\u4ea4\u4e92\u8f6e\u6b21\u548c\u5de5\u5177\u8c03\u7528\uff0c\u4f46\u9700\u8981\u66f4\u5f3a\u7684\u5de5\u5177\u8c03\u7528\u8bad\u7ec3\u548c\u6ce8\u610f\u529b\u5bf9\u9f50", "motivation": "\u63a2\u7d22\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b\u5728\u667a\u80fd\u4f53\u591a\u6b65\u51b3\u7b56\u4e2d\u7684\u6f5c\u529b\uff0c\u6bd4\u8f83\u5176\u4e0e\u81ea\u56de\u5f52\u6a21\u578b\u5728\u76f8\u540c\u667a\u80fd\u4f53\u6846\u67b6\u548c\u76d1\u7763\u4e0b\u7684\u89c4\u5212\u4e0e\u5de5\u5177\u4f7f\u7528\u884c\u4e3a\u5dee\u5f02\uff0c\u4ee5\u53ca\u8fd9\u4e9b\u5dee\u5f02\u662f\u5426\u80fd\u8f6c\u5316\u4e3a\u7aef\u5230\u7aef\u6548\u7387\u63d0\u5347", "method": "\u5728\u76f8\u540c\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\uff08DeepDiver\uff09\u4e2d\u5b9e\u4f8b\u5316\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b\u548c\u81ea\u56de\u5f52\u6a21\u578b\u9aa8\u5e72\uff0c\u4f7f\u7528\u76f8\u540c\u7684\u8f68\u8ff9\u6570\u636e\u8fdb\u884c\u5339\u914d\u7684\u667a\u80fd\u4f53\u5bfc\u5411\u5fae\u8c03\uff0c\u521b\u5efa\u53ef\u76f4\u63a5\u6bd4\u8f83\u7684\u6269\u6563\u667a\u80fd\u4f53\u548c\u81ea\u56de\u5f52\u667a\u80fd\u4f53", "result": "\u5728\u51c6\u786e\u7387\u76f8\u5f53\u7684\u60c5\u51b5\u4e0b\uff0c\u6269\u6563\u667a\u80fd\u4f53\u7aef\u5230\u7aef\u901f\u5ea6\u5e73\u5747\u6bd4\u81ea\u56de\u5f52\u667a\u80fd\u4f53\u5feb30%\u4ee5\u4e0a\uff0c\u67d0\u4e9b\u60c5\u51b5\u4e0b\u8d85\u8fc78\u500d\u52a0\u901f\uff1b\u5728\u6b63\u786e\u5b8c\u6210\u4efb\u52a1\u65f6\uff0c\u6269\u6563\u667a\u80fd\u4f53\u9700\u8981\u66f4\u5c11\u7684\u4ea4\u4e92\u8f6e\u6b21\u548c\u5de5\u5177\u8c03\u7528\uff0c\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u89c4\u5212\u547d\u4e2d\u7387\u548c\u66f4\u65e9\u6536\u655b\u5230\u6b63\u786e\u884c\u52a8\u8def\u5f84", "conclusion": "\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b\u5728\u667a\u80fd\u4f53\u51b3\u7b56\u4e2d\u5177\u6709\u6548\u7387\u4f18\u52bf\uff0c\u4f46\u90e8\u7f72\u65f6\u9700\u8981\u6ce8\u610f\u4e24\u4e2a\u5b9e\u9645\u95ee\u9898\uff1a\u9700\u8981\u66f4\u5f3a\u7684\u5de5\u5177\u8c03\u7528\u7279\u5b9a\u8bad\u7ec3\u4ee5\u907f\u514d\u7ed3\u6784\u5316\u5de5\u5177\u8c03\u7528\u5931\u8d25\uff0c\u4ee5\u53ca\u9700\u8981\u5bf9\u9f50\u6ce8\u610f\u529b\u63a9\u7801\u4ee5\u907f\u514d\u865a\u5047\u7684\u4e0a\u4e0b\u6587-\u884c\u52a8\u4fe1\u606f\u6d41\uff1b\u6269\u6563\u667a\u80fd\u4f53\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u5168\u5c40\u89c4\u5212\u4fe1\u53f7"}}
{"id": "2602.07047", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.07047", "abs": "https://arxiv.org/abs/2602.07047", "authors": ["Muhammad Rashid", "Elvio G. Amparore", "Enrico Ferrari", "Damiano Verda"], "title": "ShapBPT: Image Feature Attributions Using Data-Aware Binary Partition Trees", "comment": "AAAI-2026", "summary": "Pixel-level feature attributions are an important tool in eXplainable AI for Computer Vision (XCV), providing visual insights into how image features influence model predictions. The Owen formula for hierarchical Shapley values has been widely used to interpret machine learning (ML) models and their learned representations. However, existing hierarchical Shapley approaches do not exploit the multiscale structure of image data, leading to slow convergence and weak alignment with the actual morphological features. Moreover, no prior Shapley method has leveraged data-aware hierarchies for Computer Vision tasks, leaving a gap in model interpretability of structured visual data. To address this, this paper introduces ShapBPT, a novel data-aware XCV method based on the hierarchical Shapley formula. ShapBPT assigns Shapley coefficients to a multiscale hierarchical structure tailored for images, the Binary Partition Tree (BPT). By using this data-aware hierarchical partitioning, ShapBPT ensures that feature attributions align with intrinsic image morphology, effectively prioritizing relevant regions while reducing computational overhead. This advancement connects hierarchical Shapley methods with image data, providing a more efficient and semantically meaningful approach to visual interpretability. Experimental results confirm ShapBPT's effectiveness, demonstrating superior alignment with image structures and improved efficiency over existing XCV methods, and a 20-subject user study confirming that ShapBPT explanations are preferred by humans.", "AI": {"tldr": "ShapBPT\u662f\u4e00\u79cd\u65b0\u7684\u53ef\u89e3\u91caAI\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u5206\u5c42Shapley\u503c\u4e0e\u56fe\u50cf\u7684\u591a\u5c3a\u5ea6\u7ed3\u6784\uff08BPT\uff09\u7ed3\u5408\uff0c\u63d0\u4f9b\u66f4\u9ad8\u6548\u3001\u8bed\u4e49\u66f4\u6e05\u6670\u7684\u50cf\u7d20\u7ea7\u7279\u5f81\u5f52\u56e0\u89e3\u91ca\u3002", "motivation": "\u73b0\u6709\u5206\u5c42Shapley\u65b9\u6cd5\u672a\u80fd\u5229\u7528\u56fe\u50cf\u6570\u636e\u7684\u591a\u5c3a\u5ea6\u7ed3\u6784\uff0c\u5bfc\u81f4\u6536\u655b\u6162\u3001\u4e0e\u771f\u5b9e\u5f62\u6001\u7279\u5f81\u5bf9\u9f50\u5dee\uff0c\u4e14\u7f3a\u4e4f\u9488\u5bf9\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\u7684\u6570\u636e\u611f\u77e5\u5c42\u6b21\u7ed3\u6784\uff0c\u9650\u5236\u4e86\u7ed3\u6784\u5316\u89c6\u89c9\u6570\u636e\u7684\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u63d0\u51faShapBPT\u65b9\u6cd5\uff0c\u57fa\u4e8e\u5206\u5c42Shapley\u516c\u5f0f\uff0c\u5c06Shapley\u7cfb\u6570\u5206\u914d\u7ed9\u4e13\u95e8\u4e3a\u56fe\u50cf\u8bbe\u8ba1\u7684\u4e8c\u8fdb\u5236\u5206\u5272\u6811\uff08BPT\uff09\u591a\u5c3a\u5ea6\u5c42\u6b21\u7ed3\u6784\uff0c\u901a\u8fc7\u6570\u636e\u611f\u77e5\u7684\u5c42\u6b21\u5206\u5272\u786e\u4fdd\u7279\u5f81\u5f52\u56e0\u4e0e\u5185\u5728\u56fe\u50cf\u5f62\u6001\u5bf9\u9f50\u3002", "result": "\u5b9e\u9a8c\u8bc1\u5b9eShapBPT\u7684\u6709\u6548\u6027\uff0c\u5c55\u793a\u51fa\u4e0e\u56fe\u50cf\u7ed3\u6784\u7684\u4f18\u8d8a\u5bf9\u9f50\u6027\uff0c\u76f8\u6bd4\u73b0\u6709XCV\u65b9\u6cd5\u6548\u7387\u66f4\u9ad8\uff0c20\u4eba\u7528\u6237\u7814\u7a76\u786e\u8ba4\u4eba\u7c7b\u66f4\u504f\u597dShapBPT\u7684\u89e3\u91ca\u3002", "conclusion": "ShapBPT\u5c06\u5206\u5c42Shapley\u65b9\u6cd5\u4e0e\u56fe\u50cf\u6570\u636e\u8fde\u63a5\u8d77\u6765\uff0c\u4e3a\u89c6\u89c9\u53ef\u89e3\u91ca\u6027\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u3001\u8bed\u4e49\u66f4\u6709\u610f\u4e49\u7684\u65b9\u6cd5\uff0c\u586b\u8865\u4e86\u7ed3\u6784\u5316\u89c6\u89c9\u6570\u636e\u6a21\u578b\u89e3\u91ca\u7684\u7a7a\u767d\u3002"}}
{"id": "2602.07821", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.07821", "abs": "https://arxiv.org/abs/2602.07821", "authors": ["Shinobu Saito"], "title": "Software Space Analytics: Towards Visualization and Statistics of Internal Software Execution", "comment": null, "summary": "In software maintenance work, software architects and programmers need to identify modules that require modification or deletion. Whilst user requests and bug reports are utilised for this purpose, evaluating the execution status of modules within the software is also crucial. This paper, therefore, applies spatial statistics to assess internal software execution data. First, we define a software space dataset, viewing the software's internal structure as a space based on module call relationships. Then, using spatial statistics, we conduct the visualization of spatial clusters and the statistical testing using spatial measures. Finally, we consider the usefulness of spatial statistics in the software engineering domain and future challenges.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5c06\u7a7a\u95f4\u7edf\u8ba1\u65b9\u6cd5\u5e94\u7528\u4e8e\u8f6f\u4ef6\u5de5\u7a0b\u9886\u57df\uff0c\u901a\u8fc7\u5206\u6790\u6a21\u5757\u8c03\u7528\u5173\u7cfb\u6765\u8bc6\u522b\u9700\u8981\u4fee\u6539\u6216\u5220\u9664\u7684\u8f6f\u4ef6\u6a21\u5757\u3002", "motivation": "\u5728\u8f6f\u4ef6\u7ef4\u62a4\u5de5\u4f5c\u4e2d\uff0c\u8f6f\u4ef6\u67b6\u6784\u5e08\u548c\u7a0b\u5e8f\u5458\u9700\u8981\u8bc6\u522b\u9700\u8981\u4fee\u6539\u6216\u5220\u9664\u7684\u6a21\u5757\u3002\u867d\u7136\u7528\u6237\u8bf7\u6c42\u548c\u9519\u8bef\u62a5\u544a\u53ef\u7528\u4e8e\u6b64\u76ee\u7684\uff0c\u4f46\u8bc4\u4f30\u8f6f\u4ef6\u5185\u90e8\u6a21\u5757\u7684\u6267\u884c\u72b6\u6001\u540c\u6837\u81f3\u5173\u91cd\u8981\u3002", "method": "\u9996\u5148\u5b9a\u4e49\u8f6f\u4ef6\u7a7a\u95f4\u6570\u636e\u96c6\uff0c\u5c06\u8f6f\u4ef6\u5185\u90e8\u7ed3\u6784\u89c6\u4e3a\u57fa\u4e8e\u6a21\u5757\u8c03\u7528\u5173\u7cfb\u7684\u7a7a\u95f4\u3002\u7136\u540e\u4f7f\u7528\u7a7a\u95f4\u7edf\u8ba1\u65b9\u6cd5\u8fdb\u884c\u7a7a\u95f4\u805a\u7c7b\u53ef\u89c6\u5316\u548c\u57fa\u4e8e\u7a7a\u95f4\u5ea6\u91cf\u7684\u7edf\u8ba1\u6d4b\u8bd5\u3002", "result": "\u8bba\u6587\u5c55\u793a\u4e86\u7a7a\u95f4\u7edf\u8ba1\u5728\u8f6f\u4ef6\u5de5\u7a0b\u9886\u57df\u7684\u5e94\u7528\u6f5c\u529b\uff0c\u901a\u8fc7\u53ef\u89c6\u5316\u7a7a\u95f4\u805a\u7c7b\u548c\u7edf\u8ba1\u6d4b\u8bd5\u6765\u8bc4\u4f30\u6a21\u5757\u6267\u884c\u72b6\u6001\u3002", "conclusion": "\u7a7a\u95f4\u7edf\u8ba1\u5728\u8f6f\u4ef6\u5de5\u7a0b\u9886\u57df\u5177\u6709\u5b9e\u7528\u6027\uff0c\u4f46\u672a\u6765\u4ecd\u9762\u4e34\u6311\u6218\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u548c\u89e3\u51b3\u3002"}}
{"id": "2602.07936", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2602.07936", "abs": "https://arxiv.org/abs/2602.07936", "authors": ["Tasnia Ashrafi Heya", "Sayed Erfan Arefin"], "title": "Privacy-Preserving Covert Communication Using Encrypted Wearable Gesture Recognition", "comment": null, "summary": "Secure communication is essential in covert and safety-critical settings where verbal interactions may expose user intent or operational context. Wearable gesture-based communication enables low-effort, nonverbal interaction, but existing systems leak motion data, intermediate representations, or inference outputs to untrusted infrastructure, enabling intent inference, behavioral biometric leakage, and insider attacks. This work proposes a privacy-preserving gesture-based covert communication system that ensures, no raw sensor signals, learned features, or classification outputs are exposed to any third-party. The system employs a multi-party homomorphic learning pipeline for gesture recognition directly over encrypted motion data, preventing adversaries from inferring gesture semantics, replaying sensor traces, or accessing intermediate representations. To our knowledge, this work is the first to apply encrypted gesture recognition in a wearable-based covert communication setting. We design and evaluate haptic and visual feedback mechanisms for covert signal delivery and evaluate the system using 600 gesture samples from a commodity smartwatch, achieving over 94.44% classification accuracy and demonstrating the feasibility of the proposed system with practical deployability from high-performance systems to resource-constrained edge devices.", "AI": {"tldr": "\u63d0\u51fa\u9996\u4e2a\u57fa\u4e8e\u53ef\u7a7f\u6234\u8bbe\u5907\u7684\u9690\u79c1\u4fdd\u62a4\u9690\u853d\u901a\u4fe1\u7cfb\u7edf\uff0c\u4f7f\u7528\u540c\u6001\u52a0\u5bc6\u5b9e\u73b0\u52a0\u5bc6\u624b\u52bf\u8bc6\u522b\uff0c\u9632\u6b62\u7b2c\u4e09\u65b9\u83b7\u53d6\u539f\u59cb\u4f20\u611f\u5668\u6570\u636e\u3001\u5b66\u4e60\u7279\u5f81\u6216\u5206\u7c7b\u7ed3\u679c", "motivation": "\u5728\u9690\u853d\u548c\u5b89\u5168\u5173\u952e\u573a\u666f\u4e2d\uff0c\u4f20\u7edf\u8bed\u97f3\u901a\u4fe1\u53ef\u80fd\u66b4\u9732\u7528\u6237\u610f\u56fe\u6216\u64cd\u4f5c\u4e0a\u4e0b\u6587\u3002\u73b0\u6709\u53ef\u7a7f\u6234\u624b\u52bf\u901a\u4fe1\u7cfb\u7edf\u4f1a\u6cc4\u9732\u8fd0\u52a8\u6570\u636e\u3001\u4e2d\u95f4\u8868\u793a\u6216\u63a8\u7406\u8f93\u51fa\uff0c\u5bfc\u81f4\u610f\u56fe\u63a8\u65ad\u3001\u884c\u4e3a\u751f\u7269\u7279\u5f81\u6cc4\u9732\u548c\u5185\u90e8\u653b\u51fb\u98ce\u9669", "method": "\u91c7\u7528\u591a\u65b9\u540c\u6001\u5b66\u4e60\u7ba1\u9053\uff0c\u76f4\u63a5\u5728\u52a0\u5bc6\u8fd0\u52a8\u6570\u636e\u4e0a\u8fdb\u884c\u624b\u52bf\u8bc6\u522b\uff0c\u9632\u6b62\u5bf9\u624b\u63a8\u65ad\u624b\u52bf\u8bed\u4e49\u3001\u91cd\u653e\u4f20\u611f\u5668\u8f68\u8ff9\u6216\u8bbf\u95ee\u4e2d\u95f4\u8868\u793a\u3002\u8bbe\u8ba1\u4e86\u89e6\u89c9\u548c\u89c6\u89c9\u53cd\u9988\u673a\u5236\u7528\u4e8e\u9690\u853d\u4fe1\u53f7\u4f20\u9012", "result": "\u5728\u5546\u7528\u667a\u80fd\u624b\u8868\u91c7\u96c6\u7684600\u4e2a\u624b\u52bf\u6837\u672c\u4e0a\u8bc4\u4f30\uff0c\u5206\u7c7b\u51c6\u786e\u7387\u8d85\u8fc794.44%\uff0c\u8bc1\u660e\u4e86\u7cfb\u7edf\u53ef\u884c\u6027\uff0c\u53ef\u4ece\u9ad8\u6027\u80fd\u7cfb\u7edf\u90e8\u7f72\u5230\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u8bbe\u5907", "conclusion": "\u8fd9\u662f\u9996\u4e2a\u5728\u53ef\u7a7f\u6234\u9690\u853d\u901a\u4fe1\u573a\u666f\u4e2d\u5e94\u7528\u52a0\u5bc6\u624b\u52bf\u8bc6\u522b\u7684\u5de5\u4f5c\uff0c\u901a\u8fc7\u9690\u79c1\u4fdd\u62a4\u8bbe\u8ba1\u786e\u4fdd\u539f\u59cb\u4f20\u611f\u5668\u4fe1\u53f7\u3001\u5b66\u4e60\u7279\u5f81\u548c\u5206\u7c7b\u8f93\u51fa\u90fd\u4e0d\u4f1a\u66b4\u9732\u7ed9\u7b2c\u4e09\u65b9\uff0c\u5b9e\u73b0\u4e86\u5b9e\u7528\u7684\u53ef\u90e8\u7f72\u6027"}}
{"id": "2602.07464", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.07464", "abs": "https://arxiv.org/abs/2602.07464", "authors": ["Yijie Chen", "Yijin Liu", "Fandong Meng"], "title": "SED-SFT: Selectively Encouraging Diversity in Supervised Fine-Tuning", "comment": "The code is publicly available at https://github.com/pppa2019/SED-SFT", "summary": "Supervised Fine-Tuning (SFT) followed by Reinforcement Learning (RL) has emerged as the standard post-training paradigm for large language models (LLMs). However, the conventional SFT process, driven by Cross-Entropy (CE) loss, often induces mode collapse, where models over-concentrate on specific response patterns. This lack of distributional diversity severely restricts the exploration efficiency required for subsequent RL. While recent studies have attempted to improve SFT by replacing the CE loss, aiming to preserve diversity or refine the update policy, they fail to adequately balance diversity and accuracy, thereby yielding suboptimal performance after RL. To address the mode collapse problem, we propose SED-SFT, which adaptively encourages diversity based on the token exploration space. This framework introduces a selective entropy regularization term with a selective masking mechanism into the optimization objective. Extensive experiments across eight mathematical benchmarks demonstrate that SED-SFT significantly enhances generation diversity with a negligible computational overhead increase compared with CE loss, yielding average improvements of 2.06 and 1.20 points in subsequent RL performance over standard CE-based baselines on Llama-3.2-3B-Instruct and Qwen2.5-Math-7B-Instruct, respectively. The code is publicly available at https://github.com/pppa2019/SED-SFT", "AI": {"tldr": "SED-SFT\u901a\u8fc7\u5f15\u5165\u9009\u62e9\u6027\u71b5\u6b63\u5219\u5316\u548c\u63a9\u7801\u673a\u5236\u89e3\u51b3SFT\u4e2d\u7684\u6a21\u5f0f\u5d29\u6e83\u95ee\u9898\uff0c\u63d0\u5347\u751f\u6210\u591a\u6837\u6027\uff0c\u4ece\u800c\u6539\u5584\u540e\u7eedRL\u6027\u80fd", "motivation": "\u4f20\u7edf\u7684SFT\u4f7f\u7528\u4ea4\u53c9\u71b5\u635f\u5931\u5bfc\u81f4\u6a21\u5f0f\u5d29\u6e83\uff0c\u6a21\u578b\u8fc7\u5ea6\u96c6\u4e2d\u5728\u7279\u5b9a\u54cd\u5e94\u6a21\u5f0f\u4e0a\uff0c\u7f3a\u4e4f\u5206\u5e03\u591a\u6837\u6027\uff0c\u8fd9\u4e25\u91cd\u9650\u5236\u4e86\u540e\u7eedRL\u7684\u63a2\u7d22\u6548\u7387\u3002\u73b0\u6709\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u5e73\u8861\u591a\u6837\u6027\u548c\u51c6\u786e\u6027\u3002", "method": "\u63d0\u51faSED-SFT\u6846\u67b6\uff0c\u57fa\u4e8e\u6807\u8bb0\u63a2\u7d22\u7a7a\u95f4\u81ea\u9002\u5e94\u5730\u9f13\u52b1\u591a\u6837\u6027\u3002\u5728\u4f18\u5316\u76ee\u6807\u4e2d\u5f15\u5165\u5e26\u6709\u9009\u62e9\u6027\u63a9\u7801\u673a\u5236\u7684\u9009\u62e9\u6027\u71b5\u6b63\u5219\u5316\u9879\u3002", "result": "\u5728\u516b\u4e2a\u6570\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSED-SFT\u663e\u8457\u63d0\u9ad8\u4e86\u751f\u6210\u591a\u6837\u6027\uff0c\u8ba1\u7b97\u5f00\u9500\u589e\u52a0\u53ef\u5ffd\u7565\u3002\u76f8\u6bd4\u6807\u51c6CE\u57fa\u7ebf\uff0c\u5728Llama-3.2-3B-Instruct\u548cQwen2.5-Math-7B-Instruct\u4e0a\u540e\u7eedRL\u6027\u80fd\u5e73\u5747\u5206\u522b\u63d0\u53472.06\u548c1.20\u4e2a\u70b9\u3002", "conclusion": "SED-SFT\u6709\u6548\u89e3\u51b3\u4e86SFT\u4e2d\u7684\u6a21\u5f0f\u5d29\u6e83\u95ee\u9898\uff0c\u901a\u8fc7\u63d0\u5347\u751f\u6210\u591a\u6837\u6027\u6539\u5584\u4e86\u540e\u7eedRL\u6027\u80fd\uff0c\u4e3aLLM\u540e\u8bad\u7ec3\u63d0\u4f9b\u4e86\u66f4\u4f18\u7684SFT\u65b9\u6cd5\u3002"}}
{"id": "2602.07049", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.07049", "abs": "https://arxiv.org/abs/2602.07049", "authors": ["Jindong Li", "Dario Zanca", "Vincent Christlein", "Tim Hamann", "Jens Barth", "Peter K\u00e4mpf", "Bj\u00f6rn Eskofier"], "title": "Enhancing IMU-Based Online Handwriting Recognition via Contrastive Learning with Zero Inference Overhead", "comment": null, "summary": "Online handwriting recognition using inertial measurement units opens up handwriting on paper as input for digital devices. Doing it on edge hardware improves privacy and lowers latency, but entails memory constraints. To address this, we propose Error-enhanced Contrastive Handwriting Recognition (ECHWR), a training framework designed to improve feature representation and recognition accuracy without increasing inference costs. ECHWR utilizes a temporary auxiliary branch that aligns sensor signals with semantic text embeddings during the training phase. This alignment is maintained through a dual contrastive objective: an in-batch contrastive loss for general modality alignment and a novel error-based contrastive loss that distinguishes between correct signals and synthetic hard negatives. The auxiliary branch is discarded after training, which allows the deployed model to keep its original, efficient architecture. Evaluations on the OnHW-Words500 dataset show that ECHWR significantly outperforms state-of-the-art baselines, reducing character error rates by up to 7.4% on the writer-independent split and 10.4% on the writer-dependent split. Finally, although our ablation studies indicate that solving specific challenges require specific architectural and objective configurations, error-based contrastive loss shows its effectiveness for handling unseen writing styles.", "AI": {"tldr": "\u63d0\u51faECHWR\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u4e34\u65f6\u8f85\u52a9\u5206\u652f\u548c\u53cc\u91cd\u5bf9\u6bd4\u635f\u5931\u63d0\u5347\u57fa\u4e8e\u60ef\u6027\u6d4b\u91cf\u5355\u5143\u7684\u624b\u5199\u8bc6\u522b\u6027\u80fd\uff0c\u4e0d\u589e\u52a0\u63a8\u7406\u6210\u672c", "motivation": "\u5728\u7ebf\u624b\u5199\u8bc6\u522b\u4f7f\u7528\u60ef\u6027\u6d4b\u91cf\u5355\u5143\u53ef\u5b9e\u73b0\u7eb8\u4e0a\u624b\u5199\u4f5c\u4e3a\u6570\u5b57\u8bbe\u5907\u8f93\u5165\uff0c\u4f46\u5728\u8fb9\u7f18\u786c\u4ef6\u4e0a\u8fd0\u884c\u9762\u4e34\u5185\u5b58\u9650\u5236\uff0c\u9700\u8981\u5728\u4fdd\u6301\u63a8\u7406\u6548\u7387\u7684\u540c\u65f6\u63d0\u5347\u8bc6\u522b\u7cbe\u5ea6", "method": "\u63d0\u51faECHWR\u8bad\u7ec3\u6846\u67b6\uff0c\u5305\u542b\u4e34\u65f6\u8f85\u52a9\u5206\u652f\u5c06\u4f20\u611f\u5668\u4fe1\u53f7\u4e0e\u8bed\u4e49\u6587\u672c\u5d4c\u5165\u5bf9\u9f50\uff0c\u91c7\u7528\u53cc\u91cd\u5bf9\u6bd4\u76ee\u6807\uff1a\u6279\u91cf\u5185\u5bf9\u6bd4\u635f\u5931\u7528\u4e8e\u6a21\u6001\u5bf9\u9f50\uff0c\u65b0\u9896\u7684\u57fa\u4e8e\u9519\u8bef\u7684\u5bf9\u6bd4\u635f\u5931\u533a\u5206\u6b63\u786e\u4fe1\u53f7\u548c\u5408\u6210\u786c\u8d1f\u6837\u672c\uff0c\u8bad\u7ec3\u540e\u4e22\u5f03\u8f85\u52a9\u5206\u652f", "result": "\u5728OnHW-Words500\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u57fa\u7ebf\uff0c\u5728\u72ec\u7acb\u4e8e\u4f5c\u8005\u7684\u5212\u5206\u4e0a\u5b57\u7b26\u9519\u8bef\u7387\u964d\u4f4e7.4%\uff0c\u5728\u4f9d\u8d56\u4e8e\u4f5c\u8005\u7684\u5212\u5206\u4e0a\u964d\u4f4e10.4%\uff0c\u57fa\u4e8e\u9519\u8bef\u7684\u5bf9\u6bd4\u635f\u5931\u5bf9\u5904\u7406\u672a\u89c1\u8fc7\u7684\u4e66\u5199\u98ce\u683c\u6709\u6548", "conclusion": "ECHWR\u6846\u67b6\u80fd\u5728\u4e0d\u589e\u52a0\u63a8\u7406\u6210\u672c\u7684\u60c5\u51b5\u4e0b\u63d0\u5347\u624b\u5199\u8bc6\u522b\u6027\u80fd\uff0c\u57fa\u4e8e\u9519\u8bef\u7684\u5bf9\u6bd4\u635f\u5931\u7279\u522b\u9002\u7528\u4e8e\u5904\u7406\u672a\u89c1\u8fc7\u7684\u4e66\u5199\u98ce\u683c\uff0c\u4f46\u89e3\u51b3\u7279\u5b9a\u6311\u6218\u9700\u8981\u7279\u5b9a\u7684\u67b6\u6784\u548c\u76ee\u6807\u914d\u7f6e"}}
{"id": "2602.07871", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.07871", "abs": "https://arxiv.org/abs/2602.07871", "authors": ["Xiang Li", "Siyu Lu", "Sarro Federica", "Claire Le Goues", "He Ye"], "title": "HerAgent: Rethinking the Automated Environment Deployment via Hierarchical Test Pyramid", "comment": null, "summary": "Automated software environment setup is a prerequisite for testing, debugging, and reproducing failures, yet remains challenging in practice due to complex dependencies, heterogeneous build systems, and incomplete documentation. Recent work leverages large language models to automate this process, but typically evaluates success using weak signals such as dependency installation or partial test execution, which do not ensure that a project can actually run. In this paper, we argue that environment setup success should be evaluated through executable evidence rather than a single binary signal. We introduce the Environment Maturity Hierarchy, which defines three success levels based on progressively stronger execution requirements, culminating in successful execution of a project's main entry point. Guided by this hierarchy, we propose HerAgent, an automated environment setup approach that incrementally constructs executable environments through execution-based validation and repair. We evaluate HerAgent on four public benchmarks, where it outperforms all related work, achieving up to 79.6\\% improvement due to its holistic understanding of project structure and dependencies. On complex C/C++ projects, HerAgent surpasses prior approaches by 66.7\\%. In addition, HerAgent uniquely resolves 11-30 environment instances across the benchmarks that no prior method can configure.", "AI": {"tldr": "HerAgent\uff1a\u57fa\u4e8e\u73af\u5883\u6210\u719f\u5ea6\u5c42\u6b21\u7ed3\u6784\u7684\u81ea\u52a8\u5316\u8f6f\u4ef6\u73af\u5883\u8bbe\u7f6e\u65b9\u6cd5\uff0c\u901a\u8fc7\u6267\u884c\u9a8c\u8bc1\u548c\u4fee\u590d\u9010\u6b65\u6784\u5efa\u53ef\u6267\u884c\u73af\u5883\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u81ea\u52a8\u5316\u8f6f\u4ef6\u73af\u5883\u8bbe\u7f6e\u9762\u4e34\u4f9d\u8d56\u590d\u6742\u3001\u6784\u5efa\u7cfb\u7edf\u5f02\u6784\u3001\u6587\u6863\u4e0d\u5b8c\u6574\u7b49\u6311\u6218\u3002\u73b0\u6709\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u65b9\u6cd5\u901a\u5e38\u4f7f\u7528\u5f31\u4fe1\u53f7\uff08\u5982\u4f9d\u8d56\u5b89\u88c5\u6216\u90e8\u5206\u6d4b\u8bd5\u6267\u884c\uff09\u8bc4\u4f30\u6210\u529f\uff0c\u65e0\u6cd5\u786e\u4fdd\u9879\u76ee\u771f\u6b63\u53ef\u8fd0\u884c\u3002", "method": "\u63d0\u51fa\u73af\u5883\u6210\u719f\u5ea6\u5c42\u6b21\u7ed3\u6784\uff0c\u5b9a\u4e49\u57fa\u4e8e\u9010\u6b65\u589e\u5f3a\u6267\u884c\u8981\u6c42\u7684\u4e09\u4e2a\u6210\u529f\u7ea7\u522b\uff0c\u6700\u7ec8\u8981\u6c42\u9879\u76ee\u4e3b\u5165\u53e3\u70b9\u6210\u529f\u6267\u884c\u3002\u57fa\u4e8e\u6b64\u5c42\u6b21\u7ed3\u6784\uff0c\u5f00\u53d1HerAgent\u65b9\u6cd5\uff0c\u901a\u8fc7\u6267\u884c\u9a8c\u8bc1\u548c\u4fee\u590d\u9010\u6b65\u6784\u5efa\u53ef\u6267\u884c\u73af\u5883\u3002", "result": "\u5728\u56db\u4e2a\u516c\u5171\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cHerAgent\u4f18\u4e8e\u6240\u6709\u76f8\u5173\u5de5\u4f5c\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8fbe79.6%\u7684\u6539\u8fdb\u3002\u5728\u590d\u6742\u7684C/C++\u9879\u76ee\u4e0a\uff0c\u6bd4\u5148\u524d\u65b9\u6cd5\u63d0\u534766.7%\u3002\u6b64\u5916\uff0cHerAgent\u72ec\u7279\u89e3\u51b3\u4e8611-30\u4e2a\u73af\u5883\u5b9e\u4f8b\uff0c\u8fd9\u4e9b\u5b9e\u4f8b\u662f\u5148\u524d\u65b9\u6cd5\u65e0\u6cd5\u914d\u7f6e\u7684\u3002", "conclusion": "\u73af\u5883\u8bbe\u7f6e\u6210\u529f\u5e94\u901a\u8fc7\u53ef\u6267\u884c\u8bc1\u636e\u800c\u975e\u5355\u4e00\u4e8c\u8fdb\u5236\u4fe1\u53f7\u8bc4\u4f30\u3002HerAgent\u901a\u8fc7\u73af\u5883\u6210\u719f\u5ea6\u5c42\u6b21\u7ed3\u6784\u548c\u6267\u884c\u9a8c\u8bc1\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u81ea\u52a8\u5316\u73af\u5883\u8bbe\u7f6e\u7684\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2602.07497", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.07497", "abs": "https://arxiv.org/abs/2602.07497", "authors": ["Mo Wang", "Kaixuan Ren", "Pratik Jalan", "Ahmed Ashraf", "Tuong Vy Vu", "Rahul Seetharaman", "Shah Nawaz", "Usman Naseem"], "title": "From Native Memes to Global Moderation: Cros-Cultural Evaluation of Vision-Language Models for Hateful Meme Detection", "comment": "12 pages, 5 figures, Proceedings of the ACM Web Conference 2026 (WWW '26)", "summary": "Cultural context profoundly shapes how people interpret online content, yet vision-language models (VLMs) remain predominantly trained through Western or English-centric lenses. This limits their fairness and cross-cultural robustness in tasks like hateful meme detection. We introduce a systematic evaluation framework designed to diagnose and quantify the cross-cultural robustness of state-of-the-art VLMs across multilingual meme datasets, analyzing three axes: (i) learning strategy (zero-shot vs. one-shot), (ii) prompting language (native vs. English), and (iii) translation effects on meaning and detection. Results show that the common ``translate-then-detect'' approach deteriorate performance, while culturally aligned interventions - native-language prompting and one-shot learning - significantly enhance detection. Our findings reveal systematic convergence toward Western safety norms and provide actionable strategies to mitigate such bias, guiding the design of globally robust multimodal moderation systems.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u8bc4\u4f30\u6846\u67b6\uff0c\u7528\u4e8e\u8bca\u65ad\u548c\u91cf\u5316\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u591a\u8bed\u8a00\u8868\u60c5\u5305\u6570\u636e\u96c6\u4e0a\u7684\u8de8\u6587\u5316\u9c81\u68d2\u6027\uff0c\u53d1\u73b0\u7ffb\u8bd1\u68c0\u6d4b\u65b9\u6cd5\u4f1a\u964d\u4f4e\u6027\u80fd\uff0c\u800c\u6587\u5316\u5bf9\u9f50\u5e72\u9884\uff08\u672c\u5730\u8bed\u8a00\u63d0\u793a\u548c\u5355\u6837\u672c\u5b66\u4e60\uff09\u80fd\u663e\u8457\u63d0\u5347\u68c0\u6d4b\u6548\u679c\u3002", "motivation": "\u6587\u5316\u80cc\u666f\u6df1\u523b\u5f71\u54cd\u4eba\u4eec\u5bf9\u5728\u7ebf\u5185\u5bb9\u7684\u7406\u89e3\uff0c\u4f46\u5f53\u524d\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e3b\u8981\u57fa\u4e8e\u897f\u65b9\u6216\u82f1\u8bed\u4e2d\u5fc3\u89c6\u89d2\u8bad\u7ec3\uff0c\u8fd9\u9650\u5236\u4e86\u5b83\u4eec\u5728\u4ec7\u6068\u8868\u60c5\u5305\u68c0\u6d4b\u7b49\u4efb\u52a1\u4e2d\u7684\u516c\u5e73\u6027\u548c\u8de8\u6587\u5316\u9c81\u68d2\u6027\u3002", "method": "\u5f15\u5165\u4e00\u4e2a\u7cfb\u7edf\u8bc4\u4f30\u6846\u67b6\uff0c\u901a\u8fc7\u4e09\u4e2a\u7ef4\u5ea6\u5206\u6790\u6700\u5148\u8fdb\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u8de8\u6587\u5316\u9c81\u68d2\u6027\uff1a(1) \u5b66\u4e60\u7b56\u7565\uff08\u96f6\u6837\u672c vs \u5355\u6837\u672c\uff09\uff1b(2) \u63d0\u793a\u8bed\u8a00\uff08\u672c\u5730\u8bed\u8a00 vs \u82f1\u8bed\uff09\uff1b(3) \u7ffb\u8bd1\u5bf9\u610f\u4e49\u548c\u68c0\u6d4b\u7684\u5f71\u54cd\u3002", "result": "\u7ed3\u679c\u663e\u793a\u5e38\u89c1\u7684\"\u5148\u7ffb\u8bd1\u518d\u68c0\u6d4b\"\u65b9\u6cd5\u4f1a\u964d\u4f4e\u6027\u80fd\uff0c\u800c\u6587\u5316\u5bf9\u9f50\u5e72\u9884\uff08\u672c\u5730\u8bed\u8a00\u63d0\u793a\u548c\u5355\u6837\u672c\u5b66\u4e60\uff09\u80fd\u663e\u8457\u63d0\u5347\u68c0\u6d4b\u6548\u679c\u3002\u7814\u7a76\u53d1\u73b0\u6a21\u578b\u7cfb\u7edf\u6027\u5730\u8d8b\u5411\u897f\u65b9\u5b89\u5168\u89c4\u8303\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5b58\u5728\u7684\u7cfb\u7edf\u6027\u6587\u5316\u504f\u89c1\uff0c\u5e76\u63d0\u4f9b\u4e86\u53ef\u64cd\u4f5c\u7684\u7b56\u7565\u6765\u51cf\u8f7b\u8fd9\u79cd\u504f\u89c1\uff0c\u4e3a\u8bbe\u8ba1\u5168\u7403\u9c81\u68d2\u7684\u591a\u6a21\u6001\u5185\u5bb9\u5ba1\u6838\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002"}}
{"id": "2602.07050", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07050", "abs": "https://arxiv.org/abs/2602.07050", "authors": ["Sonia Joseph", "Quentin Garrido", "Randall Balestriero", "Matthew Kowal", "Thomas Fel", "Shahab Bakhtiari", "Blake Richards", "Mike Rabbat"], "title": "Interpreting Physics in Video World Models", "comment": null, "summary": "A long-standing question in physical reasoning is whether video-based models need to rely on factorized representations of physical variables in order to make physically accurate predictions, or whether they can implicitly represent such variables in a task-specific, distributed manner. While modern video world models achieve strong performance on intuitive physics benchmarks, it remains unclear which of these representational regimes they implement internally. Here, we present the first interpretability study to directly examine physical representations inside large-scale video encoders. Using layerwise probing, subspace geometry, patch-level decoding, and targeted attention ablations, we characterize where physical information becomes accessible and how it is organized within encoder-based video transformers.\n  Across architectures, we identify a sharp intermediate-depth transition -- which we call the Physics Emergence Zone -- at which physical variables become accessible. Physics-related representations peak shortly after this transition and degrade toward the output layers. Decomposing motion into explicit variables, we find that scalar quantities such as speed and acceleration are available from early layers onwards, whereas motion direction becomes accessible only at the Physics Emergence Zone. Notably, we find that direction is encoded through a high-dimensional population structure with circular geometry, requiring coordinated multi-feature intervention to control. These findings suggest that modern video models do not use factorized representations of physical variables like a classical physics engine. Instead, they use a distributed representation that is nonetheless sufficient for making physical predictions.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u73b0\u4ee3\u89c6\u9891\u6a21\u578b\u4e0d\u4f7f\u7528\u7ecf\u5178\u7269\u7406\u5f15\u64ce\u90a3\u6837\u7684\u56e0\u5b50\u5316\u7269\u7406\u53d8\u91cf\u8868\u793a\uff0c\u800c\u662f\u91c7\u7528\u5206\u5e03\u5f0f\u8868\u793a\uff0c\u4f46\u4ecd\u80fd\u505a\u51fa\u51c6\u786e\u7684\u7269\u7406\u9884\u6d4b", "motivation": "\u63a2\u7d22\u89c6\u9891\u6a21\u578b\u662f\u5426\u4f9d\u8d56\u56e0\u5b50\u5316\u7684\u7269\u7406\u53d8\u91cf\u8868\u793a\u6765\u505a\u51fa\u51c6\u786e\u7269\u7406\u9884\u6d4b\uff0c\u8fd8\u662f\u91c7\u7528\u4efb\u52a1\u7279\u5b9a\u7684\u5206\u5e03\u5f0f\u8868\u793a\uff0c\u4ee5\u53ca\u7269\u7406\u4fe1\u606f\u5728\u89c6\u9891\u7f16\u7801\u5668\u4e2d\u7684\u7ec4\u7ec7\u65b9\u5f0f", "method": "\u4f7f\u7528\u5206\u5c42\u63a2\u6d4b\u3001\u5b50\u7a7a\u95f4\u51e0\u4f55\u3001\u8865\u4e01\u7ea7\u89e3\u7801\u548c\u9488\u5bf9\u6027\u6ce8\u610f\u529b\u6d88\u878d\u7b49\u65b9\u6cd5\uff0c\u5206\u6790\u5927\u89c4\u6a21\u89c6\u9891\u7f16\u7801\u5668\u5185\u90e8\u7684\u7269\u7406\u8868\u793a", "result": "\u53d1\u73b0\u7269\u7406\u4fe1\u606f\u5728\u4e2d\u95f4\u6df1\u5ea6\u51fa\u73b0\u6025\u5267\u8f6c\u53d8\uff08\u7269\u7406\u6d8c\u73b0\u533a\uff09\uff0c\u6807\u91cf\u7269\u7406\u91cf\u4ece\u65e9\u671f\u5c42\u5373\u53ef\u83b7\u5f97\uff0c\u800c\u8fd0\u52a8\u65b9\u5411\u4ec5\u5728\u7269\u7406\u6d8c\u73b0\u533a\u624d\u53d8\u5f97\u53ef\u8bbf\u95ee\uff0c\u65b9\u5411\u901a\u8fc7\u5177\u6709\u5706\u5f62\u51e0\u4f55\u7ed3\u6784\u7684\u9ad8\u7ef4\u7fa4\u4f53\u7ed3\u6784\u7f16\u7801", "conclusion": "\u73b0\u4ee3\u89c6\u9891\u6a21\u578b\u4e0d\u4f7f\u7528\u7ecf\u5178\u7269\u7406\u5f15\u64ce\u7684\u56e0\u5b50\u5316\u7269\u7406\u53d8\u91cf\u8868\u793a\uff0c\u800c\u662f\u91c7\u7528\u5206\u5e03\u5f0f\u8868\u793a\uff0c\u8fd9\u79cd\u8868\u793a\u65b9\u5f0f\u8db3\u4ee5\u505a\u51fa\u7269\u7406\u9884\u6d4b"}}
{"id": "2602.07882", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.07882", "abs": "https://arxiv.org/abs/2602.07882", "authors": ["Chen Xie", "Yuling Shi", "Xiaodong Gu", "Beijun Shen"], "title": "Rethinking Code Complexity Through the Lens of Large Language Models", "comment": null, "summary": "Code complexity metrics such as cyclomatic complexity have long been used to assess software quality and maintainability. With the rapid advancement of large language models (LLMs) on code understanding and generation tasks, an important yet underexplored question arises: do these traditional complexity metrics meaningfully characterize the difficulty LLMs experience when processing code? In this work, we empirically demonstrate that, after controlling for code length, classical metrics exhibit no consistent correlation with LLM performance, revealing a fundamental mismatch with model-perceived difficulty. To address this gap, we propose LM-CC, a novel code complexity metric designed from the perspective of LLMs. The core premise of LM-CC is that LLM-perceived difficulty is driven by the nonlinearity of program semantics. Accordingly, we decompose programs into semantic units based on entropy, organize these units into a compositional hierarchy, and quantify complexity as a principled aggregation of compositional level and branching-induced divergence, capturing cumulative model uncertainty during code processing. Our extensive experiments show that LM-CC not only correlates more strongly with LLM performance than traditional metrics but also that lowering it directly enhances task performance.", "AI": {"tldr": "\u4f20\u7edf\u4ee3\u7801\u590d\u6742\u5ea6\u6307\u6807\u4e0eLLM\u5904\u7406\u4ee3\u7801\u7684\u96be\u5ea6\u4e0d\u76f8\u5173\uff0c\u4f5c\u8005\u63d0\u51fa\u57fa\u4e8eLLM\u89c6\u89d2\u7684\u65b0\u590d\u6742\u5ea6\u6307\u6807LM-CC\uff0c\u8be5\u6307\u6807\u80fd\u66f4\u597d\u9884\u6d4bLLM\u6027\u80fd", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4ee3\u7801\u7406\u89e3\u548c\u751f\u6210\u4efb\u52a1\u4e0a\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u4e00\u4e2a\u91cd\u8981\u4f46\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u7684\u95ee\u9898\u662f\uff1a\u4f20\u7edf\u7684\u4ee3\u7801\u590d\u6742\u5ea6\u6307\u6807\uff08\u5982\u5708\u590d\u6742\u5ea6\uff09\u662f\u5426\u80fd\u6709\u6548\u8868\u5f81LLM\u5904\u7406\u4ee3\u7801\u65f6\u9047\u5230\u7684\u56f0\u96be\uff1f\u4f5c\u8005\u53d1\u73b0\u4f20\u7edf\u6307\u6807\u4e0eLLM\u6027\u80fd\u4e4b\u95f4\u7f3a\u4e4f\u4e00\u81f4\u6027\u5173\u8054\u3002", "method": "\u63d0\u51faLM-CC\uff08\u8bed\u8a00\u6a21\u578b\u4ee3\u7801\u590d\u6742\u5ea6\uff09\u6307\u6807\uff0c\u5176\u6838\u5fc3\u524d\u63d0\u662fLLM\u611f\u77e5\u7684\u96be\u5ea6\u7531\u7a0b\u5e8f\u8bed\u4e49\u7684\u975e\u7ebf\u6027\u9a71\u52a8\u3002\u65b9\u6cd5\u5305\u62ec\uff1a\u57fa\u4e8e\u71b5\u5c06\u7a0b\u5e8f\u5206\u89e3\u4e3a\u8bed\u4e49\u5355\u5143\uff0c\u5c06\u8fd9\u4e9b\u5355\u5143\u7ec4\u7ec7\u6210\u7ec4\u5408\u5c42\u6b21\u7ed3\u6784\uff0c\u5e76\u901a\u8fc7\u7ec4\u5408\u5c42\u6b21\u548c\u5206\u652f\u5f15\u8d77\u7684\u53d1\u6563\u7684\u539f\u5219\u6027\u805a\u5408\u6765\u91cf\u5316\u590d\u6742\u5ea6\uff0c\u6355\u6349\u4ee3\u7801\u5904\u7406\u8fc7\u7a0b\u4e2d\u7684\u7d2f\u79ef\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5728\u63a7\u5236\u4ee3\u7801\u957f\u5ea6\u540e\uff0c\u4f20\u7edf\u590d\u6742\u5ea6\u6307\u6807\u4e0eLLM\u6027\u80fd\u6ca1\u6709\u4e00\u81f4\u76f8\u5173\u6027\u3002\u800cLM-CC\u4e0d\u4ec5\u6bd4\u4f20\u7edf\u6307\u6807\u4e0eLLM\u6027\u80fd\u7684\u76f8\u5173\u6027\u66f4\u5f3a\uff0c\u800c\u4e14\u964d\u4f4eLM-CC\u80fd\u76f4\u63a5\u63d0\u5347\u4efb\u52a1\u6027\u80fd\u3002", "conclusion": "\u4f20\u7edf\u4ee3\u7801\u590d\u6742\u5ea6\u6307\u6807\u65e0\u6cd5\u6709\u6548\u8868\u5f81LLM\u5904\u7406\u4ee3\u7801\u7684\u96be\u5ea6\uff0c\u9700\u8981\u4eceLLM\u89c6\u89d2\u91cd\u65b0\u5b9a\u4e49\u4ee3\u7801\u590d\u6742\u5ea6\u3002LM-CC\u4f5c\u4e3a\u57fa\u4e8eLLM\u611f\u77e5\u7684\u65b0\u6307\u6807\uff0c\u80fd\u66f4\u597d\u5730\u9884\u6d4b\u548c\u6539\u5584LLM\u5728\u4ee3\u7801\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u3002"}}
{"id": "2602.07499", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.07499", "abs": "https://arxiv.org/abs/2602.07499", "authors": ["Jingshen Zhang", "Xin Ying Qiu", "Lifang Lu", "Zhuhua Huang", "Yutao Hu", "Yuechang Wu", "JunYu Lu"], "title": "Let's Simplify Step by Step: Guiding LLM Towards Multilingual Unsupervised Proficiency-Controlled Sentence Simplification", "comment": "Accepted to EACL 2026 Findings", "summary": "Large language models demonstrate limited capability in proficiency-controlled sentence simplification, particularly when simplifying across large readability levels. We propose a framework that decomposes complex simplifications into manageable steps through dynamic path planning, semantic-aware exemplar selection, and chain-of-thought generation with conversation history for coherent reasoning. Evaluation on five languages across two benchmarks shows our approach improves simplification effectiveness while reducing computational steps by 22-42%. Human evaluation confirms the fundamental trade-off between simplification effectiveness and meaning preservation. Notably, even human annotators struggle to agree on semantic preservation judgments, highlighting the inherent complexity of this task. Our work shows that while step-by-step simplification improves control, preserving semantic fidelity during extensive simplification remains an open challenge.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e00\u4e2a\u901a\u8fc7\u52a8\u6001\u8def\u5f84\u89c4\u5212\u3001\u8bed\u4e49\u611f\u77e5\u793a\u4f8b\u9009\u62e9\u548c\u5bf9\u8bdd\u5386\u53f2\u94fe\u5f0f\u63a8\u7406\u7684\u6846\u67b6\uff0c\u5c06\u590d\u6742\u53e5\u5b50\u7b80\u5316\u5206\u89e3\u4e3a\u53ef\u7ba1\u7406\u6b65\u9aa4\uff0c\u5728\u4e94\u4e2a\u8bed\u8a00\u7684\u4e24\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u63d0\u5347\u7b80\u5316\u6548\u679c\u5e76\u51cf\u5c1122-42%\u8ba1\u7b97\u6b65\u9aa4\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u53ef\u63a7\u96be\u5ea6\u53e5\u5b50\u7b80\u5316\u65b9\u9762\u80fd\u529b\u6709\u9650\uff0c\u7279\u522b\u662f\u5728\u8de8\u8d8a\u8f83\u5927\u53ef\u8bfb\u6027\u6c34\u5e73\u8fdb\u884c\u7b80\u5316\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u7b80\u5316\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u8def\u5f84\u89c4\u5212\u5c06\u590d\u6742\u7b80\u5316\u5206\u89e3\u4e3a\u53ef\u7ba1\u7406\u6b65\u9aa4\uff0c\u7ed3\u5408\u8bed\u4e49\u611f\u77e5\u7684\u793a\u4f8b\u9009\u62e9\u548c\u57fa\u4e8e\u5bf9\u8bdd\u5386\u53f2\u7684\u94fe\u5f0f\u63a8\u7406\u751f\u6210\uff0c\u5b9e\u73b0\u8fde\u8d2f\u7684\u63a8\u7406\u8fc7\u7a0b\u3002", "result": "\u5728\u4e94\u4e2a\u8bed\u8a00\u7684\u4e24\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u63d0\u9ad8\u4e86\u7b80\u5316\u6548\u679c\uff0c\u540c\u65f6\u51cf\u5c11\u4e8622-42%\u7684\u8ba1\u7b97\u6b65\u9aa4\u3002\u4eba\u7c7b\u8bc4\u4f30\u786e\u8ba4\u4e86\u7b80\u5316\u6548\u679c\u4e0e\u610f\u4e49\u4fdd\u7559\u4e4b\u95f4\u7684\u57fa\u672c\u6743\u8861\uff0c\u5e76\u53d1\u73b0\u5373\u4f7f\u662f\u4eba\u7c7b\u6807\u6ce8\u8005\u5728\u8bed\u4e49\u4fdd\u7559\u5224\u65ad\u4e0a\u4e5f\u96be\u4ee5\u8fbe\u6210\u4e00\u81f4\u3002", "conclusion": "\u9010\u6b65\u7b80\u5316\u65b9\u6cd5\u786e\u5b9e\u80fd\u63d0\u9ad8\u63a7\u5236\u6027\uff0c\u4f46\u5728\u5927\u89c4\u6a21\u7b80\u5316\u8fc7\u7a0b\u4e2d\u4fdd\u6301\u8bed\u4e49\u4fdd\u771f\u5ea6\u4ecd\u7136\u662f\u4e00\u4e2a\u5f00\u653e\u6311\u6218\uff0c\u8fd9\u7a81\u663e\u4e86\u8be5\u4efb\u52a1\u7684\u5185\u5728\u590d\u6742\u6027\u3002"}}
{"id": "2602.07051", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.NE"], "pdf": "https://arxiv.org/pdf/2602.07051", "abs": "https://arxiv.org/abs/2602.07051", "authors": ["Karthik Sivakoti"], "title": "Neural Sentinel: Unified Vision Language Model (VLM) for License Plate Recognition with Human-in-the-Loop Continual Learning", "comment": null, "summary": "Traditional Automatic License Plate Recognition (ALPR) systems employ multi-stage pipelines consisting of object detection networks followed by separate Optical Character Recognition (OCR) modules, introducing compounding errors, increased latency, and architectural complexity. This research presents Neural Sentinel, a novel unified approach that leverages Vision Language Models (VLMs) to perform license plate recognition, state classification, and vehicle attribute extraction through a single forward pass. Our primary contribution lies in demonstrating that a fine-tuned PaliGemma 3B model, adapted via Low-Rank Adaptation (LoRA), can simultaneously answer multiple visual questions about vehicle images, achieving 92.3% plate recognition accuracy, which is a 14.1% improvement over EasyOCR and 9.9% improvement over PaddleOCR baselines. We introduce a Human-in-the-Loop (HITL) continual learning framework that incorporates user corrections while preventing catastrophic forgetting through experience replay, maintaining a 70:30 ratio of original training data to correction samples. The system achieves a mean inference latency of 152ms with an Expected Calibration Error (ECE) of 0.048, indicating well calibrated confidence estimates. Additionally, the VLM first architecture enables zero-shot generalization to auxiliary tasks including vehicle color detection (89%), seatbelt detection (82%), and occupancy counting (78%) without task specific training. Through extensive experimentation on real world toll plaza imagery, we demonstrate that unified vision language approaches represent a paradigm shift in ALPR systems, offering superior accuracy, reduced architectural complexity, and emergent multi-task capabilities that traditional pipeline approaches cannot achieve.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faNeural Sentinel\uff0c\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u7edf\u4e00\u8f66\u724c\u8bc6\u522b\u7cfb\u7edf\uff0c\u901a\u8fc7\u5355\u6b21\u524d\u5411\u4f20\u64ad\u540c\u65f6\u5b8c\u6210\u8f66\u724c\u8bc6\u522b\u3001\u72b6\u6001\u5206\u7c7b\u548c\u8f66\u8f86\u5c5e\u6027\u63d0\u53d6\uff0c\u76f8\u6bd4\u4f20\u7edf\u591a\u9636\u6bb5\u6d41\u6c34\u7ebf\u65b9\u6cd5\u5728\u51c6\u786e\u6027\u548c\u67b6\u6784\u590d\u6742\u5ea6\u4e0a\u6709\u663e\u8457\u6539\u8fdb\u3002", "motivation": "\u4f20\u7edfALPR\u7cfb\u7edf\u91c7\u7528\u591a\u9636\u6bb5\u6d41\u6c34\u7ebf\uff08\u76ee\u6807\u68c0\u6d4b+OCR\u6a21\u5757\uff09\uff0c\u5b58\u5728\u8bef\u5dee\u7d2f\u79ef\u3001\u5ef6\u8fdf\u9ad8\u3001\u67b6\u6784\u590d\u6742\u7684\u95ee\u9898\u3002\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u3001\u51c6\u786e\u4e14\u80fd\u5904\u7406\u591a\u4efb\u52a1\u7684\u7edf\u4e00\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f7f\u7528PaliGemma 3B\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7LoRA\u5fae\u8c03\uff0c\u5b9e\u73b0\u5355\u6b21\u524d\u5411\u4f20\u64ad\u540c\u65f6\u56de\u7b54\u591a\u4e2a\u5173\u4e8e\u8f66\u8f86\u56fe\u50cf\u7684\u89c6\u89c9\u95ee\u9898\u3002\u5f15\u5165\u4eba\u673a\u534f\u540c\u6301\u7eed\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u7ecf\u9a8c\u56de\u653e\u9632\u6b62\u707e\u96be\u6027\u9057\u5fd8\uff0c\u4fdd\u630170:30\u7684\u539f\u59cb\u8bad\u7ec3\u6570\u636e\u4e0e\u4fee\u6b63\u6837\u672c\u6bd4\u4f8b\u3002", "result": "\u8f66\u724c\u8bc6\u522b\u51c6\u786e\u7387\u8fbe\u523092.3%\uff0c\u6bd4EasyOCR\u63d0\u534714.1%\uff0c\u6bd4PaddleOCR\u63d0\u53479.9%\u3002\u5e73\u5747\u63a8\u7406\u5ef6\u8fdf152ms\uff0c\u9884\u671f\u6821\u51c6\u8bef\u5dee0.048\u3002\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\uff1a\u8f66\u8f86\u989c\u8272\u68c0\u6d4b89%\u3001\u5b89\u5168\u5e26\u68c0\u6d4b82%\u3001\u4e58\u5458\u8ba1\u657078%\u3002", "conclusion": "\u7edf\u4e00\u7684\u89c6\u89c9\u8bed\u8a00\u65b9\u6cd5\u4ee3\u8868\u4e86ALPR\u7cfb\u7edf\u7684\u8303\u5f0f\u8f6c\u53d8\uff0c\u63d0\u4f9b\u4e86\u4f18\u4e8e\u4f20\u7edf\u6d41\u6c34\u7ebf\u65b9\u6cd5\u7684\u51c6\u786e\u6027\u3001\u964d\u4f4e\u7684\u67b6\u6784\u590d\u6742\u5ea6\u4ee5\u53ca\u65b0\u5174\u7684\u591a\u4efb\u52a1\u80fd\u529b\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2602.07893", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.07893", "abs": "https://arxiv.org/abs/2602.07893", "authors": ["Zhiyuan Chen", "Soham Sanjay Deo", "Poorna Chander Reddy Puttaparthi", "Vanessa Nava-Camal", "Yiming Tang", "Xueling Zhang", "Weiyi Shang"], "title": "Is Your Private Information Logged? An Empirical Study on Android App Logs", "comment": null, "summary": "With the rapid growth of mobile apps, users' concerns about their privacy have become increasingly prominent. Android app logs serve as crucial computer resources, aiding developers in debugging and monitoring the status of Android apps, while also containing a wealth of software system information. Previous studies have acknowledged privacy leaks in software logs and Android apps as significant issues without providing a comprehensive view of the privacy leaks in Android app logs. In this study, we build a comprehensive dataset of Android app logs and conduct an empirical study to analyze the status and severity of privacy leaks in Android app logs. Our study comprises three aspects: (1) Understanding real-world developers' concerns regarding privacy issues related to software logs; (2) Studying privacy leaks in the Android app logs; (3) Investigating the characteristics of privacy-leaking Android app logs and analyzing the reasons behind them. Our study reveals five different categories of concerns from real-world developers regarding privacy issues related to software logs and the prevalence of privacy leaks in Android app logs, with the majority stemming from developers' unawareness of such leaks. Additionally, our study provides developers with suggestions to safeguard their privacy from being logged.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5206\u6790\u4e86Android\u5e94\u7528\u65e5\u5fd7\u4e2d\u7684\u9690\u79c1\u6cc4\u9732\u95ee\u9898\uff0c\u901a\u8fc7\u6784\u5efa\u5168\u9762\u7684\u6570\u636e\u96c6\u8fdb\u884c\u5b9e\u8bc1\u7814\u7a76\uff0c\u63ed\u793a\u4e86\u5f00\u53d1\u8005\u5bf9\u65e5\u5fd7\u9690\u79c1\u95ee\u9898\u7684\u5173\u6ce8\u70b9\u4ee5\u53caAndroid\u5e94\u7528\u65e5\u5fd7\u4e2d\u9690\u79c1\u6cc4\u9732\u7684\u666e\u904d\u6027\u3002", "motivation": "\u968f\u7740\u79fb\u52a8\u5e94\u7528\u7684\u5feb\u901f\u589e\u957f\uff0c\u7528\u6237\u5bf9\u9690\u79c1\u7684\u62c5\u5fe7\u65e5\u76ca\u7a81\u51fa\u3002Android\u5e94\u7528\u65e5\u5fd7\u4f5c\u4e3a\u91cd\u8981\u7684\u8ba1\u7b97\u673a\u8d44\u6e90\uff0c\u65e2\u5e2e\u52a9\u5f00\u53d1\u8005\u8c03\u8bd5\u548c\u76d1\u63a7\u5e94\u7528\u72b6\u6001\uff0c\u53c8\u5305\u542b\u4e30\u5bcc\u7684\u8f6f\u4ef6\u7cfb\u7edf\u4fe1\u606f\u3002\u5148\u524d\u7814\u7a76\u627f\u8ba4\u8f6f\u4ef6\u65e5\u5fd7\u548cAndroid\u5e94\u7528\u4e2d\u7684\u9690\u79c1\u6cc4\u9732\u662f\u91cd\u8981\u95ee\u9898\uff0c\u4f46\u672a\u80fd\u63d0\u4f9bAndroid\u5e94\u7528\u65e5\u5fd7\u4e2d\u9690\u79c1\u6cc4\u9732\u7684\u5168\u9762\u89c6\u56fe\u3002", "method": "\u7814\u7a76\u6784\u5efa\u4e86\u5168\u9762\u7684Android\u5e94\u7528\u65e5\u5fd7\u6570\u636e\u96c6\uff0c\u5e76\u8fdb\u884c\u5b9e\u8bc1\u7814\u7a76\uff0c\u4ece\u4e09\u4e2a\u65b9\u9762\u8fdb\u884c\u5206\u6790\uff1a(1) \u4e86\u89e3\u771f\u5b9e\u4e16\u754c\u5f00\u53d1\u8005\u5bf9\u8f6f\u4ef6\u65e5\u5fd7\u76f8\u5173\u9690\u79c1\u95ee\u9898\u7684\u5173\u6ce8\uff1b(2) \u7814\u7a76Android\u5e94\u7528\u65e5\u5fd7\u4e2d\u7684\u9690\u79c1\u6cc4\u9732\uff1b(3) \u8c03\u67e5\u9690\u79c1\u6cc4\u9732\u7684Android\u5e94\u7528\u65e5\u5fd7\u7279\u5f81\u5e76\u5206\u6790\u5176\u539f\u56e0\u3002", "result": "\u7814\u7a76\u63ed\u793a\u4e86\u771f\u5b9e\u4e16\u754c\u5f00\u53d1\u8005\u5bf9\u8f6f\u4ef6\u65e5\u5fd7\u9690\u79c1\u95ee\u9898\u7684\u4e94\u7c7b\u4e0d\u540c\u5173\u6ce8\u70b9\uff0c\u4ee5\u53caAndroid\u5e94\u7528\u65e5\u5fd7\u4e2d\u9690\u79c1\u6cc4\u9732\u7684\u666e\u904d\u6027\u3002\u5927\u591a\u6570\u9690\u79c1\u6cc4\u9732\u6e90\u4e8e\u5f00\u53d1\u8005\u5bf9\u6b64\u7c7b\u6cc4\u9732\u7684\u65e0\u610f\u8bc6\u3002\u7814\u7a76\u8fd8\u53d1\u73b0Android\u5e94\u7528\u65e5\u5fd7\u4e2d\u9690\u79c1\u6cc4\u9732\u7684\u73b0\u72b6\u548c\u4e25\u91cd\u7a0b\u5ea6\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5f00\u53d1\u8005\u63d0\u4f9b\u4e86\u4fdd\u62a4\u9690\u79c1\u4e0d\u88ab\u8bb0\u5f55\u7684\u5efa\u8bae\uff0c\u5f3a\u8c03\u4e86Android\u5e94\u7528\u65e5\u5fd7\u4e2d\u9690\u79c1\u6cc4\u9732\u95ee\u9898\u7684\u4e25\u91cd\u6027\uff0c\u5e76\u6307\u51fa\u5f00\u53d1\u8005\u610f\u8bc6\u4e0d\u8db3\u662f\u4e3b\u8981\u6839\u6e90\u3002\u7814\u7a76\u7ed3\u679c\u6709\u52a9\u4e8e\u63d0\u9ad8\u5f00\u53d1\u8005\u5bf9\u65e5\u5fd7\u9690\u79c1\u4fdd\u62a4\u7684\u91cd\u89c6\u3002"}}
{"id": "2602.08072", "categories": ["cs.CR", "cs.SE"], "pdf": "https://arxiv.org/pdf/2602.08072", "abs": "https://arxiv.org/abs/2602.08072", "authors": ["Md Nafiu Rahman", "Sadif Ahmed", "Zahin Wahab", "Gias Uddin", "Rifat Shahriyar"], "title": "IssueGuard: Real-Time Secret Leak Prevention Tool for GitHub Issue Reports", "comment": null, "summary": "GitHub and GitLab are widely used collaborative platforms whose issue-tracking systems contain large volumes of unstructured text, including logs, code snippets, and configuration examples. This creates a significant risk of accidental secret exposure, such as API keys and credentials, yet these platforms provide no mechanism to warn users before submission. We present \\textsc{IssueGuard}, a tool for real-time detection and prevention of secret leaks in issue reports. Implemented as a Chrome extension, \\textsc{IssueGuard} analyzes text as users type and combines regex-based candidate extraction with a fine-tuned CodeBERT model for contextual classification. This approach effectively separates real secrets from false positives and achieves an F1-score of 92.70\\% on a benchmark dataset, outperforming traditional regex-based scanners. \\textsc{IssueGuard} integrates directly into the web interface and continuously analyzes the issue editor, presenting clear visual warnings to help users avoid submitting sensitive data. The source code is publicly available at \\href{https://github.com/nafiurahman00/IssueGuard}{https://github.com/nafiurahman00/IssueGuard}, and a demonstration video is available at \\href{https://youtu.be/kvbWA8rr9cU}{https://youtu.be/kvbWA8rr9cU}.", "AI": {"tldr": "IssueGuard\u662f\u4e00\u4e2aChrome\u6269\u5c55\u5de5\u5177\uff0c\u7528\u4e8e\u5b9e\u65f6\u68c0\u6d4b\u548c\u9632\u6b62GitHub/GitLab\u95ee\u9898\u62a5\u544a\u4e2d\u610f\u5916\u6cc4\u9732API\u5bc6\u94a5\u7b49\u654f\u611f\u4fe1\u606f\uff0c\u7ed3\u5408\u6b63\u5219\u8868\u8fbe\u5f0f\u548c\u5fae\u8c03CodeBERT\u6a21\u578b\uff0c\u8fbe\u523092.70%\u7684F1\u5206\u6570\u3002", "motivation": "GitHub\u548cGitLab\u7b49\u534f\u4f5c\u5e73\u53f0\u7684issue\u8ddf\u8e2a\u7cfb\u7edf\u5305\u542b\u5927\u91cf\u975e\u7ed3\u6784\u5316\u6587\u672c\uff08\u65e5\u5fd7\u3001\u4ee3\u7801\u7247\u6bb5\u3001\u914d\u7f6e\u793a\u4f8b\uff09\uff0c\u5b58\u5728\u610f\u5916\u6cc4\u9732API\u5bc6\u94a5\u548c\u51ed\u8bc1\u7684\u98ce\u9669\uff0c\u4f46\u8fd9\u4e9b\u5e73\u53f0\u6ca1\u6709\u63d0\u4f9b\u63d0\u4ea4\u524d\u7684\u8b66\u544a\u673a\u5236\u3002", "method": "\u5b9e\u73b0\u4e3aChrome\u6269\u5c55\uff0c\u7ed3\u5408\u57fa\u4e8e\u6b63\u5219\u8868\u8fbe\u5f0f\u7684\u5019\u9009\u63d0\u53d6\u548c\u5fae\u8c03\u7684CodeBERT\u6a21\u578b\u8fdb\u884c\u4e0a\u4e0b\u6587\u5206\u7c7b\uff0c\u76f4\u63a5\u5728Web\u754c\u9762\u4e2d\u6301\u7eed\u5206\u6790issue\u7f16\u8f91\u5668\uff0c\u63d0\u4f9b\u6e05\u6670\u7684\u89c6\u89c9\u8b66\u544a\u3002", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8fbe\u523092.70%\u7684F1\u5206\u6570\uff0c\u4f18\u4e8e\u4f20\u7edf\u7684\u57fa\u4e8e\u6b63\u5219\u8868\u8fbe\u5f0f\u7684\u626b\u63cf\u5668\uff0c\u80fd\u6709\u6548\u533a\u5206\u771f\u5b9e\u5bc6\u94a5\u548c\u8bef\u62a5\u3002", "conclusion": "IssueGuard\u662f\u4e00\u4e2a\u6709\u6548\u7684\u5b9e\u65f6\u79d8\u5bc6\u6cc4\u9732\u68c0\u6d4b\u5de5\u5177\uff0c\u901a\u8fc7\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u5206\u7c7b\u65b9\u6cd5\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\uff0c\u5e2e\u52a9\u7528\u6237\u5728\u63d0\u4ea4\u524d\u907f\u514d\u654f\u611f\u6570\u636e\u6cc4\u9732\u3002"}}
{"id": "2602.07546", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.07546", "abs": "https://arxiv.org/abs/2602.07546", "authors": ["Zicong Cheng", "Ruixuan Jia", "Jia Li", "Guo-Wei Yang", "Meng-Hao Guo", "Shi-Min Hu"], "title": "Improving Variable-Length Generation in Diffusion Language Models via Length Regularization", "comment": "diffusion language models", "summary": "Diffusion Large Language Models (DLLMs) are inherently ill-suited for variable-length generation, as their inference is defined on a fixed-length canvas and implicitly assumes a known target length. When the length is unknown, as in realistic completion and infilling, naively comparing confidence across mask lengths becomes systematically biased, leading to under-generation or redundant continuations. In this paper, we show that this failure arises from an intrinsic lengthinduced bias in generation confidence estimates, leaving existing DLLMs without a robust way to determine generation length and making variablelength inference unreliable. To address this issue, we propose LR-DLLM, a length-regularized inference framework for DLLMs that treats generation length as an explicit variable and achieves reliable length determination at inference time. It decouples semantic compatibility from lengthinduced uncertainty through an explicit length regularization that corrects biased confidence estimates. Based on this, LR-DLLM enables dynamic expansion or contraction of the generation span without modifying the underlying DLLM or its training procedure. Experiments show that LRDLLM achieves 51.3% Pass@1 on HumanEvalInfilling under fully unknown lengths (+13.4% vs. DreamOn) and 51.5% average Pass@1 on four-language McEval (+14.3% vs. DreamOn).", "AI": {"tldr": "LR-DLLM\u63d0\u51fa\u4e86\u4e00\u79cd\u957f\u5ea6\u6b63\u5219\u5316\u63a8\u7406\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b\u5728\u53d8\u957f\u751f\u6210\u4e2d\u7684\u957f\u5ea6\u504f\u5dee\u95ee\u9898\uff0c\u901a\u8fc7\u663e\u5f0f\u957f\u5ea6\u6b63\u5219\u5316\u5b9e\u73b0\u53ef\u9760\u7684\u751f\u6210\u957f\u5ea6\u786e\u5b9a\u3002", "motivation": "\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b\u5728\u53d8\u957f\u751f\u6210\u4e2d\u5b58\u5728\u56fa\u6709\u7f3a\u9677\uff0c\u56e0\u4e3a\u5176\u63a8\u7406\u57fa\u4e8e\u56fa\u5b9a\u957f\u5ea6\u753b\u5e03\u5e76\u9690\u542b\u5047\u8bbe\u5df2\u77e5\u76ee\u6807\u957f\u5ea6\u3002\u5f53\u957f\u5ea6\u672a\u77e5\u65f6\uff08\u5982\u5b9e\u9645\u8865\u5168\u548c\u586b\u5145\u4efb\u52a1\uff09\uff0c\u5728\u4e0d\u540c\u63a9\u7801\u957f\u5ea6\u95f4\u7b80\u5355\u6bd4\u8f83\u7f6e\u4fe1\u5ea6\u4f1a\u4ea7\u751f\u7cfb\u7edf\u6027\u504f\u5dee\uff0c\u5bfc\u81f4\u751f\u6210\u4e0d\u8db3\u6216\u5197\u4f59\u5ef6\u7eed\u3002", "method": "\u63d0\u51faLR-DLLM\uff08\u957f\u5ea6\u6b63\u5219\u5316\u63a8\u7406\u6846\u67b6\uff09\uff0c\u5c06\u751f\u6210\u957f\u5ea6\u4f5c\u4e3a\u663e\u5f0f\u53d8\u91cf\uff0c\u901a\u8fc7\u663e\u5f0f\u957f\u5ea6\u6b63\u5219\u5316\u89e3\u8026\u8bed\u4e49\u517c\u5bb9\u6027\u548c\u957f\u5ea6\u8bf1\u5bfc\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u4fee\u6b63\u6709\u504f\u7684\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\u3002\u8be5\u6846\u67b6\u65e0\u9700\u4fee\u6539\u5e95\u5c42DLLM\u6216\u5176\u8bad\u7ec3\u8fc7\u7a0b\uff0c\u5373\u53ef\u5b9e\u73b0\u751f\u6210\u8de8\u5ea6\u7684\u52a8\u6001\u6269\u5c55\u6216\u6536\u7f29\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cLR-DLLM\u5728\u5b8c\u5168\u672a\u77e5\u957f\u5ea6\u7684\u60c5\u51b5\u4e0b\uff0c\u5728HumanEvalInfilling\u4e0a\u8fbe\u523051.3% Pass@1\uff08\u6bd4DreamOn\u63d0\u534713.4%\uff09\uff0c\u5728\u56db\u8bed\u8a00McEval\u4e0a\u5e73\u5747\u8fbe\u523051.5% Pass@1\uff08\u6bd4DreamOn\u63d0\u534714.3%\uff09\u3002", "conclusion": "LR-DLLM\u6709\u6548\u89e3\u51b3\u4e86\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b\u5728\u53d8\u957f\u751f\u6210\u4e2d\u7684\u957f\u5ea6\u504f\u5dee\u95ee\u9898\uff0c\u901a\u8fc7\u957f\u5ea6\u6b63\u5219\u5316\u63a8\u7406\u6846\u67b6\u5b9e\u73b0\u4e86\u53ef\u9760\u7684\u751f\u6210\u957f\u5ea6\u786e\u5b9a\uff0c\u663e\u8457\u63d0\u5347\u4e86\u586b\u5145\u548c\u8865\u5168\u4efb\u52a1\u7684\u6027\u80fd\u3002"}}
{"id": "2602.07052", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2602.07052", "abs": "https://arxiv.org/abs/2602.07052", "authors": ["Ziye Xie", "Oded Schlesinger", "Raj Kundu", "Jessica Y. Choi", "Pablo Iturralde", "Dennis A. Turner", "Stefan M. Goetz", "Guillermo Sapiro", "Angel V. Peterchev", "J. Matias Di Martino"], "title": "Toward Accurate and Accessible Markerless Neuronavigation", "comment": null, "summary": "Neuronavigation is widely used in biomedical research and interventions to guide the precise placement of instruments around the head to support procedures such as transcranial magnetic stimulation. Traditional systems, however, rely on subject-mounted markers that require manual registration, may shift during procedures, and can cause discomfort. We introduce and evaluate markerless approaches that replace expensive hardware and physical markers with low-cost visible and infrared light cameras incorporating stereo and depth sensing combined with algorithmic modeling of the facial geometry. Validation with $50$ human subjects yielded a median tracking discrepancy of only $2.32$ mm and $2.01\u00b0$ for the best markerless algorithms compared to a conventional marker-based system, which indicates sufficient accuracy for transcranial magnetic stimulation and a substantial improvement over prior markerless results. The results suggest that integration of the data from the various camera sensors can improve the overall accuracy further. The proposed markerless neuronavigation methods can reduce setup cost and complexity, improve patient comfort, and expand access to neuronavigation in clinical and research settings.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u6807\u8bb0\u795e\u7ecf\u5bfc\u822a\u7cfb\u7edf\uff0c\u4f7f\u7528\u4f4e\u6210\u672c\u53ef\u89c1\u5149\u548c\u7ea2\u5916\u5149\u76f8\u673a\u7ed3\u5408\u7acb\u4f53\u548c\u6df1\u5ea6\u611f\u77e5\uff0c\u66ff\u4ee3\u4f20\u7edf\u9700\u8981\u7269\u7406\u6807\u8bb0\u7684\u7cfb\u7edf\uff0c\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u7684\u5934\u90e8\u8ddf\u8e2a\u3002", "motivation": "\u4f20\u7edf\u795e\u7ecf\u5bfc\u822a\u7cfb\u7edf\u4f9d\u8d56\u9700\u8981\u624b\u52a8\u6ce8\u518c\u7684\u7269\u7406\u6807\u8bb0\uff0c\u8fd9\u4e9b\u6807\u8bb0\u53ef\u80fd\u5728\u624b\u672f\u8fc7\u7a0b\u4e2d\u79fb\u4f4d\uff0c\u9020\u6210\u60a3\u8005\u4e0d\u9002\uff0c\u4e14\u6210\u672c\u9ad8\u6602\u3002\u9700\u8981\u5f00\u53d1\u66f4\u8212\u9002\u3001\u4f4e\u6210\u672c\u7684\u65e0\u6807\u8bb0\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u4f7f\u7528\u4f4e\u6210\u672c\u53ef\u89c1\u5149\u548c\u7ea2\u5916\u5149\u76f8\u673a\uff0c\u7ed3\u5408\u7acb\u4f53\u89c6\u89c9\u548c\u6df1\u5ea6\u611f\u77e5\u6280\u672f\uff0c\u901a\u8fc7\u7b97\u6cd5\u5efa\u6a21\u9762\u90e8\u51e0\u4f55\u7279\u5f81\uff0c\u5b9e\u73b0\u65e0\u6807\u8bb0\u7684\u5934\u90e8\u8ddf\u8e2a\u548c\u5bfc\u822a\u3002", "result": "\u572850\u540d\u4eba\u7c7b\u53d7\u8bd5\u8005\u4e0a\u7684\u9a8c\u8bc1\u663e\u793a\uff0c\u6700\u4f73\u65e0\u6807\u8bb0\u7b97\u6cd5\u7684\u4e2d\u4f4d\u8ddf\u8e2a\u8bef\u5dee\u4ec5\u4e3a2.32\u6beb\u7c73\u548c2.01\u5ea6\uff0c\u76f8\u6bd4\u4f20\u7edf\u6807\u8bb0\u7cfb\u7edf\u5177\u6709\u8db3\u591f\u7cbe\u5ea6\u7528\u4e8e\u7ecf\u9885\u78c1\u523a\u6fc0\uff0c\u4e14\u663e\u8457\u4f18\u4e8e\u5148\u524d\u65e0\u6807\u8bb0\u65b9\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u7684\u65e0\u6807\u8bb0\u795e\u7ecf\u5bfc\u822a\u65b9\u6cd5\u80fd\u964d\u4f4e\u8bbe\u7f6e\u6210\u672c\u548c\u590d\u6742\u6027\uff0c\u63d0\u9ad8\u60a3\u8005\u8212\u9002\u5ea6\uff0c\u5e76\u6269\u5927\u795e\u7ecf\u5bfc\u822a\u5728\u4e34\u5e8a\u548c\u7814\u7a76\u73af\u5883\u4e2d\u7684\u53ef\u53ca\u6027\u3002\u591a\u4f20\u611f\u5668\u6570\u636e\u878d\u5408\u53ef\u8fdb\u4e00\u6b65\u63d0\u9ad8\u6574\u4f53\u7cbe\u5ea6\u3002"}}
{"id": "2602.07473", "categories": ["cs.AI", "cs.FL"], "pdf": "https://arxiv.org/pdf/2602.07473", "abs": "https://arxiv.org/abs/2602.07473", "authors": ["Nathana\u00ebl Fijalkow", "Arka Ghosh", "Roman Kniazev", "Guillermo A. P\u00e9rez", "Pierre Vandenhove"], "title": "Computing the Reachability Value of Posterior-Deterministic POMDPs", "comment": null, "summary": "Partially observable Markov decision processes (POMDPs) are a fundamental model for sequential decision-making under uncertainty. However, many verification and synthesis problems for POMDPs are undecidable or intractable. Most prominently, the seminal result of Madani et al. (2003) states that there is no algorithm that, given a POMDP and a set of target states, can compute the maximal probability of reaching the target states, or even approximate it up to a non-trivial constant. This is in stark contrast to fully observable Markov decision processes (MDPs), where the reachability value can be computed in polynomial time.\n  In this work, we introduce posterior-deterministic POMDPs, a novel class of POMDPs. Our main technical contribution is to show that for posterior-deterministic POMDPs, the maximal probability of reaching a given set of states can be approximated up to arbitrary precision.\n  A POMDP is posterior-deterministic if the next state can be uniquely determined by the current state, the action taken, and the observation received. While the actual state is generally uncertain in POMDPs, the posterior-deterministic property tells us that once the true state is known it remains known forever. This simple and natural definition includes all MDPs and captures classical non-trivial examples such as the Tiger POMDP (Kaelbling et al. 1998), making it one of the largest known classes of POMDPs for which the reachability value can be approximated.", "AI": {"tldr": "\u63d0\u51fa\u540e\u9a8c\u786e\u5b9a\u6027POMDPs\u65b0\u7c7b\u522b\uff0c\u89e3\u51b3\u4e86POMDPs\u4e2d\u53ef\u8fbe\u6982\u7387\u65e0\u6cd5\u8ba1\u7b97\u7684\u95ee\u9898\uff0c\u4f7f\u5f97\u5728\u8be5\u7c7b\u522b\u4e0b\u53ef\u8fbe\u6982\u7387\u53ef\u4ee5\u4efb\u610f\u7cbe\u5ea6\u8fd1\u4f3c\u8ba1\u7b97\u3002", "motivation": "POMDPs\u5728\u4e0d\u786e\u5b9a\u6027\u4e0b\u7684\u987a\u5e8f\u51b3\u7b56\u5efa\u6a21\u4e2d\u5f88\u91cd\u8981\uff0c\u4f46\u8bb8\u591a\u9a8c\u8bc1\u548c\u7efc\u5408\u95ee\u9898\u662f\u4e0d\u53ef\u5224\u5b9a\u6216\u96be\u5904\u7406\u7684\u3002\u7279\u522b\u662fMadani\u7b49\u4eba\u7684\u7ed3\u679c\u8868\u660e\uff0c\u65e0\u6cd5\u8ba1\u7b97\u6216\u8fd1\u4f3cPOMDPs\u4e2d\u5230\u8fbe\u76ee\u6807\u72b6\u6001\u7684\u6700\u5927\u6982\u7387\uff0c\u8fd9\u4e0e\u5b8c\u5168\u53ef\u89c2\u6d4bMDPs\u5f62\u6210\u9c9c\u660e\u5bf9\u6bd4\u3002", "method": "\u5f15\u5165\u540e\u9a8c\u786e\u5b9a\u6027POMDPs\u65b0\u7c7b\u522b\uff1a\u5982\u679c\u7ed9\u5b9a\u5f53\u524d\u72b6\u6001\u3001\u91c7\u53d6\u7684\u52a8\u4f5c\u548c\u63a5\u6536\u7684\u89c2\u6d4b\uff0c\u4e0b\u4e00\u4e2a\u72b6\u6001\u53ef\u4ee5\u552f\u4e00\u786e\u5b9a\uff0c\u5219POMDP\u662f\u540e\u9a8c\u786e\u5b9a\u6027\u7684\u3002\u8fd9\u610f\u5473\u7740\u4e00\u65e6\u771f\u5b9e\u72b6\u6001\u5df2\u77e5\uff0c\u5b83\u5c06\u6c38\u8fdc\u4fdd\u6301\u5df2\u77e5\u3002", "result": "\u5bf9\u4e8e\u540e\u9a8c\u786e\u5b9a\u6027POMDPs\uff0c\u5230\u8fbe\u7ed9\u5b9a\u72b6\u6001\u96c6\u7684\u6700\u5927\u6982\u7387\u53ef\u4ee5\u8fd1\u4f3c\u5230\u4efb\u610f\u7cbe\u5ea6\u3002\u8fd9\u5305\u62ec\u6240\u6709MDPs\u548c\u7ecf\u5178\u7684\u975e\u5e73\u51e1\u793a\u4f8b\uff08\u5982Tiger POMDP\uff09\uff0c\u4f7f\u5176\u6210\u4e3a\u5df2\u77e5\u6700\u5927\u7684POMDPs\u7c7b\u522b\u4e4b\u4e00\u3002", "conclusion": "\u540e\u9a8c\u786e\u5b9a\u6027POMDPs\u5b9a\u4e49\u4e86\u4e00\u4e2a\u5177\u6709\u826f\u597d\u8ba1\u7b97\u6027\u8d28\u7684POMDPs\u65b0\u7c7b\u522b\uff0c\u89e3\u51b3\u4e86POMDPs\u53ef\u8fbe\u6027\u5206\u6790\u4e2d\u7684\u57fa\u672c\u8ba1\u7b97\u969c\u788d\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.08165", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2602.08165", "abs": "https://arxiv.org/abs/2602.08165", "authors": ["Miguel Bicudo", "Estev\u00e3o Rabello", "Daniel Menasch\u00e9", "Paulo Segal", "Claudio Segal", "Anton Kocheturov", "Priyanjan Sharma"], "title": "A Transfer Learning Approach to Unveil the Role of Windows Common Configuration Enumerations in IEC 62443 Compliance", "comment": null, "summary": "Industrial control systems (ICS) depend on highly heterogeneous environments where Linux, proprietary real-time operating systems, and Windows coexist. Although the IEC 62443-3-3 standard provides a comprehensive framework for securing such systems, translating its requirements into concrete configuration checks remains challenging, especially for Windows platforms. In this paper, we propose a transfer learning methodology that maps Windows Common Configuration Enumerations (CCEs) to IEC 62443-3-3 System Security Requirements by leveraging labeled Linux datasets. The resulting labeled dataset enables automated compliance checks, analysis of requirement prevalence, and identification of cross-platform similarities and divergences. Our results highlight the role of CCEs as a bridge between abstract standards and concrete configurations, advancing automation, traceability, and clarity in IEC 62443-3-3 compliance for Windows environments.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u8fc1\u79fb\u5b66\u4e60\u65b9\u6cd5\uff0c\u5c06Windows\u914d\u7f6e\u679a\u4e3e\u6620\u5c04\u5230IEC 62443-3-3\u5b89\u5168\u6807\u51c6\uff0c\u5229\u7528Linux\u6570\u636e\u96c6\u5b9e\u73b0\u81ea\u52a8\u5316\u5408\u89c4\u68c0\u67e5", "motivation": "\u5de5\u4e1a\u63a7\u5236\u7cfb\u7edf\u73af\u5883\u9ad8\u5ea6\u5f02\u6784\uff0c\u5305\u542bLinux\u3001\u4e13\u6709\u5b9e\u65f6\u7cfb\u7edf\u548cWindows\u3002\u867d\u7136IEC 62443-3-3\u6807\u51c6\u63d0\u4f9b\u4e86\u5168\u9762\u7684\u5b89\u5168\u6846\u67b6\uff0c\u4f46\u5c06\u5176\u8981\u6c42\u8f6c\u5316\u4e3a\u5177\u4f53\u7684\u914d\u7f6e\u68c0\u67e5\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\uff0c\u7279\u522b\u662f\u5bf9\u4e8eWindows\u5e73\u53f0\u3002", "method": "\u91c7\u7528\u8fc1\u79fb\u5b66\u4e60\u65b9\u6cd5\uff0c\u5c06Windows\u901a\u7528\u914d\u7f6e\u679a\u4e3e\u6620\u5c04\u5230IEC 62443-3-3\u7cfb\u7edf\u5b89\u5168\u8981\u6c42\uff0c\u5229\u7528\u5df2\u6807\u8bb0\u7684Linux\u6570\u636e\u96c6\u6765\u8bad\u7ec3\u6a21\u578b\u3002", "result": "\u751f\u6210\u4e86\u6807\u8bb0\u7684\u6570\u636e\u96c6\uff0c\u652f\u6301\u81ea\u52a8\u5316\u5408\u89c4\u68c0\u67e5\u3001\u9700\u6c42\u6d41\u884c\u5ea6\u5206\u6790\uff0c\u5e76\u8bc6\u522b\u4e86\u8de8\u5e73\u53f0\u7684\u76f8\u4f3c\u6027\u548c\u5dee\u5f02\u6027\u3002CCE\u6210\u4e3a\u8fde\u63a5\u62bd\u8c61\u6807\u51c6\u548c\u5177\u4f53\u914d\u7f6e\u7684\u6865\u6881\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u63a8\u8fdb\u4e86Windows\u73af\u5883\u4e2dIEC 62443-3-3\u5408\u89c4\u7684\u81ea\u52a8\u5316\u3001\u53ef\u8ffd\u6eaf\u6027\u548c\u6e05\u6670\u5ea6\uff0c\u4e3a\u5f02\u6784\u5de5\u4e1a\u63a7\u5236\u7cfb\u7edf\u5b89\u5168\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.07594", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07594", "abs": "https://arxiv.org/abs/2602.07594", "authors": ["Yuxin Chen", "Yu Wang", "Yi Zhang", "Ziang Ye", "Zhengzhou Cai", "Yaorui Shi", "Qi Gu", "Hui Su", "Xunliang Cai", "Xiang Wang", "An Zhang", "Tat-Seng Chua"], "title": "Learning to Self-Verify Makes Language Models Better Reasoners", "comment": null, "summary": "Recent large language models (LLMs) achieve strong performance in generating promising reasoning paths for complex tasks. However, despite powerful generation ability, LLMs remain weak at verifying their own answers, revealing a persistent capability asymmetry between generation and self-verification. In this work, we conduct an in-depth investigation of this asymmetry throughout training evolution and show that, even on the same task, improving generation does not lead to corresponding improvements in self-verification. Interestingly, we find that the reverse direction of this asymmetry behaves differently: learning to self-verify can effectively improve generation performance, achieving accuracy comparable to standard generation training while yielding more efficient and effective reasoning traces. Building on this observation, we further explore integrating self-verification into generation training by formulating a multi-task reinforcement learning framework, where generation and self-verification are optimized as two independent but complementary objectives. Extensive experiments across benchmarks and models demonstrate performance gains over generation-only training in both generation and verification capabilities.", "AI": {"tldr": "LLMs\u5728\u751f\u6210\u63a8\u7406\u8def\u5f84\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u81ea\u6211\u9a8c\u8bc1\u65b9\u9762\u8f83\u5f31\uff0c\u5b58\u5728\u80fd\u529b\u4e0d\u5bf9\u79f0\u6027\u3002\u7814\u7a76\u53d1\u73b0\u81ea\u6211\u9a8c\u8bc1\u8bad\u7ec3\u80fd\u6709\u6548\u63d0\u5347\u751f\u6210\u6027\u80fd\uff0c\u800c\u751f\u6210\u8bad\u7ec3\u4e0d\u80fd\u76f8\u5e94\u63d0\u5347\u9a8c\u8bc1\u80fd\u529b\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u80fd\u751f\u6210\u6709\u524d\u666f\u7684\u63a8\u7406\u8def\u5f84\uff0c\u4f46\u5728\u9a8c\u8bc1\u81ea\u8eab\u7b54\u6848\u65b9\u9762\u4ecd\u7136\u8584\u5f31\uff0c\u5b58\u5728\u751f\u6210\u4e0e\u81ea\u6211\u9a8c\u8bc1\u4e4b\u95f4\u7684\u80fd\u529b\u4e0d\u5bf9\u79f0\u95ee\u9898\u3002\u672c\u7814\u7a76\u65e8\u5728\u6df1\u5165\u63a2\u7a76\u8fd9\u79cd\u4e0d\u5bf9\u79f0\u6027\u53ca\u5176\u8bad\u7ec3\u6f14\u5316\u8fc7\u7a0b\u3002", "method": "\u7814\u7a76\u5206\u6790\u4e86\u8bad\u7ec3\u6f14\u5316\u8fc7\u7a0b\u4e2d\u751f\u6210\u4e0e\u9a8c\u8bc1\u80fd\u529b\u7684\u4e0d\u5bf9\u79f0\u6027\uff0c\u53d1\u73b0\u81ea\u6211\u9a8c\u8bc1\u5b66\u4e60\u80fd\u6709\u6548\u63d0\u5347\u751f\u6210\u6027\u80fd\u3002\u57fa\u4e8e\u6b64\u89c2\u5bdf\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u591a\u4efb\u52a1\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u5c06\u751f\u6210\u548c\u81ea\u6211\u9a8c\u8bc1\u4f5c\u4e3a\u4e24\u4e2a\u72ec\u7acb\u4f46\u4e92\u8865\u7684\u76ee\u6807\u8fdb\u884c\u4f18\u5316\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5373\u4f7f\u5728\u76f8\u540c\u4efb\u52a1\u4e0a\uff0c\u6539\u8fdb\u751f\u6210\u80fd\u529b\u5e76\u4e0d\u4f1a\u76f8\u5e94\u63d0\u5347\u81ea\u6211\u9a8c\u8bc1\u80fd\u529b\u3002\u76f8\u53cd\uff0c\u5b66\u4e60\u81ea\u6211\u9a8c\u8bc1\u80fd\u6709\u6548\u63d0\u9ad8\u751f\u6210\u6027\u80fd\uff0c\u8fbe\u5230\u4e0e\u6807\u51c6\u751f\u6210\u8bad\u7ec3\u76f8\u5f53\u7684\u51c6\u786e\u7387\uff0c\u540c\u65f6\u4ea7\u751f\u66f4\u9ad8\u6548\u6709\u6548\u7684\u63a8\u7406\u8f68\u8ff9\u3002\u591a\u4efb\u52a1\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u548c\u6a21\u578b\u4e0a\u90fd\u663e\u793a\u51fa\u4f18\u4e8e\u7eaf\u751f\u6210\u8bad\u7ec3\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "\u751f\u6210\u4e0e\u81ea\u6211\u9a8c\u8bc1\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u7684\u80fd\u529b\u4e0d\u5bf9\u79f0\u6027\uff0c\u81ea\u6211\u9a8c\u8bc1\u8bad\u7ec3\u5bf9\u63d0\u5347\u751f\u6210\u6027\u80fd\u5177\u6709\u91cd\u8981\u4f5c\u7528\u3002\u901a\u8fc7\u591a\u4efb\u52a1\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u6574\u5408\u751f\u6210\u4e0e\u9a8c\u8bc1\u76ee\u6807\uff0c\u80fd\u540c\u65f6\u63d0\u5347\u6a21\u578b\u7684\u751f\u6210\u548c\u9a8c\u8bc1\u80fd\u529b\uff0c\u4e3aLLM\u8bad\u7ec3\u63d0\u4f9b\u4e86\u65b0\u7684\u6709\u6548\u65b9\u6cd5\u3002"}}
{"id": "2602.07057", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.07057", "abs": "https://arxiv.org/abs/2602.07057", "authors": ["Di Mo", "Mingyang Sun", "Chengxiu Yin", "Runjia Tian", "Yanhong Wu", "Liyan Xu"], "title": "RECITYGEN -- Interactive and Generative Participatory Urban Design Tool with Latent Diffusion and Segment Anything", "comment": null, "summary": "Urban design profoundly impacts public spaces and community engagement. Traditional top-down methods often overlook public input, creating a gap in design aspirations and reality. Recent advancements in digital tools, like City Information Modelling and augmented reality, have enabled a more participatory process involving more stakeholders in urban design. Further, deep learning and latent diffusion models have lowered barriers for design generation, providing even more opportunities for participatory urban design. Combining state-of-the-art latent diffusion models with interactive semantic segmentation, we propose RECITYGEN, a novel tool that allows users to interactively create variational street view images of urban environments using text prompts. In a pilot project in Beijing, users employed RECITYGEN to suggest improvements for an ongoing Urban Regeneration project. Despite some limitations, RECITYGEN has shown significant potential in aligning with public preferences, indicating a shift towards more dynamic and inclusive urban planning methods. The source code for the project can be found at RECITYGEN GitHub.", "AI": {"tldr": "RECITYGEN\u662f\u4e00\u4e2a\u7ed3\u5408\u6f5c\u5728\u6269\u6563\u6a21\u578b\u548c\u4ea4\u4e92\u5f0f\u8bed\u4e49\u5206\u5272\u7684\u5de5\u5177\uff0c\u5141\u8bb8\u7528\u6237\u901a\u8fc7\u6587\u672c\u63d0\u793a\u4ea4\u4e92\u5f0f\u751f\u6210\u57ce\u5e02\u8857\u666f\u7684\u53d8\u4f53\u56fe\u50cf\uff0c\u7528\u4e8e\u53c2\u4e0e\u5f0f\u57ce\u5e02\u8bbe\u8ba1\u3002", "motivation": "\u4f20\u7edf\u81ea\u4e0a\u800c\u4e0b\u7684\u57ce\u5e02\u8bbe\u8ba1\u65b9\u6cd5\u5f80\u5f80\u5ffd\u89c6\u516c\u4f17\u610f\u89c1\uff0c\u5bfc\u81f4\u8bbe\u8ba1\u613f\u666f\u4e0e\u73b0\u5b9e\u9700\u6c42\u4e4b\u95f4\u5b58\u5728\u5dee\u8ddd\u3002\u6570\u5b57\u5de5\u5177\u7684\u53d1\u5c55\u4e3a\u66f4\u5e7f\u6cdb\u7684\u5229\u76ca\u76f8\u5173\u8005\u53c2\u4e0e\u57ce\u5e02\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u673a\u4f1a\u3002", "method": "\u7ed3\u5408\u6700\u5148\u8fdb\u7684\u6f5c\u5728\u6269\u6563\u6a21\u578b\u548c\u4ea4\u4e92\u5f0f\u8bed\u4e49\u5206\u5272\u6280\u672f\uff0c\u5f00\u53d1\u4e86RECITYGEN\u5de5\u5177\uff0c\u7528\u6237\u53ef\u4ee5\u901a\u8fc7\u6587\u672c\u63d0\u793a\u4ea4\u4e92\u5f0f\u751f\u6210\u57ce\u5e02\u8857\u666f\u7684\u53d8\u4f53\u56fe\u50cf\u3002", "result": "\u5728\u5317\u4eac\u7684\u8bd5\u70b9\u9879\u76ee\u4e2d\uff0c\u7528\u6237\u4f7f\u7528RECITYGEN\u4e3a\u6b63\u5728\u8fdb\u884c\u7684\u57ce\u5e02\u66f4\u65b0\u9879\u76ee\u63d0\u51fa\u6539\u8fdb\u5efa\u8bae\u3002\u5c3d\u7ba1\u5b58\u5728\u4e00\u4e9b\u5c40\u9650\u6027\uff0c\u4f46\u8be5\u5de5\u5177\u663e\u793a\u51fa\u4e0e\u516c\u4f17\u504f\u597d\u9ad8\u5ea6\u5951\u5408\u7684\u6f5c\u529b\u3002", "conclusion": "RECITYGEN\u5c55\u793a\u4e86\u5411\u66f4\u52a8\u6001\u3001\u5305\u5bb9\u7684\u57ce\u5e02\u89c4\u5212\u65b9\u6cd5\u8f6c\u53d8\u7684\u6f5c\u529b\uff0c\u4e3a\u53c2\u4e0e\u5f0f\u57ce\u5e02\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u7684\u6280\u672f\u9014\u5f84\u3002"}}
{"id": "2602.07491", "categories": ["cs.AI", "cond-mat.mes-hall", "cond-mat.mtrl-sci", "cond-mat.soft", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.07491", "abs": "https://arxiv.org/abs/2602.07491", "authors": ["Isabella A. Stewart", "Tarjei Paule Hage", "Yu-Chuan Hsu", "Markus J. Buehler"], "title": "GraphAgents: Knowledge Graph-Guided Agentic AI for Cross-Domain Materials Design", "comment": null, "summary": "Large Language Models (LLMs) promise to accelerate discovery by reasoning across the expanding scientific landscape. Yet, the challenge is no longer access to information but connecting it in meaningful, domain-spanning ways. In materials science, where innovation demands integrating concepts from molecular chemistry to mechanical performance, this is especially acute. Neither humans nor single-agent LLMs can fully contend with this torrent of information, with the latter often prone to hallucinations. To address this bottleneck, we introduce a multi-agent framework guided by large-scale knowledge graphs to find sustainable substitutes for per- and polyfluoroalkyl substances (PFAS)-chemicals currently under intense regulatory scrutiny. Agents in the framework specialize in problem decomposition, evidence retrieval, design parameter extraction, and graph traversal, uncovering latent connections across distinct knowledge pockets to support hypothesis generation. Ablation studies show that the full multi-agent pipeline outperforms single-shot prompting, underscoring the value of distributed specialization and relational reasoning. We demonstrate that by tailoring graph traversal strategies, the system alternates between exploitative searches focusing on domain-critical outcomes and exploratory searches surfacing emergent cross-connections. Illustrated through the exemplar of biomedical tubing, the framework generates sustainable PFAS-free alternatives that balance tribological performance, thermal stability, chemical resistance, and biocompatibility. This work establishes a framework combining knowledge graphs with multi-agent reasoning to expand the materials design space, showcasing several initial design candidates to demonstrate the approach.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u7ed3\u5408\u77e5\u8bc6\u56fe\u8c31\u4e0e\u591a\u667a\u80fd\u4f53\u63a8\u7406\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5bfb\u627ePFAS\uff08\u5168\u6c1f\u548c\u591a\u6c1f\u70f7\u57fa\u7269\u8d28\uff09\u7684\u53ef\u6301\u7eed\u66ff\u4ee3\u54c1\uff0c\u901a\u8fc7\u5206\u5e03\u5f0f\u4e13\u4e1a\u5316\u548c\u5173\u7cfb\u63a8\u7406\u6765\u52a0\u901f\u6750\u6599\u53d1\u73b0\u8fc7\u7a0b\u3002", "motivation": "\u5728\u6750\u6599\u79d1\u5b66\u9886\u57df\uff0c\u521b\u65b0\u9700\u8981\u6574\u5408\u4ece\u5206\u5b50\u5316\u5b66\u5230\u673a\u68b0\u6027\u80fd\u7684\u591a\u4e2a\u6982\u5ff5\uff0c\u4f46\u4eba\u7c7b\u6216\u5355\u667a\u80fd\u4f53\u5927\u8bed\u8a00\u6a21\u578b\u96be\u4ee5\u5904\u7406\u5982\u6b64\u5e9e\u5927\u7684\u4fe1\u606f\u6d41\uff0c\u4e14\u540e\u8005\u5bb9\u6613\u4ea7\u751f\u5e7b\u89c9\u3002\u5f53\u524d\u6311\u6218\u5df2\u4e0d\u518d\u662f\u4fe1\u606f\u83b7\u53d6\uff0c\u800c\u662f\u5982\u4f55\u4ee5\u6709\u610f\u4e49\u3001\u8de8\u9886\u57df\u7684\u65b9\u5f0f\u8fde\u63a5\u4fe1\u606f\u3002", "method": "\u5f15\u5165\u4e86\u4e00\u4e2a\u7531\u5927\u89c4\u6a21\u77e5\u8bc6\u56fe\u8c31\u6307\u5bfc\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\u3002\u667a\u80fd\u4f53\u4e13\u95e8\u8d1f\u8d23\u95ee\u9898\u5206\u89e3\u3001\u8bc1\u636e\u68c0\u7d22\u3001\u8bbe\u8ba1\u53c2\u6570\u63d0\u53d6\u548c\u56fe\u904d\u5386\uff0c\u901a\u8fc7\u63ed\u793a\u4e0d\u540c\u77e5\u8bc6\u9886\u57df\u4e4b\u95f4\u7684\u6f5c\u5728\u8054\u7cfb\u6765\u652f\u6301\u5047\u8bbe\u751f\u6210\u3002\u901a\u8fc7\u5b9a\u5236\u56fe\u904d\u5386\u7b56\u7565\uff0c\u7cfb\u7edf\u5728\u4e13\u6ce8\u4e8e\u9886\u57df\u5173\u952e\u7ed3\u679c\u7684\u5229\u7528\u6027\u641c\u7d22\u548c\u63ed\u793a\u65b0\u5174\u8de8\u9886\u57df\u8fde\u63a5\u7684\u63a2\u7d22\u6027\u641c\u7d22\u4e4b\u95f4\u4ea4\u66ff\u8fdb\u884c\u3002", "result": "\u6d88\u878d\u7814\u7a76\u8868\u660e\uff0c\u5b8c\u6574\u7684\u591a\u667a\u80fd\u4f53\u6d41\u7a0b\u4f18\u4e8e\u5355\u6b21\u63d0\u793a\uff0c\u7a81\u663e\u4e86\u5206\u5e03\u5f0f\u4e13\u4e1a\u5316\u548c\u5173\u7cfb\u63a8\u7406\u7684\u4ef7\u503c\u3002\u4ee5\u751f\u7269\u533b\u5b66\u5bfc\u7ba1\u4e3a\u4f8b\uff0c\u8be5\u6846\u67b6\u751f\u6210\u4e86\u5e73\u8861\u6469\u64e6\u6027\u80fd\u3001\u70ed\u7a33\u5b9a\u6027\u3001\u5316\u5b66\u6297\u6027\u548c\u751f\u7269\u76f8\u5bb9\u6027\u7684\u53ef\u6301\u7eed\u65e0PFAS\u66ff\u4ee3\u54c1\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u5efa\u7acb\u4e86\u4e00\u4e2a\u7ed3\u5408\u77e5\u8bc6\u56fe\u8c31\u4e0e\u591a\u667a\u80fd\u4f53\u63a8\u7406\u7684\u6846\u67b6\uff0c\u6269\u5c55\u4e86\u6750\u6599\u8bbe\u8ba1\u7a7a\u95f4\uff0c\u5c55\u793a\u4e86\u51e0\u4e2a\u521d\u6b65\u7684\u8bbe\u8ba1\u5019\u9009\u65b9\u6848\u6765\u9a8c\u8bc1\u8be5\u65b9\u6cd5\u3002"}}
{"id": "2602.07058", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.07058", "abs": "https://arxiv.org/abs/2602.07058", "authors": ["Carolina R. Kelsch", "Leonardo S. B. Pereira", "Natnael Mola", "Luis H. Arribas", "Juan C. S. M. Avedillo"], "title": "FADE: Selective Forgetting via Sparse LoRA and Self-Distillation", "comment": null, "summary": "Machine Unlearning aims to remove the influence of specific data or concepts from trained models while preserving overall performance, a capability increasingly required by data protection regulations and responsible AI practices. Despite recent progress, unlearning in text-to-image diffusion models remains challenging due to high computational costs and the difficulty of balancing effective forgetting with retention of unrelated concepts. We introduce FADE (Fast Adapter for Data Erasure), a two-stage unlearning method for image generation that combines parameter localization with self-distillation. FADE first identifies parameters most responsible for the forget set using gradient-based saliency and constrains updates through sparse LoRA adapters, ensuring lightweight, localized modifications. In a second stage, FADE applies a self-distillation objective that overwrites the forgotten concept with a user-defined surrogate while preserving behavior on retained data. The resulting adapters are memory-efficient, reversible, and can be merged or removed at runtime, enabling flexible deployment in production systems. We evaluated FADE on the UnlearnCanvas benchmark and conducted ablation studies on Imagenette, Labeled Faces in the Wild, AtharvaTaras Dog Breeds Dataset, and SUN Attributes datasets, demonstrating State-of-the-Art unlearning performance with fine-grained control over the forgetting-retention trade-off. Our results demonstrate that FADE achieves strong concept erasure and high retainability across various domains, making it a suitable solution for selective unlearning in diffusion-based image generation models.", "AI": {"tldr": "FADE\u662f\u4e00\u79cd\u7528\u4e8e\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u7684\u4e24\u9636\u6bb5\u9057\u5fd8\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u53c2\u6570\u5b9a\u4f4d\u548c\u81ea\u84b8\u998f\u5b9e\u73b0\u9ad8\u6548\u3001\u53ef\u9006\u7684\u6982\u5ff5\u64e6\u9664\uff0c\u5728\u4fdd\u6301\u6574\u4f53\u6027\u80fd\u7684\u540c\u65f6\u79fb\u9664\u7279\u5b9a\u6570\u636e\u7684\u5f71\u54cd\u3002", "motivation": "\u968f\u7740\u6570\u636e\u4fdd\u62a4\u6cd5\u89c4\u548c\u8d1f\u8d23\u4efbAI\u5b9e\u8df5\u7684\u8981\u6c42\u589e\u52a0\uff0c\u9700\u8981\u4ece\u8bad\u7ec3\u6a21\u578b\u4e2d\u79fb\u9664\u7279\u5b9a\u6570\u636e\u6216\u6982\u5ff5\u5f71\u54cd\u7684\u80fd\u529b\u3002\u5f53\u524d\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u7684\u9057\u5fd8\u5b66\u4e60\u9762\u4e34\u9ad8\u8ba1\u7b97\u6210\u672c\u548c\u5e73\u8861\u6709\u6548\u9057\u5fd8\u4e0e\u4fdd\u7559\u65e0\u5173\u6982\u5ff5\u7684\u6311\u6218\u3002", "method": "FADE\u91c7\u7528\u4e24\u9636\u6bb5\u65b9\u6cd5\uff1a1) \u4f7f\u7528\u57fa\u4e8e\u68af\u5ea6\u7684\u663e\u8457\u6027\u8bc6\u522b\u5bf9\u9057\u5fd8\u96c6\u6700\u8d1f\u8d23\u7684\u53c2\u6570\uff0c\u901a\u8fc7\u7a00\u758fLoRA\u9002\u914d\u5668\u7ea6\u675f\u66f4\u65b0\uff1b2) \u5e94\u7528\u81ea\u84b8\u998f\u76ee\u6807\uff0c\u7528\u7528\u6237\u5b9a\u4e49\u7684\u66ff\u4ee3\u6982\u5ff5\u8986\u76d6\u9057\u5fd8\u6982\u5ff5\uff0c\u540c\u65f6\u4fdd\u7559\u5728\u4fdd\u7559\u6570\u636e\u4e0a\u7684\u884c\u4e3a\u3002", "result": "\u5728UnlearnCanvas\u57fa\u51c6\u6d4b\u8bd5\u548c\u591a\u4e2a\u6570\u636e\u96c6\uff08Imagenette\u3001LFW\u3001ATDB\u3001SUN Attributes\uff09\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0cFADE\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u9057\u5fd8\u6027\u80fd\uff0c\u5728\u9057\u5fd8-\u4fdd\u7559\u6743\u8861\u65b9\u9762\u5177\u6709\u7ec6\u7c92\u5ea6\u63a7\u5236\uff0c\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6982\u5ff5\u64e6\u9664\u80fd\u529b\u548c\u9ad8\u4fdd\u7559\u6027\u3002", "conclusion": "FADE\u63d0\u4f9b\u4e86\u4e00\u79cd\u5185\u5b58\u9ad8\u6548\u3001\u53ef\u9006\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u914d\u5668\u53ef\u5728\u8fd0\u884c\u65f6\u5408\u5e76\u6216\u79fb\u9664\uff0c\u9002\u5408\u5728\u57fa\u4e8e\u6269\u6563\u7684\u56fe\u50cf\u751f\u6210\u6a21\u578b\u4e2d\u8fdb\u884c\u9009\u62e9\u6027\u9057\u5fd8\u5b66\u4e60\uff0c\u6ee1\u8db3\u751f\u4ea7\u7cfb\u7edf\u7684\u7075\u6d3b\u90e8\u7f72\u9700\u6c42\u3002"}}
{"id": "2602.08015", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.08015", "abs": "https://arxiv.org/abs/2602.08015", "authors": ["Patricia G. F. Matsubara", "Tayana Conte"], "title": "Bridging the Gap: Adapting Evidence to Decision Frameworks to support the link between Software Engineering academia and industry", "comment": "Accepted for publication in ICSE 2026 - Future of Software Engineering", "summary": "Over twenty years ago, the Software Engineering (SE) research community have been involved with Evidence-Based Software Engineering (EBSE). EBSE aims to inform industrial practice with the best evidence from rigorous research, preferably from systematic literature reviews (SLRs). Since then, SE researchers have conducted many SLRs, perfected their SLR procedures, proposed alternative ways of presenting their results (such as Evidence Briefings), and profusely discussed how to conduct research that impacts practice. Nevertheless, there is still a feeling that SLRs' results are not reaching practitioners. Something is missing. In this vision paper, we introduce Evidence to Decision (EtD) frameworks from the health sciences, which propose gathering experts in panels to assess the existing best evidence about the impact of an intervention in all relevant outcomes and make structured recommendations based on them. The insight we can leverage from EtD frameworks is not their structure per se but all the relevant criteria for making recommendations to practitioners from SLRs. Furthermore, we provide a worked example based on an SE SLR. We also discuss the challenges the SE research and practice community may face when adopting EtD frameworks, highlighting the need for more comprehensive criteria in our recommendations to industry practitioners.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u5c06\u5065\u5eb7\u79d1\u5b66\u9886\u57df\u7684\"\u8bc1\u636e\u5230\u51b3\u7b56\"\u6846\u67b6\u5f15\u5165\u8f6f\u4ef6\u5de5\u7a0b\u9886\u57df\uff0c\u4ee5\u89e3\u51b3\u7cfb\u7edf\u6587\u732e\u7efc\u8ff0\u7ed3\u679c\u96be\u4ee5\u89e6\u8fbe\u5b9e\u8df5\u8005\u7684\u95ee\u9898\u3002", "motivation": "\u5c3d\u7ba1\u8f6f\u4ef6\u5de5\u7a0b\u9886\u57df\u5df2\u8fdb\u884c\u5927\u91cf\u7cfb\u7edf\u6587\u732e\u7efc\u8ff0\u5e76\u5b8c\u5584\u4e86\u76f8\u5173\u6d41\u7a0b\uff0c\u4f46\u7814\u7a76\u7ed3\u679c\u4ecd\u96be\u4ee5\u6709\u6548\u4f20\u8fbe\u7ed9\u884c\u4e1a\u5b9e\u8df5\u8005\uff0c\u9700\u8981\u5bfb\u627e\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u6765\u5f25\u5408\u7814\u7a76\u4e0e\u5b9e\u8df5\u7684\u9e3f\u6c9f\u3002", "method": "\u5f15\u5165\u5065\u5eb7\u79d1\u5b66\u9886\u57df\u7684\u8bc1\u636e\u5230\u51b3\u7b56\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u4e13\u5bb6\u5c0f\u7ec4\u8bc4\u4f30\u73b0\u6709\u6700\u4f73\u8bc1\u636e\uff0c\u5e76\u57fa\u4e8e\u76f8\u5173\u6807\u51c6\u5236\u5b9a\u7ed3\u6784\u5316\u5efa\u8bae\u3002\u8bba\u6587\u8fd8\u63d0\u4f9b\u4e86\u57fa\u4e8eSE\u7cfb\u7edf\u6587\u732e\u7efc\u8ff0\u7684\u5de5\u4f5c\u793a\u4f8b\u3002", "result": "\u63d0\u51faEtD\u6846\u67b6\u53ef\u4ee5\u4e3a\u8f6f\u4ef6\u5de5\u7a0b\u9886\u57df\u63d0\u4f9b\u66f4\u5168\u9762\u7684\u51b3\u7b56\u6807\u51c6\uff0c\u5e2e\u52a9\u5c06\u7cfb\u7edf\u6587\u732e\u7efc\u8ff0\u7ed3\u679c\u8f6c\u5316\u4e3a\u5bf9\u5b9e\u8df5\u8005\u6709\u7528\u7684\u5efa\u8bae\uff0c\u4f46\u5b9e\u65bd\u8fc7\u7a0b\u4e2d\u53ef\u80fd\u9762\u4e34\u6311\u6218\u3002", "conclusion": "\u8bc1\u636e\u5230\u51b3\u7b56\u6846\u67b6\u6709\u671b\u6539\u5584\u8f6f\u4ef6\u5de5\u7a0b\u7814\u7a76\u4e0e\u5b9e\u8df5\u4e4b\u95f4\u7684\u77e5\u8bc6\u8f6c\u79fb\uff0c\u4f46\u9700\u8981\u89e3\u51b3\u5b9e\u65bd\u4e2d\u7684\u6311\u6218\uff0c\u5e76\u5f00\u53d1\u66f4\u5168\u9762\u7684\u884c\u4e1a\u5b9e\u8df5\u5efa\u8bae\u6807\u51c6\u3002"}}
{"id": "2602.08384", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2602.08384", "abs": "https://arxiv.org/abs/2602.08384", "authors": ["Jianyu Zhang", "Fuyuan Zhang", "Jiayi Lu", "Jilin Hu", "Xiaoyi Yin", "Long Zhang", "Feng Yang", "Yongwang Zhao"], "title": "Towards Real-World Industrial-Scale Verification: LLM-Driven Theorem Proving on seL4", "comment": null, "summary": "Formal methods (FM) are reliable but costly to apply, often requiring years of expert effort in industrial-scale projects such as seL4, especially for theorem proving. Recent advances in large language models (LLMs) have made automated theorem proving increasingly feasible. However, most prior work focuses on mathematics-oriented benchmarks such as miniF2F, with limited evaluation on real-world verification projects. The few studies that consider industrial-scale verification mostly rely on closed-source models with hundreds of billions of parameters, which cannot be locally deployed and incur substantial usage costs. In this paper, we propose AutoReal, an LLM-driven theorem proving method for real-world industrial-scale systems with support for lightweight local deployment. We evaluate AutoReal on the seL4-Isabelle verification project as a representative and challenging case study. AutoReal incorporates two key improvements: (1) chain-of-thought (CoT)-based proof training, which teaches the LLM the reasoning behind proof steps and enables step-wise explanations alongside proofs, and (2) context augmentation, which leverages proof context from the project to enhance LLM-driven proving. Based on the AutoReal methodology, we fine-tune a base model to obtain AutoReal-Prover, a compact 7B-scale prover for industrial-scale theorem proving. AutoReal-Prover achieves a 51.67% proof success rate on 660 theorems from seL4-designated Important Theories across all 10 seL4 proof categories, substantially outperforming prior attempts on seL4 (27.06%). To evaluate generalization, we further apply AutoReal-Prover to three security-related projects from the Archive of Formal Proofs (AFP), covering all 451 theorems and achieving a proof success rate of 53.88%. Overall, this work advances the application of LLM-driven theorem proving in real-world industrial-scale verification.", "AI": {"tldr": "AutoReal\uff1a\u4e00\u79cd\u9762\u5411\u5de5\u4e1a\u7ea7\u7cfb\u7edf\u9a8c\u8bc1\u7684LLM\u9a71\u52a8\u5b9a\u7406\u8bc1\u660e\u65b9\u6cd5\uff0c\u901a\u8fc7CoT\u8bad\u7ec3\u548c\u4e0a\u4e0b\u6587\u589e\u5f3a\uff0c\u57287B\u53c2\u6570\u89c4\u6a21\u4e0a\u5b9e\u73b051.67%\u7684seL4\u5b9a\u7406\u8bc1\u660e\u6210\u529f\u7387", "motivation": "\u4f20\u7edf\u5f62\u5f0f\u5316\u65b9\u6cd5\u6210\u672c\u9ad8\u6602\uff0c\u9700\u8981\u4e13\u5bb6\u591a\u5e74\u52aa\u529b\uff1b\u73b0\u6709LLM\u5b9a\u7406\u8bc1\u660e\u7814\u7a76\u591a\u96c6\u4e2d\u4e8e\u6570\u5b66\u57fa\u51c6\uff0c\u5bf9\u5de5\u4e1a\u7ea7\u9a8c\u8bc1\u9879\u76ee\u8bc4\u4f30\u6709\u9650\uff1b\u5927\u578b\u95ed\u6e90\u6a21\u578b\u65e0\u6cd5\u672c\u5730\u90e8\u7f72\u4e14\u6210\u672c\u9ad8", "method": "\u63d0\u51faAutoReal\u65b9\u6cd5\uff1a1) \u57fa\u4e8e\u601d\u7ef4\u94fe\u7684\u8bc1\u660e\u8bad\u7ec3\uff0c\u6559LLM\u7406\u89e3\u8bc1\u660e\u6b65\u9aa4\u80cc\u540e\u7684\u63a8\u7406\u8fc7\u7a0b\uff1b2) \u4e0a\u4e0b\u6587\u589e\u5f3a\uff0c\u5229\u7528\u9879\u76ee\u4e2d\u7684\u8bc1\u660e\u4e0a\u4e0b\u6587\u63d0\u5347\u8bc1\u660e\u80fd\u529b\u3002\u57fa\u4e8e\u8be5\u65b9\u6cd5\u5fae\u8c03\u5f97\u52307B\u53c2\u6570\u7684AutoReal-Prover", "result": "\u5728seL4\u9a8c\u8bc1\u9879\u76ee\u4e2d\uff0cAutoReal-Prover\u5728660\u4e2a\u91cd\u8981\u5b9a\u7406\u4e0a\u8fbe\u523051.67%\u7684\u8bc1\u660e\u6210\u529f\u7387\uff0c\u663e\u8457\u4f18\u4e8e\u4e4b\u524d27.06%\u7684\u7ed3\u679c\uff1b\u5728AFP\u7684\u4e09\u4e2a\u5b89\u5168\u9879\u76ee\u4e2d\uff0c\u5bf9451\u4e2a\u5b9a\u7406\u8fbe\u523053.88%\u7684\u8bc1\u660e\u6210\u529f\u7387", "conclusion": "AutoReal\u63a8\u52a8\u4e86LLM\u9a71\u52a8\u5b9a\u7406\u8bc1\u660e\u5728\u771f\u5b9e\u5de5\u4e1a\u7ea7\u9a8c\u8bc1\u4e2d\u7684\u5e94\u7528\uff0c\u5b9e\u73b0\u4e86\u8f7b\u91cf\u7ea7\u672c\u5730\u90e8\u7f72\uff0c\u5728\u5de5\u4e1a\u89c4\u6a21\u7cfb\u7edf\u9a8c\u8bc1\u4e2d\u8868\u73b0\u51fa\u8272"}}
{"id": "2602.07639", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.07639", "abs": "https://arxiv.org/abs/2602.07639", "authors": ["Jaewook Lee", "Alexander Scarlatos", "Simon Woodhead", "Andrew Lan"], "title": "Letting Tutor Personas \"Speak Up\" for LLMs: Learning Steering Vectors from Dialogue via Preference Optimization", "comment": null, "summary": "With the emergence of large language models (LLMs) as a powerful class of generative artificial intelligence (AI), their use in tutoring has become increasingly prominent. Prior works on LLM-based tutoring typically learn a single tutor policy and do not capture the diversity of tutoring styles. In real-world tutor-student interactions, pedagogical intent is realized through adaptive instructional strategies, with tutors varying the level of scaffolding, instructional directiveness, feedback, and affective support in response to learners' needs. These differences can all impact dialogue dynamics and student engagement. In this paper, we explore how tutor personas embedded in human tutor-student dialogues can be used to guide LLM behavior without relying on explicitly prompted instructions. We modify Bidirectional Preference Optimization (BiPO) to learn a steering vector, an activation-space direction that steers model responses towards certain tutor personas. We find that this steering vector captures tutor-specific variation across dialogue contexts, improving semantic alignment with ground-truth tutor utterances and increasing preference-based evaluations, while largely preserving lexical similarity. Analysis of the learned directional coefficients further reveals interpretable structure across tutors, corresponding to consistent differences in tutoring behavior. These results demonstrate that activation steering offers an effective and interpretable way for controlling tutor-specific variation in LLMs using signals derived directly from human dialogue data.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u7d22\u5982\u4f55\u5229\u7528\u4eba\u7c7b\u5bfc\u5e08-\u5b66\u751f\u5bf9\u8bdd\u4e2d\u7684\u5bfc\u5e08\u89d2\u8272\u6765\u6307\u5bfc\u5927\u8bed\u8a00\u6a21\u578b\u884c\u4e3a\uff0c\u901a\u8fc7\u4fee\u6539\u53cc\u5411\u504f\u597d\u4f18\u5316\u5b66\u4e60\u6fc0\u6d3b\u7a7a\u95f4\u4e2d\u7684\u5bfc\u5411\u5411\u91cf\uff0c\u4f7f\u6a21\u578b\u54cd\u5e94\u66f4\u7b26\u5408\u7279\u5b9a\u5bfc\u5e08\u98ce\u683c\uff0c\u800c\u65e0\u9700\u663e\u5f0f\u63d0\u793a\u6307\u4ee4\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8f85\u5bfc\u7cfb\u7edf\u901a\u5e38\u5b66\u4e60\u5355\u4e00\u5bfc\u5e08\u7b56\u7565\uff0c\u672a\u80fd\u6355\u6349\u5bfc\u5e08\u98ce\u683c\u7684\u591a\u6837\u6027\u3002\u73b0\u5b9e\u4e2d\uff0c\u5bfc\u5e08\u4f1a\u6839\u636e\u5b66\u751f\u9700\u6c42\u8c03\u6574\u811a\u624b\u67b6\u6c34\u5e73\u3001\u6307\u5bfc\u65b9\u5411\u6027\u3001\u53cd\u9988\u548c\u60c5\u611f\u652f\u6301\u7b49\u6559\u5b66\u7b56\u7565\uff0c\u8fd9\u4e9b\u5dee\u5f02\u4f1a\u5f71\u54cd\u5bf9\u8bdd\u52a8\u6001\u548c\u5b66\u751f\u53c2\u4e0e\u5ea6\u3002", "method": "\u4fee\u6539\u53cc\u5411\u504f\u597d\u4f18\u5316\u65b9\u6cd5\uff0c\u5b66\u4e60\u4e00\u4e2a\u6fc0\u6d3b\u7a7a\u95f4\u4e2d\u7684\u5bfc\u5411\u5411\u91cf\uff0c\u8be5\u5411\u91cf\u80fd\u591f\u5c06\u6a21\u578b\u54cd\u5e94\u5f15\u5bfc\u5411\u7279\u5b9a\u5bfc\u5e08\u89d2\u8272\u3002\u8fd9\u79cd\u65b9\u6cd5\u76f4\u63a5\u4ece\u4eba\u7c7b\u5bf9\u8bdd\u6570\u636e\u4e2d\u63d0\u53d6\u4fe1\u53f7\uff0c\u65e0\u9700\u663e\u5f0f\u63d0\u793a\u6307\u4ee4\u3002", "result": "\u5b66\u4e60\u5230\u7684\u5bfc\u5411\u5411\u91cf\u80fd\u591f\u6355\u6349\u4e0d\u540c\u5bf9\u8bdd\u60c5\u5883\u4e0b\u7684\u5bfc\u5e08\u7279\u5b9a\u53d8\u5316\uff0c\u63d0\u9ad8\u4e0e\u771f\u5b9e\u5bfc\u5e08\u8bdd\u8bed\u7684\u8bed\u4e49\u5bf9\u9f50\u5ea6\uff0c\u589e\u52a0\u57fa\u4e8e\u504f\u597d\u7684\u8bc4\u4f30\u5206\u6570\uff0c\u540c\u65f6\u57fa\u672c\u4fdd\u6301\u8bcd\u6c47\u76f8\u4f3c\u6027\u3002\u5bf9\u5b66\u4e60\u5230\u7684\u65b9\u5411\u7cfb\u6570\u5206\u6790\u63ed\u793a\u4e86\u8de8\u5bfc\u5e08\u7684\u53ef\u89e3\u91ca\u7ed3\u6784\uff0c\u5bf9\u5e94\u4e00\u81f4\u7684\u8f85\u5bfc\u884c\u4e3a\u5dee\u5f02\u3002", "conclusion": "\u6fc0\u6d3b\u5bfc\u5411\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u4e14\u53ef\u89e3\u91ca\u7684\u65b9\u6cd5\uff0c\u5229\u7528\u76f4\u63a5\u4ece\u4eba\u7c7b\u5bf9\u8bdd\u6570\u636e\u4e2d\u63d0\u53d6\u7684\u4fe1\u53f7\u6765\u63a7\u5236\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u5bfc\u5e08\u7279\u5b9a\u53d8\u5316\u3002"}}
{"id": "2602.07062", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.07062", "abs": "https://arxiv.org/abs/2602.07062", "authors": ["Daniil Storonkin", "Ilia Dziub", "Maksim Golyadkin", "Ilya Makarov"], "title": "From Images to Decisions: Assistive Computer Vision for Non-Metallic Content Estimation in Scrap Metal", "comment": "AAAI 2026 Workshop on Addressing Challenges and Opportunities in Human-Centric Manufacturing", "summary": "Scrap quality directly affects energy use, emissions, and safety in steelmaking. Today, the share of non-metallic inclusions (contamination) is judged visually by inspectors - an approach that is subjective and hazardous due to dust and moving machinery. We present an assistive computer vision pipeline that estimates contamination (per percent) from images captured during railcar unloading and also classifies scrap type. The method formulates contamination assessment as a regression task at the railcar level and leverages sequential data through multi-instance learning (MIL) and multi-task learning (MTL). Best results include MAE 0.27 and R2 0.83 by MIL; and an MTL setup reaches MAE 0.36 with F1 0.79 for scrap class. Also we present the system in near real time within the acceptance workflow: magnet/railcar detection segments temporal layers, a versioned inference service produces railcar-level estimates with confidence scores, and results are reviewed by operators with structured overrides; corrections and uncertain cases feed an active-learning loop for continual improvement. The pipeline reduces subjective variability, improves human safety, and enables integration into acceptance and melt-planning workflows.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u8ba1\u7b97\u673a\u89c6\u89c9\u7cfb\u7edf\uff0c\u901a\u8fc7\u56fe\u50cf\u5206\u6790\u8bc4\u4f30\u5e9f\u94a2\u4e2d\u7684\u975e\u91d1\u5c5e\u5939\u6742\u7269\u6c61\u67d3\u7a0b\u5ea6\uff0c\u5e76\u5206\u7c7b\u5e9f\u94a2\u7c7b\u578b\uff0c\u7528\u4e8e\u94a2\u94c1\u751f\u4ea7\u4e2d\u7684\u8d28\u91cf\u63a7\u5236", "motivation": "\u5f53\u524d\u5e9f\u94a2\u8d28\u91cf\u4e3b\u8981\u901a\u8fc7\u4eba\u5de5\u76ee\u89c6\u68c0\u67e5\u975e\u91d1\u5c5e\u5939\u6742\u7269\u6c61\u67d3\u7a0b\u5ea6\uff0c\u8fd9\u79cd\u65b9\u6cd5\u4e3b\u89c2\u6027\u5f3a\u4e14\u5b58\u5728\u5b89\u5168\u9690\u60a3\uff08\u7c89\u5c18\u548c\u79fb\u52a8\u673a\u68b0\uff09\u3002\u9700\u8981\u5ba2\u89c2\u3001\u5b89\u5168\u7684\u81ea\u52a8\u5316\u8bc4\u4f30\u65b9\u6cd5", "method": "\u5c06\u6c61\u67d3\u8bc4\u4f30\u5efa\u6a21\u4e3a\u8f66\u53a2\u7ea7\u522b\u7684\u56de\u5f52\u4efb\u52a1\uff0c\u91c7\u7528\u591a\u5b9e\u4f8b\u5b66\u4e60\uff08MIL\uff09\u5904\u7406\u5e8f\u5217\u6570\u636e\uff0c\u7ed3\u5408\u591a\u4efb\u52a1\u5b66\u4e60\uff08MTL\uff09\u540c\u65f6\u8fdb\u884c\u6c61\u67d3\u8bc4\u4f30\u548c\u5e9f\u94a2\u5206\u7c7b\u3002\u7cfb\u7edf\u5305\u62ec\u78c1\u94c1/\u8f66\u53a2\u68c0\u6d4b\u3001\u7248\u672c\u5316\u63a8\u7406\u670d\u52a1\u3001\u7f6e\u4fe1\u5ea6\u8bc4\u5206\u548c\u4e3b\u52a8\u5b66\u4e60\u5faa\u73af", "result": "\u6700\u4f73\u7ed3\u679c\uff1aMIL\u65b9\u6cd5\u8fbe\u5230MAE 0.27\u548cR\u00b2 0.83\uff1bMTL\u8bbe\u7f6e\u8fbe\u5230MAE 0.36\uff0c\u5e9f\u94a2\u5206\u7c7bF1\u5206\u65700.79\u3002\u7cfb\u7edf\u5df2\u96c6\u6210\u5230\u9a8c\u6536\u5de5\u4f5c\u6d41\u4e2d\uff0c\u5b9e\u73b0\u8fd1\u5b9e\u65f6\u5904\u7406", "conclusion": "\u8be5\u8ba1\u7b97\u673a\u89c6\u89c9\u7ba1\u9053\u51cf\u5c11\u4e86\u4e3b\u89c2\u53d8\u5f02\u6027\uff0c\u63d0\u9ad8\u4e86\u4eba\u5458\u5b89\u5168\u6027\uff0c\u5e76\u80fd\u96c6\u6210\u5230\u9a8c\u6536\u548c\u7194\u70bc\u8ba1\u5212\u5de5\u4f5c\u6d41\u4e2d\uff0c\u901a\u8fc7\u4e3b\u52a8\u5b66\u4e60\u5b9e\u73b0\u6301\u7eed\u6539\u8fdb"}}
{"id": "2602.07543", "categories": ["cs.AI", "cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2602.07543", "abs": "https://arxiv.org/abs/2602.07543", "authors": ["Heewoong Noh", "Gyoung S. Na", "Namkyeong Lee", "Chanyoung Park"], "title": "MSP-LLM: A Unified Large Language Model Framework for Complete Material Synthesis Planning", "comment": null, "summary": "Material synthesis planning (MSP) remains a fundamental and underexplored bottleneck in AI-driven materials discovery, as it requires not only identifying suitable precursor materials but also designing coherent sequences of synthesis operations to realize a target material. Although several AI-based approaches have been proposed to address isolated subtasks of MSP, a unified methodology for solving the entire MSP task has yet to be established. We propose MSP-LLM, a unified LLM-based framework that formulates MSP as a structured process composed of two constituent subproblems: precursor prediction (PP) and synthesis operation prediction (SOP). Our approach introduces a discrete material class as an intermediate decision variable that organizes both tasks into a chemically consistent decision chain. For OP, we further incorporate hierarchical precursor types as synthesis-relevant inductive biases and employ an explicit conditioning strategy that preserves precursor-related information in the autoregressive decoding state. Extensive experiments show that MSP-LLM consistently outperforms existing methods on both PP and SOP, as well as on the complete MSP task, demonstrating an effective and scalable framework for MSP that can accelerate real-world materials discovery.", "AI": {"tldr": "MSP-LLM\uff1a\u4e00\u4e2a\u7edf\u4e00\u7684LLM\u6846\u67b6\uff0c\u5c06\u6750\u6599\u5408\u6210\u89c4\u5212\u5206\u89e3\u4e3a\u524d\u9a71\u4f53\u9884\u6d4b\u548c\u5408\u6210\u64cd\u4f5c\u9884\u6d4b\u4e24\u4e2a\u5b50\u95ee\u9898\uff0c\u901a\u8fc7\u5f15\u5165\u79bb\u6563\u6750\u6599\u7c7b\u522b\u4f5c\u4e3a\u4e2d\u95f4\u51b3\u7b56\u53d8\u91cf\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6750\u6599\u5408\u6210\u89c4\u5212\u7684\u51c6\u786e\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "motivation": "\u6750\u6599\u5408\u6210\u89c4\u5212\u662fAI\u9a71\u52a8\u6750\u6599\u53d1\u73b0\u4e2d\u7684\u5173\u952e\u74f6\u9888\uff0c\u73b0\u6709\u65b9\u6cd5\u53ea\u80fd\u89e3\u51b3\u5b64\u7acb\u5b50\u4efb\u52a1\uff0c\u7f3a\u4e4f\u7edf\u4e00\u7684\u89e3\u51b3\u65b9\u6848\u3002\u9700\u8981\u5efa\u7acb\u4e00\u4e2a\u80fd\u591f\u540c\u65f6\u5904\u7406\u524d\u9a71\u4f53\u9009\u62e9\u548c\u5408\u6210\u64cd\u4f5c\u5e8f\u5217\u8bbe\u8ba1\u7684\u5b8c\u6574\u6846\u67b6\u3002", "method": "\u63d0\u51faMSP-LLM\u6846\u67b6\uff0c\u5c06\u6750\u6599\u5408\u6210\u89c4\u5212\u5206\u89e3\u4e3a\u524d\u9a71\u4f53\u9884\u6d4b\u548c\u5408\u6210\u64cd\u4f5c\u9884\u6d4b\u4e24\u4e2a\u5b50\u95ee\u9898\u3002\u5f15\u5165\u79bb\u6563\u6750\u6599\u7c7b\u522b\u4f5c\u4e3a\u4e2d\u95f4\u51b3\u7b56\u53d8\u91cf\uff0c\u6784\u5efa\u5316\u5b66\u4e00\u81f4\u7684\u51b3\u7b56\u94fe\u3002\u5728\u5408\u6210\u64cd\u4f5c\u9884\u6d4b\u4e2d\uff0c\u91c7\u7528\u5206\u5c42\u524d\u9a71\u4f53\u7c7b\u578b\u4f5c\u4e3a\u5f52\u7eb3\u504f\u7f6e\uff0c\u5e76\u4f7f\u7528\u663e\u5f0f\u6761\u4ef6\u7b56\u7565\u5728\u81ea\u56de\u5f52\u89e3\u7801\u72b6\u6001\u4e2d\u4fdd\u7559\u524d\u9a71\u4f53\u76f8\u5173\u4fe1\u606f\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cMSP-LLM\u5728\u524d\u9a71\u4f53\u9884\u6d4b\u3001\u5408\u6210\u64cd\u4f5c\u9884\u6d4b\u4ee5\u53ca\u5b8c\u6574\u7684\u6750\u6599\u5408\u6210\u89c4\u5212\u4efb\u52a1\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u8bc1\u660e\u4e86\u8be5\u6846\u67b6\u5728\u6750\u6599\u5408\u6210\u89c4\u5212\u4e2d\u7684\u6709\u6548\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "MSP-LLM\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u6548\u4e14\u53ef\u6269\u5c55\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u80fd\u591f\u52a0\u901f\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u6750\u6599\u53d1\u73b0\u8fc7\u7a0b\uff0c\u89e3\u51b3\u4e86\u6750\u6599\u5408\u6210\u89c4\u5212\u8fd9\u4e00\u957f\u671f\u5b58\u5728\u7684\u74f6\u9888\u95ee\u9898\u3002"}}
{"id": "2602.08084", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.08084", "abs": "https://arxiv.org/abs/2602.08084", "authors": ["Mark Looi", "Marc Szepan"], "title": "Outsourcing in Global Software Development: Effects of Temporal Location and Methodologies", "comment": "Published in International Journal of Business and Social Science International Journal of Business and Social Science, Vol. 12, No. 3; March 2021, DOI: 10.30845/ijbss.v12n3p3", "summary": "Developing software globally using outsourced resources has become a common practice, with project teams often distributed in different time zones. In this study, we focus on customers that contract software development to vendors in temporally nearshore or far offshore locations. We conducted a survey to determine the effect of temporal distance on overall success, costs, project management effort, schedule, quality, communication problems, and other outcomes of interest to managers. In the survey of 80 customers and interviews with 6 of them, we also investigated the effect of software development methodology on the same outcomes. The results show that nearshore development is advantageous for overall success, quality, reduced PM effort, maintaining schedule, higher quality, and engendering fewer communication problems. Development methodology appears to only influence higher costs. We assess our findings in the context of prior GSE research and provide practical advice for customers of outsourced global software development, chief of which is to favor nearshore for communication-intensive or Agile projects.", "AI": {"tldr": "\u7814\u7a76\u6bd4\u8f83\u4e86\u65f6\u533a\u76f8\u8fd1\uff08\u8fd1\u5cb8\uff09\u548c\u65f6\u533a\u5dee\u5f02\u5927\uff08\u8fdc\u5cb8\uff09\u7684\u5916\u5305\u8f6f\u4ef6\u5f00\u53d1\u5bf9\u9879\u76ee\u6210\u529f\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u8fd1\u5cb8\u5f00\u53d1\u5728\u6574\u4f53\u6210\u529f\u3001\u8d28\u91cf\u3001\u9879\u76ee\u7ba1\u7406\u3001\u8fdb\u5ea6\u63a7\u5236\u548c\u6c9f\u901a\u65b9\u9762\u66f4\u6709\u4f18\u52bf\u3002", "motivation": "\u968f\u7740\u5168\u7403\u8f6f\u4ef6\u5916\u5305\u6210\u4e3a\u666e\u904d\u5b9e\u8df5\uff0c\u9879\u76ee\u56e2\u961f\u5206\u5e03\u5728\u4e0d\u540c\u7684\u65f6\u533a\u3002\u672c\u7814\u7a76\u5173\u6ce8\u5ba2\u6237\u5c06\u8f6f\u4ef6\u5f00\u53d1\u5916\u5305\u7ed9\u65f6\u533a\u76f8\u8fd1\uff08\u8fd1\u5cb8\uff09\u6216\u65f6\u533a\u5dee\u5f02\u5927\uff08\u8fdc\u5cb8\uff09\u7684\u4f9b\u5e94\u5546\u65f6\uff0c\u65f6\u95f4\u8ddd\u79bb\u5bf9\u9879\u76ee\u7ed3\u679c\u7684\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u8c03\u67e580\u540d\u5ba2\u6237\u548c\u8bbf\u8c08\u5176\u4e2d6\u540d\u5ba2\u6237\uff0c\u7814\u7a76\u4e86\u65f6\u95f4\u8ddd\u79bb\u5bf9\u6574\u4f53\u6210\u529f\u3001\u6210\u672c\u3001\u9879\u76ee\u7ba1\u7406\u52aa\u529b\u3001\u8fdb\u5ea6\u3001\u8d28\u91cf\u3001\u6c9f\u901a\u95ee\u9898\u7b49\u7ed3\u679c\u7684\u5f71\u54cd\uff0c\u540c\u65f6\u8003\u5bdf\u4e86\u8f6f\u4ef6\u5f00\u53d1\u65b9\u6cd5\u8bba\u5bf9\u8fd9\u4e9b\u7ed3\u679c\u7684\u5f71\u54cd\u3002", "result": "\u8fd1\u5cb8\u5f00\u53d1\u5728\u6574\u4f53\u6210\u529f\u3001\u8d28\u91cf\u3001\u51cf\u5c11\u9879\u76ee\u7ba1\u7406\u52aa\u529b\u3001\u4fdd\u6301\u8fdb\u5ea6\u3001\u63d0\u9ad8\u8d28\u91cf\u548c\u51cf\u5c11\u6c9f\u901a\u95ee\u9898\u65b9\u9762\u5177\u6709\u4f18\u52bf\u3002\u8f6f\u4ef6\u5f00\u53d1\u65b9\u6cd5\u8bba\u4f3c\u4e4e\u53ea\u5f71\u54cd\u66f4\u9ad8\u7684\u6210\u672c\u3002", "conclusion": "\u8fd1\u5cb8\u5f00\u53d1\u5bf9\u6c9f\u901a\u5bc6\u96c6\u578b\u6216\u654f\u6377\u9879\u76ee\u66f4\u4e3a\u6709\u5229\u3002\u7814\u7a76\u4e3a\u5916\u5305\u5168\u7403\u8f6f\u4ef6\u5f00\u53d1\u7684\u5ba2\u6237\u63d0\u4f9b\u4e86\u5b9e\u7528\u5efa\u8bae\uff0c\u5efa\u8bae\u4ed6\u4eec\u4f18\u5148\u8003\u8651\u8fd1\u5cb8\u5f00\u53d1\u3002"}}
{"id": "2602.08422", "categories": ["cs.CR", "cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2602.08422", "abs": "https://arxiv.org/abs/2602.08422", "authors": ["Benjamin Livshits"], "title": "LLMs + Security = Trouble", "comment": null, "summary": "We argue that when it comes to producing secure code with AI, the prevailing \"fighting fire with fire\" approach -- using probabilistic AI-based checkers or attackers to secure probabilistically generated code -- fails to address the long tail of security bugs. As a result, systems may remain exposed to zero-day vulnerabilities that can be discovered by better-resourced or more persistent adversaries.\n  While neurosymbolic approaches that combine LLMs with formal methods are attractive in principle, we argue that they are difficult to reconcile with the \"vibe coding\" workflow common in LLM-assisted development: unless the end-to-end verification pipeline is fully automated, developers are repeatedly asked to validate specifications, resolve ambiguities, and adjudicate failures, making the human-in-the-loop a likely point of weakness, compromising secure-by-construction guarantees.\n  In this paper we argue that stronger security guarantees can be obtained by enforcing security constraints during code generation (e.g., via constrained decoding), rather than relying solely on post-hoc detection and repair. This direction is particularly promising for diffusion-style code models, whose approach provides a natural elegant opportunity for modular, hierarchical security enforcement, allowing us to combine lower-latency generation techniques with generating secure-by-construction code.", "AI": {"tldr": "\u8bba\u6587\u4e3b\u5f20\u901a\u8fc7\u7ea6\u675f\u89e3\u7801\u5728\u4ee3\u7801\u751f\u6210\u9636\u6bb5\u76f4\u63a5\u5f3a\u5236\u6267\u884c\u5b89\u5168\u7ea6\u675f\uff0c\u800c\u975e\u4f9d\u8d56\u540e\u9a8c\u68c0\u6d4b\u4fee\u590d\uff0c\u4ee5\u89e3\u51b3AI\u751f\u6210\u4ee3\u7801\u7684\u5b89\u5168\u6f0f\u6d1e\u957f\u5c3e\u95ee\u9898", "motivation": "\u5f53\u524d\u4f7f\u7528\u6982\u7387\u6027AI\u68c0\u67e5\u5668\u6216\u653b\u51fb\u8005\u6765\u4fdd\u62a4\u6982\u7387\u751f\u6210\u4ee3\u7801\u7684\"\u4ee5\u706b\u653b\u706b\"\u65b9\u6cd5\u65e0\u6cd5\u89e3\u51b3\u5b89\u5168\u6f0f\u6d1e\u7684\u957f\u5c3e\u95ee\u9898\uff0c\u7cfb\u7edf\u4ecd\u53ef\u80fd\u66b4\u9732\u4e8e\u96f6\u65e5\u6f0f\u6d1e\u3002\u795e\u7ecf\u7b26\u53f7\u65b9\u6cd5\u867d\u7136\u7406\u8bba\u4e0a\u5438\u5f15\u4eba\uff0c\u4f46\u96be\u4ee5\u4e0eLLM\u8f85\u52a9\u5f00\u53d1\u4e2d\u5e38\u89c1\u7684\"\u6c1b\u56f4\u7f16\u7801\"\u5de5\u4f5c\u6d41\u534f\u8c03\uff0c\u9700\u8981\u4eba\u5de5\u5e72\u9884\u4f1a\u7834\u574f\u5b89\u5168\u6784\u9020\u4fdd\u8bc1\u3002", "method": "\u63d0\u51fa\u5728\u4ee3\u7801\u751f\u6210\u9636\u6bb5\u901a\u8fc7\u7ea6\u675f\u89e3\u7801\u5f3a\u5236\u6267\u884c\u5b89\u5168\u7ea6\u675f\uff0c\u7279\u522b\u9488\u5bf9\u6269\u6563\u5f0f\u4ee3\u7801\u6a21\u578b\uff0c\u5229\u7528\u5176\u6a21\u5757\u5316\u3001\u5206\u5c42\u7279\u6027\u5b9e\u73b0\u5b89\u5168\u6784\u9020\u4ee3\u7801\u7684\u751f\u6210", "result": "\u8be5\u65b9\u6cd5\u4e3aAI\u751f\u6210\u4ee3\u7801\u63d0\u4f9b\u4e86\u66f4\u5f3a\u7684\u5b89\u5168\u4fdd\u8bc1\uff0c\u80fd\u591f\u66f4\u6709\u6548\u5730\u5904\u7406\u5b89\u5168\u6f0f\u6d1e\u7684\u957f\u5c3e\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u6301\u8f83\u4f4e\u7684\u751f\u6210\u5ef6\u8fdf", "conclusion": "\u5728\u4ee3\u7801\u751f\u6210\u9636\u6bb5\u76f4\u63a5\u5f3a\u5236\u6267\u884c\u5b89\u5168\u7ea6\u675f\u6bd4\u540e\u9a8c\u68c0\u6d4b\u4fee\u590d\u65b9\u6cd5\u80fd\u63d0\u4f9b\u66f4\u5f3a\u7684\u5b89\u5168\u4fdd\u969c\uff0c\u6269\u6563\u5f0f\u4ee3\u7801\u6a21\u578b\u4e3a\u6b64\u63d0\u4f9b\u4e86\u4f18\u96c5\u7684\u5b9e\u73b0\u9014\u5f84\uff0c\u6709\u671b\u5b9e\u73b0\u5b89\u5168\u6784\u9020\u4ee3\u7801\u7684\u751f\u6210"}}
{"id": "2602.07673", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.07673", "abs": "https://arxiv.org/abs/2602.07673", "authors": ["Jiangnan Fang", "Cheng-Tse Liu", "Hanieh Deilamsalehy", "Nesreen K. Ahmed", "Puneet Mathur", "Nedim Lipka", "Franck Dernoncourt", "Ryan A. Rossi"], "title": "Blind to the Human Touch: Overlap Bias in LLM-Based Summary Evaluation", "comment": null, "summary": "Large language model (LLM) judges have often been used alongside traditional, algorithm-based metrics for tasks like summarization because they better capture semantic information, are better at reasoning, and are more robust to paraphrasing. However, LLM judges show biases for length and order among others, and are vulnerable to various adversarial input prompts. While recent studies have looked into these biases, few have analyzed them at a more granular level in relation to a well-defined overlap metric. In this work we provide an LLM judge bias analysis as a function of overlap with human-written responses in the domain of summarization. We test 9 recent LLMs with parameter counts ranging from 1 billion to 12 billion, including variants of Gemma 3 and LLaMA 3. We find that LLM judges increasingly prefer summaries generated by other LLMs over those written by humans as the similarities (as measured by ROUGE and BLEU) between the judged summaries decrease, and this pattern extends to all but one model tested, and exists regardless of the models' own position biases. Additionally, we find that models struggle to judge even summaries with limited overlaps, suggesting that LLM-as-a-judge in the summary domain should rely on techniques beyond a simple comparison.", "AI": {"tldr": "LLM\u8bc4\u4f30\u5668\u5728\u6458\u8981\u4efb\u52a1\u4e2d\u5b58\u5728\u504f\u89c1\uff1a\u968f\u7740\u4e0e\u4eba\u5de5\u6458\u8981\u76f8\u4f3c\u5ea6\u964d\u4f4e\uff0cLLM\u8bc4\u4f30\u5668\u8d8a\u6765\u8d8a\u504f\u597d\u5176\u4ed6LLM\u751f\u6210\u7684\u6458\u8981\u800c\u975e\u4eba\u5de5\u6458\u8981\uff0c\u4e14\u51e0\u4e4e\u6240\u6709\u6d4b\u8bd5\u6a21\u578b\u90fd\u5b58\u5728\u6b64\u6a21\u5f0f\u3002", "motivation": "\u867d\u7136LLM\u8bc4\u4f30\u5668\u5728\u6458\u8981\u4efb\u52a1\u4e2d\u6bd4\u4f20\u7edf\u7b97\u6cd5\u6307\u6807\u66f4\u80fd\u6355\u6349\u8bed\u4e49\u4fe1\u606f\u3001\u63a8\u7406\u80fd\u529b\u66f4\u5f3a\u4e14\u5bf9\u6539\u5199\u66f4\u9c81\u68d2\uff0c\u4f46\u5b83\u4eec\u5b58\u5728\u957f\u5ea6\u3001\u987a\u5e8f\u7b49\u504f\u89c1\uff0c\u4e14\u6613\u53d7\u5bf9\u6297\u6027\u8f93\u5165\u5f71\u54cd\u3002\u73b0\u6709\u7814\u7a76\u5f88\u5c11\u5728\u7ec6\u7c92\u5ea6\u5c42\u9762\u5206\u6790\u8fd9\u4e9b\u504f\u89c1\u4e0e\u660e\u786e\u91cd\u53e0\u5ea6\u6307\u6807\u7684\u5173\u7cfb\u3002", "method": "\u5728\u6458\u8981\u9886\u57df\uff0c\u5206\u6790LLM\u8bc4\u4f30\u5668\u504f\u89c1\u4e0e\u4eba\u5de5\u64b0\u5199\u56de\u7b54\u91cd\u53e0\u5ea6\u7684\u51fd\u6570\u5173\u7cfb\u3002\u6d4b\u8bd59\u4e2a\u8fd1\u671fLLM\uff08\u53c2\u6570\u4ece10\u4ebf\u5230120\u4ebf\uff09\uff0c\u5305\u62ecGemma 3\u548cLLaMA 3\u7684\u53d8\u4f53\uff0c\u4f7f\u7528ROUGE\u548cBLEU\u5ea6\u91cf\u76f8\u4f3c\u5ea6\u3002", "result": "\u53d1\u73b0\u968f\u7740\u88ab\u8bc4\u4f30\u6458\u8981\u4e4b\u95f4\u76f8\u4f3c\u5ea6\u964d\u4f4e\uff0cLLM\u8bc4\u4f30\u5668\u8d8a\u6765\u8d8a\u504f\u597d\u5176\u4ed6LLM\u751f\u6210\u7684\u6458\u8981\u800c\u975e\u4eba\u5de5\u6458\u8981\uff0c\u8fd9\u4e00\u6a21\u5f0f\u5b58\u5728\u4e8e\u9664\u4e00\u4e2a\u6a21\u578b\u5916\u7684\u6240\u6709\u6d4b\u8bd5\u6a21\u578b\u4e2d\uff0c\u4e14\u4e0d\u53d7\u6a21\u578b\u81ea\u8eab\u4f4d\u7f6e\u504f\u89c1\u5f71\u54cd\u3002\u6b64\u5916\uff0c\u6a21\u578b\u751a\u81f3\u96be\u4ee5\u8bc4\u4f30\u91cd\u53e0\u5ea6\u6709\u9650\u7684\u6458\u8981\u3002", "conclusion": "\u5728\u6458\u8981\u9886\u57df\u4f7f\u7528LLM\u4f5c\u4e3a\u8bc4\u4f30\u5668\u65f6\uff0c\u5e94\u4f9d\u8d56\u8d85\u8d8a\u7b80\u5355\u6bd4\u8f83\u7684\u6280\u672f\uff0c\u56e0\u4e3aLLM\u8bc4\u4f30\u5668\u5b58\u5728\u7cfb\u7edf\u6027\u504f\u89c1\uff0c\u96be\u4ee5\u51c6\u786e\u8bc4\u4f30\u4e0e\u4eba\u5de5\u6458\u8981\u76f8\u4f3c\u5ea6\u8f83\u4f4e\u7684\u6458\u8981\u3002"}}
{"id": "2602.08133", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.08133", "abs": "https://arxiv.org/abs/2602.08133", "authors": ["Mojtaba Mostafavi Ghahfarokhi", "Hamed Jahantigh", "Alireza Asadi", "Abbas Heydarnoori"], "title": "Integrating Code Metrics into Automated Documentation Generation for Computational Notebooks", "comment": null, "summary": "Effective code documentation is essential for collaboration, comprehension, and long-term software maintainability, yet developers often neglect it due to its repetitive nature. Automated documentation generation has evolved from heuristic and rule-based methods to neural network-based and large language model (LLM)-based approaches. However, existing methods often overlook structural and quantitative characteristics of code that influence readability and comprehension. Prior research suggests that code metrics capture information relevant to program understanding. Building on these insights, this paper investigates the role of source code metrics as auxiliary signals for automated documentation generation, focusing on computational notebooks, a popular medium among data scientists that integrates code, narrative, and results but suffers from inconsistent documentation. We propose a two-stage approach. First, the CodeSearchNet dataset construction process was refined to create a specialized dataset from over 17 million code and markdown cells. After structural and semantic filtering, approximately 36,734 high-quality (code, markdown) pairs were extracted. Second, two modeling paradigms, a lightweight CNN-RNN architecture and a few-shot GPT-3.5 architecture, were evaluated with and without metric information. Results show that incorporating code metrics improves the accuracy and contextual relevance of generated documentation, yielding gains of 6% in BLEU-1 and 3% in ROUGE-L F1 for CNN-RNN-based architecture, and 9% in BERTScore F1 for LLM-based architecture. These findings demonstrate that integrating code metrics provides valuable structural context, enhancing automated documentation generation across diverse model families.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u5c06\u6e90\u4ee3\u7801\u5ea6\u91cf\u4f5c\u4e3a\u8f85\u52a9\u4fe1\u53f7\uff0c\u7528\u4e8e\u81ea\u52a8\u751f\u6210\u8ba1\u7b97\u7b14\u8bb0\u672c\u7684\u6587\u6863\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u65b9\u6cd5\uff08\u6570\u636e\u96c6\u6784\u5efa\u548c\u6a21\u578b\u8bc4\u4f30\uff09\u8bc1\u660e\u4ee3\u7801\u5ea6\u91cf\u80fd\u63d0\u5347\u6587\u6863\u751f\u6210\u7684\u51c6\u786e\u6027\u548c\u4e0a\u4e0b\u6587\u76f8\u5173\u6027\u3002", "motivation": "\u81ea\u52a8\u6587\u6863\u751f\u6210\u65b9\u6cd5\u901a\u5e38\u5ffd\u89c6\u4ee3\u7801\u7684\u7ed3\u6784\u548c\u91cf\u5316\u7279\u5f81\uff0c\u800c\u8fd9\u4e9b\u7279\u5f81\u5bf9\u53ef\u8bfb\u6027\u548c\u7406\u89e3\u5f88\u91cd\u8981\u3002\u8ba1\u7b97\u7b14\u8bb0\u672c\u4f5c\u4e3a\u6570\u636e\u79d1\u5b66\u5bb6\u7684\u5e38\u7528\u5de5\u5177\uff0c\u96c6\u6210\u4e86\u4ee3\u7801\u3001\u53d9\u8ff0\u548c\u7ed3\u679c\uff0c\u4f46\u5b58\u5728\u6587\u6863\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\u3002\u4ee3\u7801\u5ea6\u91cf\u53ef\u80fd\u5305\u542b\u4e0e\u7a0b\u5e8f\u7406\u89e3\u76f8\u5173\u7684\u4fe1\u606f\uff0c\u53ef\u4f5c\u4e3a\u6587\u6863\u751f\u6210\u7684\u8f85\u52a9\u4fe1\u53f7\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u65b9\u6cd5\uff1a1\uff09\u6539\u8fdbCodeSearchNet\u6570\u636e\u96c6\u6784\u5efa\u6d41\u7a0b\uff0c\u4ece1700\u591a\u4e07\u4e2a\u4ee3\u7801\u548cMarkdown\u5355\u5143\u683c\u4e2d\u521b\u5efa\u4e13\u95e8\u6570\u636e\u96c6\uff0c\u7ecf\u8fc7\u7ed3\u6784\u548c\u8bed\u4e49\u8fc7\u6ee4\u540e\u63d0\u53d6\u7ea636,734\u4e2a\u9ad8\u8d28\u91cf\uff08\u4ee3\u7801\uff0cMarkdown\uff09\u5bf9\uff1b2\uff09\u8bc4\u4f30\u4e24\u79cd\u5efa\u6a21\u8303\u5f0f\uff1a\u8f7b\u91cf\u7ea7CNN-RNN\u67b6\u6784\u548c\u5c11\u6837\u672cGPT-3.5\u67b6\u6784\uff0c\u5206\u522b\u5728\u6709/\u65e0\u4ee3\u7801\u5ea6\u91cf\u4fe1\u606f\u7684\u60c5\u51b5\u4e0b\u8fdb\u884c\u5bf9\u6bd4\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u52a0\u5165\u4ee3\u7801\u5ea6\u91cf\u4fe1\u606f\u80fd\u663e\u8457\u63d0\u5347\u751f\u6210\u6587\u6863\u7684\u8d28\u91cf\uff1aCNN-RNN\u67b6\u6784\u5728BLEU-1\u4e0a\u63d0\u53476%\uff0cROUGE-L F1\u4e0a\u63d0\u53473%\uff1bLLM\u67b6\u6784\u5728BERTScore F1\u4e0a\u63d0\u53479%\u3002\u8fd9\u8bc1\u660e\u4ee3\u7801\u5ea6\u91cf\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u7ed3\u6784\u4e0a\u4e0b\u6587\uff0c\u80fd\u589e\u5f3a\u4e0d\u540c\u6a21\u578b\u5bb6\u65cf\u7684\u81ea\u52a8\u6587\u6863\u751f\u6210\u80fd\u529b\u3002", "conclusion": "\u6e90\u4ee3\u7801\u5ea6\u91cf\u4f5c\u4e3a\u8f85\u52a9\u4fe1\u53f7\u80fd\u6709\u6548\u63d0\u5347\u81ea\u52a8\u6587\u6863\u751f\u6210\u7684\u51c6\u786e\u6027\u548c\u4e0a\u4e0b\u6587\u76f8\u5173\u6027\uff0c\u4e3a\u8ba1\u7b97\u7b14\u8bb0\u672c\u7b49\u590d\u6742\u4ee3\u7801\u6587\u6863\u7684\u81ea\u52a8\u5316\u751f\u6210\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u8868\u660e\u7ed3\u6784\u7279\u5f81\u5728\u6587\u6863\u751f\u6210\u4efb\u52a1\u4e2d\u5177\u6709\u91cd\u8981\u4ef7\u503c\u3002"}}
{"id": "2602.08668", "categories": ["cs.CR", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.08668", "abs": "https://arxiv.org/abs/2602.08668", "authors": ["Scott Thornton"], "title": "Retrieval Pivot Attacks in Hybrid RAG: Measuring and Mitigating Amplified Leakage from Vector Seeds to Graph Expansion", "comment": "18 pages, 5 figures", "summary": "Hybrid Retrieval-Augmented Generation (RAG) pipelines combine vector similarity search with knowledge graph expansion for multi-hop reasoning. We show that this composition introduces a distinct security failure mode: a vector-retrieved \"seed\" chunk can pivot via entity links into sensitive graph neighborhoods, causing cross-tenant data leakage that does not occur in vector-only retrieval. We formalize this risk as Retrieval Pivot Risk (RPR) and introduce companion metrics Leakage@k, Amplification Factor, and Pivot Depth (PD) to quantify leakage magnitude and traversal structure.\n  We present seven Retrieval Pivot Attacks that exploit the vector-to-graph boundary and show that adversarial injection is not required: naturally shared entities create cross-tenant pivot paths organically. Across a synthetic multi-tenant enterprise corpus and the Enron email corpus, the undefended hybrid pipeline exhibits high pivot risk (RPR up to 0.95) with multiple unauthorized items returned per query. Leakage consistently appears at PD=2, which we attribute to the bipartite chunk-entity topology and formalize as a proposition.\n  We then show that enforcing authorization at a single location, the graph expansion boundary, eliminates measured leakage (RPR near 0) across both corpora, all attack variants, and label forgery rates up to 10 percent, with minimal overhead. Our results indicate the root cause is boundary enforcement, not inherently complex defenses: two individually secure retrieval components can compose into an insecure system unless authorization is re-checked at the transition point.", "AI": {"tldr": "\u6df7\u5408\u68c0\u7d22\u589e\u5f3a\u751f\u6210(RAG)\u7ba1\u9053\u7ed3\u5408\u5411\u91cf\u76f8\u4f3c\u6027\u641c\u7d22\u548c\u77e5\u8bc6\u56fe\u8c31\u6269\u5c55\u8fdb\u884c\u591a\u8df3\u63a8\u7406\uff0c\u4f46\u5b58\u5728\u5b89\u5168\u98ce\u9669\uff1a\u5411\u91cf\u68c0\u7d22\u7684\"\u79cd\u5b50\"\u5757\u53ef\u901a\u8fc7\u5b9e\u4f53\u94fe\u63a5\u8bbf\u95ee\u654f\u611f\u56fe\u8c31\u90bb\u57df\uff0c\u5bfc\u81f4\u8de8\u79df\u6237\u6570\u636e\u6cc4\u9732\u3002", "motivation": "\u6df7\u5408RAG\u7ba1\u9053\u7ed3\u5408\u5411\u91cf\u641c\u7d22\u548c\u77e5\u8bc6\u56fe\u8c31\u6269\u5c55\uff0c\u4f46\u8fd9\u79cd\u7ec4\u5408\u5f15\u5165\u4e86\u65b0\u7684\u5b89\u5168\u6f0f\u6d1e\u6a21\u5f0f\uff1a\u5411\u91cf\u68c0\u7d22\u7684\"\u79cd\u5b50\"\u5757\u53ef\u901a\u8fc7\u5b9e\u4f53\u94fe\u63a5\u8bbf\u95ee\u654f\u611f\u56fe\u8c31\u90bb\u57df\uff0c\u5bfc\u81f4\u8de8\u79df\u6237\u6570\u636e\u6cc4\u9732\uff0c\u8fd9\u5728\u7eaf\u5411\u91cf\u68c0\u7d22\u4e2d\u4e0d\u4f1a\u53d1\u751f\u3002", "method": "1. \u5f62\u5f0f\u5316\u5b9a\u4e49\u68c0\u7d22\u67a2\u8f74\u98ce\u9669(RPR)\uff1b2. \u5f15\u5165Leakage@k\u3001\u653e\u5927\u56e0\u5b50\u548c\u67a2\u8f74\u6df1\u5ea6(PD)\u7b49\u6307\u6807\u91cf\u5316\u6cc4\u9732\u7a0b\u5ea6\uff1b3. \u63d0\u51fa\u4e03\u79cd\u68c0\u7d22\u67a2\u8f74\u653b\u51fb\u65b9\u6cd5\uff1b4. \u5728\u5408\u6210\u591a\u79df\u6237\u4f01\u4e1a\u8bed\u6599\u5e93\u548cEnron\u90ae\u4ef6\u8bed\u6599\u5e93\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff1b5. \u5728\u56fe\u8c31\u6269\u5c55\u8fb9\u754c\u5b9e\u65bd\u6388\u6743\u63a7\u5236\u3002", "result": "1. \u672a\u9632\u5fa1\u7684\u6df7\u5408\u7ba1\u9053\u67a2\u8f74\u98ce\u9669\u9ad8\u8fbe0.95\uff0c\u6bcf\u4e2a\u67e5\u8be2\u8fd4\u56de\u591a\u4e2a\u672a\u6388\u6743\u9879\uff1b2. \u6cc4\u9732\u901a\u5e38\u51fa\u73b0\u5728PD=2\u5904\uff0c\u8fd9\u4e0e\u4e8c\u5206\u5757-\u5b9e\u4f53\u62d3\u6251\u7ed3\u6784\u6709\u5173\uff1b3. \u5728\u56fe\u8c31\u6269\u5c55\u8fb9\u754c\u5b9e\u65bd\u6388\u6743\u540e\uff0cRPR\u63a5\u8fd10\uff0c\u6cc4\u9732\u88ab\u6d88\u9664\uff1b4. \u5373\u4f7f\u6807\u7b7e\u4f2a\u9020\u7387\u9ad8\u8fbe10%\uff0c\u9632\u5fa1\u4ecd\u6709\u6548\uff0c\u4e14\u5f00\u9500\u6700\u5c0f\u3002", "conclusion": "\u6df7\u5408RAG\u7ba1\u9053\u7684\u5b89\u5168\u6f0f\u6d1e\u6839\u6e90\u5728\u4e8e\u8fb9\u754c\u6388\u6743\u7f3a\u5931\uff0c\u800c\u975e\u7ec4\u4ef6\u672c\u8eab\u4e0d\u5b89\u5168\u3002\u4e24\u4e2a\u5355\u72ec\u5b89\u5168\u7684\u68c0\u7d22\u7ec4\u4ef6\u7ec4\u5408\u540e\u53ef\u80fd\u5f62\u6210\u4e0d\u5b89\u5168\u7cfb\u7edf\uff0c\u5fc5\u987b\u5728\u8fc7\u6e21\u70b9\u91cd\u65b0\u68c0\u67e5\u6388\u6743\u3002\u5728\u56fe\u8c31\u6269\u5c55\u8fb9\u754c\u5b9e\u65bd\u6388\u6743\u53ef\u6709\u6548\u6d88\u9664\u8de8\u79df\u6237\u6570\u636e\u6cc4\u9732\u98ce\u9669\u3002"}}
{"id": "2602.07559", "categories": ["cs.AI", "cs.CC", "math.NA"], "pdf": "https://arxiv.org/pdf/2602.07559", "abs": "https://arxiv.org/abs/2602.07559", "authors": ["Kaleem Ullah Qasim", "Jiashu Zhang", "Hao Li", "Muhammad Kafeel Shaheen"], "title": "VERIFY-RL: Verifiable Recursive Decomposition for Reinforcement Learning in Mathematical Reasoning", "comment": "13 pages", "summary": "Training language models to solve complex mathematical problems benefits from curriculum learning progressively training on simpler subproblems. However, existing decomposition methods are often heuristic, offering no guarantees that subproblems are simpler, that solving them aids the parent task, or that their relationships are mathematically grounded. We observe that symbolic differentiation provides a natural structure for verified decomposition: calculus rules explicitly define how expressions reduce to simpler components with provable properties. We introduce Verify-RL, a framework where every parent-child decomposition satisfies three verifiable conditions: strictly decreasing structural complexity, solution containment, and formal rule derivation. Unlike heuristic methods where a significant fraction of decompositions are invalid our properties admit automatic verification through symbolic computation, achieving \"verification by construction\" Experiments demonstrate that eliminating invalid decompositions yields sizable gains, accuracy on the hardest problems more than doubles from 32% to 68%, with a 40% relative improvement overall.", "AI": {"tldr": "Verify-RL\u6846\u67b6\u901a\u8fc7\u7b26\u53f7\u5fae\u5206\u5b9e\u73b0\u53ef\u9a8c\u8bc1\u7684\u6570\u5b66\u95ee\u9898\u5206\u89e3\uff0c\u786e\u4fdd\u5b50\u95ee\u9898\u66f4\u7b80\u5355\u3001\u89e3\u51b3\u5b50\u95ee\u9898\u6709\u52a9\u4e8e\u7236\u4efb\u52a1\u3001\u4e14\u5206\u89e3\u5173\u7cfb\u6709\u6570\u5b66\u57fa\u7840\uff0c\u76f8\u6bd4\u542f\u53d1\u5f0f\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd", "motivation": "\u73b0\u6709\u6570\u5b66\u95ee\u9898\u5206\u89e3\u65b9\u6cd5\u901a\u5e38\u662f\u542f\u53d1\u5f0f\u7684\uff0c\u65e0\u6cd5\u4fdd\u8bc1\u5b50\u95ee\u9898\u66f4\u7b80\u5355\u3001\u89e3\u51b3\u5b50\u95ee\u9898\u6709\u52a9\u4e8e\u7236\u4efb\u52a1\u3001\u4e14\u5206\u89e3\u5173\u7cfb\u6709\u6570\u5b66\u57fa\u7840\uff0c\u5bfc\u81f4\u5927\u91cf\u65e0\u6548\u5206\u89e3", "method": "\u5229\u7528\u7b26\u53f7\u5fae\u5206\u4f5c\u4e3a\u53ef\u9a8c\u8bc1\u5206\u89e3\u7684\u81ea\u7136\u7ed3\u6784\uff1a\u5fae\u79ef\u5206\u89c4\u5219\u660e\u786e\u5b9a\u4e49\u8868\u8fbe\u5f0f\u5982\u4f55\u5206\u89e3\u4e3a\u66f4\u7b80\u5355\u7684\u7ec4\u4ef6\uff0c\u5e76\u5177\u6709\u53ef\u8bc1\u660e\u7684\u6027\u8d28\u3002\u63d0\u51faVerify-RL\u6846\u67b6\uff0c\u786e\u4fdd\u6bcf\u4e2a\u7236\u5b50\u5206\u89e3\u6ee1\u8db3\u4e09\u4e2a\u53ef\u9a8c\u8bc1\u6761\u4ef6\uff1a\u7ed3\u6784\u590d\u6742\u5ea6\u4e25\u683c\u9012\u51cf\u3001\u89e3\u5305\u542b\u6027\u3001\u5f62\u5f0f\u89c4\u5219\u63a8\u5bfc", "result": "\u6d88\u9664\u65e0\u6548\u5206\u89e3\u5e26\u6765\u663e\u8457\u6027\u80fd\u63d0\u5347\uff1a\u6700\u96be\u95ee\u9898\u7684\u51c6\u786e\u7387\u4ece32%\u7ffb\u500d\u81f368%\uff0c\u6574\u4f53\u76f8\u5bf9\u6539\u8fdb\u8fbe40%\u3002\u53ef\u9a8c\u8bc1\u6761\u4ef6\u901a\u8fc7\u7b26\u53f7\u8ba1\u7b97\u5b9e\u73b0\u81ea\u52a8\u9a8c\u8bc1\uff0c\u8fbe\u5230\"\u6784\u9020\u5373\u9a8c\u8bc1\"\u7684\u6548\u679c", "conclusion": "\u57fa\u4e8e\u7b26\u53f7\u5fae\u5206\u7684\u53ef\u9a8c\u8bc1\u5206\u89e3\u6846\u67b6\u4e3a\u6570\u5b66\u95ee\u9898\u6c42\u89e3\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u4fdd\u8bc1\uff0c\u76f8\u6bd4\u542f\u53d1\u5f0f\u65b9\u6cd5\u80fd\u663e\u8457\u63d0\u5347\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u6570\u5b66\u95ee\u9898\u4e0a\u7684\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u5f62\u5f0f\u5316\u9a8c\u8bc1\u5728\u8bfe\u7a0b\u5b66\u4e60\u4e2d\u7684\u91cd\u8981\u6027"}}
{"id": "2602.08741", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2602.08741", "abs": "https://arxiv.org/abs/2602.08741", "authors": ["Jona te Lintelo", "Lichao Wu", "Stjepan Picek"], "title": "Large Language Lobotomy: Jailbreaking Mixture-of-Experts via Expert Silencing", "comment": null, "summary": "The rapid adoption of Mixture-of-Experts (MoE) architectures marks a major shift in the deployment of Large Language Models (LLMs). MoE LLMs improve scaling efficiency by activating only a small subset of parameters per token, but their routing structure introduces new safety attack surfaces. We find that safety-critical behaviors in MoE LLMs (e.g., refusal) are concentrated in a small set of experts rather than being uniformly distributed. Building on this, we propose Large Language Lobotomy (L$^3$), a training-free, architecture-agnostic attack that compromises safety alignment by exploiting expert routing dynamics. L$^3$ learns routing patterns that correlate with refusal, attributes safety behavior to specific experts, and adaptively silences the most safety-relevant experts until harmful outputs are produced. We evaluate L$^3$ on eight state-of-the-art open-source MoE LLMs and show that our adaptive expert silencing increases average attack success from 7.3% to 70.4%, reaching up to 86.3%, outperforming prior training-free MoE jailbreak methods. Moreover, bypassing guardrails typically requires silencing fewer than 20% of layer-wise experts while largely preserving general language utility. These results reveal a fundamental tension between efficiency-driven MoE design and robust safety alignment and motivate distributing safety mechanisms more robustly in future MoE LLMs with architecture- and routing-aware methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aLarge Language Lobotomy (L\u00b3)\u7684\u8bad\u7ec3\u514d\u8d39\u3001\u67b6\u6784\u65e0\u5173\u7684\u653b\u51fb\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528MoE LLMs\u4e2d\u7684\u4e13\u5bb6\u8def\u7531\u52a8\u6001\u6765\u7834\u574f\u5b89\u5168\u5bf9\u9f50\uff0c\u80fd\u591f\u5c06\u653b\u51fb\u6210\u529f\u7387\u4ece7.3%\u63d0\u5347\u81f370.4%\u3002", "motivation": "MoE\u67b6\u6784\u867d\u7136\u63d0\u9ad8\u4e86LLMs\u7684\u6269\u5c55\u6548\u7387\uff0c\u4f46\u5176\u8def\u7531\u7ed3\u6784\u5f15\u5165\u4e86\u65b0\u7684\u5b89\u5168\u653b\u51fb\u9762\u3002\u7814\u7a76\u53d1\u73b0MoE LLMs\u4e2d\u7684\u5b89\u5168\u5173\u952e\u884c\u4e3a\uff08\u5982\u62d2\u7edd\u56de\u7b54\uff09\u96c6\u4e2d\u5728\u5c11\u6570\u4e13\u5bb6\u4e2d\u800c\u975e\u5747\u5300\u5206\u5e03\uff0c\u8fd9\u4e3a\u653b\u51fb\u63d0\u4f9b\u4e86\u673a\u4f1a\u3002", "method": "\u63d0\u51faL\u00b3\u653b\u51fb\u65b9\u6cd5\uff1a1\uff09\u5b66\u4e60\u4e0e\u62d2\u7edd\u884c\u4e3a\u76f8\u5173\u7684\u8def\u7531\u6a21\u5f0f\uff1b2\uff09\u5c06\u5b89\u5168\u884c\u4e3a\u5f52\u56e0\u4e8e\u7279\u5b9a\u4e13\u5bb6\uff1b3\uff09\u81ea\u9002\u5e94\u5730\u6c89\u9ed8\u6700\u76f8\u5173\u7684\u5b89\u5168\u4e13\u5bb6\u76f4\u5230\u4ea7\u751f\u6709\u5bb3\u8f93\u51fa\u3002\u8be5\u65b9\u6cd5\u65e0\u9700\u8bad\u7ec3\u4e14\u67b6\u6784\u65e0\u5173\u3002", "result": "\u57288\u4e2a\u6700\u5148\u8fdb\u7684\u5f00\u6e90MoE LLMs\u4e0a\u8bc4\u4f30\uff0c\u81ea\u9002\u5e94\u4e13\u5bb6\u6c89\u9ed8\u5c06\u5e73\u5747\u653b\u51fb\u6210\u529f\u7387\u4ece7.3%\u63d0\u5347\u81f370.4%\uff0c\u6700\u9ad8\u8fbe86.3%\uff0c\u4f18\u4e8e\u5148\u524d\u7684\u8bad\u7ec3\u514d\u8d39MoE\u8d8a\u72f1\u65b9\u6cd5\u3002\u7ed5\u8fc7\u9632\u62a4\u901a\u5e38\u53ea\u9700\u6c89\u9ed8\u5c11\u4e8e20%\u7684\u5c42\u7ea7\u4e13\u5bb6\uff0c\u540c\u65f6\u57fa\u672c\u4fdd\u7559\u8bed\u8a00\u901a\u7528\u80fd\u529b\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86MoE\u8bbe\u8ba1\u4e2d\u6548\u7387\u9a71\u52a8\u4e0e\u9c81\u68d2\u5b89\u5168\u5bf9\u9f50\u4e4b\u95f4\u7684\u6839\u672c\u77db\u76fe\uff0c\u5e76\u5efa\u8bae\u672a\u6765MoE LLMs\u5e94\u91c7\u7528\u67b6\u6784\u548c\u8def\u7531\u611f\u77e5\u7684\u65b9\u6cd5\u66f4\u9c81\u68d2\u5730\u5206\u5e03\u5b89\u5168\u673a\u5236\u3002"}}
{"id": "2602.07778", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.07778", "abs": "https://arxiv.org/abs/2602.07778", "authors": ["Shenglai Zeng", "Tianqi Zheng", "Chuan Tian", "Dante Everaert", "Yau-Shian Wang", "Yupin Huang", "Michael J. Morais", "Rohit Patki", "Jinjin Tian", "Xinnan Dai", "Kai Guo", "Monica Xiao Cheng", "Hui Liu"], "title": "Attn-GS: Attention-Guided Context Compression for Efficient Personalized LLMs", "comment": null, "summary": "Personalizing large language models (LLMs) to individual users requires incorporating extensive interaction histories and profiles, but input token constraints make this impractical due to high inference latency and API costs. Existing approaches rely on heuristic methods such as selecting recent interactions or prompting summarization models to compress user profiles. However, these methods treat context as a monolithic whole and fail to consider how LLMs internally process and prioritize different profile components. We investigate whether LLMs' attention patterns can effectively identify important personalization signals for intelligent context compression. Through preliminary studies on representative personalization tasks, we discover that (a) LLMs' attention patterns naturally reveal important signals, and (b) fine-tuning enhances LLMs' ability to distinguish between relevant and irrelevant information. Based on these insights, we propose Attn-GS, an attention-guided context compression framework that leverages attention feedback from a marking model to mark important personalization sentences, then guides a compression model to generate task-relevant, high-quality compressed user contexts. Extensive experiments demonstrate that Attn-GS significantly outperforms various baselines across different tasks, token limits, and settings, achieving performance close to using full context while reducing token usage by 50 times.", "AI": {"tldr": "Attn-GS\uff1a\u57fa\u4e8e\u6ce8\u610f\u529b\u5f15\u5bfc\u7684\u4e0a\u4e0b\u6587\u538b\u7f29\u6846\u67b6\uff0c\u5229\u7528LLM\u6ce8\u610f\u529b\u6a21\u5f0f\u8bc6\u522b\u91cd\u8981\u4e2a\u6027\u5316\u4fe1\u53f7\uff0c\u5728\u51cf\u5c1150\u500dtoken\u4f7f\u7528\u7684\u540c\u65f6\u4fdd\u6301\u63a5\u8fd1\u5b8c\u6574\u4e0a\u4e0b\u6587\u7684\u6027\u80fd", "motivation": "\u4e2a\u6027\u5316\u5927\u578b\u8bed\u8a00\u6a21\u578b\u9700\u8981\u6574\u5408\u5927\u91cf\u7528\u6237\u4ea4\u4e92\u5386\u53f2\u548c\u8d44\u6599\uff0c\u4f46\u8f93\u5165token\u9650\u5236\u5bfc\u81f4\u63a8\u7406\u5ef6\u8fdf\u9ad8\u3001API\u6210\u672c\u9ad8\u3002\u73b0\u6709\u542f\u53d1\u5f0f\u65b9\u6cd5\uff08\u5982\u9009\u62e9\u6700\u8fd1\u4ea4\u4e92\u6216\u4f7f\u7528\u6458\u8981\u6a21\u578b\uff09\u5c06\u4e0a\u4e0b\u6587\u89c6\u4e3a\u6574\u4f53\uff0c\u672a\u80fd\u8003\u8651LLM\u5185\u90e8\u5982\u4f55\u5904\u7406\u548c\u4f18\u5148\u5904\u7406\u4e0d\u540c\u8d44\u6599\u7ec4\u4ef6", "method": "\u63d0\u51faAttn-GS\u6ce8\u610f\u529b\u5f15\u5bfc\u4e0a\u4e0b\u6587\u538b\u7f29\u6846\u67b6\uff1a1\uff09\u901a\u8fc7\u521d\u6b65\u7814\u7a76\u53d1\u73b0LLM\u6ce8\u610f\u529b\u6a21\u5f0f\u80fd\u81ea\u7136\u63ed\u793a\u91cd\u8981\u4fe1\u53f7\uff0c\u5fae\u8c03\u80fd\u589e\u5f3a\u533a\u5206\u76f8\u5173/\u65e0\u5173\u4fe1\u606f\u7684\u80fd\u529b\uff1b2\uff09\u5229\u7528\u6807\u8bb0\u6a21\u578b\u7684\u6ce8\u610f\u529b\u53cd\u9988\u6807\u8bb0\u91cd\u8981\u4e2a\u6027\u5316\u53e5\u5b50\uff1b3\uff09\u6307\u5bfc\u538b\u7f29\u6a21\u578b\u751f\u6210\u4efb\u52a1\u76f8\u5173\u3001\u9ad8\u8d28\u91cf\u7684\u538b\u7f29\u7528\u6237\u4e0a\u4e0b\u6587", "result": "\u5728\u4e0d\u540c\u4efb\u52a1\u3001token\u9650\u5236\u548c\u8bbe\u7f6e\u4e0b\uff0cAttn-GS\u663e\u8457\u4f18\u4e8e\u5404\u79cd\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5728\u51cf\u5c1150\u500dtoken\u4f7f\u7528\u7684\u540c\u65f6\uff0c\u6027\u80fd\u63a5\u8fd1\u4f7f\u7528\u5b8c\u6574\u4e0a\u4e0b\u6587", "conclusion": "LLM\u7684\u6ce8\u610f\u529b\u6a21\u5f0f\u53ef\u4ee5\u6709\u6548\u8bc6\u522b\u91cd\u8981\u7684\u4e2a\u6027\u5316\u4fe1\u53f7\uff0c\u7528\u4e8e\u667a\u80fd\u4e0a\u4e0b\u6587\u538b\u7f29\u3002Attn-GS\u6846\u67b6\u901a\u8fc7\u6ce8\u610f\u529b\u5f15\u5bfc\u7684\u538b\u7f29\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u5927\u5e45\u964d\u4f4etoken\u4f7f\u7528\uff0c\u4e3a\u4e2a\u6027\u5316LLM\u7684\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u89e3\u51b3\u65b9\u6848"}}
{"id": "2602.07069", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07069", "abs": "https://arxiv.org/abs/2602.07069", "authors": ["Zihao Fan", "Xin Lu", "Yidi Liu", "Jie Huang", "Dong Li", "Xueyang Fu", "Zheng-Jun Zha"], "title": "Bidirectional Reward-Guided Diffusion for Real-World Image Super-Resolution", "comment": null, "summary": "Diffusion-based super-resolution can synthesize rich details, but models trained on synthetic paired data often fail on real-world LR images due to distribution shifts. We propose Bird-SR, a bidirectional reward-guided diffusion framework that formulates super-resolution as trajectory-level preference optimization via reward feedback learning (ReFL), jointly leveraging synthetic LR-HR pairs and real-world LR images. For structural fidelity easily affected in ReFL, the model is directly optimized on synthetic pairs at early diffusion steps, which also facilitates structure preservation for real-world inputs under smaller distribution gap in structure levels. For perceptual enhancement, quality-guided rewards are applied at later sampling steps to both synthetic and real LR images. To mitigate reward hacking, the rewards for synthetic results are formulated in a relative advantage space bounded by their clean counterparts, while real-world optimization is regularized via a semantic alignment constraint. Furthermore, to balance structural and perceptual learning, we adopt a dynamic fidelity-perception weighting strategy that emphasizes structure preservation at early stages and progressively shifts focus toward perceptual optimization at later diffusion steps. Extensive experiments on real-world SR benchmarks demonstrate that Bird-SR consistently outperforms state-of-the-art methods in perceptual quality while preserving structural consistency, validating its effectiveness for real-world super-resolution.", "AI": {"tldr": "Bird-SR\u662f\u4e00\u4e2a\u53cc\u5411\u5956\u52b1\u5f15\u5bfc\u7684\u6269\u6563\u6846\u67b6\uff0c\u901a\u8fc7\u5956\u52b1\u53cd\u9988\u5b66\u4e60\u5c06\u8d85\u5206\u8fa8\u7387\u5efa\u6a21\u4e3a\u8f68\u8ff9\u7ea7\u504f\u597d\u4f18\u5316\uff0c\u8054\u5408\u5229\u7528\u5408\u6210LR-HR\u5bf9\u548c\u771f\u5b9e\u4e16\u754cLR\u56fe\u50cf\uff0c\u5728\u4fdd\u6301\u7ed3\u6784\u4e00\u81f4\u6027\u7684\u540c\u65f6\u63d0\u5347\u611f\u77e5\u8d28\u91cf\u3002", "motivation": "\u57fa\u4e8e\u6269\u6563\u7684\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\u867d\u7136\u80fd\u5408\u6210\u4e30\u5bcc\u7ec6\u8282\uff0c\u4f46\u5728\u5408\u6210\u6570\u636e\u4e0a\u8bad\u7ec3\u7684\u6a21\u578b\u5f80\u5f80\u56e0\u5206\u5e03\u504f\u79fb\u800c\u5728\u771f\u5b9e\u4e16\u754cLR\u56fe\u50cf\u4e0a\u8868\u73b0\u4e0d\u4f73\u3002\u9700\u8981\u4e00\u79cd\u80fd\u540c\u65f6\u5229\u7528\u5408\u6210\u6570\u636e\u548c\u771f\u5b9e\u4e16\u754c\u56fe\u50cf\uff0c\u5728\u4fdd\u6301\u7ed3\u6784\u4fdd\u771f\u5ea6\u7684\u540c\u65f6\u63d0\u5347\u611f\u77e5\u8d28\u91cf\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faBird-SR\u6846\u67b6\uff1a1) \u5728\u65e9\u671f\u6269\u6563\u6b65\u9aa4\u76f4\u63a5\u5728\u5408\u6210\u5bf9\u4e0a\u4f18\u5316\u4ee5\u4fdd\u8bc1\u7ed3\u6784\u4fdd\u771f\u5ea6\uff1b2) \u5728\u540e\u671f\u91c7\u6837\u6b65\u9aa4\u5bf9\u5408\u6210\u548c\u771f\u5b9eLR\u56fe\u50cf\u5e94\u7528\u8d28\u91cf\u5f15\u5bfc\u5956\u52b1\uff1b3) \u901a\u8fc7\u76f8\u5bf9\u4f18\u52bf\u7a7a\u95f4\u548c\u8bed\u4e49\u5bf9\u9f50\u7ea6\u675f\u7f13\u89e3\u5956\u52b1\u653b\u51fb\uff1b4) \u91c7\u7528\u52a8\u6001\u4fdd\u771f\u5ea6-\u611f\u77e5\u6743\u91cd\u7b56\u7565\u5e73\u8861\u7ed3\u6784\u4fdd\u6301\u548c\u611f\u77e5\u4f18\u5316\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u8d85\u5206\u8fa8\u7387\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cBird-SR\u5728\u611f\u77e5\u8d28\u91cf\u65b9\u9762\u6301\u7eed\u4f18\u4e8e\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u7ed3\u6784\u4e00\u81f4\u6027\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u771f\u5b9e\u4e16\u754c\u8d85\u5206\u8fa8\u7387\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "Bird-SR\u901a\u8fc7\u53cc\u5411\u5956\u52b1\u5f15\u5bfc\u7684\u6269\u6563\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u771f\u5b9e\u4e16\u754c\u8d85\u5206\u8fa8\u7387\u4e2d\u7684\u5206\u5e03\u504f\u79fb\u95ee\u9898\uff0c\u5728\u7ed3\u6784\u4fdd\u771f\u5ea6\u548c\u611f\u77e5\u8d28\u91cf\u4e4b\u95f4\u53d6\u5f97\u4e86\u826f\u597d\u5e73\u8861\uff0c\u4e3a\u771f\u5b9e\u4e16\u754c\u56fe\u50cf\u589e\u5f3a\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.08166", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.08166", "abs": "https://arxiv.org/abs/2602.08166", "authors": ["Oscar Manglaras", "Alex Farkas", "Thomas Woolford", "Christoph Treude", "Markus Wagner"], "title": "Distributed Architecture Reconstruction of Polyglot and Multi-Repository Microservice Projects", "comment": null, "summary": "Microservice architectures encourage the use of small, independently developed services; however, this can lead to increased architectural complexity. Accurate documentation is crucial, but is challenging to maintain due to the rapid, independent evolution of services. While static architecture reconstruction provides a way to maintain up-to-date documentation, existing approaches suffer from technology limitations, mono-repo constraints, or high implementation barriers. This paper presents a novel framework for static architecture reconstruction that supports technology-specific analysis modules, called \\emph{extractors}, and supports \\emph{distributed architecture reconstruction} in multi-repo environments. We describe the core design concepts and algorithms that govern how extractors are executed, how data is passed between them, and how their outputs are unified. Furthermore, the framework is interoperable with existing static analysis tools and algorithms, allowing them to be invoked from or embedded within extractors.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u652f\u6301\u6280\u672f\u7279\u5b9a\u5206\u6790\u6a21\u5757\u548c\u591a\u4ed3\u5e93\u73af\u5883\u7684\u9759\u6001\u67b6\u6784\u91cd\u6784\u6846\u67b6\uff0c\u89e3\u51b3\u5fae\u670d\u52a1\u67b6\u6784\u6587\u6863\u7ef4\u62a4\u96be\u9898", "motivation": "\u5fae\u670d\u52a1\u67b6\u6784\u867d\u7136\u9f13\u52b1\u5c0f\u578b\u72ec\u7acb\u670d\u52a1\u5f00\u53d1\uff0c\u4f46\u589e\u52a0\u4e86\u67b6\u6784\u590d\u6742\u6027\u3002\u51c6\u786e\u6587\u6863\u5bf9\u7ef4\u62a4\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7531\u4e8e\u670d\u52a1\u5feb\u901f\u72ec\u7acb\u6f14\u5316\u800c\u96be\u4ee5\u4fdd\u6301\u66f4\u65b0\u3002\u73b0\u6709\u9759\u6001\u67b6\u6784\u91cd\u6784\u65b9\u6cd5\u5b58\u5728\u6280\u672f\u9650\u5236\u3001\u5355\u4ed3\u5e93\u7ea6\u675f\u6216\u9ad8\u5b9e\u73b0\u95e8\u69db\u7b49\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u65b0\u9896\u7684\u9759\u6001\u67b6\u6784\u91cd\u6784\u6846\u67b6\uff0c\u652f\u6301\u6280\u672f\u7279\u5b9a\u7684\u5206\u6790\u6a21\u5757\uff08\u79f0\u4e3a\u63d0\u53d6\u5668\uff09\uff0c\u5e76\u652f\u6301\u591a\u4ed3\u5e93\u73af\u5883\u4e2d\u7684\u5206\u5e03\u5f0f\u67b6\u6784\u91cd\u6784\u3002\u63cf\u8ff0\u4e86\u6838\u5fc3\u8bbe\u8ba1\u6982\u5ff5\u548c\u7b97\u6cd5\uff0c\u5305\u62ec\u63d0\u53d6\u5668\u6267\u884c\u65b9\u5f0f\u3001\u6570\u636e\u4f20\u9012\u673a\u5236\u4ee5\u53ca\u8f93\u51fa\u7edf\u4e00\u65b9\u6cd5\u3002\u6846\u67b6\u4e0e\u73b0\u6709\u9759\u6001\u5206\u6790\u5de5\u5177\u548c\u7b97\u6cd5\u4e92\u64cd\u4f5c\uff0c\u5141\u8bb8\u4ece\u63d0\u53d6\u5668\u5185\u8c03\u7528\u6216\u5d4c\u5165\u5b83\u4eec\u3002", "result": "\u8be5\u6846\u67b6\u80fd\u591f\u514b\u670d\u73b0\u6709\u65b9\u6cd5\u7684\u6280\u672f\u9650\u5236\uff0c\u652f\u6301\u591a\u4ed3\u5e93\u73af\u5883\uff0c\u964d\u4f4e\u5b9e\u73b0\u95e8\u69db\uff0c\u5e76\u63d0\u4f9b\u4e0e\u73b0\u6709\u5de5\u5177\u7684\u4e92\u64cd\u4f5c\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u9759\u6001\u67b6\u6784\u91cd\u6784\u6846\u67b6\u4e3a\u89e3\u51b3\u5fae\u670d\u52a1\u67b6\u6784\u6587\u6863\u7ef4\u62a4\u6311\u6218\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u652f\u6301\u6280\u672f\u7279\u5b9a\u5206\u6790\u6a21\u5757\u548c\u591a\u4ed3\u5e93\u73af\u5883\uff0c\u80fd\u591f\u4fdd\u6301\u67b6\u6784\u6587\u6863\u7684\u51c6\u786e\u6027\u548c\u65f6\u6548\u6027\u3002"}}
{"id": "2602.07794", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07794", "abs": "https://arxiv.org/abs/2602.07794", "authors": ["Ningyu Xu", "Qi Zhang", "Xipeng Qiu", "Xuanjing Huang"], "title": "Emergent Structured Representations Support Flexible In-Context Inference in Large Language Models", "comment": "27 pages, 16 figures", "summary": "Large language models (LLMs) exhibit emergent behaviors suggestive of human-like reasoning. While recent work has identified structured, human-like conceptual representations within these models, it remains unclear whether they functionally rely on such representations for reasoning. Here we investigate the internal processing of LLMs during in-context concept inference. Our results reveal a conceptual subspace emerging in middle to late layers, whose representational structure persists across contexts. Using causal mediation analyses, we demonstrate that this subspace is not merely an epiphenomenon but is functionally central to model predictions, establishing its causal role in inference. We further identify a layer-wise progression where attention heads in early-to-middle layers integrate contextual cues to construct and refine the subspace, which is subsequently leveraged by later layers to generate predictions. Together, these findings provide evidence that LLMs dynamically construct and use structured, latent representations in context for inference, offering insights into the computational processes underlying flexible adaptation.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4e0a\u4e0b\u6587\u6982\u5ff5\u63a8\u7406\u4e2d\u4f1a\u52a8\u6001\u6784\u5efa\u548c\u4f7f\u7528\u7ed3\u6784\u5316\u6f5c\u5728\u8868\u5f81\uff0c\u8fd9\u4e9b\u8868\u5f81\u5177\u6709\u56e0\u679c\u529f\u80fd\u4f5c\u7528\u800c\u975e\u4ec5\u4ec5\u662f\u4f34\u968f\u73b0\u8c61\u3002", "motivation": "\u867d\u7136\u5df2\u6709\u7814\u7a76\u53d1\u73b0\u5927\u8bed\u8a00\u6a21\u578b\u5185\u90e8\u5b58\u5728\u7c7b\u4f3c\u4eba\u7c7b\u7684\u7ed3\u6784\u5316\u6982\u5ff5\u8868\u5f81\uff0c\u4f46\u5c1a\u4e0d\u6e05\u695a\u8fd9\u4e9b\u8868\u5f81\u662f\u5426\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u53d1\u6325\u529f\u80fd\u6027\u4f5c\u7528\uff0c\u8fd8\u662f\u4ec5\u4ec5\u662f\u4f34\u968f\u73b0\u8c61\u3002", "method": "\u901a\u8fc7\u5206\u6790\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4e0a\u4e0b\u6587\u6982\u5ff5\u63a8\u7406\u8fc7\u7a0b\u4e2d\u7684\u5185\u90e8\u5904\u7406\uff0c\u8bc6\u522b\u6982\u5ff5\u5b50\u7a7a\u95f4\u7684\u51fa\u73b0\u4f4d\u7f6e\u548c\u7ed3\u6784\u7279\u5f81\uff0c\u5e76\u4f7f\u7528\u56e0\u679c\u4e2d\u4ecb\u5206\u6790\u9a8c\u8bc1\u5176\u529f\u80fd\u6027\u4f5c\u7528\u3002", "result": "\u53d1\u73b0\u6982\u5ff5\u5b50\u7a7a\u95f4\u5728\u4e2d\u540e\u671f\u5c42\u51fa\u73b0\uff0c\u5176\u8868\u5f81\u7ed3\u6784\u5728\u4e0d\u540c\u4e0a\u4e0b\u6587\u4e2d\u4fdd\u6301\u7a33\u5b9a\uff1b\u56e0\u679c\u5206\u6790\u8bc1\u660e\u8be5\u5b50\u7a7a\u95f4\u5bf9\u6a21\u578b\u9884\u6d4b\u5177\u6709\u56e0\u679c\u4f5c\u7528\uff1b\u89c2\u5bdf\u5230\u5c42\u95f4\u9012\u8fdb\u8fc7\u7a0b\uff1a\u65e9\u671f\u5230\u4e2d\u671f\u5c42\u7684\u6ce8\u610f\u529b\u5934\u6574\u5408\u4e0a\u4e0b\u6587\u7ebf\u7d22\u6784\u5efa\u548c\u7cbe\u70bc\u5b50\u7a7a\u95f4\uff0c\u540e\u671f\u5c42\u5229\u7528\u8be5\u5b50\u7a7a\u95f4\u751f\u6210\u9884\u6d4b\u3002", "conclusion": "\u5927\u8bed\u8a00\u6a21\u578b\u786e\u5b9e\u4f1a\u52a8\u6001\u6784\u5efa\u548c\u4f7f\u7528\u7ed3\u6784\u5316\u7684\u6f5c\u5728\u8868\u5f81\u8fdb\u884c\u4e0a\u4e0b\u6587\u63a8\u7406\uff0c\u8fd9\u4e3a\u7406\u89e3\u6a21\u578b\u7075\u6d3b\u9002\u5e94\u7684\u8ba1\u7b97\u8fc7\u7a0b\u63d0\u4f9b\u4e86\u65b0\u89c1\u89e3\u3002"}}
{"id": "2602.07082", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07082", "abs": "https://arxiv.org/abs/2602.07082", "authors": ["Haoming Wang", "Qiyao Xue", "Weichen Liu", "Wei Gao"], "title": "MosaicThinker: On-Device Visual Spatial Reasoning for Embodied AI via Iterative Construction of Space Representation", "comment": null, "summary": "When embodied AI is expanding from traditional object detection and recognition to more advanced tasks of robot manipulation and actuation planning, visual spatial reasoning from the video inputs is necessary to perceive the spatial relationships of objects and guide device actions. However, existing visual language models (VLMs) have very weak capabilities in spatial reasoning due to the lack of knowledge about 3D spatial information, especially when the reasoning task involve complex spatial relations across multiple video frames. In this paper, we present a new inference-time computing technique for on-device embodied AI, namely \\emph{MosaicThinker}, which enhances the on-device small VLM's spatial reasoning capabilities on difficult cross-frame reasoning tasks. Our basic idea is to integrate fragmented spatial information from multiple frames into a unified space representation of global semantic map, and further guide the VLM's spatial reasoning over the semantic map via a visual prompt. Experiment results show that our technique can greatly enhance the accuracy of cross-frame spatial reasoning on resource-constrained embodied AI devices, over reasoning tasks with diverse types and complexities.", "AI": {"tldr": "MosaicThinker\u662f\u4e00\u79cd\u63a8\u7406\u65f6\u8ba1\u7b97\u6280\u672f\uff0c\u901a\u8fc7\u5c06\u591a\u5e27\u89c6\u9891\u7684\u7a7a\u95f4\u4fe1\u606f\u6574\u5408\u5230\u7edf\u4e00\u7684\u8bed\u4e49\u5730\u56fe\u4e2d\uff0c\u589e\u5f3a\u5c0f\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u8de8\u5e27\u7a7a\u95f4\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u7a7a\u95f4\u63a8\u7406\u65b9\u9762\u80fd\u529b\u8f83\u5f31\uff0c\u7279\u522b\u662f\u6d89\u53ca\u591a\u5e27\u590d\u6742\u7a7a\u95f4\u5173\u7cfb\u65f6\uff0c\u8fd9\u9650\u5236\u4e86\u5177\u8eabAI\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u548c\u52a8\u4f5c\u89c4\u5212\u7b49\u9ad8\u7ea7\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u63d0\u51faMosaicThinker\u6280\u672f\uff0c\u5c06\u591a\u5e27\u89c6\u9891\u4e2d\u7684\u788e\u7247\u5316\u7a7a\u95f4\u4fe1\u606f\u6574\u5408\u5230\u7edf\u4e00\u7684\u5168\u5c40\u8bed\u4e49\u5730\u56fe\u8868\u793a\u4e2d\uff0c\u5e76\u901a\u8fc7\u89c6\u89c9\u63d0\u793a\u5f15\u5bfc\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u8bed\u4e49\u5730\u56fe\u4e0a\u8fdb\u884c\u7a7a\u95f4\u63a8\u7406\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6280\u672f\u80fd\u663e\u8457\u63d0\u9ad8\u8d44\u6e90\u53d7\u9650\u5177\u8eabAI\u8bbe\u5907\u5728\u8de8\u5e27\u7a7a\u95f4\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u51c6\u786e\u6027\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u7c7b\u578b\u548c\u590d\u6742\u5ea6\u7684\u63a8\u7406\u4efb\u52a1\u3002", "conclusion": "MosaicThinker\u901a\u8fc7\u6784\u5efa\u5168\u5c40\u8bed\u4e49\u5730\u56fe\u548c\u89c6\u89c9\u63d0\u793a\uff0c\u6709\u6548\u589e\u5f3a\u4e86\u5c0f\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\u7684\u5177\u8eabAI\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.07628", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.07628", "abs": "https://arxiv.org/abs/2602.07628", "authors": ["Keondo Park", "Younghoon Na", "Yourim Choi", "Hyunwoo Ryu", "Hyun-Woo Shin", "Hyung-Sin Kim"], "title": "SleepMaMi: A Universal Sleep Foundation Model for Integrating Macro- and Micro-structures", "comment": "8 pages, Appendix 9 pages", "summary": "While the shift toward unified foundation models has revolutionized many deep learning domains, sleep medicine remains largely restricted to task-specific models that focus on localized micro-structure features. These approaches often neglect the rich, multi-modal context of Polysomnography (PSG) and fail to capture the global macro-structure of a full night's sleep. To address this, we introduce SleepMaMi , a Sleep Foundation Model engineered to master both hour-long sleep architectures and fine-grained signal morphologies. Our framework utilizes a hierarchical dual-encoder design: a Macro-Encoder to model full-night temporal dependencies and a Micro-Encoder to capture short-term characteristics from biosignals. Macro-Encoder is trained via Demographic-Guided Contrastive Learning, which aligns overnight sleep patterns with objective subject metadata, such as age, sex and BMI to refine global representations. Micro-Encoder is optimized via a hybrid Masked Autoencoder (MAE) and multi-modal contrastive objective. Pre-trained on a massive corpus of $>$20,000 PSG recordings (158K hours),SleepMaMi outperforms existing foundation models across a diverse suite of downstream tasks, demonstrating superior generalizability and label-efficient adaptation for clinical sleep analysis.", "AI": {"tldr": "SleepMaMi\u662f\u4e00\u4e2a\u7761\u7720\u57fa\u7840\u6a21\u578b\uff0c\u901a\u8fc7\u5206\u5c42\u53cc\u7f16\u7801\u5668\u8bbe\u8ba1\u540c\u65f6\u5efa\u6a21\u6574\u591c\u7761\u7720\u5b8f\u89c2\u7ed3\u6784\u548c\u7ec6\u7c92\u5ea6\u4fe1\u53f7\u5fae\u89c2\u7279\u5f81\uff0c\u5728\u8d85\u8fc720,000\u4e2aPSG\u8bb0\u5f55\u4e0a\u9884\u8bad\u7ec3\uff0c\u5728\u591a\u4e2a\u4e0b\u6e38\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u5f53\u524d\u7761\u7720\u533b\u5b66\u4e3b\u8981\u4f7f\u7528\u4efb\u52a1\u7279\u5b9a\u6a21\u578b\uff0c\u8fd9\u4e9b\u6a21\u578b\u4e13\u6ce8\u4e8e\u5c40\u90e8\u5fae\u89c2\u7ed3\u6784\u7279\u5f81\uff0c\u5ffd\u7565\u4e86PSG\u4e30\u5bcc\u7684\u591a\u6a21\u6001\u4e0a\u4e0b\u6587\u548c\u6574\u591c\u7761\u7720\u7684\u5168\u5c40\u5b8f\u89c2\u7ed3\u6784\u3002\u9700\u8981\u7edf\u4e00\u7684\u57fa\u7840\u6a21\u578b\u6765\u540c\u65f6\u638c\u63e1\u957f\u65f6\u95f4\u7761\u7720\u67b6\u6784\u548c\u7ec6\u7c92\u5ea6\u4fe1\u53f7\u5f62\u6001\u3002", "method": "\u91c7\u7528\u5206\u5c42\u53cc\u7f16\u7801\u5668\u8bbe\u8ba1\uff1a\u5b8f\u89c2\u7f16\u7801\u5668\u5efa\u6a21\u6574\u591c\u65f6\u95f4\u4f9d\u8d56\u5173\u7cfb\uff0c\u901a\u8fc7\u4eba\u53e3\u7edf\u8ba1\u5b66\u5f15\u5bfc\u7684\u5bf9\u6bd4\u5b66\u4e60\u4e0e\u5e74\u9f84\u3001\u6027\u522b\u3001BMI\u7b49\u5ba2\u89c2\u5143\u6570\u636e\u5bf9\u9f50\uff1b\u5fae\u89c2\u7f16\u7801\u5668\u6355\u83b7\u751f\u7269\u4fe1\u53f7\u7684\u77ed\u671f\u7279\u5f81\uff0c\u901a\u8fc7\u6df7\u5408\u63a9\u7801\u81ea\u7f16\u7801\u5668\u548c\u591a\u6a21\u6001\u5bf9\u6bd4\u76ee\u6807\u4f18\u5316\u3002\u5728\u8d85\u8fc720,000\u4e2aPSG\u8bb0\u5f55\uff08158K\u5c0f\u65f6\uff09\u4e0a\u8fdb\u884c\u9884\u8bad\u7ec3\u3002", "result": "SleepMaMi\u5728\u591a\u6837\u5316\u7684\u4e0b\u6e38\u4efb\u52a1\u4e2d\u8d85\u8d8a\u4e86\u73b0\u6709\u7684\u57fa\u7840\u6a21\u578b\uff0c\u5c55\u793a\u4e86\u4f18\u8d8a\u7684\u6cdb\u5316\u80fd\u529b\u548c\u6807\u7b7e\u9ad8\u6548\u7684\u4e34\u5e8a\u7761\u7720\u5206\u6790\u9002\u5e94\u80fd\u529b\u3002", "conclusion": "SleepMaMi\u6210\u529f\u89e3\u51b3\u4e86\u7761\u7720\u533b\u5b66\u4e2d\u4efb\u52a1\u7279\u5b9a\u6a21\u578b\u7684\u5c40\u9650\u6027\uff0c\u901a\u8fc7\u7edf\u4e00\u7684\u57fa\u7840\u6a21\u578b\u6846\u67b6\u540c\u65f6\u638c\u63e1\u5b8f\u89c2\u7761\u7720\u7ed3\u6784\u548c\u5fae\u89c2\u4fe1\u53f7\u7279\u5f81\uff0c\u4e3a\u4e34\u5e8a\u7761\u7720\u5206\u6790\u63d0\u4f9b\u4e86\u66f4\u5f3a\u5927\u7684\u5de5\u5177\u3002"}}
{"id": "2602.08181", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.08181", "abs": "https://arxiv.org/abs/2602.08181", "authors": ["Oscar Manglaras", "Alex Farkas", "Thomas Woolford", "Christoph Treude", "Markus Wagner"], "title": "ModARO: A Modular Approach to Architecture Reconstruction of Distributed Microservice Codebases", "comment": null, "summary": "Microservice architectures promote small, independently developed services, but increase overall architectural complexity. It is crucial that developers understand the architecture and how changes to a service affect the overall system, but rapid and independent development of services increases the risk of architectural drift and discourages the creation and maintenance of documentation. Automatic architecture reconstruction can help avoid these issues, but it is difficult to reuse reconstruction code across multiple projects, as all use different combinations of technologies and project-specific conventions. Reconstruction of architecture-level details is further complicated by the tendency to split microservices into separate repositories, preventing a full view of the system from any one codebase. In this paper, we present and evaluate ModARO, an approach to microservice architecture reconstruction that allows writing modular reconstruction code ('extractors') for any technologies and reusing them across different projects, independent of the surrounding technology stack or whether or not the services are split into multiple codebases. We demonstrate the effectiveness of our approach by configuring ModARO to reconstruct 10 open source projects, and we validate the usefulness and usability of ModARO against a state-of-the-art baseline in a user study with 8 industry practitioners. Using this approach, developers can assemble or create extractors tailored to their technology stacks and distribute architecture reconstruction across repositories, enabling integration into repository CI/CD pipelines.", "AI": {"tldr": "ModARO\u662f\u4e00\u4e2a\u6a21\u5757\u5316\u7684\u5fae\u670d\u52a1\u67b6\u6784\u91cd\u6784\u65b9\u6cd5\uff0c\u5141\u8bb8\u7f16\u5199\u53ef\u91cd\u7528\u7684\u63d0\u53d6\u5668\u6765\u91cd\u6784\u4e0d\u540c\u6280\u672f\u6808\u7684\u5fae\u670d\u52a1\u67b6\u6784\uff0c\u89e3\u51b3\u4e86\u8de8\u9879\u76ee\u590d\u7528\u548c\u8de8\u4ed3\u5e93\u91cd\u6784\u7684\u96be\u9898\u3002", "motivation": "\u5fae\u670d\u52a1\u67b6\u6784\u867d\u7136\u4fc3\u8fdb\u4e86\u5c0f\u578b\u72ec\u7acb\u670d\u52a1\u7684\u5f00\u53d1\uff0c\u4f46\u589e\u52a0\u4e86\u6574\u4f53\u67b6\u6784\u590d\u6742\u6027\u3002\u5feb\u901f\u72ec\u7acb\u7684\u670d\u52a1\u5f00\u53d1\u589e\u52a0\u4e86\u67b6\u6784\u6f02\u79fb\u7684\u98ce\u9669\uff0c\u4e14\u96be\u4ee5\u7ef4\u62a4\u6587\u6863\u3002\u81ea\u52a8\u67b6\u6784\u91cd\u6784\u53ef\u4ee5\u907f\u514d\u8fd9\u4e9b\u95ee\u9898\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u8de8\u9879\u76ee\u590d\u7528\u91cd\u6784\u4ee3\u7801\uff0c\u56e0\u4e3a\u4e0d\u540c\u9879\u76ee\u4f7f\u7528\u4e0d\u540c\u7684\u6280\u672f\u7ec4\u5408\u548c\u9879\u76ee\u7279\u5b9a\u7ea6\u5b9a\u3002\u6b64\u5916\uff0c\u5fae\u670d\u52a1\u901a\u5e38\u88ab\u62c6\u5206\u5230\u4e0d\u540c\u7684\u4ee3\u7801\u4ed3\u5e93\uff0c\u4f7f\u5f97\u4ece\u5355\u4e00\u4ee3\u7801\u5e93\u65e0\u6cd5\u83b7\u5f97\u7cfb\u7edf\u7684\u5b8c\u6574\u89c6\u56fe\u3002", "method": "\u63d0\u51fa\u4e86ModARO\u65b9\u6cd5\uff0c\u5141\u8bb8\u7f16\u5199\u6a21\u5757\u5316\u7684\u91cd\u6784\u4ee3\u7801\uff08\"\u63d0\u53d6\u5668\"\uff09\uff0c\u8fd9\u4e9b\u63d0\u53d6\u5668\u53ef\u4ee5\u9488\u5bf9\u4efb\u4f55\u6280\u672f\u7f16\u5199\uff0c\u5e76\u80fd\u5728\u4e0d\u540c\u9879\u76ee\u4e2d\u590d\u7528\uff0c\u72ec\u7acb\u4e8e\u5468\u56f4\u7684\u6280\u672f\u6808\u4ee5\u53ca\u670d\u52a1\u662f\u5426\u88ab\u62c6\u5206\u5230\u591a\u4e2a\u4ee3\u7801\u5e93\u3002\u5f00\u53d1\u8005\u53ef\u4ee5\u7ec4\u88c5\u6216\u521b\u5efa\u9002\u5408\u5176\u6280\u672f\u6808\u7684\u63d0\u53d6\u5668\uff0c\u5e76\u5728\u4e0d\u540c\u4ed3\u5e93\u95f4\u5206\u53d1\u67b6\u6784\u91cd\u6784\u4efb\u52a1\u3002", "result": "\u901a\u8fc7\u914d\u7f6eModARO\u91cd\u6784\u4e8610\u4e2a\u5f00\u6e90\u9879\u76ee\uff0c\u5e76\u901a\u8fc7\u7528\u6237\u7814\u7a76\u9a8c\u8bc1\u4e86ModARO\u7684\u6709\u6548\u6027\u548c\u53ef\u7528\u6027\u3002\u7528\u6237\u7814\u7a76\u6d89\u53ca8\u540d\u884c\u4e1a\u4ece\u4e1a\u8005\uff0c\u4e0e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\u8fdb\u884c\u6bd4\u8f83\uff0c\u8bc1\u660e\u4e86ModARO\u7684\u5b9e\u7528\u6027\u548c\u53ef\u7528\u6027\u3002", "conclusion": "ModARO\u65b9\u6cd5\u4f7f\u5f00\u53d1\u8005\u80fd\u591f\u4e3a\u7279\u5b9a\u6280\u672f\u6808\u5b9a\u5236\u63d0\u53d6\u5668\uff0c\u5e76\u5728\u4e0d\u540c\u4ed3\u5e93\u95f4\u5206\u53d1\u67b6\u6784\u91cd\u6784\u4efb\u52a1\uff0c\u4ece\u800c\u652f\u6301\u96c6\u6210\u5230\u4ed3\u5e93\u7684CI/CD\u6d41\u6c34\u7ebf\u4e2d\u3002\u8fd9\u79cd\u65b9\u6cd5\u89e3\u51b3\u4e86\u5fae\u670d\u52a1\u67b6\u6784\u91cd\u6784\u4e2d\u7684\u8de8\u9879\u76ee\u590d\u7528\u548c\u8de8\u4ed3\u5e93\u91cd\u6784\u96be\u9898\u3002"}}
{"id": "2602.08750", "categories": ["cs.CR", "cs.SE"], "pdf": "https://arxiv.org/pdf/2602.08750", "abs": "https://arxiv.org/abs/2602.08750", "authors": ["Guy Farrelly", "Michael Chesser", "Seyit Camtepe", "Damith C. Ranasinghe"], "title": "DyMA-Fuzz: Dynamic Direct Memory Access Abstraction for Re-hosted Monolithic Firmware Fuzzing", "comment": "Accepted to ICSE 2026", "summary": "The rise of smart devices in critical domains--including automotive, medical, industrial--demands robust firmware testing. Fuzzing firmware in re-hosted environments is a promising method for automated testing at scale, but remains difficult due to the tight coupling of code with a microcontroller's peripherals. Existing fuzzing frameworks primarily address input challenges in providing inputs for Memory-Mapped I/O or interrupts, but largely overlook Direct Memory Access (DMA), a key high-throughput interface used that bypasses the CPU. We introduce DyMA-Fuzz to extend recent advances in stream-based fuzz input injection to DMA-driven interfaces in re-hosted environments. It tackles key challenges--vendor-specific descriptors, heterogeneous DMA designs, and varying descriptor locations--using runtime analysis techniques to infer DMA memory access patterns and automatically inject fuzzing data into target buffers, without manual configuration or datasheets. Evaluated on 94 firmware samples and 8 DMA-guarded CVE benchmarks, DyMA-Fuzz reveals vulnerabilities and execution paths missed by state-of-the-art tools and achieves up to 122% higher code coverage. These results highlight DyMA-Fuzz as a practical and effective advancement in automated firmware testing and a scalable solution for fuzzing complex embedded systems.", "AI": {"tldr": "DyMA-Fuzz\u662f\u4e00\u4e2a\u9488\u5bf9\u5d4c\u5165\u5f0f\u7cfb\u7edf\u56fa\u4ef6\u6d4b\u8bd5\u7684\u6a21\u7cca\u6d4b\u8bd5\u6846\u67b6\uff0c\u4e13\u95e8\u5904\u7406DMA\uff08\u76f4\u63a5\u5185\u5b58\u8bbf\u95ee\uff09\u63a5\u53e3\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u5de5\u5177\u5ffd\u7565DMA\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u8fd0\u884c\u65f6\u5206\u6790\u81ea\u52a8\u6ce8\u5165\u6d4b\u8bd5\u6570\u636e\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u4ee3\u7801\u8986\u76d6\u7387\u3002", "motivation": "\u968f\u7740\u667a\u80fd\u8bbe\u5907\u5728\u5173\u952e\u9886\u57df\uff08\u6c7d\u8f66\u3001\u533b\u7597\u3001\u5de5\u4e1a\uff09\u7684\u666e\u53ca\uff0c\u9700\u8981\u5f3a\u5927\u7684\u56fa\u4ef6\u6d4b\u8bd5\u3002\u73b0\u6709\u6a21\u7cca\u6d4b\u8bd5\u6846\u67b6\u4e3b\u8981\u5904\u7406\u5185\u5b58\u6620\u5c04I/O\u548c\u4e2d\u65ad\u8f93\u5165\uff0c\u4f46\u5ffd\u7565\u4e86DMA\u8fd9\u4e00\u5173\u952e\u7684\u9ad8\u541e\u5410\u91cf\u63a5\u53e3\uff0c\u800cDMA\u7ed5\u8fc7CPU\u76f4\u63a5\u8bbf\u95ee\u5185\u5b58\uff0c\u662f\u56fa\u4ef6\u6d4b\u8bd5\u7684\u91cd\u8981\u6311\u6218\u3002", "method": "DyMA-Fuzz\u5c06\u57fa\u4e8e\u6d41\u7684\u6a21\u7cca\u6d4b\u8bd5\u8f93\u5165\u6ce8\u5165\u6269\u5c55\u5230\u91cd\u6258\u7ba1\u73af\u5883\u4e2d\u7684DMA\u9a71\u52a8\u63a5\u53e3\u3002\u5b83\u4f7f\u7528\u8fd0\u884c\u65f6\u5206\u6790\u6280\u672f\u6765\u63a8\u65adDMA\u5185\u5b58\u8bbf\u95ee\u6a21\u5f0f\uff0c\u81ea\u52a8\u5c06\u6a21\u7cca\u6d4b\u8bd5\u6570\u636e\u6ce8\u5165\u76ee\u6807\u7f13\u51b2\u533a\uff0c\u65e0\u9700\u624b\u52a8\u914d\u7f6e\u6216\u6570\u636e\u624b\u518c\u3002\u89e3\u51b3\u4e86\u4f9b\u5e94\u5546\u7279\u5b9a\u63cf\u8ff0\u7b26\u3001\u5f02\u6784DMA\u8bbe\u8ba1\u548c\u4e0d\u540c\u63cf\u8ff0\u7b26\u4f4d\u7f6e\u7b49\u5173\u952e\u6311\u6218\u3002", "result": "\u572894\u4e2a\u56fa\u4ef6\u6837\u672c\u548c8\u4e2aDMA\u4fdd\u62a4\u7684CVE\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cDyMA-Fuzz\u53d1\u73b0\u4e86\u73b0\u6709\u6700\u5148\u8fdb\u5de5\u5177\u9057\u6f0f\u7684\u6f0f\u6d1e\u548c\u6267\u884c\u8def\u5f84\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8fbe122%\u7684\u4ee3\u7801\u8986\u76d6\u7387\u63d0\u5347\u3002", "conclusion": "DyMA-Fuzz\u662f\u81ea\u52a8\u56fa\u4ef6\u6d4b\u8bd5\u7684\u5b9e\u7528\u6709\u6548\u8fdb\u5c55\uff0c\u4e3a\u6a21\u7cca\u6d4b\u8bd5\u590d\u6742\u5d4c\u5165\u5f0f\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86DMA\u63a5\u53e3\u7684\u6d4b\u8bd5\u8986\u76d6\u80fd\u529b\u3002"}}
{"id": "2602.07095", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07095", "abs": "https://arxiv.org/abs/2602.07095", "authors": ["Wang Lin", "Feng Wang", "Majun Zhang", "Wentao Hu", "Tao Jin", "Zhou Zhao", "Fei Wu", "Jingyuan Chen", "Alan Yuille", "Sucheng Ren"], "title": "WorldEdit: Towards Open-World Image Editing with a Knowledge-Informed Benchmark", "comment": null, "summary": "Recent advances in image editing models have demonstrated remarkable capabilities in executing explicit instructions, such as attribute manipulation, style transfer, and pose synthesis. However, these models often face challenges when dealing with implicit editing instructions, which describe the cause of a visual change without explicitly detailing the resulting outcome. These limitations arise because existing models rely on uniform editing strategies that are not equipped to handle the complex world knowledge and reasoning required for implicit instructions. To address this gap, we introduce \\textbf{WorldEdit}, a dataset specifically designed to enable world-driven image editing. WorldEdit consists of high-quality editing samples, guided by paraphrased instructions that align with real-world causal logic. Furthermore, we provide \\textbf{WorldEdit-Test} for evaluating the existing model's performance on causal editing scenarios. With WorldEdit, we use a two-stage training framework for fine-tuning models like Bagel, integrating with a causal verification reward. Our results show that the proposed dataset and methods significantly narrow the gap with GPT-4o and Nano-Banana, demonstrating competitive performance not only in instruction following but also in knowledge plausibility, where many open-source systems typically struggle.", "AI": {"tldr": "WorldEdit\u662f\u4e00\u4e2a\u4e13\u95e8\u8bbe\u8ba1\u7528\u4e8e\u4e16\u754c\u9a71\u52a8\u56fe\u50cf\u7f16\u8f91\u7684\u6570\u636e\u96c6\uff0c\u65e8\u5728\u89e3\u51b3\u73b0\u6709\u6a21\u578b\u5728\u5904\u7406\u9690\u5f0f\u7f16\u8f91\u6307\u4ee4\u65f6\u7684\u5c40\u9650\u6027\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u8bad\u7ec3\u6846\u67b6\u63d0\u5347\u6a21\u578b\u5728\u56e0\u679c\u7f16\u8f91\u573a\u666f\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u56fe\u50cf\u7f16\u8f91\u6a21\u578b\u5728\u5904\u7406\u663e\u5f0f\u6307\u4ee4\uff08\u5982\u5c5e\u6027\u64cd\u4f5c\u3001\u98ce\u683c\u8f6c\u6362\u3001\u59ff\u6001\u5408\u6210\uff09\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u5904\u7406\u9690\u5f0f\u7f16\u8f91\u6307\u4ee4\u65f6\u9762\u4e34\u6311\u6218\u3002\u9690\u5f0f\u6307\u4ee4\u63cf\u8ff0\u89c6\u89c9\u53d8\u5316\u7684\u539f\u56e0\u800c\u4e0d\u8be6\u7ec6\u8bf4\u660e\u7ed3\u679c\uff0c\u9700\u8981\u590d\u6742\u7684\u4e16\u754c\u77e5\u8bc6\u548c\u63a8\u7406\u80fd\u529b\uff0c\u800c\u73b0\u6709\u6a21\u578b\u7f3a\u4e4f\u8fd9\u79cd\u80fd\u529b\u3002", "method": "1. \u5f15\u5165WorldEdit\u6570\u636e\u96c6\uff0c\u5305\u542b\u9ad8\u8d28\u91cf\u7f16\u8f91\u6837\u672c\uff0c\u6307\u5bfc\u901a\u8fc7\u7b26\u5408\u73b0\u5b9e\u4e16\u754c\u56e0\u679c\u903b\u8f91\u7684\u8f6c\u8ff0\u6307\u4ee4\uff1b2. \u63d0\u4f9bWorldEdit-Test\u7528\u4e8e\u8bc4\u4f30\u73b0\u6709\u6a21\u578b\u5728\u56e0\u679c\u7f16\u8f91\u573a\u666f\u4e2d\u7684\u6027\u80fd\uff1b3. \u4f7f\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u6846\u67b6\u5fae\u8c03Bagel\u7b49\u6a21\u578b\uff0c\u6574\u5408\u56e0\u679c\u9a8c\u8bc1\u5956\u52b1\u3002", "result": "\u63d0\u51fa\u7684\u6570\u636e\u96c6\u548c\u65b9\u6cd5\u663e\u8457\u7f29\u5c0f\u4e86\u4e0eGPT-4o\u548cNano-Banana\u7684\u5dee\u8ddd\uff0c\u5728\u6307\u4ee4\u9075\u5faa\u548c\u77e5\u8bc6\u5408\u7406\u6027\u65b9\u9762\u8868\u73b0\u51fa\u7ade\u4e89\u529b\uff0c\u800c\u8bb8\u591a\u5f00\u6e90\u7cfb\u7edf\u901a\u5e38\u5728\u8fd9\u4e9b\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\u3002", "conclusion": "WorldEdit\u6570\u636e\u96c6\u548c\u4e24\u9636\u6bb5\u8bad\u7ec3\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u56fe\u50cf\u7f16\u8f91\u6a21\u578b\u5904\u7406\u9690\u5f0f\u6307\u4ee4\u7684\u5c40\u9650\u6027\uff0c\u901a\u8fc7\u6574\u5408\u56e0\u679c\u903b\u8f91\u548c\u4e16\u754c\u77e5\u8bc6\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u5728\u590d\u6742\u7f16\u8f91\u573a\u666f\u4e2d\u7684\u6027\u80fd\u3002"}}
{"id": "2602.07642", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.07642", "abs": "https://arxiv.org/abs/2602.07642", "authors": ["Zhuoyan Xu", "Haoyang Fang", "Boran Han", "Bonan Min", "Bernie Wang", "Cuixiong Hu", "Shuai Zhang"], "title": "Efficient Table Retrieval and Understanding with Multimodal Large Language Models", "comment": "Published at EACL 2026 Findings", "summary": "Tabular data is frequently captured in image form across a wide range of real-world scenarios such as financial reports, handwritten records, and document scans. These visual representations pose unique challenges for machine understanding, as they combine both structural and visual complexities. While recent advances in Multimodal Large Language Models (MLLMs) show promising results in table understanding, they typically assume the relevant table is readily available. However, a more practical scenario involves identifying and reasoning over relevant tables from large-scale collections to answer user queries. To address this gap, we propose TabRAG, a framework that enables MLLMs to answer queries over large collections of table images. Our approach first retrieves candidate tables using jointly trained visual-text foundation models, then leverages MLLMs to perform fine-grained reranking of these candidates, and finally employs MLLMs to reason over the selected tables for answer generation. Through extensive experiments on a newly constructed dataset comprising 88,161 training and 9,819 testing samples across 8 benchmarks with 48,504 unique tables, we demonstrate that our framework significantly outperforms existing methods by 7.0% in retrieval recall and 6.1% in answer accuracy, offering a practical solution for real-world table understanding tasks.", "AI": {"tldr": "TabRAG\u6846\u67b6\u901a\u8fc7\u89c6\u89c9-\u6587\u672c\u57fa\u7840\u6a21\u578b\u68c0\u7d22\u5019\u9009\u8868\u683c\u56fe\u50cf\uff0cMLLM\u7ec6\u7c92\u5ea6\u91cd\u6392\u5e8f\uff0c\u6700\u7ec8MLLM\u63a8\u7406\u751f\u6210\u7b54\u6848\uff0c\u663e\u8457\u63d0\u5347\u8868\u683c\u56fe\u50cf\u68c0\u7d22\u548c\u95ee\u7b54\u6027\u80fd", "motivation": "\u73b0\u5b9e\u4e16\u754c\u4e2d\u8868\u683c\u5e38\u4ee5\u56fe\u50cf\u5f62\u5f0f\u5b58\u5728\uff08\u5982\u8d22\u52a1\u62a5\u8868\u3001\u624b\u5199\u8bb0\u5f55\u3001\u6587\u6863\u626b\u63cf\uff09\uff0c\u73b0\u6709MLLM\u65b9\u6cd5\u901a\u5e38\u5047\u8bbe\u8868\u683c\u5df2\u51c6\u5907\u597d\uff0c\u4f46\u5b9e\u9645\u573a\u666f\u9700\u8981\u4ece\u5927\u89c4\u6a21\u8868\u683c\u56fe\u50cf\u96c6\u5408\u4e2d\u68c0\u7d22\u76f8\u5173\u8868\u683c\u8fdb\u884c\u63a8\u7406\u56de\u7b54\u7528\u6237\u67e5\u8be2", "method": "\u63d0\u51faTabRAG\u6846\u67b6\uff1a1) \u4f7f\u7528\u8054\u5408\u8bad\u7ec3\u7684\u89c6\u89c9-\u6587\u672c\u57fa\u7840\u6a21\u578b\u68c0\u7d22\u5019\u9009\u8868\u683c\u56fe\u50cf\uff1b2) \u5229\u7528MLLM\u5bf9\u5019\u9009\u8868\u683c\u8fdb\u884c\u7ec6\u7c92\u5ea6\u91cd\u6392\u5e8f\uff1b3) \u4f7f\u7528MLLM\u5728\u9009\u5b9a\u8868\u683c\u4e0a\u8fdb\u884c\u63a8\u7406\u751f\u6210\u7b54\u6848", "result": "\u5728\u65b0\u6784\u5efa\u7684\u6570\u636e\u96c6\uff0888,161\u8bad\u7ec3\u6837\u672c\uff0c9,819\u6d4b\u8bd5\u6837\u672c\uff0c8\u4e2a\u57fa\u51c6\uff0c48,504\u4e2a\u552f\u4e00\u8868\u683c\uff09\u4e0a\u5b9e\u9a8c\u8868\u660e\uff0c\u6846\u67b6\u5728\u68c0\u7d22\u53ec\u56de\u7387\u4e0a\u63d0\u53477.0%\uff0c\u7b54\u6848\u51c6\u786e\u7387\u63d0\u53476.1%\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5", "conclusion": "TabRAG\u4e3a\u73b0\u5b9e\u4e16\u754c\u8868\u683c\u7406\u89e3\u4efb\u52a1\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u68c0\u7d22-\u91cd\u6392\u5e8f-\u63a8\u7406\u7684\u4e09\u9636\u6bb5\u6846\u67b6\uff0c\u6709\u6548\u5904\u7406\u5927\u89c4\u6a21\u8868\u683c\u56fe\u50cf\u96c6\u5408\u7684\u67e5\u8be2\u56de\u7b54\u95ee\u9898"}}
{"id": "2602.08192", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.08192", "abs": "https://arxiv.org/abs/2602.08192", "authors": ["Mirko Perkusich", "Danyllo Albuquerque", "Allysson Allex Ara\u00fajo", "Matheus Paix\u00e3o", "Rohit Gheyi", "Marcos Kalinowski", "Angelo Perkusich"], "title": "Adoption of Large Language Models in Scrum Management: Insights from Brazilian Practitioners", "comment": "Accepted for publication at the 27th International Conference on Agile Software Development (XP 2026)", "summary": "Scrum is widely adopted in software project management due to its adaptability and collaborative nature. The recent emergence of Large Language Models (LLMs) has created new opportunities to support knowledge-intensive Scrum practices. However, existing research has largely focused on technical activities such as coding and testing, with limited evidence on the use of LLMs in management-related Scrum activities. In this study, we investigate the use of LLMs in Scrum management activities through a survey of 70 Brazilian professionals. Among them, 49 actively use Scrum, and 33 reported using LLM-based assistants in their Scrum practices. The results indicate a high level of proficiency and frequent use of LLMs, with 85% of respondents reporting intermediate or advanced proficiency and 52% using them daily. LLM use concentrates on exploring Scrum practices, with artifacts and events receiving targeted yet uneven support, whereas broader management tasks appear to be adopted more cautiously. The main benefits include increased productivity (78%) and reduced manual effort (75%). However, several critical risks remain, as respondents report 'almost correct' outputs (81%), confidentiality concerns (63%), and hallucinations during use (59%). This work provides one of the first empirical characterizations of LLM use in Scrum management, identifying current practices, quantifying benefits and risks, and outlining directions for responsible adoption and integration in Agile environments.", "AI": {"tldr": "\u7814\u7a76\u8c03\u67e5\u4e86\u5df4\u897f70\u540d\u4e13\u4e1a\u4eba\u58eb\u5728Scrum\u7ba1\u7406\u6d3b\u52a8\u4e2d\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u7684\u60c5\u51b5\uff0c\u53d1\u73b0LLM\u4e3b\u8981\u5e94\u7528\u4e8e\u63a2\u7d22Scrum\u5b9e\u8df5\uff0c\u63d0\u9ad8\u751f\u4ea7\u529b\u548c\u51cf\u5c11\u4eba\u5de5\u52b3\u52a8\uff0c\u4f46\u5b58\u5728\u8f93\u51fa\u4e0d\u5b8c\u5168\u51c6\u786e\u3001\u4fdd\u5bc6\u95ee\u9898\u548c\u5e7b\u89c9\u7b49\u98ce\u9669\u3002", "motivation": "\u867d\u7136Scrum\u5728\u8f6f\u4ef6\u9879\u76ee\u7ba1\u7406\u4e2d\u5e7f\u6cdb\u5e94\u7528\uff0cLLM\u4e3a\u77e5\u8bc6\u5bc6\u96c6\u578bScrum\u5b9e\u8df5\u63d0\u4f9b\u4e86\u65b0\u673a\u4f1a\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u7f16\u7801\u548c\u6d4b\u8bd5\u7b49\u6280\u672f\u6d3b\u52a8\uff0c\u7f3a\u4e4fLLM\u5728Scrum\u7ba1\u7406\u6d3b\u52a8\u4e2d\u7684\u5e94\u7528\u8bc1\u636e\u3002", "method": "\u901a\u8fc7\u5bf970\u540d\u5df4\u897f\u4e13\u4e1a\u4eba\u58eb\u8fdb\u884c\u8c03\u67e5\u7814\u7a76\uff0c\u5176\u4e2d49\u4eba\u79ef\u6781\u4f7f\u7528Scrum\uff0c33\u4eba\u62a5\u544a\u5728Scrum\u5b9e\u8df5\u4e2d\u4f7f\u7528LLM\u52a9\u624b\uff0c\u5206\u6790\u4e86LLM\u5728Scrum\u7ba1\u7406\u6d3b\u52a8\u4e2d\u7684\u4f7f\u7528\u60c5\u51b5\u3002", "result": "\u7ed3\u679c\u663e\u793aLLM\u4f7f\u7528\u6c34\u5e73\u9ad8\u4e14\u9891\u7e41\uff1a85%\u53d7\u8bbf\u8005\u5177\u5907\u4e2d\u7ea7\u6216\u9ad8\u7ea7\u719f\u7ec3\u5ea6\uff0c52%\u6bcf\u5929\u4f7f\u7528\u3002LLM\u4e3b\u8981\u96c6\u4e2d\u4e8e\u63a2\u7d22Scrum\u5b9e\u8df5\uff0c\u5de5\u4ef6\u548c\u4e8b\u4ef6\u83b7\u5f97\u9488\u5bf9\u6027\u4f46\u4e0d\u5747\u8861\u7684\u652f\u6301\uff0c\u800c\u66f4\u5e7f\u6cdb\u7684\u7ba1\u7406\u4efb\u52a1\u91c7\u7528\u66f4\u4e3a\u8c28\u614e\u3002\u4e3b\u8981\u76ca\u5904\u5305\u62ec\u63d0\u9ad8\u751f\u4ea7\u529b\uff0878%\uff09\u548c\u51cf\u5c11\u4eba\u5de5\u52b3\u52a8\uff0875%\uff09\uff0c\u4f46\u5b58\u5728\"\u51e0\u4e4e\u6b63\u786e\"\u7684\u8f93\u51fa\uff0881%\uff09\u3001\u4fdd\u5bc6\u62c5\u5fe7\uff0863%\uff09\u548c\u4f7f\u7528\u4e2d\u7684\u5e7b\u89c9\uff0859%\uff09\u7b49\u98ce\u9669\u3002", "conclusion": "\u672c\u7814\u7a76\u9996\u6b21\u5bf9LLM\u5728Scrum\u7ba1\u7406\u4e2d\u7684\u4f7f\u7528\u8fdb\u884c\u4e86\u5b9e\u8bc1\u63cf\u8ff0\uff0c\u8bc6\u522b\u4e86\u5f53\u524d\u5b9e\u8df5\uff0c\u91cf\u5316\u4e86\u76ca\u5904\u548c\u98ce\u9669\uff0c\u5e76\u4e3a\u654f\u6377\u73af\u5883\u4e2d\u8d1f\u8d23\u4efb\u5730\u91c7\u7528\u548c\u96c6\u6210LLM\u63d0\u4f9b\u4e86\u65b9\u5411\u3002"}}
{"id": "2602.08798", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2602.08798", "abs": "https://arxiv.org/abs/2602.08798", "authors": ["Hedong Zhang", "Neusha Javidnia", "Shweta Pardeshi", "Qian Lou", "Farinaz Koushanfar"], "title": "CryptoGen: Secure Transformer Generation with Encrypted KV-Cache Reuse", "comment": "13 pages, 9 figures", "summary": "The widespread deployment of cloud-hosted generative models raises a fundamental challenge: enabling efficient autoregressive generation while preserving the privacy of both user prompts and model parameters in untrusted environments. We address this challenge in a client-server setting where an untrusted server hosts an autoregressive Transformer and the client requires cryptographic protection for both inputs and inference. We present CryptoGen, the first system to enable scalable privacy-preserving neural generation with persistent encrypted key-value (KV) cache reuse. Discriminative-task secure inference systems incur quadratic latency and memory growth when adapted to autoregressive decoding due to the lack of native encrypted KV-cache support. In contrast, CryptoGen achieves near-linear scaling by securely reusing and updating encrypted KV caches throughout generation. CryptoGen integrates homomorphic encryption and secret sharing to support both prefilling and generation. Key techniques include a unified encrypted KV-cache framework, heterogeneous SIMD encodings for different phases, optimized cipher-cipher matrix-matrix and matrix-vector operations, and efficient noise refresh and ciphertext concatenation mechanisms. Evaluation on generative Transformer models trained on WikiText-2, PTB, and LAMBADA shows that for input lengths of 128-512 tokens, CryptoGen achieves 4.4x-7.6x lower per-token latency than state-of-the-art discriminative secure inference systems, while maintaining near-linear latency and memory scaling, with advantages increasing for longer sequences. CryptoGen is released as an open-source library.", "AI": {"tldr": "CryptoGen\u662f\u9996\u4e2a\u652f\u6301\u53ef\u6269\u5c55\u9690\u79c1\u4fdd\u62a4\u795e\u7ecf\u751f\u6210\u7684\u7cfb\u7edf\uff0c\u901a\u8fc7\u5b89\u5168\u91cd\u7528\u52a0\u5bc6\u7684KV\u7f13\u5b58\uff0c\u5728\u4e0d\u53ef\u4fe1\u670d\u52a1\u5668\u73af\u5883\u4e2d\u4fdd\u62a4\u7528\u6237\u63d0\u793a\u548c\u6a21\u578b\u53c2\u6570\u7684\u9690\u79c1\u3002", "motivation": "\u4e91\u6258\u7ba1\u751f\u6210\u6a21\u578b\u7684\u5e7f\u6cdb\u90e8\u7f72\u5e26\u6765\u4e86\u57fa\u672c\u6311\u6218\uff1a\u5982\u4f55\u5728\u4e0d\u53ef\u4fe1\u73af\u5883\u4e2d\u5b9e\u73b0\u9ad8\u6548\u7684\u81ea\u56de\u5f52\u751f\u6210\uff0c\u540c\u65f6\u4fdd\u62a4\u7528\u6237\u63d0\u793a\u548c\u6a21\u578b\u53c2\u6570\u7684\u9690\u79c1\u3002\u73b0\u6709\u5224\u522b\u4efb\u52a1\u5b89\u5168\u63a8\u7406\u7cfb\u7edf\u5728\u9002\u5e94\u81ea\u56de\u5f52\u89e3\u7801\u65f6\u5b58\u5728\u4e8c\u6b21\u65b9\u5ef6\u8fdf\u548c\u5185\u5b58\u589e\u957f\u95ee\u9898\u3002", "method": "CryptoGen\u6574\u5408\u540c\u6001\u52a0\u5bc6\u548c\u79d8\u5bc6\u5171\u4eab\uff0c\u652f\u6301\u9884\u586b\u5145\u548c\u751f\u6210\u9636\u6bb5\u3002\u5173\u952e\u6280\u672f\u5305\u62ec\u7edf\u4e00\u7684\u52a0\u5bc6KV\u7f13\u5b58\u6846\u67b6\u3001\u9488\u5bf9\u4e0d\u540c\u9636\u6bb5\u7684\u5f02\u6784SIMD\u7f16\u7801\u3001\u4f18\u5316\u7684\u5bc6\u6587-\u5bc6\u6587\u77e9\u9635-\u77e9\u9635\u548c\u77e9\u9635-\u5411\u91cf\u64cd\u4f5c\uff0c\u4ee5\u53ca\u9ad8\u6548\u7684\u566a\u58f0\u5237\u65b0\u548c\u5bc6\u6587\u8fde\u63a5\u673a\u5236\u3002", "result": "\u5728WikiText-2\u3001PTB\u548cLAMBADA\u6570\u636e\u96c6\u8bad\u7ec3\u7684\u751f\u6210Transformer\u6a21\u578b\u4e0a\u8bc4\u4f30\uff0c\u5bf9\u4e8e128-512\u4e2a\u4ee4\u724c\u7684\u8f93\u5165\u957f\u5ea6\uff0cCryptoGen\u76f8\u6bd4\u6700\u5148\u8fdb\u7684\u5224\u522b\u5b89\u5168\u63a8\u7406\u7cfb\u7edf\u5b9e\u73b0\u4e864.4x-7.6x\u66f4\u4f4e\u7684\u6bcf\u4ee4\u724c\u5ef6\u8fdf\uff0c\u540c\u65f6\u4fdd\u6301\u63a5\u8fd1\u7ebf\u6027\u7684\u5ef6\u8fdf\u548c\u5185\u5b58\u6269\u5c55\uff0c\u5bf9\u4e8e\u66f4\u957f\u5e8f\u5217\u4f18\u52bf\u66f4\u660e\u663e\u3002", "conclusion": "CryptoGen\u901a\u8fc7\u5b89\u5168\u91cd\u7528\u548c\u66f4\u65b0\u52a0\u5bc6KV\u7f13\u5b58\uff0c\u5b9e\u73b0\u4e86\u63a5\u8fd1\u7ebf\u6027\u7684\u6269\u5c55\uff0c\u662f\u9996\u4e2a\u652f\u6301\u53ef\u6269\u5c55\u9690\u79c1\u4fdd\u62a4\u795e\u7ecf\u751f\u6210\u7684\u7cfb\u7edf\uff0c\u5df2\u4f5c\u4e3a\u5f00\u6e90\u5e93\u53d1\u5e03\u3002"}}
{"id": "2602.07100", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.07100", "abs": "https://arxiv.org/abs/2602.07100", "authors": ["Biao Xiong", "Zhen Peng", "Ping Wang", "Qiegen Liu", "Xian Zhong"], "title": "TLC-Plan: A Two-Level Codebook Based Network for End-to-End Vector Floorplan Generation", "comment": null, "summary": "Automated floorplan generation aims to improve design quality, architectural efficiency, and sustainability by jointly modeling global spatial organization and precise geometric detail. However, existing approaches operate in raster space and rely on post hoc vectorization, which introduces structural inconsistencies and hinders end-to-end learning. Motivated by compositional spatial reasoning, we propose TLC-Plan, a hierarchical generative model that directly synthesizes vector floorplans from input boundaries, aligning with human architectural workflows based on modular and reusable patterns. TLC-Plan employs a two-level VQ-VAE to encode global layouts as semantically labeled room bounding boxes and to refine local geometries using polygon-level codes. This hierarchy is unified in a CodeTree representation, while an autoregressive transformer samples codes conditioned on the boundary to generate diverse and topologically valid designs, without requiring explicit room topology or dimensional priors. Extensive experiments show state-of-the-art performance on RPLAN dataset (FID = 1.84, MSE = 2.06) and leading results on LIFULL dataset. The proposed framework advances constraint-aware and scalable vector floorplan generation for real-world architectural applications. Source code and trained models are released at https://github.com/rosolose/TLC-PLAN.", "AI": {"tldr": "TLC-Plan\u662f\u4e00\u4e2a\u76f4\u63a5\u4ece\u8f93\u5165\u8fb9\u754c\u5408\u6210\u77e2\u91cf\u5e73\u9762\u56fe\u7684\u5c42\u6b21\u5316\u751f\u6210\u6a21\u578b\uff0c\u901a\u8fc7\u4e24\u5c42VQ-VAE\u7f16\u7801\u5168\u5c40\u5e03\u5c40\u548c\u5c40\u90e8\u51e0\u4f55\uff0c\u4f7f\u7528CodeTree\u7edf\u4e00\u8868\u793a\uff0c\u5e76\u901a\u8fc7\u81ea\u56de\u5f52\u53d8\u6362\u5668\u751f\u6210\u591a\u6837\u4e14\u62d3\u6251\u6709\u6548\u7684\u8bbe\u8ba1\uff0c\u5728RPLAN\u6570\u636e\u96c6\u4e0a\u8fbe\u5230SOTA\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u6805\u683c\u7a7a\u95f4\u4e2d\u64cd\u4f5c\u5e76\u4f9d\u8d56\u540e\u5904\u7406\u77e2\u91cf\u5316\uff0c\u4f1a\u5bfc\u81f4\u7ed3\u6784\u4e0d\u4e00\u81f4\u5e76\u963b\u788d\u7aef\u5230\u7aef\u5b66\u4e60\u3002\u53d7\u7ec4\u5408\u7a7a\u95f4\u63a8\u7406\u542f\u53d1\uff0c\u9700\u8981\u5f00\u53d1\u4e0e\u4eba\u7c7b\u5efa\u7b51\u5de5\u4f5c\u6d41\u7a0b\uff08\u57fa\u4e8e\u6a21\u5757\u5316\u548c\u53ef\u91cd\u7528\u6a21\u5f0f\uff09\u4e00\u81f4\u7684\u77e2\u91cf\u5e73\u9762\u56fe\u751f\u6210\u65b9\u6cd5\u3002", "method": "\u63d0\u51faTLC-Plan\u5c42\u6b21\u751f\u6210\u6a21\u578b\uff1a1) \u4f7f\u7528\u4e24\u5c42VQ-VAE\u7f16\u7801\u5168\u5c40\u5e03\u5c40\uff08\u8bed\u4e49\u6807\u8bb0\u7684\u623f\u95f4\u8fb9\u754c\u6846\uff09\u548c\u5c40\u90e8\u51e0\u4f55\uff08\u591a\u8fb9\u5f62\u7ea7\u7f16\u7801\uff09\uff1b2) \u901a\u8fc7CodeTree\u7edf\u4e00\u8868\u793a\u5c42\u6b21\u7ed3\u6784\uff1b3) \u4f7f\u7528\u81ea\u56de\u5f52\u53d8\u6362\u5668\u5728\u8fb9\u754c\u6761\u4ef6\u4e0b\u91c7\u6837\u7f16\u7801\uff0c\u751f\u6210\u591a\u6837\u4e14\u62d3\u6251\u6709\u6548\u7684\u8bbe\u8ba1\uff0c\u65e0\u9700\u663e\u5f0f\u623f\u95f4\u62d3\u6251\u6216\u7ef4\u5ea6\u5148\u9a8c\u3002", "result": "\u5728RPLAN\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff08FID = 1.84\uff0cMSE = 2.06\uff09\uff0c\u5728LIFULL\u6570\u636e\u96c6\u4e0a\u4e5f\u53d6\u5f97\u9886\u5148\u7ed3\u679c\u3002\u6846\u67b6\u652f\u6301\u7ea6\u675f\u611f\u77e5\u548c\u53ef\u6269\u5c55\u7684\u77e2\u91cf\u5e73\u9762\u56fe\u751f\u6210\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u5efa\u7b51\u5e94\u7528\u3002", "conclusion": "TLC-Plan\u901a\u8fc7\u76f4\u63a5\u5408\u6210\u77e2\u91cf\u5e73\u9762\u56fe\uff0c\u907f\u514d\u4e86\u6805\u683c\u7a7a\u95f4\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5b9e\u73b0\u4e86\u4e0e\u4eba\u7c7b\u5efa\u7b51\u5de5\u4f5c\u6d41\u7a0b\u4e00\u81f4\u7684\u9ad8\u8d28\u91cf\u3001\u591a\u6837\u5316\u548c\u62d3\u6251\u6709\u6548\u7684\u5e73\u9762\u56fe\u751f\u6210\uff0c\u4e3a\u5b9e\u9645\u5efa\u7b51\u5e94\u7528\u63d0\u4f9b\u4e86\u5148\u8fdb\u7684\u7ea6\u675f\u611f\u77e5\u548c\u53ef\u6269\u5c55\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.07662", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07662", "abs": "https://arxiv.org/abs/2602.07662", "authors": ["Glenda Amaral", "Tiago Prince Sales", "Riccardo Baratella", "Daniele Porello", "Renata Guizzardi", "Giancarlo Guizzardi"], "title": "ONTrust: A Reference Ontology of Trust", "comment": "46 pages", "summary": "Trust has stood out more than ever in the light of recent innovations. Some examples are advances in artificial intelligence that make machines more and more humanlike, and the introduction of decentralized technologies (e.g. blockchains), which creates new forms of (decentralized) trust. These new developments have the potential to improve the provision of products and services, as well as to contribute to individual and collective well-being. However, their adoption depends largely on trust. In order to build trustworthy systems, along with defining laws, regulations and proper governance models for new forms of trust, it is necessary to properly conceptualize trust, so that it can be understood both by humans and machines. This paper is the culmination of a long-term research program of providing a solid ontological foundation on trust, by creating reference conceptual models to support information modeling, automated reasoning, information integration and semantic interoperability tasks. To address this, a Reference Ontology of Trust (ONTrust) was developed, grounded on the Unified Foundational Ontology and specified in OntoUML, which has been applied in several initiatives, to demonstrate, for example, how it can be used for conceptual modeling and enterprise architecture design, for language evaluation and (re)design, for trust management, for requirements engineering, and for trustworthy artificial intelligence (AI) in the context of affective Human-AI teaming. ONTrust formally characterizes the concept of trust and its different types, describes the different factors that can influence trust, as well as explains how risk emerges from trust relations. To illustrate the working of ONTrust, the ontology is applied to model two case studies extracted from the literature.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u7edf\u4e00\u57fa\u7840\u672c\u4f53\u7684\u4fe1\u4efb\u53c2\u8003\u672c\u4f53(ONTrust)\uff0c\u65e8\u5728\u4e3a\u4eba\u7c7b\u548c\u673a\u5668\u63d0\u4f9b\u5bf9\u4fe1\u4efb\u7684\u6b63\u5f0f\u6982\u5ff5\u5316\uff0c\u4ee5\u652f\u6301\u53ef\u4fe1\u7cfb\u7edf\u7684\u6784\u5efa\u3002", "motivation": "\u968f\u7740\u4eba\u5de5\u667a\u80fd\u548c\u533a\u5757\u94fe\u7b49\u6280\u672f\u7684\u53d1\u5c55\uff0c\u4fe1\u4efb\u5728\u7cfb\u7edf\u91c7\u7eb3\u4e2d\u7684\u91cd\u8981\u6027\u65e5\u76ca\u51f8\u663e\u3002\u4e3a\u4e86\u6784\u5efa\u53ef\u4fe1\u7cfb\u7edf\uff0c\u9700\u8981\u4e3a\u4eba\u7c7b\u548c\u673a\u5668\u63d0\u4f9b\u5bf9\u4fe1\u4efb\u7684\u6b63\u5f0f\u6982\u5ff5\u5316\uff0c\u4ee5\u652f\u6301\u4fe1\u606f\u5efa\u6a21\u3001\u81ea\u52a8\u63a8\u7406\u548c\u4fe1\u606f\u96c6\u6210\u7b49\u4efb\u52a1\u3002", "method": "\u5f00\u53d1\u4e86\u57fa\u4e8e\u7edf\u4e00\u57fa\u7840\u672c\u4f53(UFO)\u7684\u4fe1\u4efb\u53c2\u8003\u672c\u4f53(ONTrust)\uff0c\u4f7f\u7528OntoUML\u8fdb\u884c\u89c4\u8303\uff0c\u5e76\u5e94\u7528\u4e8e\u591a\u4e2a\u5b9e\u9645\u6848\u4f8b\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "ONTrust\u6b63\u5f0f\u5b9a\u4e49\u4e86\u4fe1\u4efb\u6982\u5ff5\u53ca\u5176\u4e0d\u540c\u7c7b\u578b\uff0c\u63cf\u8ff0\u4e86\u5f71\u54cd\u4fe1\u4efb\u7684\u5404\u79cd\u56e0\u7d20\uff0c\u89e3\u91ca\u4e86\u4fe1\u4efb\u5173\u7cfb\u4e2d\u98ce\u9669\u7684\u4ea7\u751f\u673a\u5236\uff0c\u5e76\u901a\u8fc7\u6587\u732e\u4e2d\u7684\u4e24\u4e2a\u6848\u4f8b\u7814\u7a76\u5c55\u793a\u4e86\u5176\u5e94\u7528\u6548\u679c\u3002", "conclusion": "ONTrust\u4e3a\u4fe1\u4efb\u63d0\u4f9b\u4e86\u4e00\u4e2a\u575a\u5b9e\u7684\u672c\u4f53\u57fa\u7840\uff0c\u80fd\u591f\u652f\u6301\u6982\u5ff5\u5efa\u6a21\u3001\u4f01\u4e1a\u67b6\u6784\u8bbe\u8ba1\u3001\u8bed\u8a00\u8bc4\u4f30\u3001\u4fe1\u4efb\u7ba1\u7406\u3001\u9700\u6c42\u5de5\u7a0b\u4ee5\u53ca\u53ef\u4fe1\u4eba\u5de5\u667a\u80fd\u7b49\u591a\u4e2a\u9886\u57df\u7684\u5e94\u7528\u3002"}}
{"id": "2602.07812", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.07812", "abs": "https://arxiv.org/abs/2602.07812", "authors": ["Fengting Yuchi", "Li Du", "Jason Eisner"], "title": "LLMs Know More About Numbers than They Can Say", "comment": "EACL 2026", "summary": "Although state-of-the-art LLMs can solve math problems, we find that they make errors on numerical comparisons with mixed notation: \"Which is larger, $5.7 \\times 10^2$ or $580$?\" This raises a fundamental question: Do LLMs even know how big these numbers are? We probe the hidden states of several smaller open-source LLMs. A single linear projection of an appropriate hidden layer encodes the log-magnitudes of both kinds of numerals, allowing us to recover the numbers with relative error of about 2.3% (on restricted synthetic text) or 19.06% (on scientific papers). Furthermore, the hidden state after reading a pair of numerals encodes their ranking, with a linear classifier achieving over 90% accuracy. Yet surprisingly, when explicitly asked to rank the same pairs of numerals, these LLMs achieve only 50-70% accuracy, with worse performance for models whose probes are less effective. Finally, we show that incorporating the classifier probe's log-loss as an auxiliary objective during finetuning brings an additional 3.22% improvement in verbalized accuracy over base models, demonstrating that improving models' internal magnitude representations can enhance their numerical reasoning capabilities.", "AI": {"tldr": "LLMs\u5728\u6df7\u5408\u8868\u793a\u7684\u6570\u5b57\u6bd4\u8f83\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u4f46\u9690\u85cf\u72b6\u6001\u7f16\u7801\u4e86\u6570\u5b57\u5927\u5c0f\u4fe1\u606f\uff0c\u901a\u8fc7\u7ebf\u6027\u63a2\u6d4b\u53ef\u63d0\u53d6\u6570\u5b57\u5927\u5c0f\u548c\u6392\u5e8f\uff0c\u4e14\u6539\u8fdb\u5185\u90e8\u6570\u503c\u8868\u793a\u80fd\u63d0\u5347\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u5c3d\u7ba1\u5148\u8fdbLLMs\u80fd\u89e3\u51b3\u6570\u5b66\u95ee\u9898\uff0c\u4f46\u7814\u7a76\u53d1\u73b0\u5b83\u4eec\u5728\u6df7\u5408\u8868\u793a\u7684\u6570\u5b57\u6bd4\u8f83\u4e2d\u4f1a\u51fa\u9519\uff08\u5982\"5.7\u00d710\u00b2 vs 580\"\uff09\uff0c\u8fd9\u5f15\u53d1\u4e86\u4e00\u4e2a\u57fa\u672c\u95ee\u9898\uff1aLLMs\u662f\u5426\u771f\u6b63\u7406\u89e3\u8fd9\u4e9b\u6570\u5b57\u7684\u5927\u5c0f\uff1f", "method": "\u5bf9\u591a\u4e2a\u5f00\u6e90LLMs\u7684\u9690\u85cf\u72b6\u6001\u8fdb\u884c\u63a2\u6d4b\uff0c\u4f7f\u7528\u7ebf\u6027\u6295\u5f71\u63d0\u53d6\u6570\u5b57\u7684\u5bf9\u6570\u5927\u5c0f\u4fe1\u606f\uff0c\u6784\u5efa\u7ebf\u6027\u5206\u7c7b\u5668\u5224\u65ad\u6570\u5b57\u6392\u5e8f\uff0c\u5e76\u5c06\u5206\u7c7b\u5668\u7684\u5bf9\u6570\u635f\u5931\u4f5c\u4e3a\u8f85\u52a9\u76ee\u6807\u8fdb\u884c\u5fae\u8c03\u3002", "result": "1) \u9002\u5f53\u9690\u85cf\u5c42\u7684\u7ebf\u6027\u6295\u5f71\u80fd\u7f16\u7801\u6570\u5b57\u7684\u5bf9\u6570\u5927\u5c0f\uff0c\u6062\u590d\u6570\u5b57\u7684\u76f8\u5bf9\u8bef\u5dee\u7ea62.3%\uff08\u53d7\u9650\u5408\u6210\u6587\u672c\uff09\u621619.06%\uff08\u79d1\u5b66\u8bba\u6587\uff09\uff1b2) \u9690\u85cf\u72b6\u6001\u7f16\u7801\u6570\u5b57\u6392\u5e8f\uff0c\u7ebf\u6027\u5206\u7c7b\u5668\u51c6\u786e\u7387\u8d8590%\uff1b3) \u4f46LLMs\u76f4\u63a5\u56de\u7b54\u6392\u5e8f\u65f6\u51c6\u786e\u7387\u4ec550-70%\uff1b4) \u5c06\u5206\u7c7b\u5668\u635f\u5931\u4f5c\u4e3a\u8f85\u52a9\u76ee\u6807\u5fae\u8c03\uff0c\u6bd4\u57fa\u7840\u6a21\u578b\u63d0\u53473.22%\u7684\u51c6\u786e\u7387\u3002", "conclusion": "LLMs\u7684\u9690\u85cf\u72b6\u6001\u786e\u5b9e\u7f16\u7801\u4e86\u6570\u5b57\u5927\u5c0f\u4fe1\u606f\uff0c\u4f46\u6a21\u578b\u65e0\u6cd5\u6709\u6548\u5229\u7528\u8fd9\u4e9b\u4fe1\u606f\u8fdb\u884c\u663e\u5f0f\u63a8\u7406\u3002\u901a\u8fc7\u6539\u8fdb\u5185\u90e8\u6570\u503c\u8868\u793a\u53ef\u4ee5\u589e\u5f3a\u6a21\u578b\u7684\u6570\u503c\u63a8\u7406\u80fd\u529b\uff0c\u8fd9\u4e3a\u63d0\u5347LLMs\u7684\u6570\u5b66\u80fd\u529b\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2602.07101", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.07101", "abs": "https://arxiv.org/abs/2602.07101", "authors": ["Zinan Lv", "Yeqian Qian", "Chen Sang", "Hao Liu", "Danping Zou", "Ming Yang"], "title": "Zero-Shot UAV Navigation in Forests via Relightable 3D Gaussian Splatting", "comment": "12 pages, 8 figures", "summary": "UAV navigation in unstructured outdoor environments using passive monocular vision is hindered by the substantial visual domain gap between simulation and reality. While 3D Gaussian Splatting enables photorealistic scene reconstruction from real-world data, existing methods inherently couple static lighting with geometry, severely limiting policy generalization to dynamic real-world illumination. In this paper, we propose a novel end-to-end reinforcement learning framework designed for effective zero-shot transfer to unstructured outdoors. Within a high-fidelity simulation grounded in real-world data, our policy is trained to map raw monocular RGB observations directly to continuous control commands. To overcome photometric limitations, we introduce Relightable 3D Gaussian Splatting, which decomposes scene components to enable explicit, physically grounded editing of environmental lighting within the neural representation. By augmenting training with diverse synthesized lighting conditions ranging from strong directional sunlight to diffuse overcast skies, we compel the policy to learn robust, illumination-invariant visual features. Extensive real-world experiments demonstrate that a lightweight quadrotor achieves robust, collision-free navigation in complex forest environments at speeds up to 10 m/s, exhibiting significant resilience to drastic lighting variations without fine-tuning.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u53ef\u91cd\u5149\u71673D\u9ad8\u65af\u6e85\u5c04\u7684\u7aef\u5230\u7aef\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u5b9e\u73b0\u65e0\u4eba\u673a\u5728\u975e\u7ed3\u6784\u5316\u5ba4\u5916\u73af\u5883\u7684\u96f6\u6837\u672c\u5bfc\u822a\uff0c\u514b\u670d\u4eff\u771f\u4e0e\u73b0\u5b9e\u95f4\u7684\u89c6\u89c9\u57df\u5dee\u8ddd\u548c\u5149\u7167\u53d8\u5316\u95ee\u9898\u3002", "motivation": "\u65e0\u4eba\u673a\u5728\u975e\u7ed3\u6784\u5316\u5ba4\u5916\u73af\u5883\u4f7f\u7528\u5355\u76ee\u89c6\u89c9\u5bfc\u822a\u9762\u4e34\u4eff\u771f\u4e0e\u73b0\u5b9e\u95f4\u7684\u89c6\u89c9\u57df\u5dee\u8ddd\u95ee\u9898\uff0c\u73b0\u67093D\u9ad8\u65af\u6e85\u5c04\u65b9\u6cd5\u5c06\u9759\u6001\u5149\u7167\u4e0e\u51e0\u4f55\u8026\u5408\uff0c\u9650\u5236\u4e86\u7b56\u7565\u5728\u52a8\u6001\u771f\u5b9e\u5149\u7167\u4e0b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u53ef\u91cd\u5149\u71673D\u9ad8\u65af\u6e85\u5c04\u6280\u672f\uff0c\u5206\u89e3\u573a\u666f\u7ec4\u4ef6\u5b9e\u73b0\u7269\u7406\u57fa\u7840\u7684\u5149\u7167\u7f16\u8f91\uff1b\u5728\u57fa\u4e8e\u771f\u5b9e\u6570\u636e\u7684\u9ad8\u4fdd\u771f\u4eff\u771f\u4e2d\u8bad\u7ec3\u7aef\u5230\u7aef\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\uff0c\u901a\u8fc7\u591a\u6837\u5408\u6210\u5149\u7167\u6761\u4ef6\u589e\u5f3a\u8bad\u7ec3\uff0c\u4f7f\u7b56\u7565\u5b66\u4e60\u5149\u7167\u4e0d\u53d8\u7684\u89c6\u89c9\u7279\u5f81\u3002", "result": "\u8f7b\u91cf\u7ea7\u56db\u65cb\u7ffc\u5728\u590d\u6742\u68ee\u6797\u73af\u5883\u4e2d\u5b9e\u73b0\u9ad8\u8fbe10 m/s\u7684\u9c81\u68d2\u65e0\u78b0\u649e\u5bfc\u822a\uff0c\u5bf9\u5267\u70c8\u5149\u7167\u53d8\u5316\u8868\u73b0\u51fa\u663e\u8457\u97e7\u6027\uff0c\u65e0\u9700\u5fae\u8c03\u5373\u53ef\u96f6\u6837\u672c\u8fc1\u79fb\u5230\u771f\u5b9e\u4e16\u754c\u3002", "conclusion": "\u901a\u8fc7\u53ef\u91cd\u5149\u7167\u795e\u7ecf\u8868\u793a\u548c\u5149\u7167\u589e\u5f3a\u8bad\u7ec3\uff0c\u5b9e\u73b0\u4e86\u65e0\u4eba\u673a\u5728\u975e\u7ed3\u6784\u5316\u5ba4\u5916\u73af\u5883\u7684\u9c81\u68d2\u89c6\u89c9\u5bfc\u822a\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4eff\u771f\u5230\u73b0\u5b9e\u7684\u89c6\u89c9\u57df\u5dee\u8ddd\u548c\u5149\u7167\u53d8\u5316\u6311\u6218\u3002"}}
{"id": "2602.07695", "categories": ["cs.AI", "cs.CL", "cs.IR", "cs.MM"], "pdf": "https://arxiv.org/pdf/2602.07695", "abs": "https://arxiv.org/abs/2602.07695", "authors": ["Congcong Hu", "Yuang Shi", "Fan Huang", "Yang Xiang", "Zhou Ye", "Ming Jin", "Shiyu Wang"], "title": "EventCast: Hybrid Demand Forecasting in E-Commerce with LLM-Based Event Knowledge", "comment": null, "summary": "Demand forecasting is a cornerstone of e-commerce operations, directly impacting inventory planning and fulfillment scheduling. However, existing forecasting systems often fail during high-impact periods such as flash sales, holiday campaigns, and sudden policy interventions, where demand patterns shift abruptly and unpredictably. In this paper, we introduce EventCast, a modular forecasting framework that integrates future event knowledge into time-series prediction. Unlike prior approaches that ignore future interventions or directly use large language models (LLMs) for numerical forecasting, EventCast leverages LLMs solely for event-driven reasoning. Unstructured business data, which covers campaigns, holiday schedules, and seller incentives, from existing operational databases, is processed by an LLM that converts it into interpretable textual summaries leveraging world knowledge for cultural nuances and novel event combinations. These summaries are fused with historical demand features within a dual-tower architecture, enabling accurate, explainable, and scalable forecasts. Deployed on real-world e-commerce scenarios spanning 4 countries of 160 regions over 10 months, EventCast achieves up to 86.9% and 97.7% improvement on MAE and MSE compared to the variant without event knowledge, and reduces MAE by up to 57.0% and MSE by 83.3% versus the best industrial baseline during event-driven periods. EventCast has deployed into real-world industrial pipelines since March 2025, offering a practical solution for improving operational decision-making in dynamic e-commerce environments.", "AI": {"tldr": "EventCast\u662f\u4e00\u4e2a\u5c06\u672a\u6765\u4e8b\u4ef6\u77e5\u8bc6\u96c6\u6210\u5230\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u7684\u6a21\u5757\u5316\u6846\u67b6\uff0c\u4e13\u95e8\u89e3\u51b3\u7535\u5546\u5728\u95ea\u8d2d\u3001\u8282\u5047\u65e5\u7b49\u7a81\u53d1\u4e8b\u4ef6\u671f\u95f4\u7684\u9700\u6c42\u9884\u6d4b\u95ee\u9898\uff0c\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u9884\u6d4b\u7cbe\u5ea6\u3002", "motivation": "\u73b0\u6709\u7535\u5546\u9700\u6c42\u9884\u6d4b\u7cfb\u7edf\u5728\u9ad8\u5f71\u54cd\u65f6\u671f\uff08\u5982\u95ea\u8d2d\u3001\u8282\u5047\u65e5\u4fc3\u9500\u3001\u653f\u7b56\u5e72\u9884\uff09\u5f80\u5f80\u5931\u6548\uff0c\u56e0\u4e3a\u8fd9\u4e9b\u65f6\u671f\u9700\u6c42\u6a21\u5f0f\u4f1a\u53d1\u751f\u7a81\u7136\u4e14\u4e0d\u53ef\u9884\u6d4b\u7684\u53d8\u5316\u3002\u4f20\u7edf\u65b9\u6cd5\u8981\u4e48\u5ffd\u7565\u672a\u6765\u5e72\u9884\uff0c\u8981\u4e48\u76f4\u63a5\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u6570\u503c\u9884\u6d4b\uff0c\u90fd\u5b58\u5728\u5c40\u9650\u6027\u3002", "method": "EventCast\u91c7\u7528\u6a21\u5757\u5316\u6846\u67b6\uff0c\u5229\u7528LLM\u4e13\u95e8\u8fdb\u884c\u4e8b\u4ef6\u9a71\u52a8\u63a8\u7406\u3002\u9996\u5148\u4ece\u8fd0\u8425\u6570\u636e\u5e93\u4e2d\u63d0\u53d6\u975e\u7ed3\u6784\u5316\u4e1a\u52a1\u6570\u636e\uff08\u5982\u8425\u9500\u6d3b\u52a8\u3001\u8282\u5047\u65e5\u5b89\u6392\u3001\u5356\u5bb6\u6fc0\u52b1\uff09\uff0c\u901a\u8fc7LLM\u5c06\u5176\u8f6c\u6362\u4e3a\u53ef\u89e3\u91ca\u7684\u6587\u672c\u6458\u8981\uff0c\u540c\u65f6\u5229\u7528\u4e16\u754c\u77e5\u8bc6\u5904\u7406\u6587\u5316\u5dee\u5f02\u548c\u65b0\u4e8b\u4ef6\u7ec4\u5408\u3002\u7136\u540e\u5c06\u8fd9\u4e9b\u6458\u8981\u4e0e\u5386\u53f2\u9700\u6c42\u7279\u5f81\u5728\u53cc\u5854\u67b6\u6784\u4e2d\u878d\u5408\uff0c\u5b9e\u73b0\u51c6\u786e\u3001\u53ef\u89e3\u91ca\u4e14\u53ef\u6269\u5c55\u7684\u9884\u6d4b\u3002", "result": "\u57284\u4e2a\u56fd\u5bb6160\u4e2a\u5730\u533a\u8d85\u8fc710\u4e2a\u6708\u7684\u771f\u5b9e\u7535\u5546\u573a\u666f\u90e8\u7f72\u4e2d\uff0cEventCast\u76f8\u6bd4\u65e0\u4e8b\u4ef6\u77e5\u8bc6\u7684\u53d8\u4f53\u5728MAE\u548cMSE\u4e0a\u5206\u522b\u63d0\u5347\u4e8686.9%\u548c97.7%\uff1b\u5728\u4e8b\u4ef6\u9a71\u52a8\u671f\u95f4\uff0c\u76f8\u6bd4\u6700\u4f73\u5de5\u4e1a\u57fa\u7ebf\uff0cMAE\u964d\u4f4e\u4e8657.0%\uff0cMSE\u964d\u4f4e\u4e8683.3%\u3002\u81ea2025\u5e743\u6708\u8d77\u5df2\u90e8\u7f72\u5230\u5b9e\u9645\u5de5\u4e1a\u7ba1\u9053\u4e2d\u3002", "conclusion": "EventCast\u901a\u8fc7\u5c06LLM\u4e13\u95e8\u7528\u4e8e\u4e8b\u4ef6\u9a71\u52a8\u63a8\u7406\uff0c\u800c\u975e\u76f4\u63a5\u8fdb\u884c\u6570\u503c\u9884\u6d4b\uff0c\u6210\u529f\u6574\u5408\u4e86\u672a\u6765\u4e8b\u4ef6\u77e5\u8bc6\uff0c\u4e3a\u52a8\u6001\u7535\u5546\u73af\u5883\u4e2d\u7684\u8fd0\u8425\u51b3\u7b56\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9ad8\u5f71\u54cd\u65f6\u671f\u7684\u9700\u6c42\u9884\u6d4b\u51c6\u786e\u6027\u3002"}}
{"id": "2602.08263", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.08263", "abs": "https://arxiv.org/abs/2602.08263", "authors": ["Taohong Zhu", "Lucas C. Cordeiro", "Mustafa A. Mustafa", "Youcheng Sun"], "title": "Specification Vibing for Automated Program Repair", "comment": null, "summary": "Large language model (LLM)-driven automated program repair (APR) has advanced rapidly, but most methods remain code-centric: they directly rewrite source code and thereby risk hallucinated, behaviorally inconsistent fixes. This limitation suggests the need for an alternative repair paradigm that relies on a representation more accessible to LLMs than raw code, enabling more accurate understanding, analysis, and alignment during repair. To address this gap, we propose VibeRepair, a specification-centric APR technique that treats repair as behavior-specification repair rather than ad-hoc code editing. VibeRepair first translates buggy code into a structured behavior specification that captures the program's intended runtime behavior, then infers and repairs specification misalignments, and finally synthesizes code strictly guided by the corrected behavior specification. An on-demand reasoning component enriches hard cases with program analysis and historical bug-fix evidence while controlling cost. Across Defects4J and real-world benchmarks and multiple LLMs, VibeRepair demonstrates consistently strong repair effectiveness with a significantly smaller patch space. On Defects4J v1.2, VibeRepair correctly repairs 174 bugs, exceeding the strongest state-of-the-art baseline by 28 bugs, which corresponds to a 19% improvement. On Defects4J v2.0, it repairs 178 bugs, outperforming prior approaches by 33 bugs, representing a 23% improvement. Evaluations on real-world benchmarks collected after the training period of selected LLMs further confirm its effectiveness and generalizability. By centering repair on explicit behavioral intent, VibeRepair reframes APR for the era of \"vibe\" coding: make the behavior sing, and the code will follow.", "AI": {"tldr": "VibeRepair\u662f\u4e00\u79cd\u57fa\u4e8e\u884c\u4e3a\u89c4\u8303\u7684\u81ea\u52a8\u7a0b\u5e8f\u4fee\u590d\u65b9\u6cd5\uff0c\u5c06\u4fee\u590d\u8fc7\u7a0b\u4ece\u76f4\u63a5\u4fee\u6539\u4ee3\u7801\u8f6c\u53d8\u4e3a\u4fee\u590d\u884c\u4e3a\u89c4\u8303\uff0c\u901a\u8fc7LLM\u751f\u6210\u66f4\u51c6\u786e\u3001\u884c\u4e3a\u4e00\u81f4\u7684\u4fee\u590d\u65b9\u6848\u3002", "motivation": "\u73b0\u6709LLM\u9a71\u52a8\u7684\u81ea\u52a8\u7a0b\u5e8f\u4fee\u590d\u65b9\u6cd5\u5927\u591a\u662f\u4ee3\u7801\u4e2d\u5fc3\u7684\uff0c\u76f4\u63a5\u91cd\u5199\u6e90\u4ee3\u7801\u53ef\u80fd\u5bfc\u81f4\u5e7b\u89c9\u4fee\u590d\u548c\u884c\u4e3a\u4e0d\u4e00\u81f4\u3002\u9700\u8981\u4e00\u79cd\u66f4\u6613\u88abLLM\u7406\u89e3\u7684\u8868\u793a\u5f62\u5f0f\uff0c\u4ee5\u5b9e\u73b0\u66f4\u51c6\u786e\u7684\u7406\u89e3\u3001\u5206\u6790\u548c\u4fee\u590d\u5bf9\u9f50\u3002", "method": "VibeRepair\u91c7\u7528\u89c4\u8303\u4e2d\u5fc3\u7684\u4fee\u590d\u8303\u5f0f\uff1a1) \u5c06\u9519\u8bef\u4ee3\u7801\u8f6c\u6362\u4e3a\u7ed3\u6784\u5316\u884c\u4e3a\u89c4\u8303\uff1b2) \u63a8\u65ad\u5e76\u4fee\u590d\u89c4\u8303\u504f\u5dee\uff1b3) \u5728\u4fee\u6b63\u540e\u7684\u884c\u4e3a\u89c4\u8303\u4e25\u683c\u6307\u5bfc\u4e0b\u5408\u6210\u4ee3\u7801\uff1b4) \u6309\u9700\u63a8\u7406\u7ec4\u4ef6\u901a\u8fc7\u7a0b\u5e8f\u5206\u6790\u548c\u5386\u53f2\u4fee\u590d\u8bc1\u636e\u589e\u5f3a\u56f0\u96be\u6848\u4f8b\u5904\u7406\u3002", "result": "\u5728Defects4J v1.2\u4e0a\u6b63\u786e\u4fee\u590d174\u4e2abug\uff0c\u6bd4\u6700\u5f3a\u57fa\u7ebf\u591a28\u4e2a\uff08\u63d0\u534719%\uff09\uff1b\u5728Defects4J v2.0\u4e0a\u4fee\u590d178\u4e2abug\uff0c\u6bd4\u5148\u524d\u65b9\u6cd5\u591a33\u4e2a\uff08\u63d0\u534723%\uff09\u3002\u5728\u8bad\u7ec3\u671f\u540e\u6536\u96c6\u7684\u771f\u5b9e\u4e16\u754c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4e5f\u8868\u73b0\u51fa\u6709\u6548\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u901a\u8fc7\u5c06\u4fee\u590d\u4e2d\u5fc3\u4ece\u4ee3\u7801\u8f6c\u79fb\u5230\u660e\u786e\u7684\u884c\u4e3a\u610f\u56fe\uff0cVibeRepair\u4e3a\"\u6c1b\u56f4\u7f16\u7801\"\u65f6\u4ee3\u91cd\u65b0\u5b9a\u4e49\u4e86\u81ea\u52a8\u7a0b\u5e8f\u4fee\u590d\uff1a\u8ba9\u884c\u4e3a\u5148\u884c\uff0c\u4ee3\u7801\u81ea\u7136\u8ddf\u968f\u3002"}}
{"id": "2602.08993", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2602.08993", "abs": "https://arxiv.org/abs/2602.08993", "authors": ["Eloise Christian", "Tejas Gadwalkar", "Arthur Azevedo de Amorim", "Edward V. Zieglar"], "title": "Reverse Online Guessing Attacks on PAKE Protocols", "comment": null, "summary": "Though not yet widely deployed, password-authenticated key exchange (PAKE) protocols have been the subject of several recent standardization efforts, partly because of their resistance against various guessing attacks, but also because they do not require a public-key infrastructure (PKI), making them naturally resistant against PKI failures. The goal of this paper is to reevaluate the PAKE model by noting that the absence of a PKI -- or, more generally, of a mechanism aside from the password for authenticating the server -- makes such protocols vulnerable to reverse online guessing attacks, in which an adversary attempts to validate password guesses by impersonating a server. While their logic is similar to traditional guessing, where the attacker impersonates a client, reverse guessing poses a unique risk because the burden of detection is shifted to the clients, rendering existing defenses against traditional guessing moot. Our results demonstrate that reverse guessing is particularly effective when an adversary attacks clients indiscriminately, such as in phishing or password-spraying attacks, or for applications with automated login processes or a universal password, such as WPA3-SAE. Our analysis suggests that stakeholders should, by default, authenticate the server using more stringent measures than just the user's password, and that a password-only mode of operation should be a last resort against catastrophic security failures when other authentication mechanisms are not available.", "AI": {"tldr": "\u8bba\u6587\u91cd\u65b0\u8bc4\u4f30\u4e86PAKE\u534f\u8bae\u6a21\u578b\uff0c\u6307\u51fa\u7f3a\u4e4fPKI\u6216\u670d\u52a1\u5668\u8ba4\u8bc1\u673a\u5236\u4f7f\u5f97\u534f\u8bae\u6613\u53d7\u53cd\u5411\u5728\u7ebf\u731c\u6d4b\u653b\u51fb\uff0c\u653b\u51fb\u8005\u901a\u8fc7\u5192\u5145\u670d\u52a1\u5668\u6765\u9a8c\u8bc1\u5bc6\u7801\u731c\u6d4b\uff0c\u8fd9\u79cd\u653b\u51fb\u5728\u9493\u9c7c\u3001\u5bc6\u7801\u55b7\u6d12\u7b49\u573a\u666f\u4e2d\u7279\u522b\u6709\u6548\u3002", "motivation": "\u5c3d\u7ba1PAKE\u534f\u8bae\u56e0\u62b5\u6297\u5404\u79cd\u731c\u6d4b\u653b\u51fb\u4e14\u4e0d\u9700\u8981PKI\u800c\u53d7\u5230\u6807\u51c6\u5316\u5173\u6ce8\uff0c\u4f46\u4f5c\u8005\u53d1\u73b0\u7f3a\u4e4fPKI\u6216\u670d\u52a1\u5668\u8ba4\u8bc1\u673a\u5236\u5b9e\u9645\u4e0a\u4f7f\u534f\u8bae\u5bb9\u6613\u53d7\u5230\u53cd\u5411\u5728\u7ebf\u731c\u6d4b\u653b\u51fb\uff0c\u8fd9\u79cd\u653b\u51fb\u5c06\u68c0\u6d4b\u8d1f\u62c5\u8f6c\u79fb\u5230\u5ba2\u6237\u7aef\uff0c\u73b0\u6709\u9632\u5fa1\u673a\u5236\u5931\u6548\u3002", "method": "\u901a\u8fc7\u91cd\u65b0\u8bc4\u4f30PAKE\u6a21\u578b\uff0c\u5206\u6790\u53cd\u5411\u5728\u7ebf\u731c\u6d4b\u653b\u51fb\u7684\u903b\u8f91\u548c\u98ce\u9669\uff0c\u7279\u522b\u5173\u6ce8\u653b\u51fb\u8005\u5192\u5145\u670d\u52a1\u5668\u9a8c\u8bc1\u5bc6\u7801\u731c\u6d4b\u7684\u573a\u666f\uff0c\u4ee5\u53ca\u8fd9\u79cd\u653b\u51fb\u5728\u9493\u9c7c\u3001\u5bc6\u7801\u55b7\u6d12\u3001\u81ea\u52a8\u5316\u767b\u5f55\u6d41\u7a0b\u7b49\u573a\u666f\u4e2d\u7684\u6709\u6548\u6027\u3002", "result": "\u7814\u7a76\u8868\u660e\u53cd\u5411\u731c\u6d4b\u653b\u51fb\u5728\u653b\u51fb\u8005\u65e0\u5dee\u522b\u653b\u51fb\u5ba2\u6237\u7aef\u65f6\u7279\u522b\u6709\u6548\uff0c\u5982\u9493\u9c7c\u653b\u51fb\u6216\u5bc6\u7801\u55b7\u6d12\u653b\u51fb\uff0c\u6216\u7528\u4e8e\u5177\u6709\u81ea\u52a8\u5316\u767b\u5f55\u6d41\u7a0b\u6216\u901a\u7528\u5bc6\u7801\u7684\u5e94\u7528\uff08\u5982WPA3-SAE\uff09\u3002", "conclusion": "\u5229\u76ca\u76f8\u5173\u8005\u5e94\u9ed8\u8ba4\u4f7f\u7528\u6bd4\u7528\u6237\u5bc6\u7801\u66f4\u4e25\u683c\u7684\u63aa\u65bd\u6765\u8ba4\u8bc1\u670d\u52a1\u5668\uff0c\u4ec5\u5bc6\u7801\u6a21\u5f0f\u5e94\u4f5c\u4e3a\u5176\u4ed6\u8ba4\u8bc1\u673a\u5236\u4e0d\u53ef\u7528\u65f6\u7684\u6700\u540e\u624b\u6bb5\uff0c\u4ee5\u5e94\u5bf9\u707e\u96be\u6027\u5b89\u5168\u6545\u969c\u3002"}}
{"id": "2602.07839", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.07839", "abs": "https://arxiv.org/abs/2602.07839", "authors": ["Jiaxi Liu", "Yanzuo Jiang", "Guibin Zhang", "Zihan Zhang", "Heng Chang", "Zhenfei Yin", "Qibing Ren", "Junchi Yan"], "title": "TodoEvolve: Learning to Architect Agent Planning Systems", "comment": null, "summary": "Planning has become a central capability for contemporary agent systems in navigating complex, long-horizon tasks, yet existing approaches predominantly rely on fixed, hand-crafted planning structures that lack the flexibility to adapt to the structural diversity of open-ended problems. To address this limitation, we introduce TodoEvolve, a meta-planning paradigm that autonomously synthesizes and dynamically revises task-specific planning architectures. Specifically, we first construct PlanFactory, a modular design space that standardizes diverse planning paradigms within a unified codebase encompassing topology, initialization, adaptation, and navigation, thereby providing a common interface for heterogeneous planning patterns. Leveraging PlanFactory, we collect high-quality planning trajectories and train Todo-14B via \\textit{Impedance-Guided Preference Optimization} (IGPO), a multi-objective reinforcement learning objective that encourages the generation of planning systems that are performant, stable, and token-efficient across arbitrary tasks and agent backbones. Empirical evaluations on five agentic benchmarks demonstrate that TodoEvolve consistently surpasses carefully engineered planning modules while maintaining economical API costs and runtime overhead.", "AI": {"tldr": "TodoEvolve\u662f\u4e00\u4e2a\u5143\u89c4\u5212\u8303\u5f0f\uff0c\u80fd\u81ea\u4e3b\u5408\u6210\u5e76\u52a8\u6001\u4fee\u8ba2\u4efb\u52a1\u7279\u5b9a\u7684\u89c4\u5212\u67b6\u6784\uff0c\u901a\u8fc7\u7edf\u4e00\u7684PlanFactory\u8bbe\u8ba1\u7a7a\u95f4\u548cIGPO\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5728\u591a\u4e2a\u667a\u80fd\u4f53\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u4e86\u624b\u5de5\u8bbe\u8ba1\u7684\u89c4\u5212\u6a21\u5757\u3002", "motivation": "\u73b0\u6709\u667a\u80fd\u4f53\u7cfb\u7edf\u4e3b\u8981\u4f9d\u8d56\u56fa\u5b9a\u7684\u624b\u5de5\u89c4\u5212\u7ed3\u6784\uff0c\u7f3a\u4e4f\u9002\u5e94\u5f00\u653e\u6027\u95ee\u9898\u7ed3\u6784\u591a\u6837\u6027\u7684\u7075\u6d3b\u6027\uff0c\u9700\u8981\u66f4\u81ea\u9002\u5e94\u7684\u89c4\u5212\u80fd\u529b\u3002", "method": "1. \u6784\u5efaPlanFactory\u6a21\u5757\u5316\u8bbe\u8ba1\u7a7a\u95f4\uff0c\u7edf\u4e00\u6807\u51c6\u5316\u4e0d\u540c\u89c4\u5212\u8303\u5f0f\uff1b2. \u6536\u96c6\u9ad8\u8d28\u91cf\u89c4\u5212\u8f68\u8ff9\uff1b3. \u901a\u8fc7\u963b\u6297\u5f15\u5bfc\u504f\u597d\u4f18\u5316(IGPO)\u8bad\u7ec3Todo-14B\u6a21\u578b\uff0c\u8fd9\u662f\u4e00\u4e2a\u591a\u76ee\u6807\u5f3a\u5316\u5b66\u4e60\u76ee\u6807\uff0c\u9f13\u52b1\u751f\u6210\u6027\u80fd\u597d\u3001\u7a33\u5b9a\u4e14token\u9ad8\u6548\u7684\u89c4\u5212\u7cfb\u7edf\u3002", "result": "\u5728\u4e94\u4e2a\u667a\u80fd\u4f53\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cTodoEvolve\u59cb\u7ec8\u8d85\u8d8a\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u89c4\u5212\u6a21\u5757\uff0c\u540c\u65f6\u4fdd\u6301\u7ecf\u6d4e\u7684API\u6210\u672c\u548c\u8fd0\u884c\u65f6\u5f00\u9500\u3002", "conclusion": "TodoEvolve\u901a\u8fc7\u5143\u89c4\u5212\u8303\u5f0f\u6210\u529f\u89e3\u51b3\u4e86\u73b0\u6709\u89c4\u5212\u7cfb\u7edf\u7f3a\u4e4f\u7075\u6d3b\u6027\u7684\u95ee\u9898\uff0c\u80fd\u591f\u81ea\u4e3b\u9002\u5e94\u4e0d\u540c\u4efb\u52a1\u7ed3\u6784\uff0c\u4e3a\u667a\u80fd\u4f53\u7cfb\u7edf\u63d0\u4f9b\u4e86\u66f4\u5f3a\u5927\u7684\u89c4\u5212\u80fd\u529b\u3002"}}
{"id": "2602.07749", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07749", "abs": "https://arxiv.org/abs/2602.07749", "authors": ["Zhenyu Wu", "Yanxi Long", "Jian Li", "Hua Huang"], "title": "Geo-Code: A Code Framework for Reverse Code Generation from Geometric Images Based on Two-Stage Multi-Agent Evolution", "comment": "ICML2026", "summary": "Program code serves as a bridge linking vision and logic, providing a feasible supervisory approach for enhancing the multimodal reasoning capability of large models through geometric operations such as auxiliary line construction and perspective transformation. Nevertheless, current inverse graphics methods face tremendous challenges in accurately reconstructing complex geometric details, which often results in the loss of key geometric constraints or structural distortion. To address this bottleneck, we propose Geo-coder -- the first inverse programming framework for geometric images based on a multi-agent system. Our method innovatively decouples the process into geometric modeling via pixel-wise anchoring and metric-driven code evolution: Stage 1 leverages the complementary advantages of visual operators and large models to achieve precise capture of pixel coordinates and visual attributes; Stage 2 introduces a synthesis-rendering-validation closed loop, where bidirectional visual feedback drives the self-correction of code. Extensive experiments demonstrate that Geo-coder achieves a substantial lead in both geometric reconstruction accuracy and visual consistency. Notably, by effectively preserving the core geometric semantics, the images reconstructed with our method exhibit equivalent performance to the original ones in multimodal reasoning tasks, which fully validates the robustness of the framework. Finally, to further reduce research costs, we have open-sourced the Geo-coder dataset constructed on the GeoCode framework, which contains more than 1,500 samples. On this basis, we have also open-sourced the GeocodeLM model, laying a solid data and model foundation for subsequent research in this field.", "AI": {"tldr": "Geo-coder\uff1a\u9996\u4e2a\u57fa\u4e8e\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u51e0\u4f55\u56fe\u50cf\u9006\u5411\u7f16\u7a0b\u6846\u67b6\uff0c\u901a\u8fc7\u50cf\u7d20\u7ea7\u951a\u5b9a\u548c\u5ea6\u91cf\u9a71\u52a8\u4ee3\u7801\u6f14\u5316\uff0c\u5b9e\u73b0\u590d\u6742\u51e0\u4f55\u7ec6\u8282\u7684\u7cbe\u786e\u91cd\u5efa\uff0c\u5728\u51e0\u4f55\u91cd\u5efa\u7cbe\u5ea6\u548c\u89c6\u89c9\u4e00\u81f4\u6027\u65b9\u9762\u663e\u8457\u9886\u5148\u3002", "motivation": "\u7a0b\u5e8f\u4ee3\u7801\u4f5c\u4e3a\u8fde\u63a5\u89c6\u89c9\u4e0e\u903b\u8f91\u7684\u6865\u6881\uff0c\u4e3a\u901a\u8fc7\u51e0\u4f55\u64cd\u4f5c\uff08\u5982\u8f85\u52a9\u7ebf\u6784\u5efa\u548c\u900f\u89c6\u53d8\u6362\uff09\u589e\u5f3a\u5927\u6a21\u578b\u591a\u6a21\u6001\u63a8\u7406\u80fd\u529b\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u76d1\u7763\u65b9\u6cd5\u3002\u7136\u800c\uff0c\u5f53\u524d\u9006\u5411\u56fe\u5f62\u65b9\u6cd5\u5728\u51c6\u786e\u91cd\u5efa\u590d\u6742\u51e0\u4f55\u7ec6\u8282\u65b9\u9762\u9762\u4e34\u5de8\u5927\u6311\u6218\uff0c\u5e38\u5bfc\u81f4\u5173\u952e\u51e0\u4f55\u7ea6\u675f\u4e22\u5931\u6216\u7ed3\u6784\u5931\u771f\u3002", "method": "\u63d0\u51faGeo-coder\u2014\u2014\u9996\u4e2a\u57fa\u4e8e\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u51e0\u4f55\u56fe\u50cf\u9006\u5411\u7f16\u7a0b\u6846\u67b6\u3002\u521b\u65b0\u6027\u5730\u5c06\u8fc7\u7a0b\u89e3\u8026\u4e3a\uff1a1\uff09\u901a\u8fc7\u50cf\u7d20\u7ea7\u951a\u5b9a\u8fdb\u884c\u51e0\u4f55\u5efa\u6a21\uff0c\u5229\u7528\u89c6\u89c9\u7b97\u5b50\u548c\u5927\u6a21\u578b\u7684\u4e92\u8865\u4f18\u52bf\u7cbe\u786e\u6355\u6349\u50cf\u7d20\u5750\u6807\u548c\u89c6\u89c9\u5c5e\u6027\uff1b2\uff09\u5ea6\u91cf\u9a71\u52a8\u4ee3\u7801\u6f14\u5316\uff0c\u5f15\u5165\u5408\u6210-\u6e32\u67d3-\u9a8c\u8bc1\u95ed\u73af\uff0c\u901a\u8fc7\u53cc\u5411\u89c6\u89c9\u53cd\u9988\u9a71\u52a8\u4ee3\u7801\u81ea\u6821\u6b63\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cGeo-coder\u5728\u51e0\u4f55\u91cd\u5efa\u7cbe\u5ea6\u548c\u89c6\u89c9\u4e00\u81f4\u6027\u65b9\u9762\u53d6\u5f97\u663e\u8457\u9886\u5148\u3002\u901a\u8fc7\u6709\u6548\u4fdd\u7559\u6838\u5fc3\u51e0\u4f55\u8bed\u4e49\uff0c\u91cd\u5efa\u56fe\u50cf\u5728\u591a\u6a21\u6001\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u4e0e\u539f\u59cb\u56fe\u50cf\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u5145\u5206\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u9c81\u68d2\u6027\u3002\u5f00\u6e90\u4e86\u5305\u542b1500\u591a\u4e2a\u6837\u672c\u7684Geo-coder\u6570\u636e\u96c6\u548cGeocodeLM\u6a21\u578b\u3002", "conclusion": "Geo-coder\u6210\u529f\u89e3\u51b3\u4e86\u590d\u6742\u51e0\u4f55\u7ec6\u8282\u91cd\u5efa\u7684\u74f6\u9888\u95ee\u9898\uff0c\u4e3a\u540e\u7eed\u7814\u7a76\u63d0\u4f9b\u4e86\u575a\u5b9e\u7684\u6570\u636e\u548c\u6a21\u578b\u57fa\u7840\u3002\u8be5\u6846\u67b6\u901a\u8fc7\u9006\u5411\u7f16\u7a0b\u65b9\u6cd5\u5b9e\u73b0\u4e86\u51e0\u4f55\u56fe\u50cf\u7684\u7cbe\u786e\u91cd\u5efa\uff0c\u5728\u591a\u6a21\u6001\u63a8\u7406\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2602.07842", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.07842", "abs": "https://arxiv.org/abs/2602.07842", "authors": ["Yuhan Wang", "Shiyu Ni", "Zhikai Ding", "Zihang Zhan", "Yuanzi Li", "Keping Bi"], "title": "Evaluating and Calibrating LLM Confidence on Questions with Multiple Correct Answers", "comment": null, "summary": "Confidence calibration is essential for making large language models (LLMs) reliable, yet existing training-free methods have been primarily studied under single-answer question answering. In this paper, we show that these methods break down in the presence of multiple valid answers, where disagreement among equally correct responses leads to systematic underestimation of confidence. To enable a systematic study of this phenomenon, we introduce MACE, a benchmark of 12,000 factual questions spanning six domains with varying numbers of correct answers. Experiments across 15 representative calibration methods and four LLM families (7B-72B) reveal that while accuracy increases with answer cardinality, estimated confidence consistently decreases, causing severe miscalibration for questions with mixed answer counts. To address this issue, we propose Semantic Confidence Aggregation (SCA), which aggregates confidence over multiple high-probability sampled responses. SCA achieves state-of-the-art calibration performance under mixed-answer settings while preserving strong calibration on single-answer questions.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u591a\u7b54\u6848\u573a\u666f\u4e0b\u7684\u7f6e\u4fe1\u5ea6\u6821\u51c6\u95ee\u9898\uff0c\u53d1\u73b0\u73b0\u6709\u65b9\u6cd5\u5728\u591a\u6b63\u786e\u7b54\u6848\u60c5\u51b5\u4e0b\u4f1a\u7cfb\u7edf\u6027\u5730\u4f4e\u4f30\u7f6e\u4fe1\u5ea6\uff0c\u63d0\u51fa\u4e86\u65b0\u7684\u57fa\u51c6MACE\u548c\u89e3\u51b3\u65b9\u6848SCA\u3002", "motivation": "\u73b0\u6709\u7f6e\u4fe1\u5ea6\u6821\u51c6\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u5355\u7b54\u6848\u95ee\u7b54\u573a\u666f\u7814\u7a76\uff0c\u4f46\u5728\u5b58\u5728\u591a\u4e2a\u6709\u6548\u7b54\u6848\u7684\u60c5\u51b5\u4e0b\u4f1a\u5931\u6548\uff0c\u56e0\u4e3a\u6b63\u786e\u7b54\u6848\u4e4b\u95f4\u7684\u5206\u6b67\u4f1a\u5bfc\u81f4\u7f6e\u4fe1\u5ea6\u88ab\u7cfb\u7edf\u6027\u4f4e\u4f30\u3002", "method": "1. \u5f15\u5165MACE\u57fa\u51c6\uff0c\u5305\u542b12,000\u4e2a\u4e8b\u5b9e\u6027\u95ee\u9898\uff0c\u6db5\u76d6\u516d\u4e2a\u9886\u57df\uff0c\u5177\u6709\u4e0d\u540c\u6570\u91cf\u7684\u6b63\u786e\u7b54\u6848\uff1b2. \u63d0\u51fa\u8bed\u4e49\u7f6e\u4fe1\u5ea6\u805a\u5408\uff08SCA\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u5bf9\u591a\u4e2a\u9ad8\u6982\u7387\u91c7\u6837\u54cd\u5e94\u7684\u7f6e\u4fe1\u5ea6\u8fdb\u884c\u805a\u5408\u6765\u89e3\u51b3\u591a\u7b54\u6848\u6821\u51c6\u95ee\u9898\u3002", "result": "\u5b9e\u9a8c\u6d89\u53ca15\u79cd\u4ee3\u8868\u6027\u6821\u51c6\u65b9\u6cd5\u548c4\u4e2aLLM\u5bb6\u65cf\uff087B-72B\uff09\uff0c\u7ed3\u679c\u663e\u793a\uff1a\u867d\u7136\u51c6\u786e\u7387\u968f\u7b54\u6848\u6570\u91cf\u589e\u52a0\u800c\u63d0\u9ad8\uff0c\u4f46\u4f30\u8ba1\u7684\u7f6e\u4fe1\u5ea6\u5374\u6301\u7eed\u4e0b\u964d\uff0c\u5bfc\u81f4\u6df7\u5408\u7b54\u6848\u6570\u91cf\u95ee\u9898\u7684\u4e25\u91cd\u6821\u51c6\u9519\u8bef\u3002SCA\u5728\u6df7\u5408\u7b54\u6848\u8bbe\u7f6e\u4e0b\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6821\u51c6\u6027\u80fd\uff0c\u540c\u65f6\u5728\u5355\u7b54\u6848\u95ee\u9898\u4e0a\u4fdd\u6301\u4e86\u5f3a\u6821\u51c6\u80fd\u529b\u3002", "conclusion": "\u591a\u7b54\u6848\u573a\u666f\u5bf9LLM\u7f6e\u4fe1\u5ea6\u6821\u51c6\u63d0\u51fa\u4e86\u65b0\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u6b64\u60c5\u51b5\u4e0b\u4f1a\u5931\u6548\u3002\u63d0\u51fa\u7684SCA\u65b9\u6cd5\u80fd\u6709\u6548\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u5355\u7b54\u6848\u6821\u51c6\u6027\u80fd\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u5347\u591a\u7b54\u6848\u573a\u666f\u7684\u6821\u51c6\u6548\u679c\u3002"}}
{"id": "2602.07106", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.07106", "abs": "https://arxiv.org/abs/2602.07106", "authors": ["Haoyu Zhang", "Zhipeng Li", "Yiwen Guo", "Tianshu Yu"], "title": "Ex-Omni: Enabling 3D Facial Animation Generation for Omni-modal Large Language Models", "comment": null, "summary": "Omni-modal large language models (OLLMs) aim to unify multimodal understanding and generation, yet incorporating speech with 3D facial animation remains largely unexplored despite its importance for natural interaction. A key challenge arises from the representation mismatch between discrete, token-level semantic reasoning in LLMs and the dense, fine-grained temporal dynamics required for 3D facial motion, which makes direct modeling difficult to optimize under limited data. We propose Expressive Omni (Ex-Omni), an open-source omni-modal framework that augments OLLMs with speech-accompanied 3D facial animation. Ex-Omni reduces learning difficulty by decoupling semantic reasoning from temporal generation, leveraging speech units as temporal scaffolding and a unified token-as-query gated fusion (TQGF) mechanism for controlled semantic injection. We further introduce InstructEx, a dataset aims to facilitate augment OLLMs with speech-accompanied 3D facial animation. Extensive experiments demonstrate that Ex-Omni performs competitively against existing open-source OLLMs while enabling stable aligned speech and facial animation generation.", "AI": {"tldr": "Ex-Omni\u662f\u4e00\u4e2a\u5f00\u6e90\u7684\u5168\u6a21\u6001\u6846\u67b6\uff0c\u901a\u8fc7\u89e3\u8026\u8bed\u4e49\u63a8\u7406\u4e0e\u65f6\u95f4\u751f\u6210\uff0c\u5229\u7528\u8bed\u97f3\u5355\u5143\u4f5c\u4e3a\u65f6\u95f4\u652f\u67b6\uff0c\u5b9e\u73b0\u8bed\u97f3\u4f34\u968f\u76843D\u9762\u90e8\u52a8\u753b\u751f\u6210\u3002", "motivation": "\u5f53\u524d\u5168\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08OLLMs\uff09\u5728\u591a\u6a21\u6001\u7406\u89e3\u548c\u751f\u6210\u65b9\u9762\u53d6\u5f97\u4e86\u8fdb\u5c55\uff0c\u4f46\u7ed3\u5408\u8bed\u97f3\u4e0e3D\u9762\u90e8\u52a8\u753b\u7684\u7814\u7a76\u4ecd\u7136\u4e0d\u8db3\uff0c\u8fd9\u5bf9\u4e8e\u81ea\u7136\u4ea4\u4e92\u81f3\u5173\u91cd\u8981\u3002\u4e3b\u8981\u6311\u6218\u5728\u4e8eLLMs\u7684\u79bb\u6563\u3001token\u7ea7\u8bed\u4e49\u63a8\u7406\u4e0e3D\u9762\u90e8\u8fd0\u52a8\u6240\u9700\u7684\u5bc6\u96c6\u3001\u7ec6\u7c92\u5ea6\u65f6\u95f4\u52a8\u6001\u4e4b\u95f4\u5b58\u5728\u8868\u793a\u4e0d\u5339\u914d\u3002", "method": "\u63d0\u51faEx-Omni\u6846\u67b6\uff0c\u901a\u8fc7\u89e3\u8026\u8bed\u4e49\u63a8\u7406\u4e0e\u65f6\u95f4\u751f\u6210\u6765\u964d\u4f4e\u5b66\u4e60\u96be\u5ea6\uff1a1\uff09\u5229\u7528\u8bed\u97f3\u5355\u5143\u4f5c\u4e3a\u65f6\u95f4\u652f\u67b6\uff1b2\uff09\u91c7\u7528\u7edf\u4e00\u7684token-as-query gated fusion\uff08TQGF\uff09\u673a\u5236\u8fdb\u884c\u53d7\u63a7\u8bed\u4e49\u6ce8\u5165\uff1b3\uff09\u5f15\u5165InstructEx\u6570\u636e\u96c6\u6765\u652f\u6301\u8bad\u7ec3\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cEx-Omni\u5728\u6027\u80fd\u4e0a\u4e0e\u73b0\u6709\u5f00\u6e90OLLMs\u7ade\u4e89\uff0c\u540c\u65f6\u80fd\u591f\u7a33\u5b9a\u751f\u6210\u5bf9\u9f50\u7684\u8bed\u97f3\u548c\u9762\u90e8\u52a8\u753b\u3002", "conclusion": "Ex-Omni\u6210\u529f\u89e3\u51b3\u4e86\u8bed\u97f3\u4e0e3D\u9762\u90e8\u52a8\u753b\u7ed3\u5408\u7684\u6280\u672f\u6311\u6218\uff0c\u901a\u8fc7\u89e3\u8026\u7b56\u7565\u548cTQGF\u673a\u5236\u5b9e\u73b0\u4e86\u7a33\u5b9a\u7684\u591a\u6a21\u6001\u751f\u6210\uff0c\u4e3a\u5168\u6a21\u6001\u4ea4\u4e92\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.07754", "categories": ["cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2602.07754", "abs": "https://arxiv.org/abs/2602.07754", "authors": ["Bahare Riahi", "Veronica Catete"], "title": "Humanizing AI Grading: Student-Centered Insights on Fairness, Trust, Consistency and Transparency", "comment": "13 pages, 3 figures", "summary": "This study investigates students' perceptions of Artificial Intelligence (AI) grading systems in an undergraduate computer science course (n = 27), focusing on a block-based programming final project. Guided by the ethical principles framework articulated by Jobin (2019), our study examines fairness, trust, consistency, and transparency in AI grading by comparing AI-generated feedback with original human-graded feedback. Findings reveal concerns about AI's lack of contextual understanding and personalization. We recommend that equitable and trustworthy AI systems reflect human judgment, flexibility, and empathy, serving as supplementary tools under human oversight. This work contributes to ethics-centered assessment practices by amplifying student voices and offering design principles for humanizing AI in designed learning environments.", "AI": {"tldr": "\u7814\u7a76\u8c03\u67e5\u672c\u79d1\u751f\u5bf9AI\u8bc4\u5206\u7cfb\u7edf\u7684\u770b\u6cd5\uff0c\u53d1\u73b0\u5b66\u751f\u5bf9AI\u7f3a\u4e4f\u60c5\u5883\u7406\u89e3\u548c\u4e2a\u6027\u5316\u8868\u793a\u62c5\u5fe7\uff0c\u5efa\u8baeAI\u7cfb\u7edf\u5e94\u4f53\u73b0\u4eba\u7c7b\u5224\u65ad\u3001\u7075\u6d3b\u6027\u548c\u540c\u7406\u5fc3\uff0c\u4f5c\u4e3a\u4eba\u7c7b\u76d1\u7763\u4e0b\u7684\u8865\u5145\u5de5\u5177\u3002", "motivation": "\u672c\u7814\u7a76\u65e8\u5728\u4e86\u89e3\u5b66\u751f\u5bf9AI\u8bc4\u5206\u7cfb\u7edf\u7684\u770b\u6cd5\uff0c\u7279\u522b\u5173\u6ce8AI\u8bc4\u5206\u5728\u516c\u5e73\u6027\u3001\u4fe1\u4efb\u5ea6\u3001\u4e00\u81f4\u6027\u548c\u900f\u660e\u5ea6\u65b9\u9762\u7684\u8868\u73b0\uff0c\u57fa\u4e8eJobin\uff082019\uff09\u63d0\u51fa\u7684\u4f26\u7406\u539f\u5219\u6846\u67b6\uff0c\u63a2\u7d22AI\u5728\u6559\u80b2\u8bc4\u4f30\u4e2d\u7684\u4f26\u7406\u95ee\u9898\u3002", "method": "\u7814\u7a76\u91c7\u7528\u5b9a\u6027\u65b9\u6cd5\uff0c\u8c03\u67e5\u4e8627\u540d\u672c\u79d1\u8ba1\u7b97\u673a\u79d1\u5b66\u5b66\u751f\u5bf9AI\u8bc4\u5206\u7cfb\u7edf\u7684\u770b\u6cd5\uff0c\u901a\u8fc7\u6bd4\u8f83AI\u751f\u6210\u7684\u53cd\u9988\u4e0e\u539f\u59cb\u4eba\u5de5\u8bc4\u5206\u53cd\u9988\uff0c\u5206\u6790AI\u5728\u5757\u72b6\u7f16\u7a0b\u6700\u7ec8\u9879\u76ee\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u5b66\u751f\u5bf9AI\u8bc4\u5206\u7cfb\u7edf\u5b58\u5728\u62c5\u5fe7\uff0c\u4e3b\u8981\u5173\u6ce8AI\u7f3a\u4e4f\u60c5\u5883\u7406\u89e3\u548c\u4e2a\u6027\u5316\u80fd\u529b\u3002AI\u751f\u6210\u7684\u53cd\u9988\u5728\u7406\u89e3\u5b66\u751f\u5de5\u4f5c\u80cc\u666f\u548c\u63d0\u4f9b\u4e2a\u6027\u5316\u6307\u5bfc\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\u3002", "conclusion": "\u7814\u7a76\u5efa\u8bae\u516c\u5e73\u53ef\u4fe1\u7684AI\u7cfb\u7edf\u5e94\u53cd\u6620\u4eba\u7c7b\u5224\u65ad\u3001\u7075\u6d3b\u6027\u548c\u540c\u7406\u5fc3\uff0c\u4f5c\u4e3a\u4eba\u7c7b\u76d1\u7763\u4e0b\u7684\u8865\u5145\u5de5\u5177\u3002\u8fd9\u9879\u5de5\u4f5c\u901a\u8fc7\u653e\u5927\u5b66\u751f\u58f0\u97f3\u548c\u4e3aAI\u4eba\u6027\u5316\u8bbe\u8ba1\u63d0\u4f9b\u539f\u5219\uff0c\u4e3a\u4ee5\u4f26\u7406\u4e3a\u4e2d\u5fc3\u7684\u8bc4\u4f30\u5b9e\u8df5\u505a\u51fa\u8d21\u732e\u3002"}}
{"id": "2602.08214", "categories": ["cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2602.08214", "abs": "https://arxiv.org/abs/2602.08214", "authors": ["Ziwei Wang", "Yuanhe Zhang", "Jing Chen", "Zhenhong Zhou", "Ruichao Liang", "Ruiying Du", "Ju Jia", "Cong Wu", "Yang Liu"], "title": "RECUR: Resource Exhaustion Attack via Recursive-Entropy Guided Counterfactual Utilization and Reflection", "comment": null, "summary": "Large Reasoning Models (LRMs) employ reasoning to address complex tasks. Such explicit reasoning requires extended context lengths, resulting in substantially higher resource consumption. Prior work has shown that adversarially crafted inputs can trigger redundant reasoning processes, exposing LRMs to resource-exhaustion vulnerabilities. However, the reasoning process itself, especially its reflective component, has received limited attention, even though it can lead to over-reflection and consume excessive computing power. In this paper, we introduce Recursive Entropy to quantify the risk of resource consumption in reflection, thereby revealing the safety issues inherent in inference itself. Based on Recursive Entropy, we introduce RECUR, a resource exhaustion attack via Recursive Entropy guided Counterfactual Utilization and Reflection. It constructs counterfactual questions to verify the inherent flaws and risks of LRMs. Extensive experiments demonstrate that, under benign inference, recursive entropy exhibits a pronounced decreasing trend. RECUR disrupts this trend, increasing the output length by up to 11x and decreasing throughput by 90%. Our work provides a new perspective on robust reasoning.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faRECUR\u653b\u51fb\u65b9\u6cd5\uff0c\u901a\u8fc7\u9012\u5f52\u71b5\u5f15\u5bfc\u7684\u53cd\u4e8b\u5b9e\u5229\u7528\u548c\u53cd\u601d\uff0c\u63ed\u793a\u5927\u578b\u63a8\u7406\u6a21\u578b\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u5b58\u5728\u7684\u8d44\u6e90\u8017\u5c3d\u5b89\u5168\u6f0f\u6d1e\u3002", "motivation": "\u5927\u578b\u63a8\u7406\u6a21\u578b\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u9700\u8981\u663e\u5f0f\u63a8\u7406\uff0c\u8fd9\u9700\u8981\u66f4\u957f\u7684\u4e0a\u4e0b\u6587\u957f\u5ea6\uff0c\u5bfc\u81f4\u8d44\u6e90\u6d88\u8017\u663e\u8457\u589e\u52a0\u3002\u5148\u524d\u7814\u7a76\u8868\u660e\u5bf9\u6297\u6027\u8f93\u5165\u53ef\u80fd\u89e6\u53d1\u5197\u4f59\u63a8\u7406\u8fc7\u7a0b\uff0c\u4f46\u63a8\u7406\u8fc7\u7a0b\u672c\u8eab\uff0c\u7279\u522b\u662f\u5176\u53cd\u601d\u7ec4\u4ef6\uff0c\u53d7\u5230\u7684\u5173\u6ce8\u6709\u9650\uff0c\u5c3d\u7ba1\u5b83\u53ef\u80fd\u5bfc\u81f4\u8fc7\u5ea6\u53cd\u601d\u5e76\u6d88\u8017\u8fc7\u591a\u8ba1\u7b97\u80fd\u529b\u3002", "method": "\u5f15\u5165\u9012\u5f52\u71b5\u6765\u91cf\u5316\u53cd\u601d\u4e2d\u7684\u8d44\u6e90\u6d88\u8017\u98ce\u9669\uff0c\u57fa\u4e8e\u6b64\u63d0\u51faRECUR\u653b\u51fb\u65b9\u6cd5\uff1a\u901a\u8fc7\u9012\u5f52\u71b5\u5f15\u5bfc\u7684\u53cd\u4e8b\u5b9e\u5229\u7528\u548c\u53cd\u601d\uff0c\u6784\u5efa\u53cd\u4e8b\u5b9e\u95ee\u9898\u6765\u9a8c\u8bc1LRMs\u7684\u5185\u5728\u7f3a\u9677\u548c\u98ce\u9669\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5728\u826f\u6027\u63a8\u7406\u4e0b\uff0c\u9012\u5f52\u71b5\u5448\u73b0\u660e\u663e\u4e0b\u964d\u8d8b\u52bf\uff1b\u800cRECUR\u653b\u51fb\u7834\u574f\u4e86\u8fd9\u4e00\u8d8b\u52bf\uff0c\u4f7f\u8f93\u51fa\u957f\u5ea6\u589e\u52a0\u9ad8\u8fbe11\u500d\uff0c\u541e\u5410\u91cf\u964d\u4f4e90%\u3002", "conclusion": "\u672c\u6587\u4e3a\u7a33\u5065\u63a8\u7406\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\uff0c\u63ed\u793a\u4e86\u63a8\u7406\u672c\u8eab\u5b58\u5728\u7684\u5b89\u5168\u95ee\u9898\uff0c\u7279\u522b\u662f\u901a\u8fc7\u9012\u5f52\u71b5\u91cf\u5316\u7684\u8d44\u6e90\u6d88\u8017\u98ce\u9669\u3002"}}
{"id": "2602.07909", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.07909", "abs": "https://arxiv.org/abs/2602.07909", "authors": ["Taolin Zhang", "Hang Guo", "Wang Lu", "Tao Dai", "Shu-Tao Xia", "Jindong Wang"], "title": "SparseEval: Efficient Evaluation of Large Language Models by Sparse Optimization", "comment": "ICLR2026", "summary": "As large language models (LLMs) continue to scale up, their performance on various downstream tasks has significantly improved. However, evaluating their capabilities has become increasingly expensive, as performing inference on a large number of benchmark samples incurs high computational costs. In this paper, we revisit the model-item performance matrix and show that it exhibits sparsity, that representative items can be selected as anchors, and that the task of efficient benchmarking can be formulated as a sparse optimization problem. Based on these insights, we propose SparseEval, a method that, for the first time, adopts gradient descent to optimize anchor weights and employs an iterative refinement strategy for anchor selection. We utilize the representation capacity of MLP to handle sparse optimization and propose the Anchor Importance Score and Candidate Importance Score to evaluate the value of each item for task-aware refinement. Extensive experiments demonstrate the low estimation error and high Kendall's~$\u03c4$ of our method across a variety of benchmarks, showcasing its superior robustness and practicality in real-world scenarios. Code is available at {https://github.com/taolinzhang/SparseEval}.", "AI": {"tldr": "SparseEval\uff1a\u4e00\u79cd\u57fa\u4e8e\u7a00\u758f\u4f18\u5316\u7684\u9ad8\u6548\u5927\u8bed\u8a00\u6a21\u578b\u8bc4\u4f30\u65b9\u6cd5\uff0c\u901a\u8fc7\u68af\u5ea6\u4e0b\u964d\u4f18\u5316\u951a\u70b9\u6743\u91cd\u548c\u8fed\u4ee3\u7cbe\u5316\u7b56\u7565\uff0c\u663e\u8457\u964d\u4f4e\u8bc4\u4f30\u6210\u672c", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u89c4\u6a21\u4e0d\u65ad\u6269\u5927\uff0c\u8bc4\u4f30\u5176\u6027\u80fd\u7684\u8ba1\u7b97\u6210\u672c\u8d8a\u6765\u8d8a\u9ad8\u3002\u4f20\u7edf\u7684\u57fa\u51c6\u6d4b\u8bd5\u9700\u8981\u5927\u91cf\u6837\u672c\u63a8\u7406\uff0c\u8ba1\u7b97\u5f00\u9500\u5de8\u5927\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u9ad8\u6548\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u63d0\u51faSparseEval\u65b9\u6cd5\uff1a1) \u5c06\u6a21\u578b-\u9879\u76ee\u6027\u80fd\u77e9\u9635\u89c6\u4e3a\u7a00\u758f\u77e9\u9635\uff1b2) \u9009\u62e9\u4ee3\u8868\u6027\u9879\u76ee\u4f5c\u4e3a\u951a\u70b9\uff1b3) \u5c06\u9ad8\u6548\u8bc4\u4f30\u95ee\u9898\u5f62\u5f0f\u5316\u4e3a\u7a00\u758f\u4f18\u5316\u95ee\u9898\uff1b4) \u9996\u6b21\u91c7\u7528\u68af\u5ea6\u4e0b\u964d\u4f18\u5316\u951a\u70b9\u6743\u91cd\uff1b5) \u4f7f\u7528\u8fed\u4ee3\u7cbe\u5316\u7b56\u7565\u8fdb\u884c\u951a\u70b9\u9009\u62e9\uff1b6) \u5229\u7528MLP\u7684\u8868\u5f81\u80fd\u529b\u5904\u7406\u7a00\u758f\u4f18\u5316\uff1b7) \u63d0\u51fa\u951a\u70b9\u91cd\u8981\u6027\u5206\u6570\u548c\u5019\u9009\u91cd\u8981\u6027\u5206\u6570\u8bc4\u4f30\u9879\u76ee\u4ef7\u503c\u3002", "result": "\u5728\u591a\u79cd\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5177\u6709\u4f4e\u4f30\u8ba1\u8bef\u5dee\u548c\u9ad8Kendall's \u03c4\u76f8\u5173\u6027\uff0c\u5c55\u793a\u4e86\u5176\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u7684\u4f18\u8d8a\u9c81\u68d2\u6027\u548c\u5b9e\u7528\u6027\u3002", "conclusion": "SparseEval\u901a\u8fc7\u7a00\u758f\u4f18\u5316\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u8bc4\u4f30\u7684\u9ad8\u6210\u672c\u95ee\u9898\uff0c\u4e3a\u9ad8\u6548\u6a21\u578b\u8bc4\u4f30\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2602.07149", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.07149", "abs": "https://arxiv.org/abs/2602.07149", "authors": ["Rawisara Lohanimit", "Yankun Wu", "Amelia Katirai", "Yuta Nakashima", "Noa Garcia"], "title": "Privacy in Image Datasets: A Case Study on Pregnancy Ultrasounds", "comment": null, "summary": "The rise of generative models has led to increased use of large-scale datasets collected from the internet, often with minimal or no data curation. This raises concerns about the inclusion of sensitive or private information. In this work, we explore the presence of pregnancy ultrasound images, which contain sensitive personal information and are often shared online. Through a systematic examination of LAION-400M dataset using CLIP embedding similarity, we retrieve images containing pregnancy ultrasound and detect thousands of entities of private information such as names and locations. Our findings reveal that multiple images have high-risk information that could enable re-identification or impersonation. We conclude with recommended practices for dataset curation, data privacy, and ethical use of public image datasets.", "AI": {"tldr": "\u901a\u8fc7CLIP\u5d4c\u5165\u76f8\u4f3c\u6027\u5728LAION-400M\u6570\u636e\u96c6\u4e2d\u7cfb\u7edf\u68c0\u7d22\u6000\u5b55\u8d85\u58f0\u56fe\u50cf\uff0c\u53d1\u73b0\u6570\u5343\u6761\u5305\u542b\u59d3\u540d\u3001\u4f4d\u7f6e\u7b49\u654f\u611f\u4e2a\u4eba\u4fe1\u606f\u7684\u9ad8\u98ce\u9669\u56fe\u50cf\uff0c\u63d0\u51fa\u4e86\u6570\u636e\u96c6\u7ba1\u7406\u3001\u9690\u79c1\u4fdd\u62a4\u548c\u4f26\u7406\u4f7f\u7528\u7684\u5efa\u8bae", "motivation": "\u751f\u6210\u6a21\u578b\u7684\u5174\u8d77\u5bfc\u81f4\u5927\u91cf\u4ece\u4e92\u8054\u7f51\u6536\u96c6\u7684\u6570\u636e\u96c6\u88ab\u4f7f\u7528\uff0c\u8fd9\u4e9b\u6570\u636e\u96c6\u901a\u5e38\u7f3a\u4e4f\u6570\u636e\u7b5b\u9009\uff0c\u5f15\u53d1\u4e86\u5bf9\u654f\u611f\u6216\u9690\u79c1\u4fe1\u606f\u5305\u542b\u7684\u62c5\u5fe7\u3002\u672c\u7814\u7a76\u7279\u522b\u5173\u6ce8\u6000\u5b55\u8d85\u58f0\u56fe\u50cf\u8fd9\u7c7b\u5305\u542b\u654f\u611f\u4e2a\u4eba\u4fe1\u606f\u4e14\u5e38\u88ab\u5728\u7ebf\u5206\u4eab\u7684\u5185\u5bb9", "method": "\u4f7f\u7528CLIP\u5d4c\u5165\u76f8\u4f3c\u6027\u5bf9LAION-400M\u6570\u636e\u96c6\u8fdb\u884c\u7cfb\u7edf\u68c0\u67e5\uff0c\u68c0\u7d22\u5305\u542b\u6000\u5b55\u8d85\u58f0\u7684\u56fe\u50cf\uff0c\u5e76\u68c0\u6d4b\u5176\u4e2d\u7684\u9690\u79c1\u4fe1\u606f\u5b9e\u4f53\uff08\u5982\u59d3\u540d\u548c\u4f4d\u7f6e\uff09", "result": "\u53d1\u73b0\u4e86\u6570\u5343\u4e2a\u5305\u542b\u59d3\u540d\u3001\u4f4d\u7f6e\u7b49\u9690\u79c1\u4fe1\u606f\u7684\u5b9e\u4f53\uff0c\u591a\u4e2a\u56fe\u50cf\u5305\u542b\u9ad8\u98ce\u9669\u4fe1\u606f\uff0c\u53ef\u80fd\u5bfc\u81f4\u91cd\u65b0\u8bc6\u522b\u6216\u8eab\u4efd\u5192\u5145", "conclusion": "\u63d0\u51fa\u4e86\u6570\u636e\u96c6\u7ba1\u7406\u3001\u6570\u636e\u9690\u79c1\u548c\u516c\u5171\u56fe\u50cf\u6570\u636e\u96c6\u4f26\u7406\u4f7f\u7528\u7684\u63a8\u8350\u5b9e\u8df5\uff0c\u5f3a\u8c03\u9700\u8981\u66f4\u597d\u7684\u6570\u636e\u7b5b\u9009\u548c\u9690\u79c1\u4fdd\u62a4\u63aa\u65bd"}}
{"id": "2602.08765", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08765", "abs": "https://arxiv.org/abs/2602.08765", "authors": ["Micah Villmow"], "title": "Taming Scylla: Understanding the multi-headed agentic daemon of the coding seas", "comment": "32 Pages, 7 Figures", "summary": "LLM-based tools are automating more software development tasks at a rapid pace, but there is no rigorous way to evaluate how different architectural choices -- prompts, skills, tools, multi-agent setups -- materially affect both capability and cost. This paper introduces Scylla, an evaluation framework for benchmarking agentic coding tools through structured ablation studies that uses seven testing tiers (T0-T6) progressively adding complexity to isolate what directly influences results and how. The key metric is Cost-of-Pass (CoP): the expected dollar cost to get one correct solution, which directly quantifies the trade-off between complexity and efficiency. The framework is model-agnostic, designed to work with any CLI tool; this paper demonstrates it with Claude Sonnet 4.5, using multiple LLM judges (Opus 4.5, Sonnet 4.5, Haiku 4.5) from the same vendor for evaluation consensus, where judges score results using direct tests, human-designed LLM-evaluated rubrics, and qualitative assessment. The result is a reproducible framework that quantifies trade-offs between agent complexity and actual outcomes, suggesting that architectural complexity does not always improve quality.", "AI": {"tldr": "Scylla\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u667a\u80fd\u7f16\u7801\u5de5\u5177\u7684\u8bc4\u4ef7\u6846\u67b6\uff0c\u901a\u8fc7\u4e03\u5c42\u6d4b\u8bd5\u9010\u6b65\u589e\u52a0\u590d\u6742\u5ea6\u6765\u91cf\u5316\u4e0d\u540c\u67b6\u6784\u9009\u62e9\u5bf9\u80fd\u529b\u548c\u6210\u672c\u7684\u5f71\u54cd\uff0c\u6838\u5fc3\u6307\u6807\u662fCost-of-Pass\uff08CoP\uff09\u3002", "motivation": "\u5f53\u524dLLM\u5de5\u5177\u6b63\u5728\u5feb\u901f\u81ea\u52a8\u5316\u8f6f\u4ef6\u5f00\u53d1\u4efb\u52a1\uff0c\u4f46\u7f3a\u4e4f\u4e25\u8c28\u7684\u65b9\u6cd5\u6765\u8bc4\u4f30\u4e0d\u540c\u67b6\u6784\u9009\u62e9\uff08\u5982\u63d0\u793a\u3001\u6280\u80fd\u3001\u5de5\u5177\u3001\u591a\u667a\u80fd\u4f53\u8bbe\u7f6e\uff09\u5982\u4f55\u5b9e\u8d28\u5f71\u54cd\u80fd\u529b\u548c\u6210\u672c\u3002", "method": "\u5f15\u5165Scylla\u8bc4\u4ef7\u6846\u67b6\uff0c\u4f7f\u7528\u4e03\u5c42\u6d4b\u8bd5\uff08T0-T6\uff09\u9010\u6b65\u589e\u52a0\u590d\u6742\u5ea6\u8fdb\u884c\u7ed3\u6784\u5316\u6d88\u878d\u7814\u7a76\uff0c\u6838\u5fc3\u6307\u6807\u662fCost-of-Pass\uff08CoP\uff09\u3002\u6846\u67b6\u6a21\u578b\u65e0\u5173\uff0c\u53ef\u4e0e\u4efb\u4f55CLI\u5de5\u5177\u914d\u5408\u4f7f\u7528\uff0c\u4f7f\u7528\u591a\u4e2aLLM\u8bc4\u4f30\u8005\u8fdb\u884c\u5171\u8bc6\u8bc4\u4f30\u3002", "result": "\u521b\u5efa\u4e86\u4e00\u4e2a\u53ef\u91cd\u590d\u7684\u6846\u67b6\uff0c\u91cf\u5316\u4e86\u667a\u80fd\u4f53\u590d\u6742\u5ea6\u4e0e\u5b9e\u9645\u7ed3\u679c\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u8868\u660e\u67b6\u6784\u590d\u6742\u5ea6\u5e76\u4e0d\u603b\u662f\u63d0\u9ad8\u8d28\u91cf\u3002", "conclusion": "Scylla\u6846\u67b6\u4e3a\u8bc4\u4f30\u667a\u80fd\u7f16\u7801\u5de5\u5177\u63d0\u4f9b\u4e86\u7cfb\u7edf\u65b9\u6cd5\uff0c\u80fd\u591f\u91cf\u5316\u4e0d\u540c\u67b6\u6784\u9009\u62e9\u5bf9\u6210\u672c\u548c\u80fd\u529b\u7684\u5f71\u54cd\uff0c\u5e2e\u52a9\u5f00\u53d1\u8005\u505a\u51fa\u66f4\u660e\u667a\u7684\u51b3\u7b56\u3002"}}
{"id": "2602.08229", "categories": ["cs.AI", "cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.08229", "abs": "https://arxiv.org/abs/2602.08229", "authors": ["Yifan Yang", "Jinjia Li", "Kunxi Li", "Puhao Zheng", "Yuanyi Wang", "Zheyan Qu", "Yang Yu", "Jianmin Wu", "Ming Li", "Hongxia Yang"], "title": "InfiCoEvalChain: A Blockchain-Based Decentralized Framework for Collaborative LLM Evaluation", "comment": null, "summary": "The rapid advancement of large language models (LLMs) demands increasingly reliable evaluation, yet current centralized evaluation suffers from opacity, overfitting, and hardware-induced variance. Our empirical analysis reveals an alarming inconsistency in existing evaluations: the standard deviation across ten repeated runs of a single model on HumanEval (1.67) actually exceeds the performance gap among the top-10 models on the official leaderboard (0.91), rendering current rankings statistically precarious. To mitigate these instabilities, we propose a decentralized evaluation framework that enables hardware and parameter diversity through large-scale benchmarking across heterogeneous compute nodes. By leveraging the blockchain-based protocol, the framework incentivizes global contributors to act as independent validators, using a robust reward system to ensure evaluation integrity and discourage dishonest participation. This collective verification transforms evaluation from a \"centralized black box\" into a \"decentralized endorsement\" where multi-party consensus and diverse inference environments yield a more stable, representative metric. Experimental results demonstrate that the decentralized evaluation framework reduces the standard deviation across ten runs on the same model to 0.28. This significant improvement over conventional frameworks ensures higher statistical confidence in model rankings. We have completely implemented this platform and will soon release it to the community.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u53bb\u4e2d\u5fc3\u5316\u8bc4\u4f30\u6846\u67b6\u89e3\u51b3LLM\u8bc4\u4f30\u4e2d\u7684\u4e0d\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u901a\u8fc7\u533a\u5757\u94fe\u534f\u8bae\u6fc0\u52b1\u5168\u7403\u8d21\u732e\u8005\u4f5c\u4e3a\u72ec\u7acb\u9a8c\u8bc1\u8005\uff0c\u663e\u8457\u964d\u4f4e\u8bc4\u4f30\u65b9\u5dee\u3002", "motivation": "\u5f53\u524d\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8bc4\u4f30\u5b58\u5728\u96c6\u4e2d\u5316\u8bc4\u4f30\u7684\u900f\u660e\u5ea6\u4f4e\u3001\u8fc7\u62df\u5408\u548c\u786c\u4ef6\u5dee\u5f02\u5bfc\u81f4\u7684\u65b9\u5dee\u95ee\u9898\u3002\u5b9e\u8bc1\u5206\u6790\u663e\u793a\uff0cHumanEval\u4e0a\u5355\u4e2a\u6a21\u578b\u5341\u6b21\u8fd0\u884c\u7684\u6807\u51c6\u5dee\uff081.67\uff09\u751a\u81f3\u8d85\u8fc7\u4e86\u5b98\u65b9\u6392\u884c\u699c\u4e0a\u524d10\u540d\u6a21\u578b\u7684\u6027\u80fd\u5dee\u8ddd\uff080.91\uff09\uff0c\u5bfc\u81f4\u5f53\u524d\u6392\u540d\u7edf\u8ba1\u4e0a\u4e0d\u53ef\u9760\u3002", "method": "\u63d0\u51fa\u53bb\u4e2d\u5fc3\u5316\u8bc4\u4f30\u6846\u67b6\uff0c\u901a\u8fc7\u533a\u5757\u94fe\u534f\u8bae\u6fc0\u52b1\u5168\u7403\u8d21\u732e\u8005\u4f5c\u4e3a\u72ec\u7acb\u9a8c\u8bc1\u8005\uff0c\u5728\u5f02\u6784\u8ba1\u7b97\u8282\u70b9\u4e0a\u8fdb\u884c\u5927\u89c4\u6a21\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5b9e\u73b0\u786c\u4ef6\u548c\u53c2\u6570\u591a\u6837\u6027\u3002\u91c7\u7528\u7a33\u5065\u7684\u5956\u52b1\u7cfb\u7edf\u786e\u4fdd\u8bc4\u4f30\u5b8c\u6574\u6027\uff0c\u9632\u6b62\u4e0d\u8bda\u5b9e\u53c2\u4e0e\u3002", "result": "\u53bb\u4e2d\u5fc3\u5316\u8bc4\u4f30\u6846\u67b6\u5c06\u540c\u4e00\u6a21\u578b\u5341\u6b21\u8fd0\u884c\u7684\u6807\u51c6\u5dee\u4ece1.67\u964d\u4f4e\u52300.28\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6a21\u578b\u6392\u540d\u7684\u7edf\u8ba1\u7f6e\u4fe1\u5ea6\u3002\u8be5\u5e73\u53f0\u5df2\u5b8c\u5168\u5b9e\u73b0\u5e76\u5c06\u5411\u793e\u533a\u53d1\u5e03\u3002", "conclusion": "\u53bb\u4e2d\u5fc3\u5316\u8bc4\u4f30\u6846\u67b6\u901a\u8fc7\u591a\u65b9\u5171\u8bc6\u548c\u591a\u6837\u5316\u63a8\u7406\u73af\u5883\uff0c\u5c06\u8bc4\u4f30\u4ece\"\u96c6\u4e2d\u5316\u9ed1\u76d2\"\u8f6c\u53d8\u4e3a\"\u53bb\u4e2d\u5fc3\u5316\u80cc\u4e66\"\uff0c\u63d0\u4f9b\u4e86\u66f4\u7a33\u5b9a\u3001\u66f4\u5177\u4ee3\u8868\u6027\u7684\u8bc4\u4f30\u6307\u6807\uff0c\u89e3\u51b3\u4e86\u5f53\u524dLLM\u8bc4\u4f30\u4e2d\u7684\u7edf\u8ba1\u4e0d\u7a33\u5b9a\u6027\u95ee\u9898\u3002"}}
{"id": "2602.07930", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.07930", "abs": "https://arxiv.org/abs/2602.07930", "authors": ["Irina Bigoulaeva", "Jonas Rohweder", "Subhabrata Dutta", "Iryna Gurevych"], "title": "Patches of Nonlinearity: Instruction Vectors in Large Language Models", "comment": null, "summary": "Despite the recent success of instruction-tuned language models and their ubiquitous usage, very little is known of how models process instructions internally. In this work, we address this gap from a mechanistic point of view by investigating how instruction-specific representations are constructed and utilized in different stages of post-training: Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO). Via causal mediation, we identify that instruction representation is fairly localized in models. These representations, which we call Instruction Vectors (IVs), demonstrate a curious juxtaposition of linear separability along with non-linear causal interaction, broadly questioning the scope of the linear representation hypothesis commonplace in mechanistic interpretability. To disentangle the non-linear causal interaction, we propose a novel method to localize information processing in language models that is free from the implicit linear assumptions of patching-based techniques. We find that, conditioned on the task representations formed in the early layers, different information pathways are selected in the later layers to solve that task, i.e., IVs act as circuit selectors.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u6307\u4ee4\u8c03\u4f18\u8bed\u8a00\u6a21\u578b\u5185\u90e8\u5982\u4f55\u5904\u7406\u6307\u4ee4\uff0c\u53d1\u73b0\u6307\u4ee4\u8868\u793a\u76f8\u5bf9\u5c40\u90e8\u5316\uff0c\u79f0\u4e3a\u6307\u4ee4\u5411\u91cf(IVs)\uff0c\u5b83\u4eec\u8868\u73b0\u51fa\u7ebf\u6027\u53ef\u5206\u6027\u4e0e\u975e\u7ebf\u6027\u56e0\u679c\u4ea4\u4e92\u5e76\u5b58\u7684\u7279\u70b9\uff0c\u6311\u6218\u4e86\u673a\u5236\u53ef\u89e3\u91ca\u6027\u4e2d\u7684\u7ebf\u6027\u8868\u793a\u5047\u8bbe\u3002", "motivation": "\u5c3d\u7ba1\u6307\u4ee4\u8c03\u4f18\u8bed\u8a00\u6a21\u578b\u53d6\u5f97\u4e86\u6210\u529f\u5e76\u88ab\u5e7f\u6cdb\u4f7f\u7528\uff0c\u4f46\u5bf9\u5176\u5185\u90e8\u5982\u4f55\u5904\u7406\u6307\u4ee4\u7684\u673a\u5236\u4e86\u89e3\u751a\u5c11\u3002\u672c\u6587\u65e8\u5728\u4ece\u673a\u5236\u89d2\u5ea6\u63a2\u7a76\u6307\u4ee4\u7279\u5b9a\u8868\u793a\u5728\u4e0d\u540c\u540e\u8bad\u7ec3\u9636\u6bb5\uff08\u76d1\u7763\u5fae\u8c03\u548c\u76f4\u63a5\u504f\u597d\u4f18\u5316\uff09\u4e2d\u662f\u5982\u4f55\u6784\u5efa\u548c\u5229\u7528\u7684\u3002", "method": "\u901a\u8fc7\u56e0\u679c\u4e2d\u4ecb\u5206\u6790\u8bc6\u522b\u6307\u4ee4\u8868\u793a\u5728\u6a21\u578b\u4e2d\u7684\u5c40\u90e8\u5316\u7279\u5f81\uff0c\u63d0\u51fa\u4e00\u79cd\u65b0\u9896\u7684\u65b9\u6cd5\u6765\u5b9a\u4f4d\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u4fe1\u606f\u5904\u7406\uff0c\u8be5\u65b9\u6cd5\u6446\u8131\u4e86\u57fa\u4e8e\u8865\u4e01\u6280\u672f\u7684\u9690\u5f0f\u7ebf\u6027\u5047\u8bbe\u3002\u7814\u7a76\u53d1\u73b0\u6307\u4ee4\u5411\u91cf\u5728\u65e9\u671f\u5c42\u5f62\u6210\u4efb\u52a1\u8868\u793a\u540e\uff0c\u5728\u540e\u671f\u5c42\u9009\u62e9\u4e0d\u540c\u7684\u4fe1\u606f\u901a\u8def\u6765\u5b8c\u6210\u4efb\u52a1\uff0c\u5373\u6307\u4ee4\u5411\u91cf\u5145\u5f53\u7535\u8def\u9009\u62e9\u5668\u3002", "result": "\u53d1\u73b0\u6307\u4ee4\u8868\u793a\u76f8\u5bf9\u5c40\u90e8\u5316\uff0c\u8fd9\u4e9b\u6307\u4ee4\u5411\u91cf\u8868\u73b0\u51fa\u7ebf\u6027\u53ef\u5206\u6027\u4e0e\u975e\u7ebf\u6027\u56e0\u679c\u4ea4\u4e92\u5e76\u5b58\u7684\u7279\u70b9\uff0c\u6311\u6218\u4e86\u673a\u5236\u53ef\u89e3\u91ca\u6027\u4e2d\u5e38\u89c1\u7684\u7ebf\u6027\u8868\u793a\u5047\u8bbe\u3002\u6307\u4ee4\u5411\u91cf\u5728\u65e9\u671f\u5c42\u5f62\u6210\u4efb\u52a1\u8868\u793a\uff0c\u5728\u540e\u671f\u5c42\u9009\u62e9\u4e0d\u540c\u7684\u4fe1\u606f\u901a\u8def\u6765\u5b8c\u6210\u4efb\u52a1\u3002", "conclusion": "\u6307\u4ee4\u8c03\u4f18\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u6307\u4ee4\u5904\u7406\u673a\u5236\u6bd4\u4f20\u7edf\u7ebf\u6027\u8868\u793a\u5047\u8bbe\u66f4\u4e3a\u590d\u6742\uff0c\u6307\u4ee4\u5411\u91cf\u65e2\u5177\u6709\u7ebf\u6027\u53ef\u5206\u6027\u53c8\u53c2\u4e0e\u975e\u7ebf\u6027\u56e0\u679c\u4ea4\u4e92\uff0c\u5b83\u4eec\u4f5c\u4e3a\u7535\u8def\u9009\u62e9\u5668\u5728\u4e0d\u540c\u5c42\u95f4\u534f\u8c03\u4fe1\u606f\u5904\u7406\uff0c\u8fd9\u4e3a\u7406\u89e3\u8bed\u8a00\u6a21\u578b\u5185\u90e8\u5de5\u4f5c\u673a\u5236\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\u3002"}}
{"id": "2602.07174", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.07174", "abs": "https://arxiv.org/abs/2602.07174", "authors": ["Yongheng Sun", "Jun Shu", "Jianhua Ma", "Fan Wang"], "title": "DuMeta++: Spatiotemporal Dual Meta-Learning for Generalizable Few-Shot Brain Tissue Segmentation Across Diverse Ages", "comment": null, "summary": "Accurate segmentation of brain tissues from MRI scans is critical for neuroscience and clinical applications, but achieving consistent performance across the human lifespan remains challenging due to dynamic, age-related changes in brain appearance and morphology. While prior work has sought to mitigate these shifts by using self-supervised regularization with paired longitudinal data, such data are often unavailable in practice. To address this, we propose \\emph{DuMeta++}, a dual meta-learning framework that operates without paired longitudinal data. Our approach integrates: (1) meta-feature learning to extract age-agnostic semantic representations of spatiotemporally evolving brain structures, and (2) meta-initialization learning to enable data-efficient adaptation of the segmentation model. Furthermore, we propose a memory-bank-based class-aware regularization strategy to enforce longitudinal consistency without explicit longitudinal supervision. We theoretically prove the convergence of our DuMeta++, ensuring stability. Experiments on diverse datasets (iSeg-2019, IBIS, OASIS, ADNI) under few-shot settings demonstrate that DuMeta++ outperforms existing methods in cross-age generalization. Code will be available at https://github.com/ladderlab-xjtu/DuMeta++.", "AI": {"tldr": "DuMeta++\uff1a\u4e00\u79cd\u65e0\u9700\u914d\u5bf9\u7eb5\u5411\u6570\u636e\u7684\u53cc\u5143\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u8de8\u5e74\u9f84\u8111\u7ec4\u7ec7\u5206\u5272\uff0c\u901a\u8fc7\u5143\u7279\u5f81\u5b66\u4e60\u548c\u5143\u521d\u59cb\u5316\u5b66\u4e60\u5b9e\u73b0\u66f4\u597d\u7684\u6cdb\u5316\u6027\u80fd\u3002", "motivation": "\u8111MRI\u7ec4\u7ec7\u5206\u5272\u5728\u795e\u7ecf\u79d1\u5b66\u548c\u4e34\u5e8a\u5e94\u7528\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7531\u4e8e\u5927\u8111\u5916\u89c2\u548c\u5f62\u6001\u968f\u5e74\u9f84\u53d8\u5316\u7684\u52a8\u6001\u6027\uff0c\u5b9e\u73b0\u8de8\u4eba\u7c7b\u751f\u547d\u5468\u671f\u7684\u7a33\u5b9a\u6027\u80fd\u5177\u6709\u6311\u6218\u6027\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u914d\u5bf9\u7eb5\u5411\u6570\u636e\u8fdb\u884c\u81ea\u76d1\u7763\u6b63\u5219\u5316\uff0c\u4f46\u8fd9\u7c7b\u6570\u636e\u5728\u5b9e\u8df5\u4e2d\u5f80\u5f80\u96be\u4ee5\u83b7\u5f97\u3002", "method": "\u63d0\u51faDuMeta++\u53cc\u5143\u5b66\u4e60\u6846\u67b6\uff1a1\uff09\u5143\u7279\u5f81\u5b66\u4e60\u63d0\u53d6\u5e74\u9f84\u65e0\u5173\u7684\u65f6\u7a7a\u6f14\u5316\u8111\u7ed3\u6784\u8bed\u4e49\u8868\u793a\uff1b2\uff09\u5143\u521d\u59cb\u5316\u5b66\u4e60\u5b9e\u73b0\u6570\u636e\u9ad8\u6548\u7684\u5206\u5272\u6a21\u578b\u9002\u5e94\uff1b3\uff09\u57fa\u4e8e\u8bb0\u5fc6\u5e93\u7684\u7c7b\u611f\u77e5\u6b63\u5219\u5316\u7b56\u7565\uff0c\u65e0\u9700\u663e\u5f0f\u7eb5\u5411\u76d1\u7763\u5373\u53ef\u5f3a\u5236\u7eb5\u5411\u4e00\u81f4\u6027\u3002\u7406\u8bba\u8bc1\u660e\u4e86\u7b97\u6cd5\u7684\u6536\u655b\u6027\u3002", "result": "\u5728iSeg-2019\u3001IBIS\u3001OASIS\u3001ADNI\u7b49\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5c11\u6837\u672c\u8bbe\u7f6e\u5b9e\u9a8c\u4e2d\uff0cDuMeta++\u5728\u8de8\u5e74\u9f84\u6cdb\u5316\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "DuMeta++\u65e0\u9700\u914d\u5bf9\u7eb5\u5411\u6570\u636e\u5373\u53ef\u5b9e\u73b0\u8de8\u5e74\u9f84\u8111\u7ec4\u7ec7\u5206\u5272\u7684\u7a33\u5b9a\u6027\u80fd\uff0c\u901a\u8fc7\u53cc\u5143\u5b66\u4e60\u6846\u67b6\u548c\u7c7b\u611f\u77e5\u6b63\u5219\u5316\u7b56\u7565\u6709\u6548\u89e3\u51b3\u4e86\u5e74\u9f84\u76f8\u5173\u53d8\u5316\u5e26\u6765\u7684\u6311\u6218\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2602.08866", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.08866", "abs": "https://arxiv.org/abs/2602.08866", "authors": ["Bang Xie", "Senjian Zhang", "Zhiyuan Peng", "Wei Chen", "Chenhao Ying", "Yuan Luo"], "title": "ArkEval: Benchmarking and Evaluating Automated CodeRepair for ArkTS", "comment": null, "summary": "Large language models have transformed code generation, enabling unprecedented automation in software development. As mobile ecosystems evolve, HarmonyOS has emerged as a critical platform requiring robust development tools. Software development for the HarmonyOS ecosystem relies heavily on ArkTS, a statically typed extension of TypeScript. Despite its growing importance, the ecosystem lacks robust tools for automated code repair, primarily due to the absence of a high-quality benchmark for evaluation. To address this gap, we present ArkEval, a unified framework for ArkTS automated repair workflow evaluation and benchmark construction. It provides the first comprehensive benchmark specifically designed for ArkTS automated program repair. We constructed this benchmark by mining issues from a large-scale official Huawei repository containing over 400 independent ArkTS applications. Through a rigorous multi-stage filtering process, we curated 502 reproducible issues. To ensure testability, we employed a novel LLM-based test generation and voting mechanism involving Claude and other models. Furthermore, we standardized problem statements to facilitate fair evaluation. Finally, we evaluated four state-of-the-art Large Language Models (LLMs) on our benchmark using a retrieval-augmented repair workflow. Our results highlight the current capabilities and limitations of LLMs in repairing ArkTS code, paving the way for future research in this low-resource language domain.", "AI": {"tldr": "ArkEval\uff1a\u9996\u4e2a\u9488\u5bf9HarmonyOS ArkTS\u8bed\u8a00\u7684\u81ea\u52a8\u5316\u4ee3\u7801\u4fee\u590d\u8bc4\u4f30\u6846\u67b6\u4e0e\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5305\u542b502\u4e2a\u53ef\u590d\u73b0\u95ee\u9898\uff0c\u7528\u4e8e\u8bc4\u4f30LLM\u5728ArkTS\u4ee3\u7801\u4fee\u590d\u4e2d\u7684\u80fd\u529b", "motivation": "\u968f\u7740HarmonyOS\u5e73\u53f0\u7684\u91cd\u8981\u6027\u65e5\u76ca\u589e\u957f\uff0cArkTS\u4f5c\u4e3a\u5176\u6838\u5fc3\u5f00\u53d1\u8bed\u8a00\u7f3a\u4e4f\u81ea\u52a8\u5316\u4ee3\u7801\u4fee\u590d\u5de5\u5177\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u7f3a\u5c11\u9ad8\u8d28\u91cf\u7684\u8bc4\u4f30\u57fa\u51c6\u3002\u5f53\u524d\u751f\u6001\u7cfb\u7edf\u9700\u8981\u4e13\u95e8\u7684\u5de5\u5177\u6765\u652f\u6301ArkTS\u4ee3\u7801\u7684\u81ea\u52a8\u4fee\u590d\u3002", "method": "1) \u4ece\u534e\u4e3a\u5b98\u65b9\u5305\u542b400\u591a\u4e2a\u72ec\u7acbArkTS\u5e94\u7528\u7684\u5927\u578b\u4ed3\u5e93\u4e2d\u6316\u6398\u95ee\u9898\uff1b2) \u901a\u8fc7\u4e25\u683c\u7684\u591a\u9636\u6bb5\u7b5b\u9009\u6d41\u7a0b\uff0c\u7b5b\u9009\u51fa502\u4e2a\u53ef\u590d\u73b0\u95ee\u9898\uff1b3) \u91c7\u7528\u57fa\u4e8eLLM\u7684\u6d4b\u8bd5\u751f\u6210\u548c\u6295\u7968\u673a\u5236\u786e\u4fdd\u53ef\u6d4b\u8bd5\u6027\uff1b4) \u6807\u51c6\u5316\u95ee\u9898\u63cf\u8ff0\u4ee5\u652f\u6301\u516c\u5e73\u8bc4\u4f30\uff1b5) \u4f7f\u7528\u68c0\u7d22\u589e\u5f3a\u7684\u4fee\u590d\u5de5\u4f5c\u6d41\u8bc4\u4f30\u56db\u4e2a\u6700\u5148\u8fdb\u7684LLM\u6a21\u578b", "result": "\u6784\u5efa\u4e86\u9996\u4e2a\u4e13\u95e8\u9488\u5bf9ArkTS\u81ea\u52a8\u5316\u7a0b\u5e8f\u4fee\u590d\u7684\u5168\u9762\u57fa\u51c6\u6570\u636e\u96c6ArkEval\uff0c\u5305\u542b502\u4e2a\u53ef\u590d\u73b0\u95ee\u9898\u3002\u901a\u8fc7\u8bc4\u4f30\u56db\u4e2a\u6700\u5148\u8fdb\u7684LLM\u6a21\u578b\uff0c\u63ed\u793a\u4e86\u5f53\u524dLLM\u5728\u4fee\u590dArkTS\u4ee3\u7801\u65b9\u9762\u7684\u80fd\u529b\u548c\u5c40\u9650\u6027\u3002", "conclusion": "ArkEval\u586b\u8865\u4e86HarmonyOS\u751f\u6001\u7cfb\u7edf\u4e2dArkTS\u8bed\u8a00\u81ea\u52a8\u5316\u4ee3\u7801\u4fee\u590d\u8bc4\u4f30\u7684\u7a7a\u767d\uff0c\u4e3a\u672a\u6765\u5728\u8fd9\u4e2a\u4f4e\u8d44\u6e90\u8bed\u8a00\u9886\u57df\u7684\u7814\u7a76\u94fa\u5e73\u4e86\u9053\u8def\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8ArkTS\u5f00\u53d1\u5de5\u5177\u7684\u8fdb\u6b65\u3002"}}
{"id": "2602.07954", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07954", "abs": "https://arxiv.org/abs/2602.07954", "authors": ["Krzysztof Wr\u00f3bel", "Jan Maria Kowalski", "Jerzy Surma", "Igor Ciuciura", "Maciej Szyma\u0144ski"], "title": "Bielik Guard: Efficient Polish Language Safety Classifiers for LLM Content Moderation", "comment": null, "summary": "As Large Language Models (LLMs) become increasingly deployed in Polish language applications, the need for efficient and accurate content safety classifiers has become paramount. We present Bielik Guard, a family of compact Polish language safety classifiers comprising two model variants: a 0.1B parameter model based on MMLW-RoBERTa-base and a 0.5B parameter model based on PKOBP/polish-roberta-8k. Fine-tuned on a community-annotated dataset of 6,885 Polish texts, these models classify content across five safety categories: Hate/Aggression, Vulgarities, Sexual Content, Crime, and Self-Harm. Our evaluation demonstrates that both models achieve strong performance on multiple benchmarks. The 0.5B variant offers the best overall discrimination capability with F1 scores of 0.791 (micro) and 0.785 (macro) on the test set, while the 0.1B variant demonstrates exceptional efficiency. Notably, Bielik Guard 0.1B v1.1 achieves superior precision (77.65\\%) and very low false positive rate (0.63\\%) on real user prompts, outperforming HerBERT-PL-Guard (31.55\\% precision, 4.70\\% FPR) despite identical model size. The models are publicly available and designed to provide appropriate responses rather than simple content blocking, particularly for sensitive categories like self-harm.", "AI": {"tldr": "Bielik Guard\u662f\u4e00\u7cfb\u5217\u6ce2\u5170\u8bed\u5185\u5bb9\u5b89\u5168\u5206\u7c7b\u5668\uff0c\u5305\u542b0.1B\u548c0.5B\u53c2\u6570\u4e24\u4e2a\u53d8\u4f53\uff0c\u7528\u4e8e\u68c0\u6d4b\u4ec7\u6068/\u653b\u51fb\u3001\u7c97\u4fd7\u3001\u6027\u5185\u5bb9\u3001\u72af\u7f6a\u548c\u81ea\u6b8b\u4e94\u7c7b\u4e0d\u5b89\u5168\u5185\u5bb9\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6ce2\u5170\u8bed\u5e94\u7528\u4e2d\u7684\u90e8\u7f72\u589e\u52a0\uff0c\u9700\u8981\u9ad8\u6548\u51c6\u786e\u7684\u5185\u5bb9\u5b89\u5168\u5206\u7c7b\u5668\u6765\u786e\u4fdd\u5185\u5bb9\u5b89\u5168\u3002", "method": "\u57fa\u4e8eMMLW-RoBERTa-base\uff080.1B\u53c2\u6570\uff09\u548cPKOBP/polish-roberta-8k\uff080.5B\u53c2\u6570\uff09\u6784\u5efa\uff0c\u57286,885\u4e2a\u793e\u533a\u6807\u6ce8\u7684\u6ce2\u5170\u6587\u672c\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5fae\u8c03\uff0c\u5206\u7c7b\u4e94\u7c7b\u4e0d\u5b89\u5168\u5185\u5bb9\u3002", "result": "0.5B\u53d8\u4f53\u5728\u6d4b\u8bd5\u96c6\u4e0a\u83b7\u5f970.791\uff08\u5fae\u89c2\uff09\u548c0.785\uff08\u5b8f\u89c2\uff09F1\u5206\u6570\uff1b0.1B\u53d8\u4f53\u5728\u5b9e\u9645\u7528\u6237\u63d0\u793a\u4e2d\u8fbe\u523077.65%\u7cbe\u786e\u5ea6\u548c0.63%\u4f4e\u8bef\u62a5\u7387\uff0c\u4f18\u4e8e\u76f8\u540c\u89c4\u6a21\u7684HerBERT-PL-Guard\u3002", "conclusion": "Bielik Guard\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u6ce2\u5170\u8bed\u5185\u5bb9\u5b89\u5168\u5206\u7c7b\u89e3\u51b3\u65b9\u6848\uff0c0.5B\u53d8\u4f53\u6027\u80fd\u6700\u4f73\uff0c0.1B\u53d8\u4f53\u6548\u7387\u7a81\u51fa\uff0c\u6a21\u578b\u516c\u5f00\u53ef\u7528\uff0c\u65e8\u5728\u63d0\u4f9b\u9002\u5f53\u54cd\u5e94\u800c\u975e\u7b80\u5355\u5185\u5bb9\u5c4f\u853d\u3002"}}
{"id": "2602.07198", "categories": ["cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2602.07198", "abs": "https://arxiv.org/abs/2602.07198", "authors": ["Heyuan Li", "Huimin Zhang", "Yuda Qiu", "Zhengwentai Sun", "Keru Zheng", "Lingteng Qiu", "Peihao Li", "Qi Zuo", "Ce Chen", "Yujian Zheng", "Yuming Gu", "Zilong Dong", "Xiaoguang Han"], "title": "Condition Matters in Full-head 3D GANs", "comment": "Accepted by ICLR 2026. Project page: https://lhyfst.github.io/balancehead/", "summary": "Conditioning is crucial for stable training of full-head 3D GANs. Without any conditioning signal, the model suffers from severe mode collapse, making it impractical to training. However, a series of previous full-head 3D GANs conventionally choose the view angle as the conditioning input, which leads to a bias in the learned 3D full-head space along the conditional view direction. This is evident in the significant differences in generation quality and diversity between the conditional view and non-conditional views of the generated 3D heads, resulting in global incoherence across different head regions. In this work, we propose to use view-invariant semantic feature as the conditioning input, thereby decoupling the generative capability of 3D heads from the viewing direction. To construct a view-invariant semantic condition for each training image, we create a novel synthesized head image dataset. We leverage FLUX.1 Kontext to extend existing high-quality frontal face datasets to a wide range of view angles. The image clip feature extracted from the frontal view is then used as a shared semantic condition across all views in the extended images, ensuring semantic alignment while eliminating directional bias. This also allows supervision from different views of the same subject to be consolidated under a shared semantic condition, which accelerates training and enhances the global coherence of the generated 3D heads. Moreover, as GANs often experience slower improvements in diversity once the generator learns a few modes that successfully fool the discriminator, our semantic conditioning encourages the generator to follow the true semantic distribution, thereby promoting continuous learning and diverse generation. Extensive experiments on full-head synthesis and single-view GAN inversion demonstrate that our method achieves significantly higher fidelity, diversity, and generalizability.", "AI": {"tldr": "\u63d0\u51fa\u4f7f\u7528\u89c6\u89d2\u4e0d\u53d8\u8bed\u4e49\u7279\u5f81\u4f5c\u4e3a\u6761\u4ef6\u8f93\u5165\uff0c\u89e3\u51b3\u4f20\u7edf3D\u5934\u90e8GAN\u4e2d\u56e0\u4f7f\u7528\u89c6\u89d2\u89d2\u5ea6\u4f5c\u4e3a\u6761\u4ef6\u5bfc\u81f4\u7684\u751f\u6210\u504f\u5dee\u95ee\u9898\uff0c\u63d0\u53473D\u5934\u90e8\u751f\u6210\u7684\u5168\u5c40\u4e00\u81f4\u6027\u548c\u591a\u6837\u6027\u3002", "motivation": "\u4f20\u7edf\u5168\u5934\u90e83D GAN\u4f7f\u7528\u89c6\u89d2\u89d2\u5ea6\u4f5c\u4e3a\u6761\u4ef6\u8f93\u5165\uff0c\u5bfc\u81f4\u5b66\u4e60\u76843D\u5934\u90e8\u7a7a\u95f4\u5b58\u5728\u89c6\u89d2\u65b9\u5411\u504f\u5dee\uff0c\u751f\u6210\u8d28\u91cf\u5728\u4e0d\u540c\u89c6\u89d2\u95f4\u5dee\u5f02\u663e\u8457\uff0c\u5168\u5c40\u4e00\u81f4\u6027\u5dee\u3002\u9700\u8981\u6d88\u9664\u8fd9\u79cd\u89c6\u89d2\u4f9d\u8d56\u7684\u504f\u5dee\u3002", "method": "1) \u4f7f\u7528\u89c6\u89d2\u4e0d\u53d8\u8bed\u4e49\u7279\u5f81\u4f5c\u4e3a\u6761\u4ef6\u8f93\u5165\uff0c\u800c\u975e\u89c6\u89d2\u89d2\u5ea6\uff1b2) \u521b\u5efa\u5408\u6210\u5934\u90e8\u56fe\u50cf\u6570\u636e\u96c6\uff0c\u5229\u7528FLUX.1 Kontext\u6269\u5c55\u73b0\u6709\u9ad8\u8d28\u91cf\u6b63\u9762\u4eba\u8138\u6570\u636e\u96c6\u5230\u591a\u89c6\u89d2\uff1b3) \u4f7f\u7528\u6b63\u9762\u89c6\u56fe\u63d0\u53d6\u7684\u56fe\u50cfclip\u7279\u5f81\u4f5c\u4e3a\u6240\u6709\u89c6\u89d2\u7684\u5171\u4eab\u8bed\u4e49\u6761\u4ef6\u3002", "result": "\u65b9\u6cd5\u57283D\u5934\u90e8\u5408\u6210\u548c\u5355\u89c6\u56feGAN\u53cd\u6f14\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u663e\u8457\u66f4\u9ad8\u7684\u4fdd\u771f\u5ea6\u3001\u591a\u6837\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u89e3\u51b3\u4e86\u89c6\u89d2\u504f\u5dee\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u5168\u5c40\u4e00\u81f4\u6027\u3002", "conclusion": "\u89c6\u89d2\u4e0d\u53d8\u8bed\u4e49\u6761\u4ef6\u8f93\u5165\u80fd\u6709\u6548\u89e3\u80263D\u5934\u90e8\u751f\u6210\u80fd\u529b\u4e0e\u89c6\u89d2\u65b9\u5411\u7684\u4f9d\u8d56\uff0c\u6d88\u9664\u4f20\u7edf\u65b9\u6cd5\u4e2d\u7684\u89c6\u89d2\u504f\u5dee\uff0c\u63d0\u5347\u751f\u6210\u8d28\u91cf\u548c\u591a\u6837\u6027\uff0c\u540c\u65f6\u52a0\u901f\u8bad\u7ec3\u8fc7\u7a0b\u3002"}}
{"id": "2602.07787", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07787", "abs": "https://arxiv.org/abs/2602.07787", "authors": ["Pierre-Louis Favreau", "Jean-Pierre Lo", "Clement Guiguet", "Charles Simon-Meunier", "Nicolas Dehandschoewercker", "Allen G. Roush", "Judah Goldfeder", "Ravid Shwartz-Ziv"], "title": "Do Multi-Agents Dream of Electric Screens? Achieving Perfect Accuracy on AndroidWorld Through Task Decomposition", "comment": null, "summary": "We present Minitap, a multi-agent system that achieves 100% success on the AndroidWorld benchmark, the first to fully solve all 116 tasks and surpassing human performance (80%). We first analyze why single-agent architectures fail: context pollution from mixed reasoning traces, silent text input failures undetected by the agent, and repetitive action loops without escape. Minitap addresses each failure through targeted mechanisms: cognitive separation across six specialized agents, deterministic post-validation of text input against device state, and meta-cognitive reasoning that detects cycles and triggers strategy changes. Ablations show multi-agent decomposition contributes +21 points over single-agent baselines; verified execution adds +7 points; meta-cognition adds +9 points. We release Minitap as open-source software. https://github.com/minitap-ai/mobile-use", "AI": {"tldr": "Minitap\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5728AndroidWorld\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0100%\u6210\u529f\u7387\uff0c\u9996\u6b21\u5b8c\u5168\u89e3\u51b3\u6240\u6709116\u4e2a\u4efb\u52a1\uff0c\u8d85\u8d8a\u4eba\u7c7b\u8868\u73b0\uff0880%\uff09\u3002", "motivation": "\u89e3\u51b3\u5355\u667a\u80fd\u4f53\u67b6\u6784\u5728\u79fb\u52a8\u8bbe\u5907\u4efb\u52a1\u6267\u884c\u4e2d\u7684\u5931\u8d25\u95ee\u9898\uff1a\u6df7\u5408\u63a8\u7406\u8f68\u8ff9\u5bfc\u81f4\u7684\u4e0a\u4e0b\u6587\u6c61\u67d3\u3001\u667a\u80fd\u4f53\u672a\u68c0\u6d4b\u5230\u7684\u9759\u9ed8\u6587\u672c\u8f93\u5165\u5931\u8d25\u3001\u4ee5\u53ca\u65e0\u9003\u8131\u673a\u5236\u7684\u91cd\u590d\u6742\u4f5c\u5faa\u73af\u3002", "method": "\u901a\u8fc7\u9488\u5bf9\u6027\u673a\u5236\uff1a\u516d\u4e2a\u4e13\u95e8\u5316\u667a\u80fd\u4f53\u7684\u8ba4\u77e5\u5206\u79bb\u3001\u57fa\u4e8e\u8bbe\u5907\u72b6\u6001\u7684\u6587\u672c\u8f93\u5165\u786e\u5b9a\u6027\u540e\u9a8c\u8bc1\u3001\u4ee5\u53ca\u68c0\u6d4b\u5faa\u73af\u5e76\u89e6\u53d1\u7b56\u7565\u6539\u53d8\u7684\u5143\u8ba4\u77e5\u63a8\u7406\u3002", "result": "\u5728AndroidWorld\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230100%\u6210\u529f\u7387\uff0c\u5b8c\u5168\u89e3\u51b3\u6240\u6709116\u4e2a\u4efb\u52a1\u3002\u6d88\u878d\u5b9e\u9a8c\u663e\u793a\uff1a\u591a\u667a\u80fd\u4f53\u5206\u89e3\u8d21\u732e+21\u5206\uff0c\u9a8c\u8bc1\u6267\u884c+7\u5206\uff0c\u5143\u8ba4\u77e5+9\u5206\u3002", "conclusion": "Minitap\u4f5c\u4e3a\u9996\u4e2a\u5b8c\u5168\u89e3\u51b3AndroidWorld\u57fa\u51c6\u6d4b\u8bd5\u7684\u7cfb\u7edf\uff0c\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u67b6\u6784\u3001\u9a8c\u8bc1\u6267\u884c\u548c\u5143\u8ba4\u77e5\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86\u5355\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u5c40\u9650\u6027\uff0c\u5df2\u5f00\u6e90\u53d1\u5e03\u3002"}}
{"id": "2602.08887", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08887", "abs": "https://arxiv.org/abs/2602.08887", "authors": ["Adam Trendowicz", "Daniel Seifert", "Andreas Jedlitschka", "Marcus Ciolkowski", "Anton Strahilov"], "title": "DeepQuali: Initial results of a study on the use of large language models for assessing the quality of user stories", "comment": null, "summary": "Generative artificial intelligence (GAI), specifically large language models (LLMs), are increasingly used in software engineering, mainly for coding tasks. However, requirements engineering - particularly requirements validation - has seen limited application of GAI. The current focus of using GAI for requirements is on eliciting, transforming, and classifying requirements, not on quality assessment. We propose and evaluate the LLM-based (GPT-4o) approach \"DeepQuali\", for assessing and improving requirements quality in agile software development. We applied it to projects in two small companies, where we compared LLM-based quality assessments with expert judgments. Experts also participated in walkthroughs of the solution, provided feedback, and rated their acceptance of the approach. Experts largely agreed with the LLM's quality assessments, especially regarding overall ratings and explanations. However, they did not always agree with the other experts on detailed ratings, suggesting that expertise and experience may influence judgments. Experts recognized the usefulness of the approach but criticized the lack of integration into their workflow. LLMs show potential in supporting software engineers with the quality assessment and improvement of requirements. The explicit use of quality models and explanatory feedback increases acceptance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u57fa\u4e8eGPT-4o\u7684\"DeepQuali\"\u65b9\u6cd5\uff0c\u7528\u4e8e\u654f\u6377\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u7684\u9700\u6c42\u8d28\u91cf\u8bc4\u4f30\u548c\u6539\u8fdb\uff0c\u5e76\u5728\u4e24\u5bb6\u5c0f\u578b\u516c\u53f8\u8fdb\u884c\u4e86\u5b9e\u8bc1\u8bc4\u4f30\u3002", "motivation": "\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\uff08\u7279\u522b\u662f\u5927\u8bed\u8a00\u6a21\u578b\uff09\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u4e3b\u8981\u7528\u4e8e\u7f16\u7801\u4efb\u52a1\uff0c\u4f46\u5728\u9700\u6c42\u5de5\u7a0b\uff08\u7279\u522b\u662f\u9700\u6c42\u9a8c\u8bc1\uff09\u65b9\u9762\u5e94\u7528\u6709\u9650\u3002\u5f53\u524dGAI\u5728\u9700\u6c42\u9886\u57df\u7684\u5e94\u7528\u4e3b\u8981\u96c6\u4e2d\u5728\u9700\u6c42\u83b7\u53d6\u3001\u8f6c\u6362\u548c\u5206\u7c7b\uff0c\u800c\u975e\u8d28\u91cf\u8bc4\u4f30\u3002", "method": "\u63d0\u51fa\u5e76\u8bc4\u4f30\u4e86\u57fa\u4e8eGPT-4o\u7684\"DeepQuali\"\u65b9\u6cd5\uff0c\u7528\u4e8e\u654f\u6377\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u7684\u9700\u6c42\u8d28\u91cf\u8bc4\u4f30\u548c\u6539\u8fdb\u3002\u8be5\u65b9\u6cd5\u5728\u4e24\u4e2a\u5c0f\u578b\u516c\u53f8\u7684\u9879\u76ee\u4e2d\u5e94\u7528\uff0c\u5c06LLM\u7684\u8d28\u91cf\u8bc4\u4f30\u4e0e\u4e13\u5bb6\u5224\u65ad\u8fdb\u884c\u6bd4\u8f83\uff0c\u5e76\u901a\u8fc7\u8d70\u67e5\u3001\u53cd\u9988\u6536\u96c6\u548c\u63a5\u53d7\u5ea6\u8bc4\u7ea7\u6765\u8bc4\u4f30\u65b9\u6cd5\u3002", "result": "\u4e13\u5bb6\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u540c\u610fLLM\u7684\u8d28\u91cf\u8bc4\u4f30\uff0c\u7279\u522b\u662f\u5728\u6574\u4f53\u8bc4\u7ea7\u548c\u89e3\u91ca\u65b9\u9762\u3002\u4f46\u4e13\u5bb6\u4e4b\u95f4\u5728\u8be6\u7ec6\u8bc4\u7ea7\u4e0a\u5e76\u4e0d\u603b\u662f\u4e00\u81f4\uff0c\u8868\u660e\u4e13\u4e1a\u77e5\u8bc6\u548c\u7ecf\u9a8c\u53ef\u80fd\u5f71\u54cd\u5224\u65ad\u3002\u4e13\u5bb6\u8ba4\u53ef\u8be5\u65b9\u6cd5\u7684\u5b9e\u7528\u6027\uff0c\u4f46\u6279\u8bc4\u5176\u7f3a\u4e4f\u5de5\u4f5c\u6d41\u7a0b\u96c6\u6210\u3002", "conclusion": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u652f\u6301\u8f6f\u4ef6\u5de5\u7a0b\u5e08\u8fdb\u884c\u9700\u6c42\u8d28\u91cf\u8bc4\u4f30\u548c\u6539\u8fdb\u65b9\u9762\u5177\u6709\u6f5c\u529b\u3002\u660e\u786e\u4f7f\u7528\u8d28\u91cf\u6a21\u578b\u548c\u89e3\u91ca\u6027\u53cd\u9988\u53ef\u4ee5\u63d0\u9ad8\u63a5\u53d7\u5ea6\uff0c\u4f46\u9700\u8981\u66f4\u597d\u5730\u96c6\u6210\u5230\u73b0\u6709\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u3002"}}
{"id": "2602.08401", "categories": ["cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2602.08401", "abs": "https://arxiv.org/abs/2602.08401", "authors": ["Liwen Wang", "Zongjie Li", "Yuchong Xie", "Shuai Wang", "Dongdong She", "Wei Wang", "Juergen Rahmel"], "title": "On Protecting Agentic Systems' Intellectual Property via Watermarking", "comment": null, "summary": "The evolution of Large Language Models (LLMs) into agentic systems that perform autonomous reasoning and tool use has created significant intellectual property (IP) value. We demonstrate that these systems are highly vulnerable to imitation attacks, where adversaries steal proprietary capabilities by training imitation models on victim outputs. Crucially, existing LLM watermarking techniques fail in this domain because real-world agentic systems often operate as grey boxes, concealing the internal reasoning traces required for verification. This paper presents AGENTWM, the first watermarking framework designed specifically for agentic models. AGENTWM exploits the semantic equivalence of action sequences, injecting watermarks by subtly biasing the distribution of functionally identical tool execution paths. This mechanism allows AGENTWM to embed verifiable signals directly into the visible action trajectory while remaining indistinguishable to users. We develop an automated pipeline to generate robust watermark schemes and a rigorous statistical hypothesis testing procedure for verification. Extensive evaluations across three complex domains demonstrate that AGENTWM achieves high detection accuracy with negligible impact on agent performance. Our results confirm that AGENTWM effectively protects agentic IP against adaptive adversaries, who cannot remove the watermarks without severely degrading the stolen model's utility.", "AI": {"tldr": "AGENTWM\u662f\u9996\u4e2a\u4e13\u95e8\u4e3a\u667a\u80fd\u4f53\u6a21\u578b\u8bbe\u8ba1\u7684\u6c34\u5370\u6846\u67b6\uff0c\u901a\u8fc7\u5229\u7528\u52a8\u4f5c\u5e8f\u5217\u7684\u8bed\u4e49\u7b49\u4ef7\u6027\uff0c\u5728\u529f\u80fd\u76f8\u540c\u7684\u5de5\u5177\u6267\u884c\u8def\u5f84\u4e2d\u6ce8\u5165\u6c34\u5370\uff0c\u6709\u6548\u4fdd\u62a4\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u77e5\u8bc6\u4ea7\u6743\u514d\u53d7\u6a21\u4eff\u653b\u51fb\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5411\u80fd\u591f\u8fdb\u884c\u81ea\u4e3b\u63a8\u7406\u548c\u5de5\u5177\u4f7f\u7528\u7684\u667a\u80fd\u4f53\u7cfb\u7edf\u6f14\u8fdb\uff0c\u521b\u9020\u4e86\u91cd\u8981\u7684\u77e5\u8bc6\u4ea7\u6743\u4ef7\u503c\u3002\u8fd9\u4e9b\u7cfb\u7edf\u5bb9\u6613\u53d7\u5230\u6a21\u4eff\u653b\u51fb\uff0c\u800c\u73b0\u6709\u7684LLM\u6c34\u5370\u6280\u672f\u5728\u667a\u80fd\u4f53\u9886\u57df\u5931\u6548\uff0c\u56e0\u4e3a\u73b0\u5b9e\u4e2d\u7684\u667a\u80fd\u4f53\u7cfb\u7edf\u901a\u5e38\u4f5c\u4e3a\u7070\u76d2\u8fd0\u884c\uff0c\u9690\u85cf\u4e86\u9a8c\u8bc1\u6240\u9700\u7684\u5185\u90e8\u63a8\u7406\u8f68\u8ff9\u3002", "method": "AGENTWM\u5229\u7528\u52a8\u4f5c\u5e8f\u5217\u7684\u8bed\u4e49\u7b49\u4ef7\u6027\uff0c\u901a\u8fc7\u5fae\u5999\u5730\u504f\u5411\u529f\u80fd\u76f8\u540c\u7684\u5de5\u5177\u6267\u884c\u8def\u5f84\u7684\u5206\u5e03\u6765\u6ce8\u5165\u6c34\u5370\u3002\u5f00\u53d1\u4e86\u81ea\u52a8\u751f\u6210\u9c81\u68d2\u6c34\u5370\u65b9\u6848\u7684\u6d41\u6c34\u7ebf\uff0c\u4ee5\u53ca\u4e25\u683c\u7684\u7edf\u8ba1\u5047\u8bbe\u68c0\u9a8c\u7a0b\u5e8f\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u5728\u4e09\u4e2a\u590d\u6742\u9886\u57df\u7684\u5e7f\u6cdb\u8bc4\u4f30\u8868\u660e\uff0cAGENTWM\u5b9e\u73b0\u4e86\u9ad8\u68c0\u6d4b\u51c6\u786e\u7387\uff0c\u540c\u65f6\u5bf9\u667a\u80fd\u4f53\u6027\u80fd\u5f71\u54cd\u6781\u5c0f\u3002AGENTWM\u80fd\u6709\u6548\u4fdd\u62a4\u667a\u80fd\u4f53\u77e5\u8bc6\u4ea7\u6743\uff0c\u5bf9\u6297\u6027\u653b\u51fb\u8005\u65e0\u6cd5\u5728\u4e0d\u4e25\u91cd\u964d\u4f4e\u88ab\u76d7\u6a21\u578b\u6548\u7528\u7684\u60c5\u51b5\u4e0b\u79fb\u9664\u6c34\u5370\u3002", "conclusion": "AGENTWM\u662f\u9996\u4e2a\u4e13\u95e8\u4e3a\u667a\u80fd\u4f53\u6a21\u578b\u8bbe\u8ba1\u7684\u6c34\u5370\u6846\u67b6\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u73b0\u6709LLM\u6c34\u5370\u6280\u672f\u5728\u667a\u80fd\u4f53\u9886\u57df\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u4fdd\u62a4\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u77e5\u8bc6\u4ea7\u6743\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.07963", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07963", "abs": "https://arxiv.org/abs/2602.07963", "authors": ["Vaibhav Shukla", "Hardik Sharma", "Adith N Reganti", "Soham Wasmatkar", "Bagesh Kumar", "Vrijendra Singh"], "title": "Lost in Translation? A Comparative Study on the Cross-Lingual Transfer of Composite Harms", "comment": "Accepted at the AICS Workshop, AAAI 2026", "summary": "Most safety evaluations of large language models (LLMs) remain anchored in English. Translation is often used as a shortcut to probe multilingual behavior, but it rarely captures the full picture, especially when harmful intent or structure morphs across languages. Some types of harm survive translation almost intact, while others distort or disappear. To study this effect, we introduce CompositeHarm, a translation-based benchmark designed to examine how safety alignment holds up as both syntax and semantics shift. It combines two complementary English datasets, AttaQ, which targets structured adversarial attacks, and MMSafetyBench, which covers contextual, real-world harms, and extends them into six languages: English, Hindi, Assamese, Marathi, Kannada, and Gujarati. Using three large models, we find that attack success rates rise sharply in Indic languages, especially under adversarial syntax, while contextual harms transfer more moderately. To ensure scalability and energy efficiency, our study adopts lightweight inference strategies inspired by edge-AI design principles, reducing redundant evaluation passes while preserving cross-lingual fidelity. This design makes large-scale multilingual safety testing both computationally feasible and environmentally conscious. Overall, our results show that translated benchmarks are a necessary first step, but not a sufficient one, toward building grounded, resource-aware, language-adaptive safety systems.", "AI": {"tldr": "\u8be5\u7814\u7a76\u521b\u5efa\u4e86CompositeHarm\u57fa\u51c6\u6d4b\u8bd5\uff0c\u901a\u8fc7\u7ffb\u8bd1\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5728\u591a\u8bed\u8a00\u73af\u5883\u4e0b\u7684\u5b89\u5168\u5bf9\u9f50\u8868\u73b0\uff0c\u53d1\u73b0\u653b\u51fb\u6210\u529f\u7387\u5728\u5370\u5ea6\u8bed\u8a00\u4e2d\u663e\u8457\u4e0a\u5347\uff0c\u7279\u522b\u662f\u5728\u5bf9\u6297\u6027\u8bed\u6cd5\u4e0b\uff0c\u800c\u4e0a\u4e0b\u6587\u5371\u5bb3\u8f6c\u79fb\u8f83\u4e3a\u6e29\u548c\u3002", "motivation": "\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5b89\u5168\u8bc4\u4f30\u4e3b\u8981\u57fa\u4e8e\u82f1\u8bed\uff0c\u7ffb\u8bd1\u4f5c\u4e3a\u591a\u8bed\u8a00\u884c\u4e3a\u63a2\u6d4b\u7684\u6377\u5f84\uff0c\u4f46\u65e0\u6cd5\u5b8c\u6574\u6355\u6349\u6709\u5bb3\u610f\u56fe\u6216\u7ed3\u6784\u5728\u4e0d\u540c\u8bed\u8a00\u95f4\u7684\u53d8\u5316\u3002\u67d0\u4e9b\u5371\u5bb3\u5728\u7ffb\u8bd1\u540e\u51e0\u4e4e\u4fdd\u6301\u4e0d\u53d8\uff0c\u800c\u5176\u4ed6\u5371\u5bb3\u5219\u4f1a\u626d\u66f2\u6216\u6d88\u5931\uff0c\u9700\u8981\u7cfb\u7edf\u7814\u7a76\u8fd9\u79cd\u6548\u5e94\u3002", "method": "\u5f15\u5165CompositeHarm\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7ed3\u5408\u4e24\u4e2a\u4e92\u8865\u7684\u82f1\u6587\u6570\u636e\u96c6\uff08AttaQ\u9488\u5bf9\u7ed3\u6784\u5316\u5bf9\u6297\u653b\u51fb\uff0cMMSafetyBench\u8986\u76d6\u4e0a\u4e0b\u6587\u73b0\u5b9e\u4e16\u754c\u5371\u5bb3\uff09\uff0c\u5c06\u5176\u6269\u5c55\u5230\u516d\u79cd\u8bed\u8a00\uff1a\u82f1\u8bed\u3001\u5370\u5730\u8bed\u3001\u963f\u8428\u59c6\u8bed\u3001\u9a6c\u62c9\u5730\u8bed\u3001\u5361\u7eb3\u8fbe\u8bed\u548c\u53e4\u5409\u62c9\u7279\u8bed\u3002\u91c7\u7528\u4e09\u4e2a\u5927\u6a21\u578b\u8fdb\u884c\u8bc4\u4f30\uff0c\u5e76\u91c7\u7528\u53d7\u8fb9\u7f18AI\u8bbe\u8ba1\u539f\u5219\u542f\u53d1\u7684\u8f7b\u91cf\u7ea7\u63a8\u7406\u7b56\u7565\uff0c\u51cf\u5c11\u5197\u4f59\u8bc4\u4f30\u6b21\u6570\u540c\u65f6\u4fdd\u6301\u8de8\u8bed\u8a00\u4fdd\u771f\u5ea6\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u653b\u51fb\u6210\u529f\u7387\u5728\u5370\u5ea6\u8bed\u8a00\u4e2d\u6025\u5267\u4e0a\u5347\uff0c\u7279\u522b\u662f\u5728\u5bf9\u6297\u6027\u8bed\u6cd5\u4e0b\uff0c\u800c\u4e0a\u4e0b\u6587\u5371\u5bb3\u8f6c\u79fb\u8f83\u4e3a\u6e29\u548c\u3002\u8f7b\u91cf\u7ea7\u63a8\u7406\u7b56\u7565\u4f7f\u5927\u89c4\u6a21\u591a\u8bed\u8a00\u5b89\u5168\u6d4b\u8bd5\u5728\u8ba1\u7b97\u4e0a\u53ef\u884c\u4e14\u73af\u4fdd\u3002", "conclusion": "\u7ffb\u8bd1\u57fa\u51c6\u6d4b\u8bd5\u662f\u6784\u5efa\u6709\u57fa\u7840\u3001\u8d44\u6e90\u611f\u77e5\u3001\u8bed\u8a00\u81ea\u9002\u5e94\u5b89\u5168\u7cfb\u7edf\u7684\u5fc5\u8981\u7b2c\u4e00\u6b65\uff0c\u4f46\u8fd8\u4e0d\u591f\u5145\u5206\u3002\u9700\u8981\u66f4\u6df1\u5165\u7684\u591a\u8bed\u8a00\u5b89\u5168\u8bc4\u4f30\u65b9\u6cd5\u6765\u786e\u4fdd\u6a21\u578b\u5728\u5404\u79cd\u8bed\u8a00\u73af\u5883\u4e0b\u7684\u5b89\u5168\u6027\u3002"}}
{"id": "2602.07212", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.07212", "abs": "https://arxiv.org/abs/2602.07212", "authors": ["Xinyu Liu", "Darryl C. Jacob", "Yuxin Liu", "Xinsong Du", "Muchao Ye", "Bolei Zhou", "Pan He"], "title": "Understanding Real-World Traffic Safety through RoadSafe365 Benchmark", "comment": null, "summary": "Although recent traffic benchmarks have advanced multimodal data analysis, they generally lack systematic evaluation aligned with official safety standards. To fill this gap, we introduce RoadSafe365, a large-scale vision-language benchmark that supports fine-grained analysis of traffic safety from extensive and diverse real-world video data collections. Unlike prior works that focus primarily on coarse accident identification, RoadSafe365 is independently curated and systematically organized using a hierarchical taxonomy that refines and extends foundational definitions of crash, incident, and violation to bridge official traffic safety standards with data-driven traffic understanding systems. RoadSafe365 provides rich attribute annotations across diverse traffic event types, environmental contexts, and interaction scenarios, yielding 36,196 annotated clips from both dashcam and surveillance cameras. Each clip is paired with multiple-choice question-answer sets, comprising 864K candidate options, 8.4K unique answers, and 36K detailed scene descriptions collectively designed for vision-language understanding and reasoning. We establish strong baselines and observe consistent gains when fine-tuning on RoadSafe365. Cross-domain experiments on both real and synthetic datasets further validate its effectiveness. Designed for large-scale training and standardized evaluation, RoadSafe365 provides a comprehensive benchmark to advance reproducible research in real-world traffic safety analysis.", "AI": {"tldr": "RoadSafe365\u662f\u4e00\u4e2a\u5927\u89c4\u6a21\u89c6\u89c9\u8bed\u8a00\u57fa\u51c6\uff0c\u7528\u4e8e\u7ec6\u7c92\u5ea6\u4ea4\u901a\u5b89\u5168\u6027\u5206\u6790\uff0c\u5305\u542b36,196\u4e2a\u6807\u6ce8\u89c6\u9891\u7247\u6bb5\u548c864K\u5019\u9009\u9009\u9879\uff0c\u586b\u8865\u4e86\u73b0\u6709\u57fa\u51c6\u7f3a\u4e4f\u5b98\u65b9\u5b89\u5168\u6807\u51c6\u7cfb\u7edf\u8bc4\u4f30\u7684\u7a7a\u767d\u3002", "motivation": "\u73b0\u6709\u4ea4\u901a\u57fa\u51c6\u867d\u7136\u63a8\u8fdb\u4e86\u591a\u6a21\u6001\u6570\u636e\u5206\u6790\uff0c\u4f46\u7f3a\u4e4f\u4e0e\u5b98\u65b9\u5b89\u5168\u6807\u51c6\u5bf9\u9f50\u7684\u7cfb\u7edf\u6027\u8bc4\u4f30\u3002\u9700\u8981\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u5efa\u7acb\u80fd\u591f\u8fde\u63a5\u5b98\u65b9\u4ea4\u901a\u5b89\u5168\u6807\u51c6\u4e0e\u6570\u636e\u9a71\u52a8\u4ea4\u901a\u7406\u89e3\u7cfb\u7edf\u7684\u57fa\u51c6\u3002", "method": "\u6784\u5efaRoadSafe365\u57fa\u51c6\uff0c\u91c7\u7528\u5206\u5c42\u5206\u7c7b\u6cd5\u7cfb\u7edf\u7ec4\u7ec7\uff0c\u7ec6\u5316\u548c\u6269\u5c55\u4e86\u78b0\u649e\u3001\u4e8b\u6545\u548c\u8fdd\u89c4\u7684\u57fa\u7840\u5b9a\u4e49\u3002\u4ece\u884c\u8f66\u8bb0\u5f55\u4eea\u548c\u76d1\u63a7\u6444\u50cf\u5934\u6536\u96c6\u771f\u5b9e\u4e16\u754c\u89c6\u9891\u6570\u636e\uff0c\u63d0\u4f9b\u4e30\u5bcc\u7684\u5c5e\u6027\u6807\u6ce8\uff0c\u5305\u62ec\u4ea4\u901a\u4e8b\u4ef6\u7c7b\u578b\u3001\u73af\u5883\u80cc\u666f\u548c\u4ea4\u4e92\u573a\u666f\u3002\u6bcf\u4e2a\u7247\u6bb5\u90fd\u914d\u6709\u591a\u9009\u9898-\u7b54\u6848\u96c6\u548c\u8be6\u7ec6\u573a\u666f\u63cf\u8ff0\u3002", "result": "\u5efa\u7acb\u4e86\u5305\u542b36,196\u4e2a\u6807\u6ce8\u7247\u6bb5\u7684\u5927\u89c4\u6a21\u57fa\u51c6\uff0c\u5305\u542b864K\u5019\u9009\u9009\u9879\u30018.4K\u72ec\u7279\u7b54\u6848\u548c36K\u8be6\u7ec6\u573a\u666f\u63cf\u8ff0\u3002\u5728RoadSafe365\u4e0a\u5fae\u8c03\u663e\u793a\u4e86\u4e00\u81f4\u7684\u6027\u80fd\u63d0\u5347\uff0c\u8de8\u9886\u57df\u5b9e\u9a8c\uff08\u5305\u62ec\u771f\u5b9e\u548c\u5408\u6210\u6570\u636e\u96c6\uff09\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "RoadSafe365\u4e3a\u5927\u89c4\u6a21\u8bad\u7ec3\u548c\u6807\u51c6\u5316\u8bc4\u4f30\u8bbe\u8ba1\uff0c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5168\u9762\u7684\u57fa\u51c6\uff0c\u80fd\u591f\u63a8\u8fdb\u771f\u5b9e\u4e16\u754c\u4ea4\u901a\u5b89\u5168\u6027\u5206\u6790\u7684\u53ef\u91cd\u590d\u7814\u7a76\uff0c\u5f25\u5408\u4e86\u5b98\u65b9\u5b89\u5168\u6807\u51c6\u4e0e\u6570\u636e\u9a71\u52a8\u7cfb\u7edf\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002"}}
{"id": "2602.07824", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.07824", "abs": "https://arxiv.org/abs/2602.07824", "authors": ["Yiwei Qin", "Zhen Huang", "Tiantian Mi", "Weiye Si", "Chenyang Zhou", "Qipeng Guo", "Siyuan Feng", "Pengfei Liu"], "title": "Data Darwinism Part I: Unlocking the Value of Scientific Data for Pre-training", "comment": null, "summary": "Data quality determines foundation model performance, yet systematic processing frameworks are lacking. We introduce Data Darwinism, a ten-level taxonomy (L0-L9) that conceptualizes data-model co-evolution: advanced models produce superior data for next-generation systems. We validate this on scientific literature by constructing Darwin-Science, a 900B-token corpus (L0-L5). We identify a learnability gap in raw scientific text, which we bridge via L4 (Generative Refinement) and L5 (Cognitive Completion) using frontier LLMs to explicate reasoning and terminology.\n  To ensure rigorous attribution, we pre-trained daVinci-origin-3B/7B models from scratch, excluding scientific content to create contamination-free baselines. After 600B tokens of continued pre-training, Darwin-Science outperforms baselines by +2.12 (3B) and +2.95 (7B) points across 20+ benchmarks, rising to +5.60 and +8.40 points on domain-aligned tasks. Systematic progression to L5 yields a +1.36 total gain, confirming that higher-level processing unlocks latent data value. We release the Darwin-Science corpus and daVinci-origin models to enable principled, co-evolutionary development.", "AI": {"tldr": "\u63d0\u51faData Darwinism\u5341\u7ea7\u5206\u7c7b\u6cd5\uff0c\u901a\u8fc7\u6570\u636e-\u6a21\u578b\u534f\u540c\u8fdb\u5316\u6846\u67b6\u63d0\u5347\u57fa\u7840\u6a21\u578b\u6027\u80fd\uff0c\u5728\u79d1\u5b66\u6587\u732e\u9886\u57df\u6784\u5efaDarwin-Science\u8bed\u6599\u5e93\u5e76\u9a8c\u8bc1\u6709\u6548\u6027", "motivation": "\u6570\u636e\u8d28\u91cf\u51b3\u5b9a\u57fa\u7840\u6a21\u578b\u6027\u80fd\uff0c\u4f46\u7f3a\u4e4f\u7cfb\u7edf\u5316\u7684\u6570\u636e\u5904\u7406\u6846\u67b6\u3002\u9700\u8981\u5efa\u7acb\u6570\u636e\u4e0e\u6a21\u578b\u534f\u540c\u8fdb\u5316\u7684\u7406\u8bba\u6846\u67b6\uff0c\u8ba9\u5148\u8fdb\u6a21\u578b\u4e3a\u4e0b\u4e00\u4ee3\u7cfb\u7edf\u751f\u6210\u66f4\u4f18\u8d28\u6570\u636e", "method": "\u63d0\u51faData Darwinism\u5341\u7ea7\u5206\u7c7b\u6cd5\uff08L0-L9\uff09\uff0c\u5728\u79d1\u5b66\u6587\u732e\u9886\u57df\u6784\u5efaDarwin-Science\u8bed\u6599\u5e93\uff08900B tokens\uff0cL0-L5\uff09\u3002\u4f7f\u7528\u524d\u6cbfLLM\u8fdb\u884cL4\uff08\u751f\u6210\u7cbe\u70bc\uff09\u548cL5\uff08\u8ba4\u77e5\u8865\u5168\uff09\u5904\u7406\uff0c\u89e3\u51b3\u539f\u59cb\u79d1\u5b66\u6587\u672c\u7684\u53ef\u5b66\u4e60\u6027\u5dee\u8ddd\u3002\u4ece\u5934\u9884\u8bad\u7ec3daVinci-origin-3B/7B\u6a21\u578b\u4f5c\u4e3a\u65e0\u6c61\u67d3\u57fa\u7ebf\uff0c\u7136\u540e\u8fdb\u884c600B tokens\u7684\u6301\u7eed\u9884\u8bad\u7ec3", "result": "Darwin-Science\u572820\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6bd4\u57fa\u7ebf\u6a21\u578b\u63d0\u5347+2.12\uff083B\uff09\u548c+2.95\uff087B\uff09\u5206\uff0c\u5728\u9886\u57df\u5bf9\u9f50\u4efb\u52a1\u4e0a\u63d0\u5347+5.60\u548c+8.40\u5206\u3002\u7cfb\u7edf\u6027\u5730\u63a8\u8fdb\u5230L5\u7ea7\u522b\u5e26\u6765+1.36\u7684\u603b\u589e\u76ca\uff0c\u8bc1\u5b9e\u9ad8\u7ea7\u6570\u636e\u5904\u7406\u80fd\u89e3\u9501\u6f5c\u5728\u6570\u636e\u4ef7\u503c", "conclusion": "Data Darwinism\u6846\u67b6\u6709\u6548\u9a8c\u8bc1\u4e86\u6570\u636e-\u6a21\u578b\u534f\u540c\u8fdb\u5316\u7684\u6982\u5ff5\uff0c\u9ad8\u7ea7\u6570\u636e\u5904\u7406\u80fd\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002\u53d1\u5e03Darwin-Science\u8bed\u6599\u5e93\u548cdaVinci-origin\u6a21\u578b\uff0c\u652f\u6301\u57fa\u4e8e\u539f\u5219\u7684\u534f\u540c\u8fdb\u5316\u5f00\u53d1"}}
{"id": "2602.08915", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.08915", "abs": "https://arxiv.org/abs/2602.08915", "authors": ["Giovanni Pinna", "Jingzhi Gong", "David Williams", "Federica Sarro"], "title": "Comparing AI Coding Agents: A Task-Stratified Analysis of Pull Request Acceptance", "comment": "Accepted by MSR'26 Mining Challenge Track", "summary": "The rapid adoption of AI-powered coding assistants is transforming software development practices, yet systematic comparisons of their effectiveness across different task types and over time remain limited. This paper presents an empirical study comparing five popular agents (OpenAI Codex, GitHub Copilot, Devin, Cursor, and Claude Code), analyzing 7,156 pull requests (PRs) from the AIDev dataset. Temporal trend analysis reveals heterogeneous evolution patterns: Devin exhibits the only consistent positive trend in acceptance rate (+0.77% per week over 32 weeks), whereas other agents remain largely stable. Our analysis suggests that the PR task type is a dominant factor influencing acceptance rates: documentation tasks achieve 82.1% acceptance compared to 66.1% for new features - a 16 percentage point gap that exceeds typical inter-agent variance for most tasks. OpenAI Codex achieves consistently high acceptance rates across all nine task categories (59.6%-88.6%), with stratified Chi-square tests confirming statistically significant advantages over other agents in several task categories. However, no single agent performs best across all task types: Claude Code leads in documentation (92.3%) and features (72.6%), while Cursor excels in fix tasks (80.4%).", "AI": {"tldr": "\u8be5\u7814\u7a76\u5bf95\u4e2aAI\u7f16\u7a0b\u52a9\u624b\u57287,156\u4e2aPR\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u8fdb\u884c\u4e86\u5b9e\u8bc1\u6bd4\u8f83\uff0c\u53d1\u73b0\u4efb\u52a1\u7c7b\u578b\u662f\u63a5\u53d7\u7387\u7684\u4e3b\u8981\u5f71\u54cd\u56e0\u7d20\uff0c\u6587\u6863\u4efb\u52a1\u63a5\u53d7\u7387\u6700\u9ad8(82.1%)\uff0c\u4e0d\u540c\u52a9\u624b\u5728\u4e0d\u540c\u4efb\u52a1\u7c7b\u578b\u4e0a\u5404\u6709\u4f18\u52bf\u3002", "motivation": "\u968f\u7740AI\u7f16\u7a0b\u52a9\u624b\u7684\u5feb\u901f\u666e\u53ca\uff0c\u7f3a\u4e4f\u5bf9\u5176\u5728\u4e0d\u540c\u4efb\u52a1\u7c7b\u578b\u548c\u65f6\u95f4\u7ef4\u5ea6\u4e0a\u6709\u6548\u6027\u7684\u7cfb\u7edf\u6bd4\u8f83\uff0c\u9700\u8981\u5b9e\u8bc1\u7814\u7a76\u6765\u6307\u5bfc\u5f00\u53d1\u8005\u9009\u62e9\u5408\u9002\u7684\u5de5\u5177\u3002", "method": "\u4f7f\u7528AIDev\u6570\u636e\u96c6\u76847,156\u4e2aPR\u6570\u636e\uff0c\u6bd4\u8f83OpenAI Codex\u3001GitHub Copilot\u3001Devin\u3001Cursor\u548cClaude Code\u4e94\u4e2aAI\u52a9\u624b\uff0c\u8fdb\u884c\u65f6\u95f4\u8d8b\u52bf\u5206\u6790\u548c\u4efb\u52a1\u7c7b\u578b\u5206\u5c42\u5206\u6790\u3002", "result": "\u65f6\u95f4\u8d8b\u52bf\u663e\u793aDevin\u662f\u552f\u4e00\u6301\u7eed\u6539\u8fdb\u7684\u52a9\u624b(\u6bcf\u5468+0.77%)\uff1b\u6587\u6863\u4efb\u52a1\u63a5\u53d7\u7387(82.1%)\u663e\u8457\u9ad8\u4e8e\u65b0\u529f\u80fd\u4efb\u52a1(66.1%)\uff1bOpenAI Codex\u5728\u6240\u67099\u4e2a\u4efb\u52a1\u7c7b\u522b\u4e2d\u8868\u73b0\u7a33\u5b9a(59.6%-88.6%)\uff1b\u4e0d\u540c\u52a9\u624b\u5728\u4e0d\u540c\u4efb\u52a1\u7c7b\u578b\u4e0a\u5404\u6709\u4f18\u52bf\u3002", "conclusion": "AI\u7f16\u7a0b\u52a9\u624b\u7684\u8868\u73b0\u9ad8\u5ea6\u4f9d\u8d56\u4e8e\u4efb\u52a1\u7c7b\u578b\uff0c\u6ca1\u6709\u5355\u4e00\u52a9\u624b\u5728\u6240\u6709\u4efb\u52a1\u4e0a\u90fd\u6700\u4f18\uff0c\u5f00\u53d1\u8005\u5e94\u6839\u636e\u5177\u4f53\u4efb\u52a1\u7c7b\u578b\u9009\u62e9\u5408\u9002\u7684\u5de5\u5177\uff0c\u540c\u65f6\u9700\u8981\u5173\u6ce8\u52a9\u624b\u968f\u65f6\u95f4\u6f14\u5316\u7684\u6027\u80fd\u53d8\u5316\u3002"}}
{"id": "2602.07978", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.07978", "abs": "https://arxiv.org/abs/2602.07978", "authors": ["Rui Feng", "Zhiyao Luo", "Liuyu Wu", "Wei Wang", "Yuting Song", "Yong Liu", "Kok Pin Ng", "Jianqing Li", "Xingyao Wang"], "title": "Cross-Linguistic Persona-Driven Data Synthesis for Robust Multimodal Cognitive Decline Detection", "comment": "18 pages, 7 figures, 6 tables", "summary": "Speech-based digital biomarkers represent a scalable, non-invasive frontier for the early identification of Mild Cognitive Impairment (MCI). However, the development of robust diagnostic models remains impeded by acute clinical data scarcity and a lack of interpretable reasoning. Current solutions frequently struggle with cross-lingual generalization and fail to provide the transparent rationales essential for clinical trust. To address these barriers, we introduce SynCog, a novel framework integrating controllable zero-shot multimodal data synthesis with Chain-of-Thought (CoT) deduction fine-tuning. Specifically, SynCog simulates diverse virtual subjects with varying cognitive profiles to effectively alleviate clinical data scarcity. This generative paradigm enables the rapid, zero-shot expansion of clinical corpora across diverse languages, effectively bypassing data bottlenecks in low-resource settings and bolstering the diagnostic performance of Multimodal Large Language Models (MLLMs). Leveraging this synthesized dataset, we fine-tune a foundational multimodal backbone using a CoT deduction strategy, empowering the model to explicitly articulate diagnostic thought processes rather than relying on black-box predictions. Extensive experiments on the ADReSS and ADReSSo benchmarks demonstrate that augmenting limited clinical data with synthetic phenotypes yields competitive diagnostic performance, achieving Macro-F1 scores of 80.67% and 78.46%, respectively, outperforming current baseline models. Furthermore, evaluation on an independent real-world Mandarin cohort (CIR-E) demonstrates robust cross-linguistic generalization, attaining a Macro-F1 of 48.71%. These findings constitute a critical step toward providing clinically trustworthy and linguistically inclusive cognitive assessment tools for global healthcare.", "AI": {"tldr": "SynCog\u6846\u67b6\u901a\u8fc7\u53ef\u63a7\u96f6\u6837\u672c\u591a\u6a21\u6001\u6570\u636e\u5408\u6210\u548c\u601d\u7ef4\u94fe\u63a8\u7406\u5fae\u8c03\uff0c\u89e3\u51b3MCI\u8bca\u65ad\u4e2d\u7684\u6570\u636e\u7a00\u7f3a\u548c\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u95ee\u9898\uff0c\u5728\u591a\u8bed\u8a00\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u8bed\u97f3\u7684\u6570\u5b57\u751f\u7269\u6807\u5fd7\u7269\u5728\u8f7b\u5ea6\u8ba4\u77e5\u969c\u788d\u65e9\u671f\u8bc6\u522b\u4e2d\u9762\u4e34\u4e34\u5e8a\u6570\u636e\u7a00\u7f3a\u3001\u8de8\u8bed\u8a00\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u4ee5\u53ca\u7f3a\u4e4f\u53ef\u89e3\u91ca\u63a8\u7406\u7b49\u6311\u6218\uff0c\u9650\u5236\u4e86\u4e34\u5e8a\u5e94\u7528\u7684\u4fe1\u4efb\u5ea6\u3002", "method": "\u63d0\u51faSynCog\u6846\u67b6\uff1a1) \u901a\u8fc7\u53ef\u63a7\u96f6\u6837\u672c\u591a\u6a21\u6001\u6570\u636e\u5408\u6210\u6a21\u62df\u4e0d\u540c\u8ba4\u77e5\u7279\u5f81\u7684\u865a\u62df\u53d7\u8bd5\u8005\uff0c\u7f13\u89e3\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff1b2) \u4f7f\u7528\u601d\u7ef4\u94fe\u63a8\u7406\u7b56\u7565\u5fae\u8c03\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u660e\u786e\u8868\u8fbe\u8bca\u65ad\u601d\u7ef4\u8fc7\u7a0b\u800c\u975e\u9ed1\u7bb1\u9884\u6d4b\u3002", "result": "\u5728ADReSS\u548cADReSSo\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u901a\u8fc7\u5408\u6210\u6570\u636e\u589e\u5f3a\u83b7\u5f9780.67%\u548c78.46%\u7684Macro-F1\u5206\u6570\uff0c\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u6a21\u578b\uff1b\u5728\u72ec\u7acb\u771f\u5b9e\u4e16\u754c\u666e\u901a\u8bdd\u961f\u5217(CIR-E)\u4e2d\u5b9e\u73b048.71%\u7684Macro-F1\uff0c\u5c55\u793a\u4e86\u5f3a\u5927\u7684\u8de8\u8bed\u8a00\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "SynCog\u6846\u67b6\u4e3a\u89e3\u51b3\u4e34\u5e8a\u6570\u636e\u7a00\u7f3a\u548c\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\uff0c\u4e3a\u5b9e\u73b0\u4e34\u5e8a\u53ef\u4fe1\u4e14\u8bed\u8a00\u5305\u5bb9\u7684\u5168\u7403\u8ba4\u77e5\u8bc4\u4f30\u5de5\u5177\u8fc8\u51fa\u4e86\u5173\u952e\u4e00\u6b65\u3002"}}
{"id": "2602.07251", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07251", "abs": "https://arxiv.org/abs/2602.07251", "authors": ["Haley Duba-Sullivan", "Steven R. Young", "Emma J. Reid"], "title": "The Double-Edged Sword of Data-Driven Super-Resolution: Adversarial Super-Resolution Models", "comment": null, "summary": "Data-driven super-resolution (SR) methods are often integrated into imaging pipelines as preprocessing steps to improve downstream tasks such as classification and detection. However, these SR models introduce a previously unexplored attack surface into imaging pipelines. In this paper, we present AdvSR, a framework demonstrating that adversarial behavior can be embedded directly into SR model weights during training, requiring no access to inputs at inference time. Unlike prior attacks that perturb inputs or rely on backdoor triggers, AdvSR operates entirely at the model level. By jointly optimizing for reconstruction quality and targeted adversarial outcomes, AdvSR produces models that appear benign under standard image quality metrics while inducing downstream misclassification. We evaluate AdvSR on three SR architectures (SRCNN, EDSR, SwinIR) paired with a YOLOv11 classifier and demonstrate that AdvSR models can achieve high attack success rates with minimal quality degradation. These findings highlight a new model-level threat for imaging pipelines, with implications for how practitioners source and validate models in safety-critical applications.", "AI": {"tldr": "AdvSR\u662f\u4e00\u79cd\u5c06\u5bf9\u6297\u6027\u884c\u4e3a\u5d4c\u5165\u8d85\u5206\u8fa8\u7387\u6a21\u578b\u6743\u91cd\u7684\u6846\u67b6\uff0c\u65e0\u9700\u63a8\u7406\u65f6\u8bbf\u95ee\u8f93\u5165\uff0c\u53ef\u5728\u4fdd\u6301\u56fe\u50cf\u8d28\u91cf\u7684\u540c\u65f6\u8bf1\u5bfc\u4e0b\u6e38\u5206\u7c7b\u9519\u8bef", "motivation": "\u6570\u636e\u9a71\u52a8\u7684\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\u5e38\u4f5c\u4e3a\u6210\u50cf\u7ba1\u9053\u9884\u5904\u7406\u6b65\u9aa4\uff0c\u4f46\u8fd9\u4e9bSR\u6a21\u578b\u5f15\u5165\u4e86\u65b0\u7684\u653b\u51fb\u9762\u3002\u73b0\u6709\u653b\u51fb\u901a\u5e38\u6270\u52a8\u8f93\u5165\u6216\u4f9d\u8d56\u540e\u95e8\u89e6\u53d1\u5668\uff0c\u800cAdvSR\u63a2\u7d22\u5728\u6a21\u578b\u5c42\u9762\u5d4c\u5165\u5bf9\u6297\u884c\u4e3a\u7684\u65b0\u5a01\u80c1", "method": "AdvSR\u5728\u8bad\u7ec3\u671f\u95f4\u76f4\u63a5\u4f18\u5316SR\u6a21\u578b\u6743\u91cd\uff0c\u8054\u5408\u4f18\u5316\u91cd\u5efa\u8d28\u91cf\u548c\u76ee\u6807\u5bf9\u6297\u7ed3\u679c\u3002\u901a\u8fc7\u540c\u65f6\u6700\u5c0f\u5316\u91cd\u5efa\u635f\u5931\u548c\u6700\u5927\u5316\u4e0b\u6e38\u5206\u7c7b\u9519\u8bef\uff0c\u4ea7\u751f\u5728\u6807\u51c6\u56fe\u50cf\u8d28\u91cf\u6307\u6807\u4e0b\u770b\u4f3c\u826f\u6027\u4f46\u80fd\u8bf1\u5bfc\u8bef\u5206\u7c7b\u7684\u6a21\u578b", "result": "\u5728\u4e09\u79cdSR\u67b6\u6784\uff08SRCNN\u3001EDSR\u3001SwinIR\uff09\u4e0eYOLOv11\u5206\u7c7b\u5668\u914d\u5bf9\u8bc4\u4f30\u4e2d\uff0cAdvSR\u6a21\u578b\u80fd\u4ee5\u6700\u5c0f\u8d28\u91cf\u9000\u5316\u5b9e\u73b0\u9ad8\u653b\u51fb\u6210\u529f\u7387\uff0c\u5c55\u793a\u4e86\u6a21\u578b\u5c42\u9762\u7684\u65b0\u5a01\u80c1", "conclusion": "AdvSR\u63ed\u793a\u4e86\u6210\u50cf\u7ba1\u9053\u4e2d\u65b0\u7684\u6a21\u578b\u7ea7\u5b89\u5168\u5a01\u80c1\uff0c\u5bf9\u5b89\u5168\u5173\u952e\u5e94\u7528\u4e2d\u6a21\u578b\u6765\u6e90\u548c\u9a8c\u8bc1\u65b9\u5f0f\u6709\u91cd\u8981\u5f71\u54cd\uff0c\u9700\u8981\u66f4\u4e25\u683c\u7684\u6a21\u578b\u9a8c\u8bc1\u673a\u5236"}}
{"id": "2602.07830", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07830", "abs": "https://arxiv.org/abs/2602.07830", "authors": ["Jiahui Zhou", "Dan Li", "Boxin Li", "Xiao Zhang", "Erli Meng", "Lin Li", "Zhuomin Chen", "Jian Lou", "See-Kiong Ng"], "title": "Time Series Reasoning via Process-Verifiable Thinking Data Synthesis and Scheduling for Tailored LLM Reasoning", "comment": null, "summary": "Time series is a pervasive data type across various application domains, rendering the reasonable solving of diverse time series tasks a long-standing goal. Recent advances in large language models (LLMs), especially their reasoning abilities unlocked through reinforcement learning (RL), have opened new opportunities for tackling tasks with long Chain-of-Thought (CoT) reasoning. However, leveraging LLM reasoning for time series remains in its infancy, hindered by the absence of carefully curated time series CoT data for training, limited data efficiency caused by underexplored data scheduling, and the lack of RL algorithms tailored for exploiting such time series CoT data. In this paper, we introduce VeriTime, a framework that tailors LLMs for time series reasoning through data synthesis, data scheduling, and RL training. First, we propose a data synthesis pipeline that constructs a TS-text multimodal dataset with process-verifiable annotations. Second, we design a data scheduling mechanism that arranges training samples according to a principled hierarchy of difficulty and task taxonomy. Third, we develop a two-stage reinforcement finetuning featuring fine-grained, multi-objective rewards that leverage verifiable process-level CoT data. Extensive experiments show that VeriTime substantially boosts LLM performance across diverse time series reasoning tasks. Notably, it enables compact 3B, 4B models to achieve reasoning capabilities on par with or exceeding those of larger proprietary LLMs.", "AI": {"tldr": "VeriTime\u662f\u4e00\u4e2a\u901a\u8fc7\u6570\u636e\u5408\u6210\u3001\u6570\u636e\u8c03\u5ea6\u548c\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u6765\u5b9a\u5236LLM\u8fdb\u884c\u65f6\u95f4\u5e8f\u5217\u63a8\u7406\u7684\u6846\u67b6\uff0c\u4f7f\u5c0f\u578b\u6a21\u578b\u5728\u65f6\u95f4\u5e8f\u5217\u4efb\u52a1\u4e0a\u8fbe\u5230\u6216\u8d85\u8fc7\u5927\u578b\u4e13\u6709LLM\u7684\u6027\u80fd\u3002", "motivation": "\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u5728\u5404\u9886\u57df\u666e\u904d\u5b58\u5728\uff0c\u4f46\u5229\u7528LLM\u63a8\u7406\u80fd\u529b\u5904\u7406\u65f6\u95f4\u5e8f\u5217\u4efb\u52a1\u4ecd\u5904\u4e8e\u65e9\u671f\u9636\u6bb5\uff0c\u4e3b\u8981\u53d7\u9650\u4e8e\u7f3a\u4e4f\u7cbe\u5fc3\u7b56\u5212\u7684\u65f6\u95f4\u5e8f\u5217CoT\u8bad\u7ec3\u6570\u636e\u3001\u6570\u636e\u6548\u7387\u4f4e\u4e0b\u4ee5\u53ca\u7f3a\u4e4f\u4e13\u95e8\u9488\u5bf9\u65f6\u95f4\u5e8f\u5217CoT\u6570\u636e\u7684RL\u7b97\u6cd5\u3002", "method": "1) \u63d0\u51fa\u6570\u636e\u5408\u6210\u7ba1\u9053\uff0c\u6784\u5efa\u5177\u6709\u8fc7\u7a0b\u53ef\u9a8c\u8bc1\u6ce8\u91ca\u7684TS-\u6587\u672c\u591a\u6a21\u6001\u6570\u636e\u96c6\uff1b2) \u8bbe\u8ba1\u6570\u636e\u8c03\u5ea6\u673a\u5236\uff0c\u6309\u96be\u5ea6\u5c42\u6b21\u548c\u4efb\u52a1\u5206\u7c7b\u539f\u5219\u5b89\u6392\u8bad\u7ec3\u6837\u672c\uff1b3) \u5f00\u53d1\u4e24\u9636\u6bb5\u5f3a\u5316\u5fae\u8c03\uff0c\u5229\u7528\u53ef\u9a8c\u8bc1\u8fc7\u7a0b\u7ea7CoT\u6570\u636e\u7684\u7ec6\u7c92\u5ea6\u591a\u76ee\u6807\u5956\u52b1\u3002", "result": "VeriTime\u663e\u8457\u63d0\u5347\u4e86LLM\u5728\u5404\u79cd\u65f6\u95f4\u5e8f\u5217\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\uff0c\u4f7f\u7d27\u51d1\u76843B\u30014B\u6a21\u578b\u80fd\u591f\u8fbe\u5230\u6216\u8d85\u8fc7\u5927\u578b\u4e13\u6709LLM\u7684\u63a8\u7406\u80fd\u529b\u3002", "conclusion": "VeriTime\u6846\u67b6\u901a\u8fc7\u7cfb\u7edf\u5316\u7684\u6570\u636e\u5408\u6210\u3001\u8c03\u5ea6\u548c\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u65f6\u95f4\u5e8f\u5217\u63a8\u7406\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u4e3aLLM\u5728\u65f6\u95f4\u5e8f\u5217\u9886\u57df\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.08874", "categories": ["cs.CL", "cs.CR"], "pdf": "https://arxiv.org/pdf/2602.08874", "abs": "https://arxiv.org/abs/2602.08874", "authors": ["Yu Fu", "Haz Sameen Shahgir", "Huanli Gong", "Zhipeng Wei", "N. Benjamin Erichson", "Yue Dong"], "title": "Is Reasoning Capability Enough for Safety in Long-Context Language Models?", "comment": "25 pages, 7 figures", "summary": "Large language models (LLMs) increasingly combine long-context processing with advanced reasoning, enabling them to retrieve and synthesize information distributed across tens of thousands of tokens. A hypothesis is that stronger reasoning capability should improve safety by helping models recognize harmful intent even when it is not stated explicitly. We test this hypothesis in long-context settings where harmful intent is implicit and must be inferred through reasoning, and find that it does not hold. We introduce compositional reasoning attacks, a new threat model in which a harmful query is decomposed into incomplete fragments that scattered throughout a long context. The model is then prompted with a neutral reasoning query that induces retrieval and synthesis, causing the harmful intent to emerge only after composition. Evaluating 14 frontier LLMs on contexts up to 64k tokens, we uncover three findings: (1) models with stronger general reasoning capability are not more robust to compositional reasoning attacks, often assembling the intent yet failing to refuse; (2) safety alignment consistently degrades as context length increases; and (3) inference-time reasoning effort is a key mitigating factor: increasing inference-time compute reduces attack success by over 50 percentage points on GPT-oss-120b model. Together, these results suggest that safety does not automatically scale with reasoning capability, especially under long-context inference.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u5728\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u653b\u51fb\u4e2d\uff0c\u66f4\u5f3a\u7684\u63a8\u7406\u80fd\u529b\u5e76\u4e0d\u80fd\u81ea\u52a8\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5b89\u5168\u6027\uff0c\u53cd\u800c\u53ef\u80fd\u5e2e\u52a9\u6a21\u578b\u7ec4\u88c5\u51fa\u6709\u5bb3\u610f\u56fe\u5374\u65e0\u6cd5\u62d2\u7edd\u6267\u884c\u3002", "motivation": "\u6d4b\u8bd5\u4e00\u4e2a\u5047\u8bbe\uff1a\u66f4\u5f3a\u7684\u63a8\u7406\u80fd\u529b\u5e94\u8be5\u80fd\u901a\u8fc7\u5e2e\u52a9\u6a21\u578b\u8bc6\u522b\u9690\u542b\u7684\u6709\u5bb3\u610f\u56fe\u6765\u63d0\u5347\u5b89\u5168\u6027\u3002\u7814\u7a76\u5728\u957f\u4e0a\u4e0b\u6587\u73af\u5883\u4e2d\uff0c\u5f53\u6709\u5bb3\u610f\u56fe\u662f\u9690\u542b\u7684\u3001\u9700\u8981\u901a\u8fc7\u63a8\u7406\u624d\u80fd\u8bc6\u522b\u65f6\uff0c\u8fd9\u4e00\u5047\u8bbe\u662f\u5426\u6210\u7acb\u3002", "method": "\u5f15\u5165\u7ec4\u5408\u63a8\u7406\u653b\u51fb\u8fd9\u4e00\u65b0\u7684\u5a01\u80c1\u6a21\u578b\uff0c\u5c06\u6709\u5bb3\u67e5\u8be2\u5206\u89e3\u6210\u4e0d\u5b8c\u6574\u7684\u7247\u6bb5\uff0c\u5206\u6563\u5728\u957f\u4e0a\u4e0b\u6587\u4e2d\u3002\u7136\u540e\u4f7f\u7528\u4e2d\u6027\u7684\u63a8\u7406\u67e5\u8be2\u8bf1\u5bfc\u6a21\u578b\u68c0\u7d22\u548c\u5408\u6210\u8fd9\u4e9b\u7247\u6bb5\uff0c\u4f7f\u6709\u5bb3\u610f\u56fe\u5728\u7ec4\u5408\u540e\u624d\u663e\u73b0\u3002\u8bc4\u4f30\u4e8614\u4e2a\u524d\u6cbf\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u4e0a\u4e0b\u6587\u957f\u5ea6\u8fbe64k tokens\u3002", "result": "\u4e09\u4e2a\u4e3b\u8981\u53d1\u73b0\uff1a1\uff09\u5177\u6709\u66f4\u5f3a\u901a\u7528\u63a8\u7406\u80fd\u529b\u7684\u6a21\u578b\u5bf9\u7ec4\u5408\u63a8\u7406\u653b\u51fb\u5e76\u4e0d\u66f4\u9c81\u68d2\uff0c\u7ecf\u5e38\u7ec4\u88c5\u51fa\u6709\u5bb3\u610f\u56fe\u5374\u65e0\u6cd5\u62d2\u7edd\uff1b2\uff09\u968f\u7740\u4e0a\u4e0b\u6587\u957f\u5ea6\u589e\u52a0\uff0c\u5b89\u5168\u5bf9\u9f50\u6548\u679c\u6301\u7eed\u4e0b\u964d\uff1b3\uff09\u63a8\u7406\u65f6\u7684\u8ba1\u7b97\u52aa\u529b\u662f\u5173\u952e\u7f13\u89e3\u56e0\u7d20\uff0c\u589e\u52a0\u63a8\u7406\u65f6\u8ba1\u7b97\u53ef\u5c06GPT-oss-120b\u6a21\u578b\u7684\u653b\u51fb\u6210\u529f\u7387\u964d\u4f4e\u8d85\u8fc750\u4e2a\u767e\u5206\u70b9\u3002", "conclusion": "\u5b89\u5168\u6027\u4e0d\u4f1a\u968f\u7740\u63a8\u7406\u80fd\u529b\u7684\u589e\u5f3a\u800c\u81ea\u52a8\u63d0\u5347\uff0c\u7279\u522b\u662f\u5728\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u573a\u666f\u4e0b\u3002\u9700\u8981\u4e13\u95e8\u7684\u5b89\u5168\u63aa\u65bd\u6765\u5e94\u5bf9\u7ec4\u5408\u63a8\u7406\u653b\u51fb\uff0c\u4e0d\u80fd\u4f9d\u8d56\u63a8\u7406\u80fd\u529b\u672c\u8eab\u6765\u4fdd\u8bc1\u5b89\u5168\u3002"}}
{"id": "2602.07996", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.07996", "abs": "https://arxiv.org/abs/2602.07996", "authors": ["Arash Marioriyad", "Omid Ghahroodi", "Ehsaneddin Asgari", "Mohammad Hossein Rohban", "Mahdieh Soleymani Baghshah"], "title": "The Judge Who Never Admits: Hidden Shortcuts in LLM-based Evaluation", "comment": null, "summary": "Large language models (LLMs) are increasingly used as automatic judges to evaluate system outputs in tasks such as reasoning, question answering, and creative writing. A faithful judge should base its verdicts solely on content quality, remain invariant to irrelevant context, and transparently reflect the factors driving its decisions. We test this ideal via controlled cue perturbations-synthetic metadata labels injected into evaluation prompts-for six judge models: GPT-4o, Gemini-2.0-Flash, Gemma-3-27B, Qwen3-235B, Claude-3-Haiku, and Llama3-70B. Experiments span two complementary datasets with distinct evaluation regimes: ELI5 (factual QA) and LitBench (open-ended creative writing). We study six cue families: source, temporal, age, gender, ethnicity, and educational status. Beyond measuring verdict shift rates (VSR), we introduce cue acknowledgment rate (CAR) to quantify whether judges explicitly reference the injected cues in their natural-language rationales. Across cues with strong behavioral effects-e.g., provenance hierarchies (Expert > Human > LLM > Unknown), recency preferences (New > Old), and educational-status favoritism-CAR is typically at or near zero, indicating that shortcut reliance is largely unreported even when it drives decisions. Crucially, CAR is also dataset-dependent: explicit cue recognition is more likely to surface in the factual ELI5 setting for some models and cues, but often collapses in the open-ended LitBench regime, where large verdict shifts can persist despite zero acknowledgment. The combination of substantial verdict sensitivity and limited cue acknowledgment reveals an explanation gap in LLM-as-judge pipelines, raising concerns about reliability of model-based evaluation in both research and deployment.", "AI": {"tldr": "LLM\u4f5c\u4e3a\u81ea\u52a8\u8bc4\u4f30\u5668\u65f6\uff0c\u5176\u5224\u65ad\u4f1a\u53d7\u5230\u65e0\u5173\u4e0a\u4e0b\u6587\u7ebf\u7d22\uff08\u5982\u6765\u6e90\u3001\u65f6\u95f4\u3001\u4eba\u53e3\u7edf\u8ba1\u4fe1\u606f\uff09\u7684\u663e\u8457\u5f71\u54cd\uff0c\u4f46\u8fd9\u4e9b\u5f71\u54cd\u5f88\u5c11\u5728\u8bc4\u4f30\u7406\u7531\u4e2d\u660e\u786e\u627f\u8ba4\uff0c\u63ed\u793a\u4e86\u6a21\u578b\u8bc4\u4f30\u7684\u53ef\u9760\u6027\u95ee\u9898\u3002", "motivation": "\u7814\u7a76LLM\u4f5c\u4e3a\u81ea\u52a8\u8bc4\u4f30\u5668\u65f6\u662f\u5426\u80fd\u591f\u5fe0\u5b9e\u3001\u516c\u6b63\u5730\u8bc4\u4f30\u7cfb\u7edf\u8f93\u51fa\uff0c\u5373\u662f\u5426\u4ec5\u57fa\u4e8e\u5185\u5bb9\u8d28\u91cf\u505a\u51fa\u5224\u65ad\uff0c\u4e0d\u53d7\u65e0\u5173\u4e0a\u4e0b\u6587\u5f71\u54cd\uff0c\u5e76\u80fd\u900f\u660e\u53cd\u6620\u51b3\u7b56\u56e0\u7d20\u3002", "method": "\u901a\u8fc7\u63a7\u5236\u6027\u7ebf\u7d22\u6270\u52a8\u5b9e\u9a8c\uff0c\u5411\u8bc4\u4f30\u63d0\u793a\u4e2d\u6ce8\u5165\u5408\u6210\u5143\u6570\u636e\u6807\u7b7e\uff08\u6765\u6e90\u3001\u65f6\u95f4\u3001\u5e74\u9f84\u3001\u6027\u522b\u3001\u79cd\u65cf\u3001\u6559\u80b2\u72b6\u6001\uff09\uff0c\u6d4b\u8bd56\u4e2aLLM\u8bc4\u4f30\u6a21\u578b\u5728ELI5\uff08\u4e8b\u5b9e\u95ee\u7b54\uff09\u548cLitBench\uff08\u521b\u610f\u5199\u4f5c\uff09\u4e24\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\uff0c\u6d4b\u91cf\u5224\u51b3\u504f\u79fb\u7387\u548c\u7ebf\u7d22\u627f\u8ba4\u7387\u3002", "result": "LLM\u8bc4\u4f30\u5668\u5bf9\u65e0\u5173\u7ebf\u7d22\u8868\u73b0\u51fa\u663e\u8457\u654f\u611f\u6027\uff0c\u5982\u6765\u6e90\u5c42\u6b21\u504f\u597d\uff08\u4e13\u5bb6>\u4eba\u7c7b>LLM>\u672a\u77e5\uff09\u3001\u65f6\u6548\u6027\u504f\u597d\uff08\u65b0>\u65e7\uff09\u3001\u6559\u80b2\u72b6\u6001\u504f\u597d\u7b49\uff0c\u4f46\u7ebf\u7d22\u627f\u8ba4\u7387\u901a\u5e38\u63a5\u8fd1\u96f6\uff0c\u8868\u660e\u5373\u4f7f\u7ebf\u7d22\u9a71\u52a8\u51b3\u7b56\u4e5f\u5f88\u5c11\u5728\u7406\u7531\u4e2d\u660e\u786e\u627f\u8ba4\u3002\u7ebf\u7d22\u627f\u8ba4\u7387\u8fd8\u4f9d\u8d56\u4e8e\u6570\u636e\u96c6\uff0c\u5728\u4e8b\u5b9e\u6027ELI5\u4e2d\u67d0\u4e9b\u6a21\u578b\u548c\u7ebf\u7d22\u66f4\u53ef\u80fd\u627f\u8ba4\uff0c\u4f46\u5728\u5f00\u653e\u6027\u7684LitBench\u4e2d\u5373\u4f7f\u5224\u51b3\u5927\u5e45\u504f\u79fb\u4e5f\u51e0\u4e4e\u4e0d\u627f\u8ba4\u3002", "conclusion": "LLM\u4f5c\u4e3a\u8bc4\u4f30\u5668\u5b58\u5728\u663e\u8457\u7684\u5224\u51b3\u654f\u611f\u6027\u4e0e\u6709\u9650\u7684\u7ebf\u7d22\u627f\u8ba4\u4e4b\u95f4\u7684\u89e3\u91ca\u5dee\u8ddd\uff0c\u8fd9\u5bf9\u7814\u7a76\u548c\u90e8\u7f72\u4e2d\u57fa\u4e8e\u6a21\u578b\u7684\u8bc4\u4f30\u53ef\u9760\u6027\u63d0\u51fa\u4e86\u91cd\u8981\u5173\u5207\u3002"}}
{"id": "2602.07260", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.07260", "abs": "https://arxiv.org/abs/2602.07260", "authors": ["Hongyu Kan", "Kristofor Pas", "Ivan Medri", "Naqib Sad Pathan", "Natasha Ironside", "Shinjini Kundu", "Jingjia He", "Gustavo Kunde Rohde"], "title": "3D Transport-based Morphometry (3D-TBM) for medical image analysis", "comment": null, "summary": "Transport-Based Morphometry (TBM) has emerged as a new framework for 3D medical image analysis. By embedding images into a transport domain via invertible transformations, TBM facilitates effective classification, regression, and other tasks using transport-domain features. Crucially, the inverse mapping enables the projection of analytic results back into the original image space, allowing researchers to directly interpret clinical features associated with model outputs in a spatially meaningful way. To facilitate broader adoption of TBM in clinical imaging research, we present 3D-TBM, a tool designed for morphological analysis of 3D medical images. The framework includes data preprocessing, computation of optimal transport embeddings, and analytical methods such as visualization of main transport directions, together with techniques for discerning discriminating directions and related analysis methods. We also provide comprehensive documentation and practical tutorials to support researchers interested in applying 3D-TBM in their own medical imaging studies. The source code is publicly available through PyTransKit.", "AI": {"tldr": "3D-TBM\uff1a\u57fa\u4e8e\u6700\u4f18\u4f20\u8f93\u76843D\u533b\u5b66\u56fe\u50cf\u5f62\u6001\u5206\u6790\u5de5\u5177\uff0c\u901a\u8fc7\u53ef\u9006\u53d8\u6362\u5c06\u56fe\u50cf\u5d4c\u5165\u4f20\u8f93\u57df\u8fdb\u884c\u5206\u6790\uff0c\u5e76\u5c06\u7ed3\u679c\u6295\u5f71\u56de\u539f\u59cb\u56fe\u50cf\u7a7a\u95f4\u8fdb\u884c\u7a7a\u95f4\u89e3\u91ca", "motivation": "\u4fc3\u8fdb\u57fa\u4e8e\u4f20\u8f93\u7684\u5f62\u6001\u6d4b\u91cf\u5b66\uff08TBM\uff09\u5728\u4e34\u5e8a\u5f71\u50cf\u7814\u7a76\u4e2d\u7684\u66f4\u5e7f\u6cdb\u5e94\u7528\uff0c\u4e3a3D\u533b\u5b66\u56fe\u50cf\u5206\u6790\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u5f62\u6001\u5b66\u5206\u6790\u6846\u67b6", "method": "\u5f00\u53d13D-TBM\u5de5\u5177\uff0c\u5305\u62ec\u6570\u636e\u9884\u5904\u7406\u3001\u6700\u4f18\u4f20\u8f93\u5d4c\u5165\u8ba1\u7b97\u3001\u4e3b\u8981\u4f20\u8f93\u65b9\u5411\u53ef\u89c6\u5316\u3001\u5224\u522b\u65b9\u5411\u8bc6\u522b\u7b49\u5206\u6790\u65b9\u6cd5\uff0c\u63d0\u4f9b\u5b8c\u6574\u6587\u6863\u548c\u6559\u7a0b", "result": "\u5f00\u53d1\u4e86\u516c\u5f00\u53ef\u7528\u76843D-TBM\u5de5\u5177\uff0c\u6e90\u4ee3\u7801\u901a\u8fc7PyTransKit\u63d0\u4f9b\uff0c\u652f\u6301\u7814\u7a76\u4eba\u5458\u5728\u81ea\u5df1\u7684\u533b\u5b66\u5f71\u50cf\u7814\u7a76\u4e2d\u5e94\u7528\u8be5\u6846\u67b6", "conclusion": "3D-TBM\u4e3a3D\u533b\u5b66\u56fe\u50cf\u7684\u5f62\u6001\u5206\u6790\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u57fa\u4e8e\u4f20\u8f93\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u53ef\u9006\u6620\u5c04\u5b9e\u73b0\u5206\u6790\u7ed3\u679c\u7684\u7a7a\u95f4\u53ef\u89e3\u91ca\u6027\uff0c\u6709\u52a9\u4e8e\u4e34\u5e8a\u7279\u5f81\u8bc6\u522b\u548c\u7814\u7a76"}}
{"id": "2602.07849", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07849", "abs": "https://arxiv.org/abs/2602.07849", "authors": ["Xin Wang", "Hualin Zhou", "Sheng Guang Wang", "Ting Dang", "Yu Zhang", "Hong Jia", "Tao Gu"], "title": "LQA: A Lightweight Quantized-Adaptive Framework for Vision-Language Models on the Edge", "comment": "16 pages, 9 figures ,9 tables, preprint", "summary": "Deploying Vision-Language Models (VLMs) on edge devices is challenged by resource constraints and performance degradation under distribution shifts. While test-time adaptation (TTA) can counteract such shifts, existing methods are too resource-intensive for on-device deployment. To address this challenge, we propose LQA, a lightweight, quantized-adaptive framework for VLMs that combines a modality-aware quantization strategy with gradient-free test-time adaptation. We introduce Selective Hybrid Quantization (SHQ) and a quantized, gradient-free adaptation mechanism to enable robust and efficient VLM deployment on resource-constrained hardware. Experiments across both synthetic and real-world distribution shifts show that LQA improves overall adaptation performance by 4.5\\%, uses less memory than full-precision models, and significantly outperforms gradient-based TTA methods, achieving up to 19.9$\\times$ lower memory usage across seven open-source datasets. These results demonstrate that LQA offers a practical pathway for robust, privacy-preserving, and efficient VLM deployment on edge devices.", "AI": {"tldr": "LQA\u662f\u4e00\u4e2a\u8f7b\u91cf\u5316\u7684\u91cf\u5316\u81ea\u9002\u5e94\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u90e8\u7f72\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u9009\u62e9\u6027\u6df7\u5408\u91cf\u5316\u548c\u65e0\u68af\u5ea6\u6d4b\u8bd5\u65f6\u9002\u5e94\uff0c\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u5b9e\u73b0\u9c81\u68d2\u9ad8\u6548\u7684VLM\u90e8\u7f72\u3002", "motivation": "\u8fb9\u7f18\u8bbe\u5907\u90e8\u7f72\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u9762\u4e34\u8d44\u6e90\u9650\u5236\u548c\u5206\u5e03\u504f\u79fb\u5bfc\u81f4\u7684\u6027\u80fd\u4e0b\u964d\u95ee\u9898\uff0c\u73b0\u6709\u6d4b\u8bd5\u65f6\u9002\u5e94\u65b9\u6cd5\u8d44\u6e90\u6d88\u8017\u8fc7\u5927\uff0c\u4e0d\u9002\u5408\u8fb9\u7f18\u8bbe\u5907\u90e8\u7f72\u3002", "method": "\u63d0\u51faLQA\u6846\u67b6\uff0c\u5305\u542b\u9009\u62e9\u6027\u6df7\u5408\u91cf\u5316\u7b56\u7565\u548c\u91cf\u5316\u65e0\u68af\u5ea6\u9002\u5e94\u673a\u5236\uff0c\u7ed3\u5408\u6a21\u6001\u611f\u77e5\u91cf\u5316\u7b56\u7565\u5b9e\u73b0\u8f7b\u91cf\u5316\u7684\u6d4b\u8bd5\u65f6\u9002\u5e94\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u4e16\u754c\u5206\u5e03\u504f\u79fb\u5b9e\u9a8c\u4e2d\uff0cLQA\u5c06\u6574\u4f53\u9002\u5e94\u6027\u80fd\u63d0\u53474.5%\uff0c\u5185\u5b58\u4f7f\u7528\u4f4e\u4e8e\u5168\u7cbe\u5ea6\u6a21\u578b\uff0c\u6bd4\u57fa\u4e8e\u68af\u5ea6\u7684TTA\u65b9\u6cd5\u5185\u5b58\u4f7f\u7528\u964d\u4f4e\u9ad8\u8fbe19.9\u500d\u3002", "conclusion": "LQA\u4e3a\u8fb9\u7f18\u8bbe\u5907\u4e0a\u9c81\u68d2\u3001\u9690\u79c1\u4fdd\u62a4\u4e14\u9ad8\u6548\u7684VLM\u90e8\u7f72\u63d0\u4f9b\u4e86\u5b9e\u7528\u8def\u5f84\uff0c\u7279\u522b\u9002\u5408\u8d44\u6e90\u53d7\u9650\u7684\u786c\u4ef6\u73af\u5883\u3002"}}
{"id": "2602.08005", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08005", "abs": "https://arxiv.org/abs/2602.08005", "authors": ["Jitai Hao", "Qiang Huang", "Yaowei Wang", "Min Zhang", "Jun Yu"], "title": "DeltaKV: Residual-Based KV Cache Compression via Long-Range Similarity", "comment": "preprint", "summary": "The deployment of efficient long-context LLMs in applications like autonomous agents, long-chain reasoning, and creative writing is fundamentally bottlenecked by the linear growth of KV cache memory. Existing compression and eviction methods often struggle to balance accuracy, compression ratio, and hardware efficiency. We propose DeltaKV, a residual-based KV cache compression framework motivated by two empirical findings: long-range inter-token similarity and highly shared latent components in KV representations. Instead of discarding tokens, DeltaKV encodes semantic residuals relative to retrieved historical references, preserving fidelity while substantially reducing storage. To translate compression gains into real system speedups, we further introduce Sparse-vLLM, a high-performance inference engine with decoupled memory management and kernels optimized for sparse and irregular KV layouts. Experiments show that DeltaKV reduces KV cache memory to 29\\% of the original while maintaining near-lossless accuracy on LongBench, SCBench, and AIME. When integrated with Sparse-vLLM, it achieves up to 2$\\times$ throughput improvement over vLLM in long-context scenarios, demonstrating a practical path toward scalable long-context LLM deployment. Code, model checkpoints, and datasets are available at https://github.com/CURRENTF/Sparse-vLLM.", "AI": {"tldr": "DeltaKV\u901a\u8fc7\u6b8b\u5dee\u7f16\u7801\u538b\u7f29KV\u7f13\u5b58\uff0c\u7ed3\u5408Sparse-vLLM\u63a8\u7406\u5f15\u64ce\uff0c\u5728\u4fdd\u6301\u7cbe\u5ea6\u7684\u540c\u65f6\u5c06\u5185\u5b58\u964d\u81f329%\uff0c\u541e\u5410\u91cf\u63d0\u53472\u500d", "motivation": "\u957f\u4e0a\u4e0b\u6587LLM\u90e8\u7f72\u9762\u4e34KV\u7f13\u5b58\u5185\u5b58\u7ebf\u6027\u589e\u957f\u7684\u74f6\u9888\uff0c\u73b0\u6709\u538b\u7f29\u65b9\u6cd5\u96be\u4ee5\u5e73\u8861\u7cbe\u5ea6\u3001\u538b\u7f29\u6bd4\u548c\u786c\u4ef6\u6548\u7387", "method": "\u57fa\u4e8e\u4e24\u4e2a\u7ecf\u9a8c\u53d1\u73b0\uff08\u957f\u8ddd\u79bbtoken\u76f8\u4f3c\u6027\u548cKV\u8868\u793a\u4e2d\u7684\u5171\u4eab\u6f5c\u5728\u7ec4\u4ef6\uff09\uff0c\u63d0\u51faDeltaKV\u6b8b\u5dee\u538b\u7f29\u6846\u67b6\uff0c\u901a\u8fc7\u7f16\u7801\u76f8\u5bf9\u4e8e\u5386\u53f2\u53c2\u8003\u7684\u8bed\u4e49\u6b8b\u5dee\u6765\u51cf\u5c11\u5b58\u50a8\uff1b\u8fdb\u4e00\u6b65\u5f00\u53d1Sparse-vLLM\u63a8\u7406\u5f15\u64ce\uff0c\u5177\u6709\u89e3\u8026\u5185\u5b58\u7ba1\u7406\u548c\u9488\u5bf9\u7a00\u758f\u4e0d\u89c4\u5219KV\u5e03\u5c40\u7684\u4f18\u5316\u5185\u6838", "result": "DeltaKV\u5c06KV\u7f13\u5b58\u5185\u5b58\u964d\u81f3\u539f\u59cb\u768429%\uff0c\u5728LongBench\u3001SCBench\u548cAIME\u4e0a\u4fdd\u6301\u63a5\u8fd1\u65e0\u635f\u7684\u7cbe\u5ea6\uff1b\u7ed3\u5408Sparse-vLLM\u540e\uff0c\u5728\u957f\u4e0a\u4e0b\u6587\u573a\u666f\u4e2d\u76f8\u6bd4vLLM\u5b9e\u73b0\u9ad8\u8fbe2\u500d\u7684\u541e\u5410\u91cf\u63d0\u5347", "conclusion": "DeltaKV\u548cSparse-vLLM\u4e3a\u53ef\u6269\u5c55\u7684\u957f\u4e0a\u4e0b\u6587LLM\u90e8\u7f72\u63d0\u4f9b\u4e86\u5b9e\u7528\u8def\u5f84\uff0c\u5728\u4fdd\u6301\u7cbe\u5ea6\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u5185\u5b58\u6548\u7387\u548c\u63a8\u7406\u6027\u80fd"}}
{"id": "2602.07262", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.07262", "abs": "https://arxiv.org/abs/2602.07262", "authors": ["Junbo Jacob Lian", "Feng Xiong", "Yujun Sun", "Kaichen Ouyang", "Mingyang Yu", "Shengwei Fu", "Zhong Rui", "Zhang Yujun", "Huiling Chen"], "title": "TwistNet-2D: Learning Second-Order Channel Interactions via Spiral Twisting for Texture Recognition", "comment": "Code is available at https://github.com/junbolian/TwistNet-2D", "summary": "Second-order feature statistics are central to texture recognition, yet current methods face a fundamental tension: bilinear pooling and Gram matrices capture global channel correlations but collapse spatial structure, while self-attention models spatial context through weighted aggregation rather than explicit pairwise feature interactions. We introduce TwistNet-2D, a lightweight module that computes \\emph{local} pairwise channel products under directional spatial displacement, jointly encoding where features co-occur and how they interact. The core component, Spiral-Twisted Channel Interaction (STCI), shifts one feature map along a prescribed direction before element-wise channel multiplication, thereby capturing the cross-position co-occurrence patterns characteristic of structured and periodic textures. Aggregating four directional heads with learned channel reweighting and injecting the result through a sigmoid-gated residual path, \\TwistNet incurs only 3.5% additional parameters and 2% additional FLOPs over ResNet-18, yet consistently surpasses both parameter-matched and substantially larger baselines -- including ConvNeXt, Swin Transformer, and hybrid CNN--Transformer architectures -- across four texture and fine-grained recognition benchmarks.", "AI": {"tldr": "TwistNet-2D\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u6a21\u5757\uff0c\u901a\u8fc7\u5c40\u90e8\u6210\u5bf9\u901a\u9053\u4e58\u79ef\u548c\u65b9\u5411\u6027\u7a7a\u95f4\u4f4d\u79fb\u6765\u6355\u6349\u7eb9\u7406\u7279\u5f81\uff0c\u5728\u4fdd\u6301\u4f4e\u8ba1\u7b97\u6210\u672c\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u7eb9\u7406\u8bc6\u522b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7eb9\u7406\u8bc6\u522b\u65b9\u6cd5\u5b58\u5728\u6839\u672c\u77db\u76fe\uff1a\u53cc\u7ebf\u6027\u6c60\u5316\u548cGram\u77e9\u9635\u80fd\u6355\u6349\u5168\u5c40\u901a\u9053\u76f8\u5173\u6027\u4f46\u4f1a\u7834\u574f\u7a7a\u95f4\u7ed3\u6784\uff0c\u800c\u81ea\u6ce8\u610f\u529b\u901a\u8fc7\u52a0\u6743\u805a\u5408\u5efa\u6a21\u7a7a\u95f4\u4e0a\u4e0b\u6587\u800c\u975e\u663e\u5f0f\u7684\u6210\u5bf9\u7279\u5f81\u4ea4\u4e92\u3002\u9700\u8981\u4e00\u79cd\u80fd\u540c\u65f6\u7f16\u7801\u7279\u5f81\u5171\u73b0\u4f4d\u7f6e\u548c\u4ea4\u4e92\u65b9\u5f0f\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faTwistNet-2D\u6a21\u5757\uff0c\u6838\u5fc3\u662f\u87ba\u65cb\u626d\u66f2\u901a\u9053\u4ea4\u4e92(STCI)\uff1a\u5c06\u7279\u5f81\u56fe\u6cbf\u9884\u5b9a\u65b9\u5411\u4f4d\u79fb\u540e\u8fdb\u884c\u9010\u901a\u9053\u5143\u7d20\u4e58\u6cd5\uff0c\u6355\u6349\u7ed3\u6784\u5316\u7eb9\u7406\u7684\u8de8\u4f4d\u7f6e\u5171\u73b0\u6a21\u5f0f\u3002\u901a\u8fc7\u56db\u4e2a\u65b9\u5411\u5934\u805a\u5408\uff0c\u7ed3\u5408\u5b66\u4e60\u5230\u7684\u901a\u9053\u91cd\u52a0\u6743\uff0c\u5e76\u901a\u8fc7sigmoid\u95e8\u63a7\u6b8b\u5dee\u8def\u5f84\u6ce8\u5165\u3002", "result": "TwistNet-2D\u4ec5\u589e\u52a0ResNet-18\u76843.5%\u53c2\u6570\u548c2%FLOPs\uff0c\u4f46\u5728\u56db\u4e2a\u7eb9\u7406\u548c\u7ec6\u7c92\u5ea6\u8bc6\u522b\u57fa\u51c6\u4e0a\u6301\u7eed\u8d85\u8d8a\u53c2\u6570\u5339\u914d\u548c\u66f4\u5927\u7684\u57fa\u7ebf\u6a21\u578b\uff0c\u5305\u62ecConvNeXt\u3001Swin Transformer\u548c\u6df7\u5408CNN-Transformer\u67b6\u6784\u3002", "conclusion": "TwistNet-2D\u901a\u8fc7\u5c40\u90e8\u6210\u5bf9\u901a\u9053\u4ea4\u4e92\u548c\u65b9\u5411\u6027\u7a7a\u95f4\u4f4d\u79fb\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u7eb9\u7406\u8bc6\u522b\u4e2d\u5168\u5c40\u76f8\u5173\u6027\u4e0e\u7a7a\u95f4\u7ed3\u6784\u4fdd\u6301\u7684\u77db\u76fe\uff0c\u4ee5\u6781\u4f4e\u7684\u8ba1\u7b97\u6210\u672c\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002"}}
{"id": "2602.07852", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.07852", "abs": "https://arxiv.org/abs/2602.07852", "authors": ["Anna Soligo", "Edward Turner", "Senthooran Rajamanoharan", "Neel Nanda"], "title": "Emergent Misalignment is Easy, Narrow Misalignment is Hard", "comment": "Published at ICLR 2026", "summary": "Finetuning large language models on narrowly harmful datasets can cause them to become emergently misaligned, giving stereotypically `evil' responses across diverse unrelated settings. Concerningly, a pre-registered survey of experts failed to predict this result, highlighting our poor understanding of the inductive biases governing learning and generalisation in LLMs. We use emergent misalignment (EM) as a case study to investigate these inductive biases and find that models can just learn the narrow dataset task, but that the general solution appears to be more stable and more efficient. To establish this, we build on the result that different EM finetunes converge to the same linear representation of general misalignment, which can be used to mediate misaligned behaviour. We find a linear representation of the narrow solution also exists, and can be learned by introducing a KL divergence loss. Comparing these representations reveals that general misalignment achieves lower loss, is more robust to perturbations, and is more influential in the pre-training distribution. This work isolates a concrete representation of general misalignment for monitoring and mitigation. More broadly, it offers a detailed case study and preliminary metrics for investigating how inductive biases shape generalisation in LLMs. We open-source all code, datasets and model finetunes.", "AI": {"tldr": "\u5fae\u8c03\u5927\u8bed\u8a00\u6a21\u578b\u5728\u72ed\u7a84\u6709\u5bb3\u6570\u636e\u96c6\u4e0a\u4f1a\u5bfc\u81f4\u6d8c\u73b0\u6027\u9519\u4f4d\uff0c\u6a21\u578b\u5728\u591a\u79cd\u65e0\u5173\u573a\u666f\u4e0b\u7ed9\u51fa\u523b\u677f\u7684\"\u90aa\u6076\"\u56de\u5e94\u3002\u4e13\u5bb6\u8c03\u67e5\u672a\u80fd\u9884\u6d4b\u6b64\u7ed3\u679c\uff0c\u63ed\u793a\u4e86\u6211\u4eec\u5bf9LLM\u5b66\u4e60\u548c\u6cdb\u5316\u5f52\u7eb3\u504f\u597d\u7684\u7406\u89e3\u4e0d\u8db3\u3002", "motivation": "\u7814\u7a76\u5927\u8bed\u8a00\u6a21\u578b\u5728\u72ed\u7a84\u6709\u5bb3\u6570\u636e\u96c6\u4e0a\u5fae\u8c03\u65f6\u51fa\u73b0\u7684\u6d8c\u73b0\u6027\u9519\u4f4d\u73b0\u8c61\uff0c\u63a2\u7d22LLM\u5b66\u4e60\u548c\u6cdb\u5316\u7684\u5f52\u7eb3\u504f\u597d\uff0c\u7406\u89e3\u4e3a\u4ec0\u4e48\u6a21\u578b\u4f1a\u4ece\u7279\u5b9a\u4efb\u52a1\u5b66\u4e60\u6269\u5c55\u5230\u666e\u904d\u9519\u4f4d\u884c\u4e3a\u3002", "method": "\u4f7f\u7528\u6d8c\u73b0\u6027\u9519\u4f4d\u4f5c\u4e3a\u6848\u4f8b\u7814\u7a76\uff0c\u53d1\u73b0\u4e0d\u540c\u9519\u4f4d\u5fae\u8c03\u6536\u655b\u5230\u76f8\u540c\u7684\u7ebf\u6027\u8868\u793a\u3002\u6784\u5efa\u72ed\u7a84\u89e3\u51b3\u65b9\u6848\u7684\u7ebf\u6027\u8868\u793a\uff08\u901a\u8fc7KL\u6563\u5ea6\u635f\u5931\u5b66\u4e60\uff09\uff0c\u6bd4\u8f83\u4e24\u79cd\u8868\u793a\u7684\u7279\u6027\u3002", "result": "\u666e\u904d\u9519\u4f4d\u8868\u793a\u5177\u6709\u66f4\u4f4e\u7684\u635f\u5931\u3001\u66f4\u5f3a\u7684\u6270\u52a8\u9c81\u68d2\u6027\uff0c\u4e14\u5728\u9884\u8bad\u7ec3\u5206\u5e03\u4e2d\u66f4\u5177\u5f71\u54cd\u529b\u3002\u666e\u904d\u89e3\u51b3\u65b9\u6848\u6bd4\u72ed\u7a84\u89e3\u51b3\u65b9\u6848\u66f4\u7a33\u5b9a\u3001\u66f4\u9ad8\u6548\u3002", "conclusion": "\u672c\u7814\u7a76\u5206\u79bb\u51fa\u666e\u904d\u9519\u4f4d\u7684\u5177\u4f53\u8868\u793a\uff0c\u53ef\u7528\u4e8e\u76d1\u63a7\u548c\u7f13\u89e3\u3002\u4e3a\u7814\u7a76LLM\u6cdb\u5316\u4e2d\u7684\u5f52\u7eb3\u504f\u597d\u63d0\u4f9b\u4e86\u8be6\u7ec6\u6848\u4f8b\u548c\u521d\u6b65\u6307\u6807\uff0c\u5f00\u6e90\u4e86\u6240\u6709\u4ee3\u7801\u3001\u6570\u636e\u96c6\u548c\u6a21\u578b\u5fae\u8c03\u3002"}}
{"id": "2602.08028", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.08028", "abs": "https://arxiv.org/abs/2602.08028", "authors": ["Po-Chun Chen", "Hen-Hsen Huang", "Hsin-Hsi Chen"], "title": "Diverge to Induce Prompting: Multi-Rationale Induction for Zero-Shot Reasoning", "comment": "Accepted to Findings of IJCNLP-AACL 2025", "summary": "To address the instability of unguided reasoning paths in standard Chain-of-Thought prompting, recent methods guide large language models (LLMs) by first eliciting a single reasoning strategy. However, relying on just one strategy for each question can still limit performance across diverse tasks. We propose Diverge-to-Induce Prompting (DIP), a framework that first prompts an LLM to generate multiple diverse high-level rationales for each question. Each rationale is then elaborated into a detailed, step-by-step draft plan. Finally, these draft plans are induced into a final plan. DIP enhances zero-shot reasoning accuracy without reliance on resource-intensive sampling. Experiments show that DIP outperforms single-strategy prompting, demonstrating the effectiveness of multi-plan induction for prompt-based reasoning.", "AI": {"tldr": "DIP\u6846\u67b6\u901a\u8fc7\u751f\u6210\u591a\u6837\u5316\u9ad8\u5c42\u63a8\u7406\u7b56\u7565\u5e76\u6574\u5408\u4e3a\u6700\u7ec8\u8ba1\u5212\uff0c\u63d0\u5347\u96f6\u6837\u672c\u63a8\u7406\u51c6\u786e\u6027\uff0c\u65e0\u9700\u4f9d\u8d56\u8d44\u6e90\u5bc6\u96c6\u578b\u91c7\u6837\u3002", "motivation": "\u6807\u51c6\u601d\u7ef4\u94fe\u63d0\u793a\u4e2d\u65e0\u6307\u5bfc\u63a8\u7406\u8def\u5f84\u4e0d\u7a33\u5b9a\uff0c\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u5355\u4e00\u63a8\u7406\u7b56\u7565\u9650\u5236\u4e86\u5728\u4e0d\u540c\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u8868\u73b0\u3002", "method": "\u63d0\u51faDIP\u6846\u67b6\uff1a1) \u751f\u6210\u591a\u4e2a\u591a\u6837\u5316\u9ad8\u5c42\u63a8\u7406\u7b56\u7565\uff1b2) \u5c06\u6bcf\u4e2a\u7b56\u7565\u6269\u5c55\u4e3a\u8be6\u7ec6\u7684\u9010\u6b65\u8349\u7a3f\u8ba1\u5212\uff1b3) \u5c06\u8fd9\u4e9b\u8349\u7a3f\u8ba1\u5212\u5f52\u7eb3\u4e3a\u6700\u7ec8\u8ba1\u5212\u3002", "result": "\u5b9e\u9a8c\u8868\u660eDIP\u4f18\u4e8e\u5355\u4e00\u7b56\u7565\u63d0\u793a\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86\u591a\u8ba1\u5212\u5f52\u7eb3\u5728\u57fa\u4e8e\u63d0\u793a\u7684\u63a8\u7406\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u901a\u8fc7\u751f\u6210\u591a\u6837\u5316\u63a8\u7406\u7b56\u7565\u5e76\u6574\u5408\u4e3a\u6700\u7ec8\u8ba1\u5212\uff0cDIP\u6846\u67b6\u80fd\u591f\u6709\u6548\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u7684\u96f6\u6837\u672c\u63a8\u7406\u51c6\u786e\u6027\u3002"}}
{"id": "2602.07272", "categories": ["cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2602.07272", "abs": "https://arxiv.org/abs/2602.07272", "authors": ["Bowen Xue", "Saeed Hadadan", "Zheng Zeng", "Fabrice Rousselle", "Zahra Montazeri", "Milos Hasan"], "title": "VideoNeuMat: Neural Material Extraction from Generative Video Models", "comment": null, "summary": "Creating photorealistic materials for 3D rendering requires exceptional artistic skill. Generative models for materials could help, but are currently limited by the lack of high-quality training data. While recent video generative models effortlessly produce realistic material appearances, this knowledge remains entangled with geometry and lighting. We present VideoNeuMat, a two-stage pipeline that extracts reusable neural material assets from video diffusion models. First, we finetune a large video model (Wan 2.1 14B) to generate material sample videos under controlled camera and lighting trajectories, effectively creating a \"virtual gonioreflectometer\" that preserves the model's material realism while learning a structured measurement pattern. Second, we reconstruct compact neural materials from these videos through a Large Reconstruction Model (LRM) finetuned from a smaller Wan 1.3B video backbone. From 17 generated video frames, our LRM performs single-pass inference to predict neural material parameters that generalize to novel viewing and lighting conditions. The resulting materials exhibit realism and diversity far exceeding the limited synthetic training data, demonstrating that material knowledge can be successfully transferred from internet-scale video models into standalone, reusable neural 3D assets.", "AI": {"tldr": "VideoNeuMat\uff1a\u4ece\u89c6\u9891\u6269\u6563\u6a21\u578b\u4e2d\u63d0\u53d6\u53ef\u91cd\u7528\u795e\u7ecf\u6750\u8d28\u8d44\u4ea7\u7684\u4e24\u9636\u6bb5\u6d41\u7a0b\uff0c\u5c06\u89c6\u9891\u6a21\u578b\u7684\u6750\u8d28\u77e5\u8bc6\u8f6c\u5316\u4e3a\u72ec\u7acb\u53ef\u7528\u76843D\u795e\u7ecf\u6750\u8d28", "motivation": "\u521b\u5efa\u903c\u771f\u76843D\u6e32\u67d3\u6750\u8d28\u9700\u8981\u6781\u9ad8\u7684\u827a\u672f\u6280\u5de7\uff0c\u800c\u751f\u6210\u6a21\u578b\u56e0\u7f3a\u4e4f\u9ad8\u8d28\u91cf\u8bad\u7ec3\u6570\u636e\u800c\u53d7\u9650\u3002\u89c6\u9891\u751f\u6210\u6a21\u578b\u80fd\u4ea7\u751f\u903c\u771f\u7684\u6750\u8d28\u5916\u89c2\uff0c\u4f46\u8fd9\u4e9b\u77e5\u8bc6\u4e0e\u51e0\u4f55\u548c\u5149\u7167\u7ea0\u7f20\u5728\u4e00\u8d77\uff0c\u65e0\u6cd5\u76f4\u63a5\u91cd\u7528\u3002", "method": "\u4e24\u9636\u6bb5\u6d41\u7a0b\uff1a1\uff09\u5fae\u8c03\u5927\u578b\u89c6\u9891\u6a21\u578b\uff08Wan 2.1 14B\uff09\u5728\u53d7\u63a7\u76f8\u673a\u548c\u5149\u7167\u8f68\u8ff9\u4e0b\u751f\u6210\u6750\u8d28\u6837\u672c\u89c6\u9891\uff0c\u521b\u5efa\"\u865a\u62df\u6d4b\u89d2\u53cd\u5c04\u8ba1\"\uff1b2\uff09\u901a\u8fc7\u4ece\u8f83\u5c0fWan 1.3B\u89c6\u9891\u9aa8\u5e72\u5fae\u8c03\u7684\u5927\u578b\u91cd\u5efa\u6a21\u578b\uff08LRM\uff09\uff0c\u4ece17\u4e2a\u751f\u6210\u89c6\u9891\u5e27\u4e2d\u91cd\u5efa\u7d27\u51d1\u7684\u795e\u7ecf\u6750\u8d28\u53c2\u6570\u3002", "result": "\u4ece17\u4e2a\u751f\u6210\u5e27\u4e2d\u5355\u6b21\u63a8\u7406\u9884\u6d4b\u795e\u7ecf\u6750\u8d28\u53c2\u6570\uff0c\u80fd\u591f\u6cdb\u5316\u5230\u65b0\u7684\u89c6\u89d2\u548c\u5149\u7167\u6761\u4ef6\u3002\u751f\u6210\u7684\u6750\u8d28\u5728\u771f\u5b9e\u611f\u548c\u591a\u6837\u6027\u4e0a\u8fdc\u8d85\u6709\u9650\u7684\u5408\u6210\u8bad\u7ec3\u6570\u636e\u3002", "conclusion": "\u6210\u529f\u5c06\u4e92\u8054\u7f51\u89c4\u6a21\u89c6\u9891\u6a21\u578b\u4e2d\u7684\u6750\u8d28\u77e5\u8bc6\u8f6c\u79fb\u5230\u72ec\u7acb\u53ef\u91cd\u7528\u7684\u795e\u7ecf3D\u8d44\u4ea7\u4e2d\uff0c\u8bc1\u660e\u4e86\u4ece\u89c6\u9891\u6269\u6563\u6a21\u578b\u63d0\u53d6\u53ef\u91cd\u7528\u795e\u7ecf\u6750\u8d28\u8d44\u4ea7\u7684\u53ef\u884c\u6027\u3002"}}
{"id": "2602.07883", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07883", "abs": "https://arxiv.org/abs/2602.07883", "authors": ["Jingqi Zhou", "Sheng Wang", "DeZhao Deng", "Junwen Lu", "Junwei Su", "Qintong Li", "Jiahui Gao", "Hao Wu", "Jiyue Jiang", "Lingpeng Kong", "Chuan Wu"], "title": "ToolSelf: Unifying Task Execution and Self-Reconfiguration via Tool-Driven Intrinsic Adaptation", "comment": null, "summary": "Agentic systems powered by Large Language Models (LLMs) have demonstrated remarkable potential in tackling complex, long-horizon tasks. However, their efficacy is fundamentally constrained by static configurations governing agent behaviors, which are fixed prior to execution and fail to adapt to evolving task dynamics. Existing approaches, relying on manual orchestration or heuristic-based patches, often struggle with poor generalization and fragmented optimization. To transcend these limitations, we propose ToolSelf, a novel paradigm enabling tool-driven runtime self-reconfiguration. By abstracting configuration updates as a callable tool, ToolSelf unifies task execution and self-adjustment into a single action space, achieving a phase transition from external rules to intrinsic parameters. Agents can thereby autonomously update their sub-goals and context based on task progression, and correspondingly adapt their strategy and toolbox, transforming from passive executors into dual managers of both task and self. We further devise Configuration-Aware Two-stage Training (CAT), combining rejection sampling fine-tuning with trajectory-level reinforcement learning to internalize this meta-capability. Extensive experiments across diverse benchmarks demonstrate that ToolSelf rivals specialized workflows while generalizing to novel tasks, achieving a 24.1% average performance gain and illuminating a path toward truly self-adaptive agents.", "AI": {"tldr": "ToolSelf\uff1a\u4e00\u79cd\u65b0\u578b\u667a\u80fd\u4f53\u8303\u5f0f\uff0c\u5c06\u914d\u7f6e\u66f4\u65b0\u62bd\u8c61\u4e3a\u53ef\u8c03\u7528\u5de5\u5177\uff0c\u5b9e\u73b0\u8fd0\u884c\u65f6\u81ea\u6211\u91cd\u914d\u7f6e\uff0c\u4f7fLLM\u667a\u80fd\u4f53\u80fd\u591f\u81ea\u4e3b\u9002\u5e94\u4efb\u52a1\u52a8\u6001\u53d8\u5316", "motivation": "\u5f53\u524d\u57fa\u4e8eLLM\u7684\u667a\u80fd\u4f53\u7cfb\u7edf\u53d7\u9650\u4e8e\u9759\u6001\u914d\u7f6e\uff0c\u8fd9\u4e9b\u914d\u7f6e\u5728\u6267\u884c\u524d\u56fa\u5b9a\uff0c\u65e0\u6cd5\u9002\u5e94\u4efb\u52a1\u52a8\u6001\u53d8\u5316\u3002\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u4eba\u5de5\u7f16\u6392\u6216\u542f\u53d1\u5f0f\u8865\u4e01\uff0c\u6cdb\u5316\u80fd\u529b\u5dee\u4e14\u4f18\u5316\u788e\u7247\u5316", "method": "\u63d0\u51faToolSelf\u8303\u5f0f\uff0c\u5c06\u914d\u7f6e\u66f4\u65b0\u62bd\u8c61\u4e3a\u53ef\u8c03\u7528\u5de5\u5177\uff0c\u7edf\u4e00\u4efb\u52a1\u6267\u884c\u548c\u81ea\u6211\u8c03\u6574\u5230\u5355\u4e00\u52a8\u4f5c\u7a7a\u95f4\u3002\u8bbe\u8ba1\u914d\u7f6e\u611f\u77e5\u4e24\u9636\u6bb5\u8bad\u7ec3\uff08CAT\uff09\uff0c\u7ed3\u5408\u62d2\u7edd\u91c7\u6837\u5fae\u8c03\u548c\u8f68\u8ff9\u7ea7\u5f3a\u5316\u5b66\u4e60", "result": "\u5728\u591a\u6837\u5316\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cToolSelf\u5ab2\u7f8e\u4e13\u7528\u5de5\u4f5c\u6d41\u540c\u65f6\u80fd\u6cdb\u5316\u5230\u65b0\u4efb\u52a1\uff0c\u5e73\u5747\u6027\u80fd\u63d0\u534724.1%\uff0c\u5c55\u793a\u4e86\u5411\u771f\u6b63\u81ea\u9002\u5e94\u667a\u80fd\u4f53\u7684\u8def\u5f84", "conclusion": "ToolSelf\u901a\u8fc7\u5de5\u5177\u9a71\u52a8\u7684\u8fd0\u884c\u65f6\u81ea\u6211\u91cd\u914d\u7f6e\uff0c\u4f7f\u667a\u80fd\u4f53\u4ece\u88ab\u52a8\u6267\u884c\u8005\u8f6c\u53d8\u4e3a\u4efb\u52a1\u548c\u81ea\u6211\u7684\u53cc\u91cd\u7ba1\u7406\u8005\uff0c\u4e3a\u6784\u5efa\u771f\u6b63\u81ea\u9002\u5e94\u7684\u667a\u80fd\u4f53\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f"}}
{"id": "2602.07277", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.07277", "abs": "https://arxiv.org/abs/2602.07277", "authors": ["Rishabh Sharma", "Gijs Hogervorst", "Wayne E. Mackey", "David J. Heeger", "Stefano Martiniani"], "title": "Cross-View World Models", "comment": "12 pages, 7 figures", "summary": "World models enable agents to plan by imagining future states, but existing approaches operate from a single viewpoint, typically egocentric, even when other perspectives would make planning easier; navigation, for instance, benefits from a bird's-eye view. We introduce Cross-View World Models (XVWM), trained with a cross-view prediction objective: given a sequence of frames from one viewpoint, predict the future state from the same or a different viewpoint after an action is taken. Enforcing cross-view consistency acts as geometric regularization: because the input and output views may share little or no visual overlap, to predict across viewpoints, the model must learn view-invariant representations of the environment's 3D structure. We train on synchronized multi-view gameplay data from Aimlabs, an aim-training platform providing precisely aligned multi-camera recordings with high-frequency action labels. The resulting model gives agents parallel imagination streams across viewpoints, enabling planning in whichever frame of reference best suits the task while executing from the egocentric view. Our results show that multi-view consistency provides a strong learning signal for spatially grounded representations. Finally, predicting the consequences of one's actions from another viewpoint may offer a foundation for perspective-taking in multi-agent settings.", "AI": {"tldr": "XVWM\u901a\u8fc7\u8de8\u89c6\u89d2\u9884\u6d4b\u76ee\u6807\u8bad\u7ec3\u4e16\u754c\u6a21\u578b\uff0c\u4f7f\u667a\u80fd\u4f53\u80fd\u591f\u4ece\u4e0d\u540c\u89c6\u89d2\uff08\u5982\u9e1f\u77b0\u56fe\uff09\u8fdb\u884c\u89c4\u5212\uff0c\u540c\u65f6\u4fdd\u6301\u81ea\u6211\u4e2d\u5fc3\u89c6\u89d2\u6267\u884c\uff0c\u5229\u7528\u591a\u89c6\u89d2\u4e00\u81f4\u6027\u4f5c\u4e3a\u51e0\u4f55\u6b63\u5219\u5316\u5b66\u4e60\u73af\u58833D\u7ed3\u6784", "motivation": "\u73b0\u6709\u4e16\u754c\u6a21\u578b\u901a\u5e38\u53ea\u4ece\u5355\u4e00\u81ea\u6211\u4e2d\u5fc3\u89c6\u89d2\u64cd\u4f5c\uff0c\u5373\u4f7f\u5176\u4ed6\u89c6\u89d2\uff08\u5982\u9e1f\u77b0\u56fe\uff09\u5728\u67d0\u4e9b\u4efb\u52a1\uff08\u5982\u5bfc\u822a\uff09\u4e2d\u80fd\u663e\u8457\u7b80\u5316\u89c4\u5212\u8fc7\u7a0b\u3002\u9700\u8981\u5f00\u53d1\u80fd\u591f\u8de8\u4e0d\u540c\u89c6\u89d2\u8fdb\u884c\u9884\u6d4b\u548c\u89c4\u5212\u7684\u4e16\u754c\u6a21\u578b", "method": "\u63d0\u51fa\u8de8\u89c6\u89d2\u4e16\u754c\u6a21\u578b\uff08XVWM\uff09\uff0c\u4f7f\u7528\u8de8\u89c6\u89d2\u9884\u6d4b\u76ee\u6807\u8fdb\u884c\u8bad\u7ec3\uff1a\u7ed9\u5b9a\u4e00\u4e2a\u89c6\u89d2\u7684\u5e27\u5e8f\u5217\uff0c\u9884\u6d4b\u6267\u884c\u52a8\u4f5c\u540e\u76f8\u540c\u6216\u4e0d\u540c\u89c6\u89d2\u7684\u672a\u6765\u72b6\u6001\u3002\u5728Aimlabs\u5e73\u53f0\u7684\u591a\u89c6\u89d2\u6e38\u620f\u6570\u636e\u4e0a\u8bad\u7ec3\uff0c\u8be5\u5e73\u53f0\u63d0\u4f9b\u7cbe\u786e\u5bf9\u9f50\u7684\u591a\u6444\u50cf\u5934\u8bb0\u5f55\u548c\u9ad8\u9891\u52a8\u4f5c\u6807\u7b7e", "result": "\u6a21\u578b\u4e3a\u667a\u80fd\u4f53\u63d0\u4f9b\u8de8\u89c6\u89d2\u7684\u5e76\u884c\u60f3\u8c61\u6d41\uff0c\u4f7f\u667a\u80fd\u4f53\u80fd\u591f\u5728\u6700\u9002\u5408\u4efb\u52a1\u7684\u53c2\u8003\u7cfb\u4e2d\u8fdb\u884c\u89c4\u5212\uff0c\u540c\u65f6\u4ece\u81ea\u6211\u4e2d\u5fc3\u89c6\u89d2\u6267\u884c\u3002\u591a\u89c6\u89d2\u4e00\u81f4\u6027\u4e3a\u7a7a\u95f4\u57fa\u7840\u8868\u793a\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u5b66\u4e60\u4fe1\u53f7", "conclusion": "\u8de8\u89c6\u89d2\u4e16\u754c\u6a21\u578b\u901a\u8fc7\u51e0\u4f55\u6b63\u5219\u5316\u5b66\u4e60\u73af\u58833D\u7ed3\u6784\uff0c\u4f7f\u667a\u80fd\u4f53\u80fd\u591f\u4ece\u4e0d\u540c\u89c6\u89d2\u8fdb\u884c\u89c4\u5212\u3002\u4ece\u4ed6\u4eba\u89c6\u89d2\u9884\u6d4b\u81ea\u8eab\u884c\u52a8\u540e\u679c\u53ef\u80fd\u4e3a\u591a\u667a\u80fd\u4f53\u73af\u5883\u4e2d\u7684\u89c6\u89d2\u91c7\u62e9\u5960\u5b9a\u57fa\u7840"}}
{"id": "2602.07885", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.07885", "abs": "https://arxiv.org/abs/2602.07885", "authors": ["Zhenyuan Zhang", "Xianzhang Jia", "Zhiqin Yang", "Zhenbo Song", "Wei Xue", "Sirui Han", "Yike Guo"], "title": "MemFly: On-the-Fly Memory Optimization via Information Bottleneck", "comment": null, "summary": "Long-term memory enables large language model agents to tackle complex tasks through historical interactions. However, existing frameworks encounter a fundamental dilemma between compressing redundant information efficiently and maintaining precise retrieval for downstream tasks. To bridge this gap, we propose MemFly, a framework grounded in information bottleneck principles that facilitates on-the-fly memory evolution for LLMs. Our approach minimizes compression entropy while maximizing relevance entropy via a gradient-free optimizer, constructing a stratified memory structure for efficient storage. To fully leverage MemFly, we develop a hybrid retrieval mechanism that seamlessly integrates semantic, symbolic, and topological pathways, incorporating iterative refinement to handle complex multi-hop queries. Comprehensive experiments demonstrate that MemFly substantially outperforms state-of-the-art baselines in memory coherence, response fidelity, and accuracy.", "AI": {"tldr": "MemFly\u662f\u4e00\u4e2a\u57fa\u4e8e\u4fe1\u606f\u74f6\u9888\u539f\u5219\u7684LLM\u8bb0\u5fc6\u6846\u67b6\uff0c\u901a\u8fc7\u68af\u5ea6\u81ea\u7531\u4f18\u5316\u5668\u6784\u5efa\u5206\u5c42\u8bb0\u5fc6\u7ed3\u6784\uff0c\u7ed3\u5408\u6df7\u5408\u68c0\u7d22\u673a\u5236\uff0c\u5728\u8bb0\u5fc6\u4e00\u81f4\u6027\u3001\u54cd\u5e94\u4fdd\u771f\u5ea6\u548c\u51c6\u786e\u6027\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u8bb0\u5fc6\u6846\u67b6\u9762\u4e34\u4e00\u4e2a\u57fa\u672c\u56f0\u5883\uff1a\u65e2\u8981\u9ad8\u6548\u538b\u7f29\u5197\u4f59\u4fe1\u606f\uff0c\u53c8\u8981\u4e3a\u4e0b\u6e38\u4efb\u52a1\u4fdd\u6301\u7cbe\u786e\u68c0\u7d22\u3002\u9700\u8981\u5f25\u5408\u8fd9\u4e00\u5dee\u8ddd\u3002", "method": "\u57fa\u4e8e\u4fe1\u606f\u74f6\u9888\u539f\u5219\uff0c\u901a\u8fc7\u68af\u5ea6\u81ea\u7531\u4f18\u5316\u5668\u6700\u5c0f\u5316\u538b\u7f29\u71b5\u540c\u65f6\u6700\u5927\u5316\u76f8\u5173\u71b5\uff0c\u6784\u5efa\u5206\u5c42\u8bb0\u5fc6\u7ed3\u6784\u3002\u5f00\u53d1\u6df7\u5408\u68c0\u7d22\u673a\u5236\uff0c\u6574\u5408\u8bed\u4e49\u3001\u7b26\u53f7\u548c\u62d3\u6251\u8def\u5f84\uff0c\u5305\u542b\u8fed\u4ee3\u7ec6\u5316\u5904\u7406\u590d\u6742\u591a\u8df3\u67e5\u8be2\u3002", "result": "\u7efc\u5408\u5b9e\u9a8c\u8868\u660e\uff0cMemFly\u5728\u8bb0\u5fc6\u4e00\u81f4\u6027\u3001\u54cd\u5e94\u4fdd\u771f\u5ea6\u548c\u51c6\u786e\u6027\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "MemFly\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u73b0\u6709\u8bb0\u5fc6\u6846\u67b6\u7684\u56f0\u5883\uff0c\u901a\u8fc7\u4fe1\u606f\u74f6\u9888\u539f\u5219\u548c\u6df7\u5408\u68c0\u7d22\u673a\u5236\u5b9e\u73b0\u4e86\u9ad8\u6548\u538b\u7f29\u548c\u7cbe\u786e\u68c0\u7d22\u7684\u5e73\u8861\u3002"}}
{"id": "2602.08517", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2602.08517", "abs": "https://arxiv.org/abs/2602.08517", "authors": ["Shaoang Zhang", "Yazhe Niu"], "title": "TreeTensor: Boost AI System on Nested Data with Constrained Tree-Like Tensor", "comment": null, "summary": "Tensor is the most basic and essential data structure of nowadays artificial intelligence (AI) system. The natural properties of Tensor, especially the memory-continuity and slice-independence, make it feasible for training system to leverage parallel computing unit like GPU to process data simultaneously in batch, spatial or temporal dimensions. However, if we look beyond perception tasks, the data in a complicated cognitive AI system usually has hierarchical structures (i.e. nested data) with various modalities. They are inconvenient and inefficient to program directly with conventional Tensor with fixed shape. To address this issue, we summarize two main computational patterns of nested data, and then propose a general nested data container: TreeTensor. Through various constraints and magic utilities of TreeTensor, one can apply arbitrary functions and operations to nested data with almost zero cost, including some famous machine learning libraries, such as Scikit-Learn, Numpy and PyTorch. Our approach utilizes a constrained tree-structure perspective to systematically model data relationships, and it can also easily be combined with other methods to extend more usages, such as asynchronous execution and variable-length data computation. Detailed examples and benchmarks show TreeTensor not only provides powerful usability in various problems, especially one of the most complicated AI systems at present: AlphaStar for StarCraftII, but also exhibits excellent runtime efficiency without any overhead. Our project is available at https://github.com/opendilab/DI-treetensor.", "AI": {"tldr": "TreeTensor\u662f\u4e00\u4e2a\u901a\u7528\u7684\u5d4c\u5957\u6570\u636e\u5bb9\u5668\uff0c\u7528\u4e8e\u5904\u7406\u5177\u6709\u5c42\u6b21\u7ed3\u6784\u7684\u591a\u6a21\u6001\u6570\u636e\uff0c\u80fd\u591f\u96f6\u6210\u672c\u5730\u5e94\u7528\u5404\u79cd\u51fd\u6570\u548c\u64cd\u4f5c\u5230\u5d4c\u5957\u6570\u636e\u4e0a\u3002", "motivation": "\u4f20\u7edf\u5f20\u91cf\uff08Tensor\uff09\u5728\u5904\u7406\u5177\u6709\u5c42\u6b21\u7ed3\u6784\u7684\u5d4c\u5957\u6570\u636e\u65f6\u5b58\u5728\u4e0d\u4fbf\u548c\u4f4e\u6548\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u590d\u6742\u7684\u8ba4\u77e5AI\u7cfb\u7edf\u4e2d\uff0c\u6570\u636e\u901a\u5e38\u5177\u6709\u5404\u79cd\u6a21\u6001\u7684\u5d4c\u5957\u7ed3\u6784\u3002", "method": "\u901a\u8fc7\u603b\u7ed3\u5d4c\u5957\u6570\u636e\u7684\u4e24\u79cd\u4e3b\u8981\u8ba1\u7b97\u6a21\u5f0f\uff0c\u63d0\u51faTreeTensor\u8fd9\u4e00\u901a\u7528\u5d4c\u5957\u6570\u636e\u5bb9\u5668\uff0c\u5229\u7528\u7ea6\u675f\u6811\u7ed3\u6784\u89c6\u89d2\u7cfb\u7edf\u5efa\u6a21\u6570\u636e\u5173\u7cfb\uff0c\u5e76\u63d0\u4f9b\u5404\u79cd\u9b54\u6cd5\u5de5\u5177\u6765\u96f6\u6210\u672c\u5730\u5e94\u7528\u51fd\u6570\u548c\u64cd\u4f5c\u3002", "result": "TreeTensor\u5728\u5404\u79cd\u95ee\u9898\u4e2d\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u53ef\u7528\u6027\uff0c\u7279\u522b\u662f\u5728\u76ee\u524d\u6700\u590d\u6742\u7684AI\u7cfb\u7edf\u4e4b\u4e00\u2014\u2014\u661f\u9645\u4e89\u9738II\u7684AlphaStar\u4e2d\uff0c\u540c\u65f6\u8868\u73b0\u51fa\u4f18\u5f02\u7684\u8fd0\u884c\u65f6\u6548\u7387\u4e14\u65e0\u4efb\u4f55\u5f00\u9500\u3002", "conclusion": "TreeTensor\u4e3a\u89e3\u51b3\u5d4c\u5957\u6570\u636e\u5904\u7406\u95ee\u9898\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u4e0e\u73b0\u6709\u673a\u5668\u5b66\u4e60\u5e93\u65e0\u7f1d\u96c6\u6210\uff0c\u5e76\u53ef\u901a\u8fc7\u4e0e\u5176\u4ed6\u65b9\u6cd5\u7ed3\u5408\u6269\u5c55\u66f4\u591a\u7528\u9014\u3002"}}
{"id": "2602.08048", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.08048", "abs": "https://arxiv.org/abs/2602.08048", "authors": ["Arshia Hemmat", "Philip Torr", "Yongqiang Chen", "Junchi Yu"], "title": "TDGNet: Hallucination Detection in Diffusion Language Models via Temporal Dynamic Graphs", "comment": null, "summary": "Diffusion language models (D-LLMs) offer parallel denoising and bidirectional context, but hallucination detection for D-LLMs remains underexplored. Prior detectors developed for auto-regressive LLMs typically rely on single-pass cues and do not directly transfer to diffusion generation, where factuality evidence is distributed across the denoising trajectory and may appear, drift, or be self-corrected over time. We introduce TDGNet, a temporal dynamic graph framework that formulates hallucination detection as learning over evolving token-level attention graphs. At each denoising step, we sparsify the attention graph and update per-token memories via message passing, then apply temporal attention to aggregate trajectory-wide evidence for final prediction. Experiments on LLaDA-8B and Dream-7B across QA benchmarks show consistent AUROC improvements over output-based, latent-based, and static-graph baselines, with single-pass inference and modest overhead. These results highlight the importance of temporal reasoning on attention graphs for robust hallucination detection in diffusion language models.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faTDGNet\uff0c\u4e00\u79cd\u7528\u4e8e\u6269\u6563\u8bed\u8a00\u6a21\u578b\u5e7b\u89c9\u68c0\u6d4b\u7684\u65f6\u5e8f\u52a8\u6001\u56fe\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u6790\u53bb\u566a\u8fc7\u7a0b\u4e2d\u7684\u6ce8\u610f\u529b\u56fe\u6f14\u5316\u6765\u68c0\u6d4b\u5e7b\u89c9\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u663e\u8457\u6539\u8fdb\u3002", "motivation": "\u6269\u6563\u8bed\u8a00\u6a21\u578b\u5177\u6709\u5e76\u884c\u53bb\u566a\u548c\u53cc\u5411\u4e0a\u4e0b\u6587\u4f18\u52bf\uff0c\u4f46\u5176\u5e7b\u89c9\u68c0\u6d4b\u7814\u7a76\u4e0d\u8db3\u3002\u73b0\u6709\u7684\u81ea\u56de\u5f52LLM\u68c0\u6d4b\u5668\u4f9d\u8d56\u5355\u6b21\u63a8\u7406\u7ebf\u7d22\uff0c\u4e0d\u9002\u7528\u4e8e\u6269\u6563\u751f\u6210\uff0c\u56e0\u4e3a\u4e8b\u5b9e\u8bc1\u636e\u5206\u5e03\u5728\u53bb\u566a\u8f68\u8ff9\u4e2d\uff0c\u53ef\u80fd\u968f\u65f6\u95f4\u51fa\u73b0\u3001\u6f02\u79fb\u6216\u81ea\u6211\u4fee\u6b63\u3002", "method": "\u63d0\u51faTDGNet\u6846\u67b6\uff0c\u5c06\u5e7b\u89c9\u68c0\u6d4b\u5efa\u6a21\u4e3a\u5728\u6f14\u5316\u4ee4\u724c\u7ea7\u6ce8\u610f\u529b\u56fe\u4e0a\u7684\u5b66\u4e60\u95ee\u9898\u3002\u5728\u6bcf\u4e2a\u53bb\u566a\u6b65\u9aa4\u4e2d\uff0c\u7a00\u758f\u5316\u6ce8\u610f\u529b\u56fe\u5e76\u901a\u8fc7\u6d88\u606f\u4f20\u9012\u66f4\u65b0\u6bcf\u4e2a\u4ee4\u724c\u7684\u8bb0\u5fc6\uff0c\u7136\u540e\u4f7f\u7528\u65f6\u5e8f\u6ce8\u610f\u529b\u805a\u5408\u8f68\u8ff9\u8303\u56f4\u5185\u7684\u8bc1\u636e\u8fdb\u884c\u6700\u7ec8\u9884\u6d4b\u3002", "result": "\u5728LLaDA-8B\u548cDream-7B\u6a21\u578b\u4e0a\u7684QA\u57fa\u51c6\u6d4b\u8bd5\u663e\u793a\uff0cTDGNet\u76f8\u6bd4\u57fa\u4e8e\u8f93\u51fa\u3001\u57fa\u4e8e\u6f5c\u5728\u8868\u793a\u548c\u9759\u6001\u56fe\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5728AUROC\u6307\u6807\u4e0a\u53d6\u5f97\u4e00\u81f4\u6539\u8fdb\uff0c\u5177\u6709\u5355\u6b21\u63a8\u7406\u548c\u9002\u5ea6\u8ba1\u7b97\u5f00\u9500\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u5bf9\u6ce8\u610f\u529b\u56fe\u8fdb\u884c\u65f6\u5e8f\u63a8\u7406\u5bf9\u4e8e\u6269\u6563\u8bed\u8a00\u6a21\u578b\u7684\u9c81\u68d2\u5e7b\u89c9\u68c0\u6d4b\u81f3\u5173\u91cd\u8981\uff0cTDGNet\u6846\u67b6\u6709\u6548\u5229\u7528\u4e86\u6269\u6563\u751f\u6210\u8fc7\u7a0b\u4e2d\u7684\u52a8\u6001\u7279\u6027\u3002"}}
{"id": "2602.07301", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.07301", "abs": "https://arxiv.org/abs/2602.07301", "authors": ["Aruna Jithesh", "Chinmayi Karumuri", "Venkata Kiran Reddy Kotha", "Meghana Doddapuneni", "Taehee Jeong"], "title": "Diabetic Retinopathy Lesion Segmentation through Attention Mechanisms", "comment": null, "summary": "Diabetic Retinopathy (DR) is an eye disease which arises due to diabetes mellitus. It might cause vision loss and blindness. To prevent irreversible vision loss, early detection through systematic screening is crucial. Although researchers have developed numerous automated deep learning-based algorithms for DR screening, their clinical applicability remains limited, particularly in lesion segmentation. Our method provides pixel-level annotations for lesions, which practically supports Ophthalmologist to screen DR from fundus images. In this work, we segmented four types of DR-related lesions: microaneurysms, soft exudates, hard exudates, and hemorrhages on 757 images from DDR dataset. To enhance lesion segmentation, an attention mechanism was integrated with DeepLab-V3+. Compared to the baseline model, the Attention-DeepLab model increases mean average precision (mAP) from 0.3010 to 0.3326 and the mean Intersection over Union (IoU) from 0.1791 to 0.1928. The model also increased microaneurysm detection from 0.0205 to 0.0763, a clinically significant improvement. The detection of microaneurysms is the earliest visible symptom of DR.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6ce8\u610f\u529b\u673a\u5236\u7684DeepLab-V3+\u6a21\u578b\uff0c\u7528\u4e8e\u7cd6\u5c3f\u75c5\u89c6\u7f51\u819c\u75c5\u53d8\uff08DR\uff09\u76f8\u5173\u75c5\u53d8\u7684\u50cf\u7d20\u7ea7\u5206\u5272\uff0c\u5728DDR\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u7cd6\u5c3f\u75c5\u89c6\u7f51\u819c\u75c5\u53d8\uff08DR\uff09\u662f\u5bfc\u81f4\u89c6\u529b\u4e27\u5931\u548c\u5931\u660e\u7684\u4e25\u91cd\u773c\u75c5\uff0c\u65e9\u671f\u7b5b\u67e5\u81f3\u5173\u91cd\u8981\u3002\u867d\u7136\u5df2\u6709\u8bb8\u591a\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u81ea\u52a8\u7b5b\u67e5\u7b97\u6cd5\uff0c\u4f46\u5728\u75c5\u53d8\u5206\u5272\u65b9\u9762\u7684\u4e34\u5e8a\u5e94\u7528\u4ecd\u6709\u9650\u5236\u3002\u672c\u6587\u65e8\u5728\u63d0\u4f9b\u50cf\u7d20\u7ea7\u75c5\u53d8\u6807\u6ce8\uff0c\u4ee5\u5b9e\u9645\u652f\u6301\u773c\u79d1\u533b\u751f\u4ece\u773c\u5e95\u56fe\u50cf\u4e2d\u7b5b\u67e5DR\u3002", "method": "\u5728757\u5f20DDR\u6570\u636e\u96c6\u56fe\u50cf\u4e0a\u5206\u5272\u56db\u79cdDR\u76f8\u5173\u75c5\u53d8\uff1a\u5fae\u52a8\u8109\u7624\u3001\u8f6f\u6027\u6e17\u51fa\u7269\u3001\u786c\u6027\u6e17\u51fa\u7269\u548c\u51fa\u8840\u3002\u91c7\u7528\u6ce8\u610f\u529b\u673a\u5236\u4e0eDeepLab-V3+\u7ed3\u5408\u7684\u65b9\u6cd5\uff0c\u6784\u5efaAttention-DeepLab\u6a21\u578b\u6765\u589e\u5f3a\u75c5\u53d8\u5206\u5272\u80fd\u529b\u3002", "result": "\u4e0e\u57fa\u7ebf\u6a21\u578b\u76f8\u6bd4\uff0cAttention-DeepLab\u6a21\u578b\u5c06\u5e73\u5747\u7cbe\u5ea6\uff08mAP\uff09\u4ece0.3010\u63d0\u5347\u81f30.3326\uff0c\u5e73\u5747\u4ea4\u5e76\u6bd4\uff08IoU\uff09\u4ece0.1791\u63d0\u5347\u81f30.1928\u3002\u5fae\u52a8\u8109\u7624\u68c0\u6d4b\u4ece0.0205\u663e\u8457\u63d0\u5347\u81f30.0763\uff0c\u8fd9\u5728\u4e34\u5e8a\u4e0a\u5177\u6709\u91cd\u8981\u610f\u4e49\uff0c\u56e0\u4e3a\u5fae\u52a8\u8109\u7624\u662fDR\u6700\u65e9\u53ef\u89c1\u7684\u75c7\u72b6\u3002", "conclusion": "\u96c6\u6210\u6ce8\u610f\u529b\u673a\u5236\u7684DeepLab-V3+\u6a21\u578b\u5728DR\u75c5\u53d8\u5206\u5272\u65b9\u9762\u8868\u73b0\u51fa\u6027\u80fd\u63d0\u5347\uff0c\u7279\u522b\u662f\u5728\u5fae\u52a8\u8109\u7624\u68c0\u6d4b\u65b9\u9762\u53d6\u5f97\u4e86\u4e34\u5e8a\u663e\u8457\u6539\u8fdb\uff0c\u6709\u52a9\u4e8e\u65e9\u671fDR\u7b5b\u67e5\u548c\u9884\u9632\u4e0d\u53ef\u9006\u7684\u89c6\u529b\u4e27\u5931\u3002"}}
{"id": "2602.08100", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08100", "abs": "https://arxiv.org/abs/2602.08100", "authors": ["Jasmine Cui", "Charles Ye"], "title": "Emergent Search and Backtracking in Latent Reasoning Models", "comment": null, "summary": "What happens when a language model thinks without words? Standard reasoning LLMs verbalize intermediate steps as chain-of-thought; latent reasoning transformers (LRTs) instead perform deliberation entirely in continuous hidden space. We investigate an LRT, decoding the model's evolving beliefs at every step on a multiple-choice QA benchmark. We find that the model spontaneously learns a structured search process in latent space. Deliberation follows a consistent trajectory: an exploration phase where probability mass spreads across candidates, tentative commitment to a frontrunner, and either convergence or backtracking. Backtracking is prevalent (32% of instances), beneficial (34% accuracy gain over non-backtracking instances), and predominantly directed away from the semantically closest distractor toward the correct answer. The search is adaptive: replacing distractors with implausible alternatives shortens exploration by 54%. Latent reasoning models achieve in activation space what chain-of-thought achieves through words: the ability to be wrong, notice, and recover.", "AI": {"tldr": "LRTs\u5728\u9690\u85cf\u7a7a\u95f4\u4e2d\u8fdb\u884c\u65e0\u8bcd\u63a8\u7406\uff0c\u81ea\u53d1\u5f62\u6210\u7ed3\u6784\u5316\u641c\u7d22\u8fc7\u7a0b\uff1a\u63a2\u7d22\u9636\u6bb5\u3001\u521d\u6b65\u627f\u8bfa\u3001\u6536\u655b\u6216\u56de\u6eaf\uff0c\u56de\u6eaf\u80fd\u663e\u8457\u63d0\u5347\u51c6\u786e\u6027", "motivation": "\u7814\u7a76\u8bed\u8a00\u6a21\u578b\u5728\u65e0\u8bcd\u60c5\u51b5\u4e0b\u7684\u63a8\u7406\u8fc7\u7a0b\uff0c\u63a2\u7d22\u6f5c\u5728\u63a8\u7406\u53d8\u6362\u5668\uff08LRTs\uff09\u5982\u4f55\u5728\u8fde\u7eed\u9690\u85cf\u7a7a\u95f4\u4e2d\u8fdb\u884c\u601d\u8003\uff0c\u4e0e\u4f20\u7edf\u7684\u94fe\u5f0f\u601d\u7ef4\uff08CoT\uff09\u5f62\u6210\u5bf9\u6bd4", "method": "\u4f7f\u7528\u6f5c\u5728\u63a8\u7406\u53d8\u6362\u5668\uff08LRTs\uff09\u5728\u591a\u9879\u9009\u62e9QA\u57fa\u51c6\u4e0a\u89e3\u7801\u6a21\u578b\u6bcf\u4e00\u6b65\u6f14\u53d8\u7684\u4fe1\u5ff5\uff0c\u5206\u6790\u5176\u5728\u9690\u85cf\u7a7a\u95f4\u4e2d\u7684\u7ed3\u6784\u5316\u641c\u7d22\u8fc7\u7a0b", "result": "\u6a21\u578b\u81ea\u53d1\u5b66\u4e60\u5230\u7ed3\u6784\u5316\u641c\u7d22\u8fc7\u7a0b\uff1a\u63a2\u7d22\u9636\u6bb5\u6982\u7387\u8d28\u91cf\u5206\u6563\u3001\u521d\u6b65\u627f\u8bfa\u9886\u5148\u9009\u9879\u3001\u6536\u655b\u6216\u56de\u6eaf\uff1b\u56de\u6eaf\u666e\u904d\uff0832%\uff09\u3001\u6709\u76ca\uff08\u51c6\u786e\u7387\u63d0\u534734%\uff09\uff0c\u4e14\u4e3b\u8981\u4ece\u8bed\u4e49\u6700\u63a5\u8fd1\u7684\u5e72\u6270\u9879\u8f6c\u5411\u6b63\u786e\u7b54\u6848\uff1b\u641c\u7d22\u5177\u6709\u9002\u5e94\u6027\uff0c\u66ff\u6362\u5e72\u6270\u9879\u53ef\u7f29\u77ed\u63a2\u7d22\u65f6\u95f454%", "conclusion": "\u6f5c\u5728\u63a8\u7406\u6a21\u578b\u5728\u6fc0\u6d3b\u7a7a\u95f4\u4e2d\u5b9e\u73b0\u4e86\u94fe\u5f0f\u601d\u7ef4\u901a\u8fc7\u6587\u5b57\u5b9e\u73b0\u7684\u529f\u80fd\uff1a\u80fd\u591f\u72af\u9519\u3001\u5bdf\u89c9\u5e76\u6062\u590d\uff0c\u5c55\u793a\u4e86\u65e0\u8bcd\u63a8\u7406\u7684\u6709\u6548\u6027\u548c\u7075\u6d3b\u6027"}}
{"id": "2602.07310", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.07310", "abs": "https://arxiv.org/abs/2602.07310", "authors": ["Kyle Williams", "Andrew Seltzman"], "title": "Optimization of Precipitate Segmentation Through Linear Genetic Programming of Image Processing", "comment": "39 pages, 12 figures, 1 table", "summary": "Current analysis of additive manufactured niobium-based copper alloys relies on hand annotation due to varying contrast, noise, and image artifacts present in micrographs, slowing iteration speed in alloy development. We present a filtering and segmentation algorithm for detecting precipitates in FIB cross-section micrographs, optimized using linear genetic programming (LGP), which accounts for the various artifacts. To this end, the optimization environment uses a domain-specific language for image processing to iterate on solutions. Programs in this language are a list of image-filtering blocks with tunable parameters that sequentially process an input image, allowing for reliable generation and mutation by a genetic algorithm. Our environment produces optimized human-interpretable MATLAB code representing an image filtering pipeline. Under ideal conditions--a population size of 60 and a maximum program length of 5 blocks--our system was able to find a near-human accuracy solution with an average evaluation error of 1.8% when comparing segmentations pixel-by-pixel to a human baseline using an XOR error evaluation. Our automation work enabled faster iteration cycles and furthered exploration of the material composition and processing space: our optimized pipeline algorithm processes a 3.6 megapixel image in about 2 seconds on average. This ultimately enables convergence on strong, low-activation, precipitation hardened copper alloys for additive manufactured fusion reactor parts.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u79cd\u57fa\u4e8e\u7ebf\u6027\u9057\u4f20\u7f16\u7a0b\u7684\u56fe\u50cf\u8fc7\u6ee4\u548c\u5206\u5272\u7b97\u6cd5\uff0c\u7528\u4e8e\u81ea\u52a8\u68c0\u6d4b\u589e\u6750\u5236\u9020\u94cc\u57fa\u94dc\u5408\u91d1\u4e2d\u7684\u6790\u51fa\u7269\uff0c\u66ff\u4ee3\u4e86\u4f20\u7edf\u7684\u624b\u52a8\u6807\u6ce8\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5408\u91d1\u5f00\u53d1\u8fed\u4ee3\u901f\u5ea6\u3002", "motivation": "\u5f53\u524d\u589e\u6750\u5236\u9020\u94cc\u57fa\u94dc\u5408\u91d1\u7684\u5206\u6790\u4f9d\u8d56\u624b\u52a8\u6807\u6ce8\uff0c\u7531\u4e8e\u663e\u5fae\u56fe\u50cf\u4e2d\u5b58\u5728\u5bf9\u6bd4\u5ea6\u53d8\u5316\u3001\u566a\u58f0\u548c\u56fe\u50cf\u4f2a\u5f71\u7b49\u95ee\u9898\uff0c\u8fd9\u4e25\u91cd\u62d6\u6162\u4e86\u5408\u91d1\u5f00\u53d1\u7684\u8fed\u4ee3\u901f\u5ea6\u3002\u9700\u8981\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u6765\u52a0\u901f\u6750\u6599\u6210\u5206\u548c\u5de5\u827a\u7a7a\u95f4\u7684\u63a2\u7d22\u3002", "method": "\u91c7\u7528\u7ebf\u6027\u9057\u4f20\u7f16\u7a0b\u4f18\u5316\u56fe\u50cf\u8fc7\u6ee4\u548c\u5206\u5272\u7b97\u6cd5\uff0c\u4f7f\u7528\u7279\u5b9a\u9886\u57df\u8bed\u8a00\u6784\u5efa\u56fe\u50cf\u5904\u7406\u6d41\u7a0b\u3002\u8be5\u8bed\u8a00\u4e2d\u7684\u7a0b\u5e8f\u662f\u4e00\u7cfb\u5217\u53ef\u8c03\u53c2\u6570\u7684\u56fe\u50cf\u8fc7\u6ee4\u5757\uff0c\u901a\u8fc7\u9057\u4f20\u7b97\u6cd5\u8fdb\u884c\u53ef\u9760\u751f\u6210\u548c\u53d8\u5f02\uff0c\u6700\u7ec8\u751f\u6210\u53ef\u89e3\u91ca\u7684MATLAB\u4ee3\u7801\u8868\u793a\u7684\u56fe\u50cf\u5904\u7406\u7ba1\u9053\u3002", "result": "\u5728\u7406\u60f3\u6761\u4ef6\u4e0b\uff08\u79cd\u7fa4\u5927\u5c0f60\uff0c\u6700\u5927\u7a0b\u5e8f\u957f\u5ea65\u4e2a\u5757\uff09\uff0c\u7cfb\u7edf\u627e\u5230\u4e86\u63a5\u8fd1\u4eba\u7c7b\u51c6\u786e\u5ea6\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e73\u5747\u8bc4\u4f30\u8bef\u5dee\u4e3a1.8%\uff08\u901a\u8fc7\u50cf\u7d20\u7ea7XOR\u8bef\u5dee\u8bc4\u4f30\u4e0e\u4eba\u5de5\u57fa\u51c6\u6bd4\u8f83\uff09\u3002\u4f18\u5316\u540e\u7684\u7ba1\u9053\u7b97\u6cd5\u5e73\u5747\u5904\u7406360\u4e07\u50cf\u7d20\u56fe\u50cf\u4ec5\u9700\u7ea62\u79d2\u3002", "conclusion": "\u8be5\u81ea\u52a8\u5316\u5de5\u4f5c\u5b9e\u73b0\u4e86\u66f4\u5feb\u7684\u8fed\u4ee3\u5468\u671f\uff0c\u4fc3\u8fdb\u4e86\u6750\u6599\u6210\u5206\u548c\u5de5\u827a\u7a7a\u95f4\u7684\u63a2\u7d22\uff0c\u6700\u7ec8\u6709\u52a9\u4e8e\u5f00\u53d1\u7528\u4e8e\u589e\u6750\u5236\u9020\u805a\u53d8\u53cd\u5e94\u5806\u90e8\u4ef6\u7684\u5f3a\u97e7\u3001\u4f4e\u6d3b\u5316\u3001\u6c89\u6dc0\u786c\u5316\u94dc\u5408\u91d1\u3002"}}
{"id": "2602.08124", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.08124", "abs": "https://arxiv.org/abs/2602.08124", "authors": ["Ke Xu", "Shera Potka", "Alex Thomo"], "title": "Gender and Race Bias in Consumer Product Recommendations by Large Language Models", "comment": "Accepted at the 39th International Conference on Advanced Information Networking and Applications (AINA 2025)", "summary": "Large Language Models are increasingly employed in generating consumer product recommendations, yet their potential for embedding and amplifying gender and race biases remains underexplored. This paper serves as one of the first attempts to examine these biases within LLM-generated recommendations. We leverage prompt engineering to elicit product suggestions from LLMs for various race and gender groups and employ three analytical methods-Marked Words, Support Vector Machines, and Jensen-Shannon Divergence-to identify and quantify biases. Our findings reveal significant disparities in the recommendations for demographic groups, underscoring the need for more equitable LLM recommendation systems.", "AI": {"tldr": "\u8be5\u7814\u7a76\u9996\u6b21\u7cfb\u7edf\u6027\u5730\u63a2\u7d22\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u751f\u6210\u4ea7\u54c1\u63a8\u8350\u65f6\u5b58\u5728\u7684\u6027\u522b\u548c\u79cd\u65cf\u504f\u89c1\u95ee\u9898\uff0c\u901a\u8fc7\u591a\u79cd\u5206\u6790\u65b9\u6cd5\u63ed\u793a\u4e86\u63a8\u8350\u7ed3\u679c\u4e2d\u7684\u663e\u8457\u4eba\u53e3\u7edf\u8ba1\u5b66\u5dee\u5f02\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8d8a\u6765\u8d8a\u591a\u5730\u7528\u4e8e\u751f\u6210\u6d88\u8d39\u8005\u4ea7\u54c1\u63a8\u8350\uff0c\u4f46\u5176\u53ef\u80fd\u5d4c\u5165\u548c\u653e\u5927\u6027\u522b\u4e0e\u79cd\u65cf\u504f\u89c1\u7684\u6f5c\u529b\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002\u672c\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u6210\u4e3a\u9996\u6279\u7cfb\u7edf\u7814\u7a76LLM\u63a8\u8350\u7cfb\u7edf\u4e2d\u504f\u89c1\u95ee\u9898\u7684\u5c1d\u8bd5\u4e4b\u4e00\u3002", "method": "\u7814\u7a76\u91c7\u7528\u63d0\u793a\u5de5\u7a0b\u65b9\u6cd5\uff0c\u5f15\u5bfcLLM\u4e3a\u4e0d\u540c\u79cd\u65cf\u548c\u6027\u522b\u7fa4\u4f53\u751f\u6210\u4ea7\u54c1\u63a8\u8350\uff0c\u7136\u540e\u4f7f\u7528\u4e09\u79cd\u5206\u6790\u65b9\u6cd5\uff1a\u6807\u8bb0\u8bcd\u5206\u6790\u3001\u652f\u6301\u5411\u91cf\u673a\u548cJensen-Shannon\u6563\u5ea6\u6765\u8bc6\u522b\u548c\u91cf\u5316\u504f\u89c1\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u4e0d\u540c\u4eba\u53e3\u7edf\u8ba1\u5b66\u7fa4\u4f53\u83b7\u5f97\u7684\u63a8\u8350\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u63ed\u793a\u4e86LLM\u63a8\u8350\u7cfb\u7edf\u4e2d\u5b58\u5728\u7684\u4e0d\u516c\u5e73\u73b0\u8c61\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u5f3a\u8c03\u4e86\u5f00\u53d1\u66f4\u516c\u5e73\u7684LLM\u63a8\u8350\u7cfb\u7edf\u7684\u5fc5\u8981\u6027\uff0c\u4e3a\u540e\u7eed\u7684\u504f\u89c1\u7f13\u89e3\u7814\u7a76\u63d0\u4f9b\u4e86\u5b9e\u8bc1\u57fa\u7840\u3002"}}
{"id": "2602.07311", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07311", "abs": "https://arxiv.org/abs/2602.07311", "authors": ["Difei Gu", "Yunhe Gao", "Gerasimos Chatzoudis", "Zihan Dong", "Guoning Zhang", "Bangwei Guo", "Yang Zhou", "Mu Zhou", "Dimitris Metaxas"], "title": "LUCID-SAE: Learning Unified Vision-Language Sparse Codes for Interpretable Concept Discovery", "comment": null, "summary": "Sparse autoencoders (SAEs) offer a natural path toward comparable explanations across different representation spaces. However, current SAEs are trained per modality, producing dictionaries whose features are not directly understandable and whose explanations do not transfer across domains. In this study, we introduce LUCID (Learning Unified vision-language sparse Codes for Interpretable concept Discovery), a unified vision-language sparse autoencoder that learns a shared latent dictionary for image patch and text token representations, while reserving private capacity for modality-specific details. We achieve feature alignment by coupling the shared codes with a learned optimal transport matching objective without the need of labeling. LUCID yields interpretable shared features that support patch-level grounding, establish cross-modal neuron correspondence, and enhance robustness against the concept clustering problem in similarity-based evaluation. Leveraging the alignment properties, we develop an automated dictionary interpretation pipeline based on term clustering without manual observations. Our analysis reveals that LUCID's shared features capture diverse semantic categories beyond objects, including actions, attributes, and abstract concepts, demonstrating a comprehensive approach to interpretable multimodal representations.", "AI": {"tldr": "LUCID\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u89c6\u89c9-\u8bed\u8a00\u7a00\u758f\u81ea\u7f16\u7801\u5668\uff0c\u901a\u8fc7\u5171\u4eab\u6f5c\u5728\u5b57\u5178\u5b66\u4e60\u8de8\u6a21\u6001\u5bf9\u9f50\u7684\u7279\u5f81\u8868\u793a\uff0c\u5b9e\u73b0\u53ef\u89e3\u91ca\u7684\u6982\u5ff5\u53d1\u73b0\u3002", "motivation": "\u73b0\u6709\u7a00\u758f\u81ea\u7f16\u7801\u5668\u6309\u6a21\u6001\u5355\u72ec\u8bad\u7ec3\uff0c\u4ea7\u751f\u7684\u7279\u5f81\u5b57\u5178\u7f3a\u4e4f\u76f4\u63a5\u53ef\u89e3\u91ca\u6027\uff0c\u4e14\u89e3\u91ca\u65e0\u6cd5\u8de8\u57df\u8fc1\u79fb\u3002\u9700\u8981\u4e00\u79cd\u7edf\u4e00\u7684\u65b9\u6cd5\u6765\u5b66\u4e60\u8de8\u6a21\u6001\u5bf9\u9f50\u7684\u53ef\u89e3\u91ca\u7279\u5f81\u8868\u793a\u3002", "method": "\u63d0\u51faLUCID\u6846\u67b6\uff1a1) \u5b66\u4e60\u56fe\u50cf\u5757\u548c\u6587\u672c\u6807\u8bb0\u7684\u5171\u4eab\u6f5c\u5728\u5b57\u5178\uff0c\u540c\u65f6\u4fdd\u7559\u6a21\u6001\u7279\u5b9a\u7ec6\u8282\u7684\u79c1\u6709\u5bb9\u91cf\uff1b2) \u901a\u8fc7\u6700\u4f18\u4f20\u8f93\u5339\u914d\u76ee\u6807\u5b9e\u73b0\u7279\u5f81\u5bf9\u9f50\uff0c\u65e0\u9700\u6807\u6ce8\uff1b3) \u5f00\u53d1\u57fa\u4e8e\u672f\u8bed\u805a\u7c7b\u7684\u81ea\u52a8\u5b57\u5178\u89e3\u91ca\u6d41\u7a0b\u3002", "result": "LUCID\u4ea7\u751f\u53ef\u89e3\u91ca\u7684\u5171\u4eab\u7279\u5f81\uff0c\u652f\u6301\u5757\u7ea7\u5b9a\u4f4d\uff0c\u5efa\u7acb\u8de8\u6a21\u6001\u795e\u7ecf\u5143\u5bf9\u5e94\u5173\u7cfb\uff0c\u589e\u5f3a\u5bf9\u76f8\u4f3c\u6027\u8bc4\u4f30\u4e2d\u6982\u5ff5\u805a\u7c7b\u95ee\u9898\u7684\u9c81\u68d2\u6027\u3002\u5171\u4eab\u7279\u5f81\u6355\u83b7\u4e86\u8d85\u8d8a\u5bf9\u8c61\u7684\u591a\u6837\u5316\u8bed\u4e49\u7c7b\u522b\uff0c\u5305\u62ec\u52a8\u4f5c\u3001\u5c5e\u6027\u548c\u62bd\u8c61\u6982\u5ff5\u3002", "conclusion": "LUCID\u4e3a\u53ef\u89e3\u91ca\u7684\u591a\u6a21\u6001\u8868\u793a\u63d0\u4f9b\u4e86\u4e00\u79cd\u5168\u9762\u65b9\u6cd5\uff0c\u901a\u8fc7\u7edf\u4e00\u7684\u89c6\u89c9-\u8bed\u8a00\u7a00\u758f\u7f16\u7801\u5b9e\u73b0\u8de8\u6a21\u6001\u6982\u5ff5\u53d1\u73b0\u548c\u89e3\u91ca\uff0c\u4e3a\u7406\u89e3\u591a\u6a21\u6001\u6a21\u578b\u5185\u90e8\u8868\u793a\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2602.07919", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.07919", "abs": "https://arxiv.org/abs/2602.07919", "authors": ["Mansi", "Avinash Kori", "Francesca Toni", "Soteris Demetriou"], "title": "Selective Fine-Tuning for Targeted and Robust Concept Unlearning", "comment": "Given the brittle nature of existing methods in unlearning harmful content in diffusion models, we propose TRuST, a novel approach for dynamically estimating target concept neurons and unlearning them by selectively fine-tuning", "summary": "Text guided diffusion models are used by millions of users, but can be easily exploited to produce harmful content. Concept unlearning methods aim at reducing the models' likelihood of generating harmful content. Traditionally, this has been tackled at an individual concept level, with only a handful of recent works considering more realistic concept combinations. However, state of the art methods depend on full finetuning, which is computationally expensive. Concept localisation methods can facilitate selective finetuning, but existing techniques are static, resulting in suboptimal utility. In order to tackle these challenges, we propose TRUST (Targeted Robust Selective fine Tuning), a novel approach for dynamically estimating target concept neurons and unlearning them through selective finetuning, empowered by a Hessian based regularization. We show experimentally, against a number of SOTA baselines, that TRUST is robust against adversarial prompts, preserves generation quality to a significant degree, and is also significantly faster than the SOTA. Our method achieves unlearning of not only individual concepts but also combinations of concepts and conditional concepts, without any specific regularization.", "AI": {"tldr": "TRUST\u662f\u4e00\u79cd\u65b0\u9896\u7684\u9488\u5bf9\u6027\u9c81\u68d2\u9009\u62e9\u6027\u5fae\u8c03\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u4f30\u8ba1\u76ee\u6807\u6982\u5ff5\u795e\u7ecf\u5143\u5e76\u5229\u7528Hessian\u6b63\u5219\u5316\u8fdb\u884c\u9009\u62e9\u6027\u5fae\u8c03\uff0c\u5b9e\u73b0\u9ad8\u6548\u7684\u6982\u5ff5\u9057\u5fd8", "motivation": "\u6587\u672c\u5f15\u5bfc\u6269\u6563\u6a21\u578b\u6613\u88ab\u5229\u7528\u751f\u6210\u6709\u5bb3\u5185\u5bb9\uff0c\u4f20\u7edf\u6982\u5ff5\u9057\u5fd8\u65b9\u6cd5\u5728\u5355\u4e2a\u6982\u5ff5\u5c42\u9762\u5904\u7406\uff0c\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u5168\u6a21\u578b\u5fae\u8c03\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u6982\u5ff5\u5b9a\u4f4d\u65b9\u6cd5\u9759\u6001\u4e14\u6548\u679c\u6b20\u4f73", "method": "\u63d0\u51faTRUST\u65b9\u6cd5\uff1a1\uff09\u52a8\u6001\u4f30\u8ba1\u76ee\u6807\u6982\u5ff5\u795e\u7ecf\u5143\uff1b2\uff09\u901a\u8fc7\u9009\u62e9\u6027\u5fae\u8c03\u8fdb\u884c\u6982\u5ff5\u9057\u5fd8\uff1b3\uff09\u91c7\u7528Hessian\u6b63\u5219\u5316\u589e\u5f3a\u9c81\u68d2\u6027", "result": "TRUST\u5728\u5bf9\u6297\u6027\u63d0\u793a\u4e0b\u8868\u73b0\u9c81\u68d2\uff0c\u663e\u8457\u4fdd\u6301\u751f\u6210\u8d28\u91cf\uff0c\u6bd4SOTA\u65b9\u6cd5\u66f4\u5feb\uff0c\u80fd\u9057\u5fd8\u5355\u4e2a\u6982\u5ff5\u3001\u6982\u5ff5\u7ec4\u5408\u548c\u6761\u4ef6\u6982\u5ff5\uff0c\u65e0\u9700\u7279\u5b9a\u6b63\u5219\u5316", "conclusion": "TRUST\u4e3a\u6269\u6563\u6a21\u578b\u7684\u6982\u5ff5\u9057\u5fd8\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u9c81\u68d2\u4e14\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u8ba1\u7b97\u6210\u672c\u548c\u6548\u679c\u9650\u5236\u95ee\u9898"}}
{"id": "2602.08149", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08149", "abs": "https://arxiv.org/abs/2602.08149", "authors": ["Sahana Ramnath", "Nima Chitsazan", "Mingyang Zhou", "Chia-Hsuan Lee", "Shi-Xiong Zhang", "Stephen Rawls", "Sambit Sahu", "Sangwoo Cho", "Xiang Ren", "Genta Indra Winata", "Akshaj Kumar Veldanda"], "title": "DIAL-SUMMER: A Structured Evaluation Framework of Hierarchical Errors in Dialogue Summaries", "comment": null, "summary": "Dialogues are a predominant mode of communication for humans, and it is immensely helpful to have automatically generated summaries of them (e.g., to revise key points discussed in a meeting, to review conversations between customer agents and product users). Prior works on dialogue summary evaluation largely ignore the complexities specific to this task: (i) shift in structure, from multiple speakers discussing information in a scattered fashion across several turns, to a summary's sentences, and (ii) shift in narration viewpoint, from speakers' first/second-person narration, standardized third-person narration in the summary. In this work, we introduce our framework DIALSUMMER to address the above. We propose DIAL-SUMMER's taxonomy of errors to comprehensively evaluate dialogue summaries at two hierarchical levels: DIALOGUE-LEVEL that focuses on the broader speakers/turns, and WITHIN-TURN-LEVEL that focuses on the information talked about inside a turn. We then present DIAL-SUMMER's dataset composed of dialogue summaries manually annotated with our taxonomy's fine-grained errors. We conduct empirical analyses of these annotated errors, and observe interesting trends (e.g., turns occurring in middle of the dialogue are the most frequently missed in the summary, extrinsic hallucinations largely occur at the end of the summary). We also conduct experiments on LLM-Judges' capability at detecting these errors, through which we demonstrate the challenging nature of our dataset, the robustness of our taxonomy, and the need for future work in this field to enhance LLMs' performance in the same. Code and inference dataset coming soon.", "AI": {"tldr": "DIALSUMMER\u6846\u67b6\u9488\u5bf9\u5bf9\u8bdd\u6458\u8981\u8bc4\u4f30\u63d0\u51fa\u5206\u5c42\u9519\u8bef\u5206\u7c7b\u6cd5\uff0c\u89e3\u51b3\u5bf9\u8bdd\u7ed3\u6784\u8f6c\u6362\u548c\u53d9\u8ff0\u89c6\u89d2\u8f6c\u6362\u7684\u590d\u6742\u6027\uff0c\u5e76\u521b\u5efa\u4eba\u5de5\u6807\u6ce8\u6570\u636e\u96c6\u5206\u6790\u9519\u8bef\u6a21\u5f0f\u3002", "motivation": "\u73b0\u6709\u5bf9\u8bdd\u6458\u8981\u8bc4\u4f30\u65b9\u6cd5\u5ffd\u7565\u4e86\u5bf9\u8bdd\u7279\u6709\u7684\u590d\u6742\u6027\uff1a\u4ece\u591a\u8bf4\u8bdd\u8005\u5206\u6563\u8ba8\u8bba\u5230\u6458\u8981\u53e5\u5b50\u7684\u7ed3\u6784\u8f6c\u6362\uff0c\u4ee5\u53ca\u4ece\u7b2c\u4e00/\u7b2c\u4e8c\u4eba\u79f0\u5230\u7b2c\u4e09\u4eba\u79f0\u7684\u53d9\u8ff0\u89c6\u89d2\u8f6c\u6362\u3002", "method": "\u63d0\u51faDIALSUMMER\u6846\u67b6\uff0c\u5305\u542b\u4e24\u5c42\u9519\u8bef\u5206\u7c7b\u6cd5\uff1a\u5bf9\u8bdd\u5c42\u9762\u5173\u6ce8\u8bf4\u8bdd\u8005/\u8f6e\u6b21\uff0c\u8f6e\u6b21\u5185\u5c42\u9762\u5173\u6ce8\u5355\u8f6e\u6b21\u4fe1\u606f\uff1b\u521b\u5efa\u4eba\u5de5\u6807\u6ce8\u6570\u636e\u96c6\uff0c\u5206\u6790\u9519\u8bef\u8d8b\u52bf\u5e76\u6d4b\u8bd5LLM-Judge\u7684\u9519\u8bef\u68c0\u6d4b\u80fd\u529b\u3002", "result": "\u5206\u6790\u53d1\u73b0\u6709\u8da3\u8d8b\u52bf\uff1a\u5bf9\u8bdd\u4e2d\u95f4\u8f6e\u6b21\u6700\u5bb9\u6613\u88ab\u9057\u6f0f\uff0c\u5916\u90e8\u5e7b\u89c9\u591a\u51fa\u73b0\u5728\u6458\u8981\u672b\u5c3e\uff1bLLM-Judge\u5728\u9519\u8bef\u68c0\u6d4b\u4e0a\u8868\u73b0\u6709\u9650\uff0c\u663e\u793a\u6570\u636e\u96c6\u7684\u6311\u6218\u6027\u548c\u672a\u6765\u6539\u8fdb\u9700\u6c42\u3002", "conclusion": "DIALSUMMER\u6846\u67b6\u4e3a\u5bf9\u8bdd\u6458\u8981\u8bc4\u4f30\u63d0\u4f9b\u5168\u9762\u65b9\u6cd5\uff0c\u5176\u6570\u636e\u96c6\u548c\u5206\u7c7b\u6cd5\u5c55\u793a\u4e86\u5bf9\u8bdd\u6458\u8981\u4efb\u52a1\u7684\u590d\u6742\u6027\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u63d0\u5347LLM\u5728\u6b64\u9886\u57df\u7684\u6027\u80fd\u3002"}}
{"id": "2602.07943", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07943", "abs": "https://arxiv.org/abs/2602.07943", "authors": ["Ivaxi Sheth", "Zhijing Jin", "Bryan Wilder", "Dominik Janzing", "Mario Fritz"], "title": "IV Co-Scientist: Multi-Agent LLM Framework for Causal Instrumental Variable Discovery", "comment": "18 pages", "summary": "In the presence of confounding between an endogenous variable and the outcome, instrumental variables (IVs) are used to isolate the causal effect of the endogenous variable. Identifying valid instruments requires interdisciplinary knowledge, creativity, and contextual understanding, making it a non-trivial task. In this paper, we investigate whether large language models (LLMs) can aid in this task. We perform a two-stage evaluation framework. First, we test whether LLMs can recover well-established instruments from the literature, assessing their ability to replicate standard reasoning. Second, we evaluate whether LLMs can identify and avoid instruments that have been empirically or theoretically discredited. Building on these results, we introduce IV Co-Scientist, a multi-agent system that proposes, critiques, and refines IVs for a given treatment-outcome pair. We also introduce a statistical test to contextualize consistency in the absence of ground truth. Our results show the potential of LLMs to discover valid instrumental variables from a large observational database.", "AI": {"tldr": "LLMs\u80fd\u591f\u5e2e\u52a9\u8bc6\u522b\u6709\u6548\u7684\u5de5\u5177\u53d8\u91cf\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u8bc4\u4f30\u6846\u67b6\u9a8c\u8bc1\u5176\u80fd\u529b\uff0c\u5e76\u5f00\u53d1\u4e86IV Co-Scientist\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u6765\u63d0\u51fa\u3001\u6279\u8bc4\u548c\u4f18\u5316\u5de5\u5177\u53d8\u91cf\u9009\u62e9\u3002", "motivation": "\u5728\u5b58\u5728\u5185\u751f\u53d8\u91cf\u4e0e\u7ed3\u679c\u6df7\u6742\u7684\u60c5\u51b5\u4e0b\uff0c\u5de5\u5177\u53d8\u91cf\uff08IVs\uff09\u7528\u4e8e\u9694\u79bb\u5185\u751f\u53d8\u91cf\u7684\u56e0\u679c\u6548\u5e94\u3002\u8bc6\u522b\u6709\u6548\u7684\u5de5\u5177\u53d8\u91cf\u9700\u8981\u8de8\u5b66\u79d1\u77e5\u8bc6\u3001\u521b\u9020\u529b\u548c\u4e0a\u4e0b\u6587\u7406\u89e3\uff0c\u8fd9\u662f\u4e00\u9879\u975e\u5e73\u51e1\u7684\u4efb\u52a1\u3002\u672c\u6587\u7814\u7a76\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u662f\u5426\u80fd\u5e2e\u52a9\u5b8c\u6210\u8fd9\u9879\u4efb\u52a1\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u8bc4\u4f30\u6846\u67b6\uff1a\u9996\u5148\u6d4b\u8bd5LLMs\u662f\u5426\u80fd\u4ece\u6587\u732e\u4e2d\u6062\u590d\u5df2\u786e\u7acb\u7684\u5de5\u5177\u53d8\u91cf\uff0c\u8bc4\u4f30\u5176\u590d\u5236\u6807\u51c6\u63a8\u7406\u7684\u80fd\u529b\uff1b\u5176\u6b21\u8bc4\u4f30LLMs\u662f\u5426\u80fd\u8bc6\u522b\u548c\u907f\u514d\u5df2\u88ab\u7ecf\u9a8c\u6216\u7406\u8bba\u5426\u5b9a\u7684\u5de5\u5177\u53d8\u91cf\u3002\u57fa\u4e8e\u8fd9\u4e9b\u7ed3\u679c\uff0c\u5f00\u53d1\u4e86IV Co-Scientist\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u8be5\u7cfb\u7edf\u80fd\u63d0\u51fa\u3001\u6279\u8bc4\u548c\u4f18\u5316\u7ed9\u5b9a\u5904\u7406-\u7ed3\u679c\u5bf9\u7684\u5de5\u5177\u53d8\u91cf\u3002\u8fd8\u5f15\u5165\u4e86\u4e00\u79cd\u7edf\u8ba1\u68c0\u9a8c\u6765\u5728\u6ca1\u6709\u771f\u5b9e\u503c\u7684\u60c5\u51b5\u4e0b\u8bc4\u4f30\u4e00\u81f4\u6027\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0cLLMs\u5177\u6709\u4ece\u5927\u578b\u89c2\u6d4b\u6570\u636e\u5e93\u4e2d\u8bc6\u522b\u6709\u6548\u5de5\u5177\u53d8\u91cf\u7684\u6f5c\u529b\u3002LLMs\u80fd\u591f\u6210\u529f\u6062\u590d\u6587\u732e\u4e2d\u5df2\u786e\u7acb\u7684\u5de5\u5177\u53d8\u91cf\uff0c\u5e76\u80fd\u907f\u514d\u4f7f\u7528\u5df2\u88ab\u5426\u5b9a\u7684\u5de5\u5177\u53d8\u91cf\u3002", "conclusion": "LLMs\u5728\u5de5\u5177\u53d8\u91cf\u8bc6\u522b\u65b9\u9762\u5c55\u73b0\u51fa\u6f5c\u529b\uff0cIV Co-Scientist\u7cfb\u7edf\u4e3a\u56e0\u679c\u63a8\u65ad\u4e2d\u7684\u5de5\u5177\u53d8\u91cf\u9009\u62e9\u63d0\u4f9b\u4e86\u65b0\u7684\u81ea\u52a8\u5316\u65b9\u6cd5\uff0c\u6709\u52a9\u4e8e\u63d0\u9ad8\u56e0\u679c\u6548\u5e94\u4f30\u8ba1\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2602.08208", "categories": ["cs.CL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2602.08208", "abs": "https://arxiv.org/abs/2602.08208", "authors": ["Cameron R. Jones", "Agnese Lombardi", "Kyle Mahowald", "Benjamin K. Bergen"], "title": "LLMs and people both learn to form conventions -- just not with each other", "comment": "10 pages, 4 figures", "summary": "Humans align to one another in conversation -- adopting shared conventions that ease communication. We test whether LLMs form the same kinds of conventions in a multimodal communication game. Both humans and LLMs display evidence of convention-formation (increasing the accuracy and consistency of their turns while decreasing their length) when communicating in same-type dyads (humans with humans, AI with AI). However, heterogenous human-AI pairs fail -- suggesting differences in communicative tendencies. In Experiment 2, we ask whether LLMs can be induced to behave more like human conversants, by prompting them to produce superficially humanlike behavior. While the length of their messages matches that of human pairs, accuracy and lexical overlap in human-LLM pairs continues to lag behind that of both human-human and AI-AI pairs. These results suggest that conversational alignment requires more than just the ability to mimic previous interactions, but also shared interpretative biases toward the meanings that are conveyed.", "AI": {"tldr": "LLMs\u548c\u4eba\u7c7b\u5728\u540c\u7c7b\u5bf9\u8bdd\u4e2d\u90fd\u80fd\u5f62\u6210\u6c9f\u901a\u60ef\u4f8b\uff0c\u4f46\u5728\u4eba\u673a\u6df7\u5408\u5bf9\u8bdd\u4e2d\u5374\u65e0\u6cd5\u6709\u6548\u5bf9\u9f50\uff0c\u5373\u4f7f\u8ba9LLMs\u6a21\u4eff\u4eba\u7c7b\u8868\u9762\u884c\u4e3a\u4e5f\u65e0\u6cd5\u8fbe\u5230\u4eba\u7c7b\u95f4\u5bf9\u8bdd\u7684\u534f\u8c03\u6c34\u5e73\u3002", "motivation": "\u7814\u7a76LLMs\u662f\u5426\u80fd\u5728\u591a\u6a21\u6001\u6c9f\u901a\u6e38\u620f\u4e2d\u50cf\u4eba\u7c7b\u4e00\u6837\u5f62\u6210\u6c9f\u901a\u60ef\u4f8b\uff0c\u4ee5\u53ca\u4eba\u673a\u6df7\u5408\u5bf9\u8bdd\u4e2d\u7684\u5bf9\u9f50\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u591a\u6a21\u6001\u6c9f\u901a\u6e38\u620f\u5b9e\u9a8c\uff0c\u6bd4\u8f83\u4eba\u7c7b-\u4eba\u7c7b\u3001AI-AI\u3001\u4eba\u7c7b-AI\u4e09\u79cd\u5bf9\u8bdd\u7ec4\u5408\u7684\u8868\u73b0\uff0c\u6d4b\u91cf\u51c6\u786e\u6027\u3001\u4e00\u81f4\u6027\u548c\u6d88\u606f\u957f\u5ea6\u7b49\u6307\u6807\u3002", "result": "\u540c\u7c7b\u5bf9\u8bdd\uff08\u4eba\u7c7b-\u4eba\u7c7b\u3001AI-AI\uff09\u90fd\u663e\u793a\u51fa\u60ef\u4f8b\u5f62\u6210\u8ff9\u8c61\uff08\u51c6\u786e\u6027\u63d0\u9ad8\u3001\u4e00\u81f4\u6027\u589e\u5f3a\u3001\u6d88\u606f\u7f29\u77ed\uff09\uff0c\u4f46\u4eba\u673a\u6df7\u5408\u5bf9\u8bdd\u5931\u8d25\u3002\u5373\u4f7f\u63d0\u793aLLMs\u6a21\u4eff\u4eba\u7c7b\u8868\u9762\u884c\u4e3a\uff0c\u4eba\u673a\u5bf9\u8bdd\u7684\u51c6\u786e\u6027\u548c\u8bcd\u6c47\u91cd\u53e0\u4ecd\u843d\u540e\u4e8e\u540c\u7c7b\u5bf9\u8bdd\u3002", "conclusion": "\u5bf9\u8bdd\u5bf9\u9f50\u4e0d\u4ec5\u9700\u8981\u6a21\u4eff\u5148\u524d\u4e92\u52a8\u7684\u80fd\u529b\uff0c\u8fd8\u9700\u8981\u5bf9\u4f20\u8fbe\u610f\u4e49\u6709\u5171\u4eab\u7684\u89e3\u91ca\u6027\u504f\u89c1\uff0cLLMs\u4e0e\u4eba\u7c7b\u5728\u8fd9\u65b9\u9762\u7684\u5dee\u5f02\u963b\u788d\u4e86\u6709\u6548\u6c9f\u901a\u60ef\u4f8b\u7684\u5f62\u6210\u3002"}}
{"id": "2602.07428", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.07428", "abs": "https://arxiv.org/abs/2602.07428", "authors": ["Chengqi Dong", "Zhiyuan Cao", "Tuoshi Qi", "Kexin Wu", "Yixing Gao", "Fan Tang"], "title": "Row-Column Separated Attention Based Low-Light Image/Video Enhancement", "comment": null, "summary": "U-Net structure is widely used for low-light image/video enhancement. The enhanced images result in areas with large local noise and loss of more details without proper guidance for global information. Attention mechanisms can better focus on and use global information. However, attention to images could significantly increase the number of parameters and computations. We propose a Row-Column Separated Attention module (RCSA) inserted after an improved U-Net. The RCSA module's input is the mean and maximum of the row and column of the feature map, which utilizes global information to guide local information with fewer parameters. We propose two temporal loss functions to apply the method to low-light video enhancement and maintain temporal consistency. Extensive experiments on the LOL, MIT Adobe FiveK image, and SDSD video datasets demonstrate the effectiveness of our approach. The code is publicly available at https://github.com/cq-dong/URCSA.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6539\u8fdbU-Net\u548c\u884c\u5217\u5206\u79bb\u6ce8\u610f\u529b\u6a21\u5757\u7684\u4f4e\u5149\u56fe\u50cf/\u89c6\u9891\u589e\u5f3a\u65b9\u6cd5\uff0c\u901a\u8fc7\u5168\u5c40\u4fe1\u606f\u6307\u5bfc\u5c40\u90e8\u4fe1\u606f\uff0c\u51cf\u5c11\u53c2\u6570\u548c\u8ba1\u7b97\u91cf\uff0c\u5e76\u5f15\u5165\u65f6\u95f4\u635f\u5931\u51fd\u6570\u4fdd\u6301\u89c6\u9891\u65f6\u5e8f\u4e00\u81f4\u6027\u3002", "motivation": "\u4f20\u7edfU-Net\u7ed3\u6784\u5728\u4f4e\u5149\u56fe\u50cf\u589e\u5f3a\u4e2d\u7f3a\u4e4f\u5168\u5c40\u4fe1\u606f\u6307\u5bfc\uff0c\u5bfc\u81f4\u5c40\u90e8\u566a\u58f0\u5927\u548c\u7ec6\u8282\u4e22\u5931\uff1b\u6ce8\u610f\u529b\u673a\u5236\u80fd\u66f4\u597d\u5229\u7528\u5168\u5c40\u4fe1\u606f\u4f46\u53c2\u6570\u548c\u8ba1\u7b97\u91cf\u5927\u3002", "method": "1. \u6539\u8fdbU-Net\u7ed3\u6784\uff1b2. \u63d0\u51fa\u884c\u5217\u5206\u79bb\u6ce8\u610f\u529b\u6a21\u5757(RCSA)\uff0c\u8f93\u5165\u7279\u5f81\u56fe\u884c\u5217\u7684\u5747\u503c\u548c\u6700\u5927\u503c\uff0c\u4ee5\u8f83\u5c11\u53c2\u6570\u5229\u7528\u5168\u5c40\u4fe1\u606f\u6307\u5bfc\u5c40\u90e8\u4fe1\u606f\uff1b3. \u63d0\u51fa\u4e24\u79cd\u65f6\u95f4\u635f\u5931\u51fd\u6570\u7528\u4e8e\u4f4e\u5149\u89c6\u9891\u589e\u5f3a\uff0c\u4fdd\u6301\u65f6\u5e8f\u4e00\u81f4\u6027\u3002", "result": "\u5728LOL\u3001MIT Adobe FiveK\u56fe\u50cf\u6570\u636e\u96c6\u548cSDSD\u89c6\u9891\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684URCSA\u65b9\u6cd5\u901a\u8fc7\u884c\u5217\u5206\u79bb\u6ce8\u610f\u529b\u6a21\u5757\u6709\u6548\u5229\u7528\u5168\u5c40\u4fe1\u606f\u6307\u5bfc\u5c40\u90e8\u589e\u5f3a\uff0c\u5728\u51cf\u5c11\u53c2\u6570\u7684\u540c\u65f6\u63d0\u5347\u4e86\u4f4e\u5149\u56fe\u50cf/\u89c6\u9891\u589e\u5f3a\u8d28\u91cf\uff0c\u5e76\u4fdd\u6301\u4e86\u89c6\u9891\u65f6\u5e8f\u4e00\u81f4\u6027\u3002"}}
{"id": "2602.08220", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.08220", "abs": "https://arxiv.org/abs/2602.08220", "authors": ["Boyi Zeng", "Yiqin Hao", "He Li", "Shixiang Song", "Feichen Song", "Zitong Wang", "Siyuan Huang", "Yi Xu", "ZiWei He", "Xinbing Wang", "Zhouhan Lin"], "title": "Pretraining with Token-Level Adaptive Latent Chain-of-Thought", "comment": null, "summary": "Scaling large language models by increasing parameters and training data is increasingly constrained by limited high-quality corpora and rising communication costs. This work explores an alternative axis: increasing per-token computation without expanding parameters, by internalizing latent Chain-of-Thought (CoT) into pretraining. We propose Pretraining with Token-Level Adaptive Latent CoT (adaptive latent CoT), where the model generates a variable-length latent CoT trajectory before emitting each token -- allocating longer trajectories to difficult tokens and shorter (or even zero) trajectories to easy ones. Importantly, this behavior emerges naturally from one-stage pretraining on general text and reduces computation in both training and inference via token-wise adaptive halting. Experiments with Llama architectures show that adaptive latent CoT consistently improves language modeling perplexity and broad downstream accuracy, even with fewer training FLOPs than prior recurrent baselines.", "AI": {"tldr": "\u63d0\u51fa\u81ea\u9002\u5e94\u6f5c\u5728\u601d\u7ef4\u94fe\u9884\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e3a\u6bcf\u4e2atoken\u751f\u6210\u53ef\u53d8\u957f\u5ea6\u7684\u6f5c\u5728\u63a8\u7406\u8f68\u8ff9\uff0c\u5728\u4e0d\u589e\u52a0\u53c2\u6570\u7684\u60c5\u51b5\u4e0b\u63d0\u5347\u8ba1\u7b97\u6548\u7387", "motivation": "\u4f20\u7edf\u901a\u8fc7\u589e\u52a0\u53c2\u6570\u548c\u8bad\u7ec3\u6570\u636e\u6765\u6269\u5c55\u5927\u8bed\u8a00\u6a21\u578b\u7684\u65b9\u6cd5\u53d7\u5230\u9ad8\u8d28\u91cf\u8bed\u6599\u5e93\u6709\u9650\u548c\u901a\u4fe1\u6210\u672c\u4e0a\u5347\u7684\u9650\u5236\uff0c\u9700\u8981\u63a2\u7d22\u65b0\u7684\u6269\u5c55\u7ef4\u5ea6", "method": "\u63d0\u51fa\u81ea\u9002\u5e94\u6f5c\u5728\u601d\u7ef4\u94fe\u9884\u8bad\u7ec3\uff0c\u6a21\u578b\u5728\u751f\u6210\u6bcf\u4e2atoken\u524d\u5148\u751f\u6210\u53ef\u53d8\u957f\u5ea6\u7684\u6f5c\u5728\u63a8\u7406\u8f68\u8ff9\uff0c\u4e3a\u56f0\u96betoken\u5206\u914d\u66f4\u957f\u8f68\u8ff9\uff0c\u4e3a\u7b80\u5355token\u5206\u914d\u8f83\u77ed\u751a\u81f3\u96f6\u8f68\u8ff9\uff0c\u901a\u8fc7\u5355\u9636\u6bb5\u9884\u8bad\u7ec3\u81ea\u7136\u5b9e\u73b0", "result": "\u5728Llama\u67b6\u6784\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u81ea\u9002\u5e94\u6f5c\u5728\u601d\u7ef4\u94fe\u80fd\u6301\u7eed\u6539\u5584\u8bed\u8a00\u5efa\u6a21\u56f0\u60d1\u5ea6\u548c\u5e7f\u6cdb\u4e0b\u6e38\u4efb\u52a1\u51c6\u786e\u7387\uff0c\u5373\u4f7f\u8bad\u7ec3FLOPs\u5c11\u4e8e\u5148\u524d\u5faa\u73af\u57fa\u7ebf", "conclusion": "\u901a\u8fc7\u5185\u90e8\u5316\u6f5c\u5728\u601d\u7ef4\u94fe\u5230\u9884\u8bad\u7ec3\u4e2d\uff0c\u5728\u4e0d\u589e\u52a0\u53c2\u6570\u7684\u60c5\u51b5\u4e0b\u589e\u52a0\u6bcf\u4e2atoken\u7684\u8ba1\u7b97\u91cf\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u4e14\u8ba1\u7b97\u9ad8\u6548\u7684\u6a21\u578b\u6269\u5c55\u65b0\u7ef4\u5ea6"}}
{"id": "2602.07444", "categories": ["cs.CV", "eess.SP"], "pdf": "https://arxiv.org/pdf/2602.07444", "abs": "https://arxiv.org/abs/2602.07444", "authors": ["Ondrej Hlinka", "Georg Kaniak", "Christian Kapeller"], "title": "Perspective-aware fusion of incomplete depth maps and surface normals for accurate 3D reconstruction", "comment": "submitted to IET Electronics Letters", "summary": "We address the problem of reconstructing 3D surfaces from depth and surface normal maps acquired by a sensor system based on a single perspective camera. Depth and normal maps can be obtained through techniques such as structured-light scanning and photometric stereo, respectively. We propose a perspective-aware log-depth fusion approach that extends existing orthographic gradient-based depth-normals fusion methods by explicitly accounting for perspective projection, leading to metrically accurate 3D reconstructions. Additionally, the method handles missing depth measurements by leveraging available surface normal information to inpaint gaps. Experiments on the DiLiGenT-MV data set demonstrate the effectiveness of our approach and highlight the importance of perspective-aware depth-normals fusion.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u900f\u89c6\u611f\u77e5\u7684\u5bf9\u6570\u6df1\u5ea6\u878d\u5408\u65b9\u6cd5\uff0c\u4ece\u5355\u89c6\u89d2\u76f8\u673a\u83b7\u53d6\u7684\u6df1\u5ea6\u548c\u6cd5\u7ebf\u56fe\u91cd\u5efa3D\u8868\u9762\uff0c\u5904\u7406\u7f3a\u5931\u6df1\u5ea6\u6d4b\u91cf\u5e76\u5b9e\u73b0\u5ea6\u91cf\u7cbe\u786e\u7684\u91cd\u5efa\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u68af\u5ea6\u7684\u6df1\u5ea6-\u6cd5\u7ebf\u878d\u5408\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u6b63\u4ea4\u6295\u5f71\uff0c\u800c\u5b9e\u9645\u4f20\u611f\u5668\u7cfb\u7edf\u901a\u5e38\u4f7f\u7528\u900f\u89c6\u6295\u5f71\uff0c\u8fd9\u5bfc\u81f4\u91cd\u5efa\u7ed3\u679c\u5728\u5ea6\u91cf\u4e0a\u4e0d\u51c6\u786e\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u663e\u5f0f\u5904\u7406\u900f\u89c6\u6295\u5f71\u7684\u878d\u5408\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u900f\u89c6\u611f\u77e5\u7684\u5bf9\u6570\u6df1\u5ea6\u878d\u5408\u65b9\u6cd5\uff0c\u5c06\u73b0\u6709\u7684\u6b63\u4ea4\u68af\u5ea6\u57fa\u6df1\u5ea6-\u6cd5\u7ebf\u878d\u5408\u65b9\u6cd5\u6269\u5c55\u5230\u900f\u89c6\u6295\u5f71\u573a\u666f\u3002\u8be5\u65b9\u6cd5\u5229\u7528\u5bf9\u6570\u6df1\u5ea6\u8868\u793a\uff0c\u663e\u5f0f\u8003\u8651\u900f\u89c6\u6295\u5f71\u6548\u5e94\uff0c\u5e76\u5229\u7528\u53ef\u7528\u7684\u8868\u9762\u6cd5\u7ebf\u4fe1\u606f\u6765\u586b\u8865\u6df1\u5ea6\u6d4b\u91cf\u4e2d\u7684\u7f3a\u5931\u533a\u57df\u3002", "result": "\u5728DiLiGenT-MV\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5e76\u7a81\u51fa\u4e86\u900f\u89c6\u611f\u77e5\u6df1\u5ea6-\u6cd5\u7ebf\u878d\u5408\u7684\u91cd\u8981\u6027\u3002\u8be5\u65b9\u6cd5\u80fd\u591f\u5b9e\u73b0\u5ea6\u91cf\u7cbe\u786e\u76843D\u91cd\u5efa\uff0c\u5e76\u6709\u6548\u5904\u7406\u6df1\u5ea6\u6d4b\u91cf\u4e2d\u7684\u7f3a\u5931\u533a\u57df\u3002", "conclusion": "\u63d0\u51fa\u7684\u900f\u89c6\u611f\u77e5\u5bf9\u6570\u6df1\u5ea6\u878d\u5408\u65b9\u6cd5\u80fd\u591f\u4ece\u5355\u89c6\u89d2\u76f8\u673a\u83b7\u53d6\u7684\u6df1\u5ea6\u548c\u6cd5\u7ebf\u56fe\u5b9e\u73b0\u5ea6\u91cf\u7cbe\u786e\u76843D\u8868\u9762\u91cd\u5efa\uff0c\u663e\u5f0f\u5904\u7406\u900f\u89c6\u6295\u5f71\u6548\u5e94\u5e76\u586b\u8865\u6df1\u5ea6\u6d4b\u91cf\u7f3a\u5931\uff0c\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2602.07983", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.07983", "abs": "https://arxiv.org/abs/2602.07983", "authors": ["Jishu Sen Gupta", "Harini SI", "Somesh Kumar Singh", "Syed Mohamad Tawseeq", "Yaman Kumar Singla", "David Doermann", "Rajiv Ratn Shah", "Balaji Krishnamurthy"], "title": "Accelerating Social Science Research via Agentic Hypothesization and Experimentation", "comment": null, "summary": "Data-driven social science research is inherently slow, relying on iterative cycles of observation, hypothesis generation, and experimental validation. While recent data-driven methods promise to accelerate parts of this process, they largely fail to support end-to-end scientific discovery. To address this gap, we introduce EXPERIGEN, an agentic framework that operationalizes end-to-end discovery through a Bayesian optimization inspired two-phase search, in which a Generator proposes candidate hypotheses and an Experimenter evaluates them empirically. Across multiple domains, EXPERIGEN consistently discovers 2-4x more statistically significant hypotheses that are 7-17 percent more predictive than prior approaches, and naturally extends to complex data regimes including multimodal and relational datasets. Beyond statistical performance, hypotheses must be novel, empirically grounded, and actionable to drive real scientific progress. To evaluate these qualities, we conduct an expert review of machine-generated hypotheses, collecting feedback from senior faculty. Among 25 reviewed hypotheses, 88 percent were rated moderately or strongly novel, 70 percent were deemed impactful and worth pursuing, and most demonstrated rigor comparable to senior graduate-level research. Finally, recognizing that ultimate validation requires real-world evidence, we conduct the first A/B test of LLM-generated hypotheses, observing statistically significant results with p less than 1e-6 and a large effect size of 344 percent.", "AI": {"tldr": "EXPERIGEN\u662f\u4e00\u4e2a\u7aef\u5230\u7aef\u7684\u79d1\u5b66\u53d1\u73b0\u6846\u67b6\uff0c\u901a\u8fc7\u751f\u6210\u5668\u63d0\u51fa\u5047\u8bbe\u3001\u5b9e\u9a8c\u8005\u8bc4\u4f30\u5047\u8bbe\u7684\u4e24\u9636\u6bb5\u641c\u7d22\uff0c\u5728\u591a\u4e2a\u9886\u57df\u53d1\u73b02-4\u500d\u66f4\u591a\u7edf\u8ba1\u663e\u8457\u4e14\u9884\u6d4b\u80fd\u529b\u63d0\u53477-17%\u7684\u5047\u8bbe\uff0c\u5e76\u901a\u8fc7\u4e13\u5bb6\u8bc4\u5ba1\u548c\u771f\u5b9eA/B\u6d4b\u8bd5\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "motivation": "\u6570\u636e\u9a71\u52a8\u7684\u793e\u4f1a\u79d1\u5b66\u7814\u7a76\u8fc7\u7a0b\u7f13\u6162\uff0c\u4f9d\u8d56\u89c2\u5bdf\u3001\u5047\u8bbe\u751f\u6210\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u7684\u8fed\u4ee3\u5faa\u73af\u3002\u73b0\u6709\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u867d\u7136\u80fd\u52a0\u901f\u90e8\u5206\u8fc7\u7a0b\uff0c\u4f46\u672a\u80fd\u652f\u6301\u7aef\u5230\u7aef\u7684\u79d1\u5b66\u53d1\u73b0\u3002\u9700\u8981\u5f00\u53d1\u80fd\u591f\u5b8c\u6574\u652f\u6301\u79d1\u5b66\u53d1\u73b0\u8fc7\u7a0b\u7684\u6846\u67b6\u3002", "method": "\u63d0\u51faEXPERIGEN\u6846\u67b6\uff0c\u91c7\u7528\u53d7\u8d1d\u53f6\u65af\u4f18\u5316\u542f\u53d1\u7684\u4e24\u9636\u6bb5\u641c\u7d22\uff1a\u751f\u6210\u5668\u63d0\u51fa\u5019\u9009\u5047\u8bbe\uff0c\u5b9e\u9a8c\u8005\u8fdb\u884c\u5b9e\u8bc1\u8bc4\u4f30\u3002\u8be5\u6846\u67b6\u53ef\u6269\u5c55\u5230\u591a\u6a21\u6001\u548c\u5173\u7cfb\u6570\u636e\u96c6\u7b49\u590d\u6742\u6570\u636e\u73af\u5883\u3002", "result": "\u5728\u591a\u4e2a\u9886\u57df\uff0cEXPERIGEN\u53d1\u73b02-4\u500d\u66f4\u591a\u7edf\u8ba1\u663e\u8457\u7684\u5047\u8bbe\uff0c\u9884\u6d4b\u80fd\u529b\u6bd4\u5148\u524d\u65b9\u6cd5\u63d0\u9ad87-17%\u3002\u4e13\u5bb6\u8bc4\u5ba1\u663e\u793a88%\u7684\u5047\u8bbe\u5177\u6709\u4e2d\u7b49\u6216\u5f3a\u65b0\u9896\u6027\uff0c70%\u88ab\u8ba4\u4e3a\u6709\u5f71\u54cd\u529b\u4e14\u503c\u5f97\u7814\u7a76\u3002A/B\u6d4b\u8bd5\u663e\u793a\u7edf\u8ba1\u663e\u8457\u7ed3\u679c(p<1e-6)\uff0c\u6548\u5e94\u5927\u5c0f\u8fbe344%\u3002", "conclusion": "EXPERIGEN\u80fd\u591f\u6709\u6548\u652f\u6301\u7aef\u5230\u7aef\u7684\u79d1\u5b66\u53d1\u73b0\uff0c\u4e0d\u4ec5\u63d0\u5347\u7edf\u8ba1\u6027\u80fd\uff0c\u8fd8\u80fd\u751f\u6210\u65b0\u9896\u3001\u6709\u5b9e\u8bc1\u57fa\u7840\u4e14\u53ef\u64cd\u4f5c\u7684\u5047\u8bbe\uff0c\u4e3a\u6570\u636e\u9a71\u52a8\u7684\u793e\u4f1a\u79d1\u5b66\u7814\u7a76\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u81ea\u52a8\u5316\u5de5\u5177\u3002"}}
{"id": "2602.08221", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08221", "abs": "https://arxiv.org/abs/2602.08221", "authors": ["Xuhua Ma", "Richong Zhang", "Zhijie Nie"], "title": "CoRect: Context-Aware Logit Contrast for Hidden State Rectification to Resolve Knowledge Conflicts", "comment": null, "summary": "Retrieval-Augmented Generation (RAG) often struggles with knowledge conflicts, where model-internal parametric knowledge overrides retrieved evidence, leading to unfaithful outputs. Existing approaches are often limited, relying either on superficial decoding adjustments or weight editing that necessitates ground-truth targets. Through layer-wise analysis, we attribute this failure to a parametric suppression phenomenon: specifically, in deep layers, certain FFN layers overwrite context-sensitive representations with memorized priors. To address this, we propose CoRect (Context-Aware Logit Contrast for Hidden State Rectification). By contrasting logits from contextualized and non-contextualized forward passes, CoRect identifies layers that exhibit high parametric bias without requiring ground-truth labels. It then rectifies the hidden states to preserve evidence-grounded information. Across question answering (QA) and summarization benchmarks, CoRect consistently improves faithfulness and reduces hallucinations compared to strong baselines.", "AI": {"tldr": "CoRect\u901a\u8fc7\u5bf9\u6bd4\u4e0a\u4e0b\u6587\u5316\u548c\u975e\u4e0a\u4e0b\u6587\u5316\u7684\u524d\u5411\u4f20\u64adlogits\u6765\u8bc6\u522b\u5b58\u5728\u9ad8\u53c2\u6570\u504f\u7f6e\u7684\u5c42\uff0c\u7136\u540e\u4fee\u6b63\u9690\u85cf\u72b6\u6001\u4ee5\u4fdd\u7559\u57fa\u4e8e\u8bc1\u636e\u7684\u4fe1\u606f\uff0c\u4ece\u800c\u89e3\u51b3RAG\u4e2d\u7684\u77e5\u8bc6\u51b2\u7a81\u95ee\u9898\u3002", "motivation": "\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u5728\u5904\u7406\u77e5\u8bc6\u51b2\u7a81\u65f6\u5b58\u5728\u56f0\u96be\uff0c\u6a21\u578b\u5185\u90e8\u7684\u53c2\u6570\u5316\u77e5\u8bc6\u4f1a\u8986\u76d6\u68c0\u7d22\u5230\u7684\u8bc1\u636e\uff0c\u5bfc\u81f4\u8f93\u51fa\u4e0d\u5fe0\u5b9e\u3002\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u4f9d\u8d56\u80a4\u6d45\u7684\u89e3\u7801\u8c03\u6574\uff0c\u8981\u4e48\u9700\u8981\u771f\u5b9e\u76ee\u6807\u8fdb\u884c\u6743\u91cd\u7f16\u8f91\uff0c\u5b58\u5728\u5c40\u9650\u6027\u3002", "method": "\u901a\u8fc7\u5c42\u95f4\u5206\u6790\u53d1\u73b0\u53c2\u6570\u6291\u5236\u73b0\u8c61\uff1a\u5728\u6df1\u5c42\u4e2d\uff0c\u67d0\u4e9bFFN\u5c42\u4f1a\u7528\u8bb0\u5fc6\u7684\u5148\u9a8c\u8986\u76d6\u4e0a\u4e0b\u6587\u654f\u611f\u8868\u793a\u3002\u63d0\u51faCoRect\u65b9\u6cd5\uff0c\u901a\u8fc7\u5bf9\u6bd4\u4e0a\u4e0b\u6587\u5316\u548c\u975e\u4e0a\u4e0b\u6587\u5316\u524d\u5411\u4f20\u64ad\u7684logits\u6765\u8bc6\u522b\u9ad8\u53c2\u6570\u504f\u7f6e\u7684\u5c42\uff0c\u7136\u540e\u4fee\u6b63\u9690\u85cf\u72b6\u6001\u4ee5\u4fdd\u7559\u57fa\u4e8e\u8bc1\u636e\u7684\u4fe1\u606f\u3002", "result": "\u5728\u95ee\u7b54\u548c\u6458\u8981\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCoRect\u76f8\u6bd4\u5f3a\u57fa\u7ebf\u65b9\u6cd5\uff0c\u6301\u7eed\u63d0\u9ad8\u4e86\u5fe0\u5b9e\u5ea6\u5e76\u51cf\u5c11\u4e86\u5e7b\u89c9\u3002", "conclusion": "CoRect\u901a\u8fc7\u8bc6\u522b\u548c\u4fee\u6b63\u5b58\u5728\u53c2\u6570\u504f\u7f6e\u7684\u5c42\uff0c\u6709\u6548\u89e3\u51b3\u4e86RAG\u4e2d\u7684\u77e5\u8bc6\u51b2\u7a81\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u751f\u6210\u5185\u5bb9\u7684\u5fe0\u5b9e\u5ea6\uff0c\u4e14\u4e0d\u9700\u8981\u771f\u5b9e\u6807\u7b7e\u3002"}}
{"id": "2602.07446", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.07446", "abs": "https://arxiv.org/abs/2602.07446", "authors": ["Naqcho Ali Mehdi"], "title": "PTB-XL-Image-17K: A Large-Scale Synthetic ECG Image Dataset with Comprehensive Ground Truth for Deep Learning-Based Digitization", "comment": "8 pages, 4 figures, dataset paper", "summary": "Electrocardiogram (ECG) digitization-converting paper-based or scanned ECG images back into time-series signals-is critical for leveraging decades of legacy clinical data in modern deep learning applications. However, progress has been hindered by the lack of large-scale datasets providing both ECG images and their corresponding ground truth signals with comprehensive annotations. We introduce PTB-XL-Image-17K, a complete synthetic ECG image dataset comprising 17,271 high-quality 12-lead ECG images generated from the PTB-XL signal database. Our dataset uniquely provides five complementary data types per sample: (1) realistic ECG images with authentic grid patterns and annotations (50% with visible grid, 50% without), (2) pixel-level segmentation masks, (3) ground truth time-series signals, (4) bounding box annotations in YOLO format for both lead regions and lead name labels, and (5) comprehensive metadata including visual parameters and patient information. We present an open-source Python framework enabling customizable dataset generation with controllable parameters including paper speed (25/50 mm/s), voltage scale (5/10 mm/mV), sampling rate (500 Hz), grid appearance (4 colors), and waveform characteristics. The dataset achieves 100% generation success rate with an average processing time of 1.35 seconds per sample. PTB-XL-Image-17K addresses critical gaps in ECG digitization research by providing the first large-scale resource supporting the complete pipeline: lead detection, waveform segmentation, and signal extraction with full ground truth for rigorous evaluation. The dataset, generation framework, and documentation are publicly available at https://github.com/naqchoalimehdi/PTB-XL-Image-17K and https://doi.org/10.5281/zenodo.18197519.", "AI": {"tldr": "PTB-XL-Image-17K\u662f\u4e00\u4e2a\u5305\u542b17,271\u4e2a\u9ad8\u8d28\u91cf12\u5bfc\u8054\u5fc3\u7535\u56fe\u56fe\u50cf\u7684\u5408\u6210\u6570\u636e\u96c6\uff0c\u4e3a\u5fc3\u7535\u56fe\u6570\u5b57\u5316\u7814\u7a76\u63d0\u4f9b\u9996\u4e2a\u5927\u89c4\u6a21\u8d44\u6e90\uff0c\u652f\u6301\u5b8c\u6574\u7684\u68c0\u6d4b\u3001\u5206\u5272\u548c\u4fe1\u53f7\u63d0\u53d6\u6d41\u7a0b\u3002", "motivation": "\u5fc3\u7535\u56fe\u6570\u5b57\u5316\u5bf9\u4e8e\u5229\u7528\u6570\u5341\u5e74\u9057\u7559\u4e34\u5e8a\u6570\u636e\u5728\u73b0\u4ee3\u6df1\u5ea6\u5b66\u4e60\u5e94\u7528\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7f3a\u4e4f\u5927\u89c4\u6a21\u540c\u65f6\u5305\u542b\u5fc3\u7535\u56fe\u56fe\u50cf\u548c\u5bf9\u5e94\u771f\u5b9e\u4fe1\u53f7\u6807\u6ce8\u7684\u6570\u636e\u96c6\u963b\u788d\u4e86\u7814\u7a76\u8fdb\u5c55\u3002", "method": "\u57fa\u4e8ePTB-XL\u4fe1\u53f7\u6570\u636e\u5e93\uff0c\u5f00\u53d1\u5f00\u6e90Python\u6846\u67b6\u751f\u6210\u5408\u6210\u5fc3\u7535\u56fe\u56fe\u50cf\uff0c\u63d0\u4f9b\u4e94\u79cd\u4e92\u8865\u6570\u636e\u7c7b\u578b\uff1a\u771f\u5b9e\u5fc3\u7535\u56fe\u56fe\u50cf\u3001\u50cf\u7d20\u7ea7\u5206\u5272\u63a9\u7801\u3001\u65f6\u95f4\u5e8f\u5217\u4fe1\u53f7\u3001YOLO\u683c\u5f0f\u8fb9\u754c\u6846\u6807\u6ce8\u4ee5\u53ca\u5305\u542b\u89c6\u89c9\u53c2\u6570\u548c\u60a3\u8005\u4fe1\u606f\u7684\u5143\u6570\u636e\u3002", "result": "\u6570\u636e\u96c6\u5305\u542b17,271\u4e2a\u6837\u672c\uff0c\u751f\u6210\u6210\u529f\u7387100%\uff0c\u5e73\u5747\u5904\u7406\u65f6\u95f4\u6bcf\u6837\u672c1.35\u79d2\uff0c\u63d0\u4f9b\u53ef\u81ea\u5b9a\u4e49\u53c2\u6570\u5305\u62ec\u8d70\u7eb8\u901f\u5ea6\u3001\u7535\u538b\u6807\u5ea6\u3001\u91c7\u6837\u7387\u3001\u7f51\u683c\u5916\u89c2\u548c\u6ce2\u5f62\u7279\u5f81\u3002", "conclusion": "PTB-XL-Image-17K\u586b\u8865\u4e86\u5fc3\u7535\u56fe\u6570\u5b57\u5316\u7814\u7a76\u7684\u5173\u952e\u7a7a\u767d\uff0c\u4e3a\u9996\u4e2a\u652f\u6301\u5b8c\u6574\u6d41\u7a0b\u7684\u5927\u89c4\u6a21\u8d44\u6e90\uff0c\u5305\u62ec\u5bfc\u8054\u68c0\u6d4b\u3001\u6ce2\u5f62\u5206\u5272\u548c\u4fe1\u53f7\u63d0\u53d6\uff0c\u4e3a\u4e25\u683c\u8bc4\u4f30\u63d0\u4f9b\u5b8c\u6574\u771f\u5b9e\u6570\u636e\u3002"}}
{"id": "2602.08009", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.08009", "abs": "https://arxiv.org/abs/2602.08009", "authors": ["Rui Li", "Zeyu Zhang", "Xiaohe Bo", "Quanyu Dai", "Chaozhuo Li", "Feng Wen", "Xu Chen"], "title": "Towards Adaptive, Scalable, and Robust Coordination of LLM Agents: A Dynamic Ad-Hoc Networking Perspective", "comment": null, "summary": "Multi-agent architectures built on large language models (LLMs) have demonstrated the potential to realize swarm intelligence through well-crafted collaboration. However, the substantial burden of manual orchestration inherently raises an imperative to automate the design of agentic workflows. We frame such an agent coordination challenge as a classic problem in dynamic ad-hoc networking: How to establish adaptive and reliable communication among a scalable number of agentic hosts? In response to this unresolved dilemma, we introduce RAPS, a reputation-aware publish-subscribe paradigm for adaptive, scalable, and robust coordination of LLM agents. RAPS is grounded in the Distributed Publish-Subscribe Protocol, allowing LLM agents to exchange messages based on their declared intents rather than predefined topologies. Beyond this substrate, RAPS further incorporates two coherent overlays: (i) Reactive Subscription, enabling agents to dynamically refine their intents; and (ii) Bayesian Reputation, empowering each agent with a local watchdog to detect and isolate malicious peers. Extensive experiments over five benchmarks showcase that our design effectively reconciles adaptivity, scalability, and robustness in a unified multi-agent coordination framework.", "AI": {"tldr": "RAPS\uff1a\u57fa\u4e8e\u58f0\u8a89\u611f\u77e5\u7684\u53d1\u5e03-\u8ba2\u9605\u8303\u5f0f\uff0c\u7528\u4e8e\u5b9e\u73b0LLM\u591a\u667a\u80fd\u4f53\u7684\u81ea\u9002\u5e94\u3001\u53ef\u6269\u5c55\u548c\u9c81\u68d2\u534f\u8c03", "motivation": "\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u591a\u667a\u80fd\u4f53\u67b6\u6784\u5c55\u73b0\u4e86\u7fa4\u4f53\u667a\u80fd\u6f5c\u529b\uff0c\u4f46\u624b\u52a8\u7f16\u6392\u5de5\u4f5c\u91cf\u5927\uff0c\u9700\u8981\u81ea\u52a8\u5316\u8bbe\u8ba1\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u3002\u6838\u5fc3\u6311\u6218\u662f\u5982\u4f55\u5728\u53ef\u6269\u5c55\u7684\u667a\u80fd\u4f53\u4e4b\u95f4\u5efa\u7acb\u81ea\u9002\u5e94\u3001\u53ef\u9760\u7684\u901a\u4fe1\u3002", "method": "\u63d0\u51faRAPS\u6846\u67b6\uff0c\u57fa\u4e8e\u5206\u5e03\u5f0f\u53d1\u5e03-\u8ba2\u9605\u534f\u8bae\uff0c\u8ba9\u667a\u80fd\u4f53\u57fa\u4e8e\u58f0\u660e\u7684\u610f\u56fe\u4ea4\u6362\u6d88\u606f\u800c\u975e\u9884\u5b9a\u4e49\u62d3\u6251\u3002\u5305\u542b\u4e24\u4e2a\u53e0\u52a0\u5c42\uff1a1\uff09\u53cd\u5e94\u5f0f\u8ba2\u9605\uff0c\u8ba9\u667a\u80fd\u4f53\u52a8\u6001\u4f18\u5316\u610f\u56fe\uff1b2\uff09\u8d1d\u53f6\u65af\u58f0\u8a89\u673a\u5236\uff0c\u4e3a\u6bcf\u4e2a\u667a\u80fd\u4f53\u63d0\u4f9b\u672c\u5730\u76d1\u63a7\u5668\u6765\u68c0\u6d4b\u548c\u9694\u79bb\u6076\u610f\u8282\u70b9\u3002", "result": "\u5728\u4e94\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u8bbe\u8ba1\u5728\u7edf\u4e00\u7684\u591a\u667a\u80fd\u4f53\u534f\u8c03\u6846\u67b6\u4e2d\u6709\u6548\u5e73\u8861\u4e86\u81ea\u9002\u5e94\u6027\u3001\u53ef\u6269\u5c55\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "RAPS\u901a\u8fc7\u58f0\u8a89\u611f\u77e5\u7684\u53d1\u5e03-\u8ba2\u9605\u8303\u5f0f\uff0c\u89e3\u51b3\u4e86LLM\u591a\u667a\u80fd\u4f53\u534f\u8c03\u4e2d\u7684\u81ea\u9002\u5e94\u3001\u53ef\u6269\u5c55\u548c\u9c81\u68d2\u6027\u95ee\u9898\uff0c\u4e3a\u81ea\u52a8\u5316\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.07449", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.07449", "abs": "https://arxiv.org/abs/2602.07449", "authors": ["Tan Yu", "Qian Qiao", "Le Shen", "Ke Zhou", "Jincheng Hu", "Dian Sheng", "Bo Hu", "Haoming Qin", "Jun Gao", "Changhai Zhou", "Shunshun Yin", "Siyuan Liu"], "title": "SoulX-FlashHead: Oracle-guided Generation of Infinite Real-time Streaming Talking Heads", "comment": "11 pages, 3 figures", "summary": "Achieving a balance between high-fidelity visual quality and low-latency streaming remains a formidable challenge in audio-driven portrait generation. Existing large-scale models often suffer from prohibitive computational costs, while lightweight alternatives typically compromise on holistic facial representations and temporal stability. In this paper, we propose SoulX-FlashHead, a unified 1.3B-parameter framework designed for real-time, infinite-length, and high-fidelity streaming video generation. To address the instability of audio features in streaming scenarios, we introduce Streaming-Aware Spatiotemporal Pre-training equipped with a Temporal Audio Context Cache mechanism, which ensures robust feature extraction from short audio fragments. Furthermore, to mitigate the error accumulation and identity drift inherent in long-sequence autoregressive generation, we propose Oracle-Guided Bidirectional Distillation, leveraging ground-truth motion priors to provide precise physical guidance. We also present VividHead, a large-scale, high-quality dataset containing 782 hours of strictly aligned footage to support robust training. Extensive experiments demonstrate that SoulX-FlashHead achieves state-of-the-art performance on HDTF and VFHQ benchmarks. Notably, our Lite variant achieves an inference speed of 96 FPS on a single NVIDIA RTX 4090, facilitating ultra-fast interaction without sacrificing visual coherence.", "AI": {"tldr": "SoulX-FlashHead\uff1a\u4e00\u4e2a1.3B\u53c2\u6570\u7684\u5b9e\u65f6\u9ad8\u4fdd\u771f\u97f3\u9891\u9a71\u52a8\u8096\u50cf\u89c6\u9891\u751f\u6210\u6846\u67b6\uff0c\u652f\u6301\u65e0\u9650\u957f\u5ea6\u6d41\u5f0f\u751f\u6210\uff0c\u5728HDTF\u548cVFHQ\u57fa\u51c6\u4e0a\u8fbe\u5230SOTA\u6027\u80fd\uff0cLite\u53d8\u4f53\u5728RTX 4090\u4e0a\u5b9e\u73b096 FPS\u63a8\u7406\u901f\u5ea6\u3002", "motivation": "\u73b0\u6709\u97f3\u9891\u9a71\u52a8\u8096\u50cf\u751f\u6210\u65b9\u6cd5\u9762\u4e34\u9ad8\u4fdd\u771f\u89c6\u89c9\u8d28\u91cf\u4e0e\u4f4e\u5ef6\u8fdf\u6d41\u5f0f\u5904\u7406\u4e4b\u95f4\u7684\u5e73\u8861\u96be\u9898\u3002\u5927\u578b\u6a21\u578b\u8ba1\u7b97\u6210\u672c\u8fc7\u9ad8\uff0c\u8f7b\u91cf\u7ea7\u65b9\u6848\u5219\u727a\u7272\u4e86\u6574\u4f53\u9762\u90e8\u8868\u5f81\u548c\u65f6\u95f4\u7a33\u5b9a\u6027\u3002", "method": "\u63d0\u51fa1.3B\u53c2\u6570\u7edf\u4e00\u6846\u67b6SoulX-FlashHead\uff0c\u5305\u542b\uff1a1\uff09\u6d41\u5f0f\u611f\u77e5\u65f6\u7a7a\u9884\u8bad\u7ec3\u4e0e\u65f6\u5e8f\u97f3\u9891\u4e0a\u4e0b\u6587\u7f13\u5b58\u673a\u5236\uff0c\u4ece\u77ed\u97f3\u9891\u7247\u6bb5\u63d0\u53d6\u7a33\u5065\u7279\u5f81\uff1b2\uff09\u5148\u77e5\u5f15\u5bfc\u53cc\u5411\u84b8\u998f\uff0c\u5229\u7528\u771f\u5b9e\u8fd0\u52a8\u5148\u9a8c\u63d0\u4f9b\u7cbe\u786e\u7269\u7406\u6307\u5bfc\uff0c\u7f13\u89e3\u957f\u5e8f\u5217\u81ea\u56de\u5f52\u751f\u6210\u7684\u8bef\u5dee\u7d2f\u79ef\u548c\u8eab\u4efd\u6f02\u79fb\uff1b3\uff09\u6784\u5efaVividHead\u6570\u636e\u96c6\uff08782\u5c0f\u65f6\u4e25\u683c\u5bf9\u9f50\u7d20\u6750\uff09\u652f\u6301\u8bad\u7ec3\u3002", "result": "\u5728HDTF\u548cVFHQ\u57fa\u51c6\u4e0a\u5b9e\u73b0\u6700\u5148\u8fdb\u6027\u80fd\u3002Lite\u53d8\u4f53\u5728\u5355\u5f20NVIDIA RTX 4090\u4e0a\u8fbe\u523096 FPS\u63a8\u7406\u901f\u5ea6\uff0c\u5b9e\u73b0\u8d85\u5feb\u4ea4\u4e92\u800c\u4e0d\u727a\u7272\u89c6\u89c9\u8fde\u8d2f\u6027\u3002", "conclusion": "SoulX-FlashHead\u6210\u529f\u89e3\u51b3\u4e86\u97f3\u9891\u9a71\u52a8\u8096\u50cf\u751f\u6210\u4e2d\u9ad8\u4fdd\u771f\u8d28\u91cf\u4e0e\u5b9e\u65f6\u6d41\u5f0f\u5904\u7406\u4e4b\u95f4\u7684\u5e73\u8861\u95ee\u9898\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u6d41\u5f0f\u611f\u77e5\u8bad\u7ec3\u673a\u5236\u548c\u5148\u77e5\u5f15\u5bfc\u84b8\u998f\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u65e0\u9650\u957f\u5ea6\u3001\u9ad8\u8d28\u91cf\u7684\u89c6\u9891\u751f\u6210\u3002"}}
{"id": "2602.08013", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08013", "abs": "https://arxiv.org/abs/2602.08013", "authors": ["Yuqiao Meng", "Luoxi Tang", "Dazheng Zhang", "Rafael Brens", "Elvys J. Romero", "Nancy Guo", "Safa Elkefi", "Zhaohan Xi"], "title": "Small Agent Group is the Future of Digital Health", "comment": null, "summary": "The rapid adoption of large language models (LLMs) in digital health has been driven by a \"scaling-first\" philosophy, i.e., the assumption that clinical intelligence increases with model size and data. However, real-world clinical needs include not only effectiveness, but also reliability and reasonable deployment cost. Since clinical decision-making is inherently collaborative, we challenge the monolithic scaling paradigm and ask whether a Small Agent Group (SAG) can support better clinical reasoning. SAG shifts from single-model intelligence to collective expertise by distributing reasoning, evidence-based analysis, and critical audit through a collaborative deliberation process. To assess the clinical utility of SAG, we conduct extensive evaluations using diverse clinical metrics spanning effectiveness, reliability, and deployment cost. Our results show that SAG achieves superior performance compared to a single giant model, both with and without additional optimization or retrieval-augmented generation. These findings suggest that the synergistic reasoning represented by SAG can substitute for model parameter growth in clinical settings. Overall, SAG offers a scalable solution to digital health that better balances effectiveness, reliability, and deployment efficiency.", "AI": {"tldr": "\u672c\u6587\u6311\u6218\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6570\u5b57\u5065\u5eb7\u9886\u57df\u7684\"\u89c4\u6a21\u4f18\u5148\"\u8303\u5f0f\uff0c\u63d0\u51fa\u5c0f\u578b\u667a\u80fd\u4f53\u7fa4\u4f53(SAG)\u901a\u8fc7\u534f\u540c\u63a8\u7406\u673a\u5236\uff0c\u5728\u4e34\u5e8a\u51b3\u7b56\u4e2d\u5b9e\u73b0\u6bd4\u5355\u4e00\u5927\u578b\u6a21\u578b\u66f4\u597d\u7684\u6548\u679c\u3001\u53ef\u9760\u6027\u548c\u90e8\u7f72\u6210\u672c\u5e73\u8861\u3002", "motivation": "\u5f53\u524d\u6570\u5b57\u5065\u5eb7\u9886\u57df\u8fc7\u5ea6\u4f9d\u8d56\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\"\u89c4\u6a21\u4f18\u5148\"\u7406\u5ff5\uff0c\u8ba4\u4e3a\u6a21\u578b\u8d8a\u5927\u3001\u6570\u636e\u8d8a\u591a\u4e34\u5e8a\u667a\u80fd\u5c31\u8d8a\u5f3a\u3002\u4f46\u771f\u5b9e\u4e34\u5e8a\u9700\u6c42\u4e0d\u4ec5\u9700\u8981\u6709\u6548\u6027\uff0c\u8fd8\u9700\u8981\u53ef\u9760\u6027\u548c\u5408\u7406\u7684\u90e8\u7f72\u6210\u672c\u3002\u4e34\u5e8a\u51b3\u7b56\u672c\u8d28\u4e0a\u662f\u534f\u4f5c\u8fc7\u7a0b\uff0c\u56e0\u6b64\u9700\u8981\u63a2\u7d22\u662f\u5426\u5c0f\u578b\u667a\u80fd\u4f53\u7fa4\u4f53\u80fd\u652f\u6301\u66f4\u597d\u7684\u4e34\u5e8a\u63a8\u7406\u3002", "method": "\u63d0\u51fa\u5c0f\u578b\u667a\u80fd\u4f53\u7fa4\u4f53(SAG)\u65b9\u6cd5\uff0c\u4ece\u5355\u4e00\u6a21\u578b\u667a\u80fd\u8f6c\u5411\u96c6\u4f53\u4e13\u4e1a\u77e5\u8bc6\uff0c\u901a\u8fc7\u534f\u4f5c\u5ba1\u8bae\u8fc7\u7a0b\u5206\u914d\u63a8\u7406\u3001\u5faa\u8bc1\u5206\u6790\u548c\u5173\u952e\u5ba1\u6838\u3002\u4f7f\u7528\u591a\u6837\u5316\u7684\u4e34\u5e8a\u6307\u6807\u8fdb\u884c\u5e7f\u6cdb\u8bc4\u4f30\uff0c\u6db5\u76d6\u6709\u6548\u6027\u3001\u53ef\u9760\u6027\u548c\u90e8\u7f72\u6210\u672c\u3002", "result": "SAG\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u5355\u4e00\u5927\u578b\u6a21\u578b\uff0c\u65e0\u8bba\u662f\u5426\u8fdb\u884c\u989d\u5916\u4f18\u5316\u6216\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u3002\u7ed3\u679c\u8868\u660eSAG\u6240\u4ee3\u8868\u7684\u534f\u540c\u63a8\u7406\u53ef\u4ee5\u66ff\u4ee3\u4e34\u5e8a\u73af\u5883\u4e2d\u6a21\u578b\u53c2\u6570\u7684\u589e\u957f\u3002", "conclusion": "SAG\u4e3a\u6570\u5b57\u5065\u5eb7\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u66f4\u597d\u5730\u5e73\u8861\u6709\u6548\u6027\u3001\u53ef\u9760\u6027\u548c\u90e8\u7f72\u6548\u7387\uff0c\u6311\u6218\u4e86\u4f20\u7edf\u7684\u5355\u4e00\u6a21\u578b\u89c4\u6a21\u6269\u5c55\u8303\u5f0f\u3002"}}
{"id": "2602.08237", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.08237", "abs": "https://arxiv.org/abs/2602.08237", "authors": ["Yao Xiao", "Lei Wang", "Yue Deng", "Guanzheng Chen", "Ziqi Jin", "Jung-jae Kim", "Xiaoli Li", "Roy Ka-wei Lee", "Lidong Bing"], "title": "Document Reconstruction Unlocks Scalable Long-Context RLVR", "comment": null, "summary": "Reinforcement Learning with Verifiable Rewards~(RLVR) has become a prominent paradigm to enhance the capabilities (i.e.\\ long-context) of Large Language Models~(LLMs). However, it often relies on gold-standard answers or explicit evaluation rubrics provided by powerful teacher models or human experts, which are costly and time-consuming. In this work, we investigate unsupervised approaches to enhance the long-context capabilities of LLMs, eliminating the need for heavy human annotations or teacher models' supervision. Specifically, we first replace a few paragraphs with special placeholders in a long document. LLMs are trained through reinforcement learning to reconstruct the document by correctly identifying and sequencing missing paragraphs from a set of candidate options. This training paradigm enables the model to capture global narrative coherence, significantly boosting long-context performance. We validate the effectiveness of our method on two widely used benchmarks, RULER and LongBench~v2. While acquiring noticeable gains on RULER, it can also achieve a reasonable improvement on LongBench~v2 without any manually curated long-context QA data. Furthermore, we conduct extensive ablation studies to analyze the impact of reward design, data curation strategies, training schemes, and data scaling effects on model performance. We publicly release our code, data, and models.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u76d1\u7763\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5RLVR\uff0c\u901a\u8fc7\u8ba9LLM\u5728\u957f\u6587\u6863\u4e2d\u8bc6\u522b\u548c\u6392\u5e8f\u7f3a\u5931\u6bb5\u843d\u6765\u63d0\u5347\u957f\u4e0a\u4e0b\u6587\u80fd\u529b\uff0c\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\u6216\u6559\u5e08\u6a21\u578b\u76d1\u7763\u3002", "motivation": "\u73b0\u6709\u7684\u5f3a\u5316\u5b66\u4e60\u9700\u8981\u4f9d\u8d56\u6602\u8d35\u7684\u9ec4\u91d1\u6807\u51c6\u7b54\u6848\u6216\u6559\u5e08\u6a21\u578b\u8bc4\u4f30\u6807\u51c6\uff0c\u6210\u672c\u9ad8\u4e14\u8017\u65f6\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u65e0\u76d1\u7763\u65b9\u6cd5\u6765\u589e\u5f3aLLM\u7684\u957f\u4e0a\u4e0b\u6587\u80fd\u529b\uff0c\u6d88\u9664\u5bf9\u5927\u91cf\u4eba\u5de5\u6807\u6ce8\u6216\u6559\u5e08\u6a21\u578b\u76d1\u7763\u7684\u9700\u6c42\u3002", "method": "\u9996\u5148\u5728\u957f\u6587\u6863\u4e2d\u7528\u7279\u6b8a\u5360\u4f4d\u7b26\u66ff\u6362\u51e0\u4e2a\u6bb5\u843d\uff0c\u7136\u540e\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3LLM\u4ece\u5019\u9009\u9009\u9879\u96c6\u4e2d\u6b63\u786e\u8bc6\u522b\u548c\u6392\u5e8f\u7f3a\u5931\u6bb5\u843d\u6765\u91cd\u6784\u6587\u6863\u3002\u8fd9\u79cd\u8bad\u7ec3\u8303\u5f0f\u4f7f\u6a21\u578b\u80fd\u591f\u6355\u6349\u5168\u5c40\u53d9\u4e8b\u8fde\u8d2f\u6027\u3002", "result": "\u5728RULER\u548cLongBench v2\u4e24\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002\u5728RULER\u4e0a\u83b7\u5f97\u663e\u8457\u63d0\u5347\uff0c\u5728LongBench v2\u4e0a\u4e5f\u80fd\u5b9e\u73b0\u5408\u7406\u6539\u8fdb\uff0c\u4e14\u65e0\u9700\u624b\u52a8\u6574\u7406\u7684\u957f\u4e0a\u4e0b\u6587QA\u6570\u636e\u3002\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u6d88\u878d\u7814\u7a76\u5206\u6790\u5956\u52b1\u8bbe\u8ba1\u3001\u6570\u636e\u6574\u7406\u7b56\u7565\u3001\u8bad\u7ec3\u65b9\u6848\u548c\u6570\u636e\u7f29\u653e\u6548\u5e94\u7684\u5f71\u54cd\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6709\u6548\u7684\u65e0\u76d1\u7763\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347LLM\u7684\u957f\u4e0a\u4e0b\u6587\u80fd\u529b\uff0c\u65e0\u9700\u6602\u8d35\u7684\u4eba\u5de5\u6807\u6ce8\u6216\u6559\u5e08\u6a21\u578b\u76d1\u7763\uff0c\u5e76\u516c\u5f00\u4e86\u4ee3\u7801\u3001\u6570\u636e\u548c\u6a21\u578b\u3002"}}
{"id": "2602.07458", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.07458", "abs": "https://arxiv.org/abs/2602.07458", "authors": ["Yancheng Long", "Yankai Yang", "Hongyang Wei", "Wei Chen", "Tianke Zhang", "Haonan fan", "Changyi Liu", "Kaiyu Jiang", "Jiankang Chen", "Kaiyu Tang", "Bin Wen", "Fan Yang", "Tingting Gao", "Han Li", "Shuo Yang"], "title": "SpatialReward: Bridging the Perception Gap in Online RL for Image Editing via Explicit Spatial Reasoning", "comment": null, "summary": "Online Reinforcement Learning (RL) offers a promising avenue for complex image editing but is currently constrained by the scarcity of reliable and fine-grained reward signals. Existing evaluators frequently struggle with a critical perception gap we term \"Attention Collapse,\" where models neglect cross-image comparisons and fail to capture fine-grained details, resulting in inaccurate perception and miscalibrated scores. To address these limitations, we propose SpatialReward, a reward model that enforces precise verification via explicit spatial reasoning. By anchoring reasoning to predicted edit regions, SpatialReward grounds semantic judgments in pixel-level evidence, significantly enhancing evaluative accuracy. Trained on a curated 260k spatial-aware dataset, our model achieves state-of-the-art performance on MMRB2 and EditReward-Bench, and outperforms proprietary evaluators on our proposed MultiEditReward-Bench. Furthermore, SpatialReward serves as a robust signal in online RL, boosting OmniGen2 by +0.90 on GEdit-Bench--surpassing the leading discriminative model and doubling the gain of GPT-4.1 (+0.45). These results demonstrate that spatial reasoning is essential for unlocking effective alignment in image editing.", "AI": {"tldr": "SpatialReward\u901a\u8fc7\u7a7a\u95f4\u63a8\u7406\u89e3\u51b3\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u56fe\u50cf\u7f16\u8f91\u4e2d\u7684\u5956\u52b1\u4fe1\u53f7\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u8bc4\u4f30\u51c6\u786e\u6027\u548c\u7f16\u8f91\u6548\u679c", "motivation": "\u5f53\u524d\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u56fe\u50cf\u7f16\u8f91\u9762\u4e34\u53ef\u9760\u7ec6\u7c92\u5ea6\u5956\u52b1\u4fe1\u53f7\u7a00\u7f3a\u7684\u95ee\u9898\uff0c\u73b0\u6709\u8bc4\u4f30\u5668\u5b58\u5728\"\u6ce8\u610f\u529b\u5d29\u6e83\"\u73b0\u8c61\uff0c\u5ffd\u89c6\u8de8\u56fe\u50cf\u6bd4\u8f83\u548c\u7ec6\u7c92\u5ea6\u7ec6\u8282\uff0c\u5bfc\u81f4\u611f\u77e5\u4e0d\u51c6\u786e\u548c\u5206\u6570\u6821\u51c6\u9519\u8bef", "method": "\u63d0\u51faSpatialReward\u5956\u52b1\u6a21\u578b\uff0c\u901a\u8fc7\u663e\u5f0f\u7a7a\u95f4\u63a8\u7406\u5b9e\u73b0\u7cbe\u786e\u9a8c\u8bc1\uff0c\u5c06\u63a8\u7406\u951a\u5b9a\u5230\u9884\u6d4b\u7684\u7f16\u8f91\u533a\u57df\uff0c\u4f7f\u8bed\u4e49\u5224\u65ad\u57fa\u4e8e\u50cf\u7d20\u7ea7\u8bc1\u636e", "result": "\u5728260k\u7a7a\u95f4\u611f\u77e5\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\uff0c\u5728MMRB2\u548cEditReward-Bench\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5728MultiEditReward-Bench\u4e0a\u8d85\u8d8a\u4e13\u6709\u8bc4\u4f30\u5668\uff1b\u4f5c\u4e3a\u5728\u7ebfRL\u4fe1\u53f7\u4f7fOmniGen2\u5728GEdit-Bench\u4e0a\u63d0\u5347+0.90\uff0c\u8d85\u8d8a\u9886\u5148\u5224\u522b\u6a21\u578b\u5e76\u4e24\u500d\u4e8eGPT-4.1\u7684\u63d0\u5347", "conclusion": "\u7a7a\u95f4\u63a8\u7406\u5bf9\u4e8e\u5b9e\u73b0\u56fe\u50cf\u7f16\u8f91\u4e2d\u7684\u6709\u6548\u5bf9\u9f50\u81f3\u5173\u91cd\u8981\uff0cSpatialReward\u4e3a\u89e3\u51b3\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u56fe\u50cf\u7f16\u8f91\u7684\u5956\u52b1\u4fe1\u53f7\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848"}}
{"id": "2602.08238", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.08238", "abs": "https://arxiv.org/abs/2602.08238", "authors": ["Nathaniel Imel", "Noga Zaslavasky"], "title": "On convexity and efficiency in semantic systems", "comment": null, "summary": "There are two widely held characterizations of human semantic category systems: (1) they form convex partitions of conceptual spaces, and (2) they are efficient for communication. While prior work observed that convexity and efficiency co-occur in color naming, the analytical relation between them and why they co-occur have not been well understood. We address this gap by combining analytical and empirical analyses that build on the Information Bottleneck (IB) framework for semantic efficiency. First, we show that convexity and efficiency are distinct in the sense that neither entails the other: there are convex systems which are inefficient, and optimally-efficient systems that are non-convex. Crucially, however, the IB-optimal systems are mostly convex in the domain of color naming, explaining the main empirical basis for the convexity approach. Second, we show that efficiency is a stronger predictor for discriminating attested color naming systems from hypothetical variants, with convexity adding negligible improvement on top of that. Finally, we discuss a range of empirical phenomena that convexity cannot account for but efficiency can. Taken together, our work suggests that while convexity and efficiency can yield similar structural observations, they are fundamentally distinct, with efficiency providing a more comprehensive account of semantic typology.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5206\u6790\u4e86\u4eba\u7c7b\u8bed\u4e49\u8303\u7574\u7cfb\u7edf\u7684\u4e24\u4e2a\u7279\u5f81\uff1a\u51f8\u6027\u548c\u6548\u7387\u6027\u3002\u901a\u8fc7\u4fe1\u606f\u74f6\u9888\u6846\u67b6\u53d1\u73b0\u4e24\u8005\u672c\u8d28\u4e0d\u540c\uff0c\u6548\u7387\u6027\u66f4\u80fd\u89e3\u91ca\u989c\u8272\u547d\u540d\u7cfb\u7edf\u7684\u5b9e\u8bc1\u73b0\u8c61\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u7406\u89e3\u4eba\u7c7b\u8bed\u4e49\u8303\u7574\u7cfb\u7edf\u4e2d\u51f8\u6027\u548c\u6548\u7387\u6027\u8fd9\u4e24\u4e2a\u5e7f\u6cdb\u8ba4\u53ef\u7279\u5f81\u4e4b\u95f4\u7684\u5173\u7cfb\u3002\u867d\u7136\u4e4b\u524d\u89c2\u5bdf\u5230\u989c\u8272\u547d\u540d\u4e2d\u4e24\u8005\u5171\u5b58\uff0c\u4f46\u5b83\u4eec\u7684\u5206\u6790\u5173\u7cfb\u53ca\u5171\u5b58\u539f\u56e0\u5c1a\u672a\u5f97\u5230\u5145\u5206\u7406\u89e3\u3002", "method": "\u91c7\u7528\u4fe1\u606f\u74f6\u9888\u6846\u67b6\u8fdb\u884c\u7406\u8bba\u548c\u5b9e\u8bc1\u5206\u6790\u3002\u9996\u5148\u8bc1\u660e\u51f8\u6027\u548c\u6548\u7387\u6027\u5728\u7406\u8bba\u4e0a\u662f\u4e0d\u540c\u7684\uff0c\u7136\u540e\u5206\u6790IB\u6700\u4f18\u7cfb\u7edf\u5728\u989c\u8272\u547d\u540d\u9886\u57df\u7684\u8868\u73b0\uff0c\u6700\u540e\u6bd4\u8f83\u4e24\u8005\u5bf9\u5b9e\u8bc1\u6570\u636e\u7684\u9884\u6d4b\u80fd\u529b\u3002", "result": "1. \u51f8\u6027\u548c\u6548\u7387\u6027\u5728\u7406\u8bba\u4e0a\u76f8\u4e92\u72ec\u7acb\uff1a\u5b58\u5728\u51f8\u4f46\u4f4e\u6548\u7684\u7cfb\u7edf\uff0c\u4e5f\u5b58\u5728\u6700\u4f18\u6548\u7387\u4f46\u975e\u51f8\u7684\u7cfb\u7edf\uff1b2. \u5728\u989c\u8272\u547d\u540d\u9886\u57df\uff0cIB\u6700\u4f18\u7cfb\u7edf\u5927\u591a\u662f\u51f8\u7684\uff0c\u89e3\u91ca\u4e86\u51f8\u6027\u65b9\u6cd5\u7684\u5b9e\u8bc1\u57fa\u7840\uff1b3. \u6548\u7387\u6027\u6bd4\u51f8\u6027\u66f4\u80fd\u533a\u5206\u5b9e\u9645\u989c\u8272\u547d\u540d\u7cfb\u7edf\u4e0e\u5047\u8bbe\u53d8\u4f53\uff0c\u51f8\u6027\u5728\u6548\u7387\u6027\u57fa\u7840\u4e0a\u51e0\u4e4e\u6ca1\u6709\u989d\u5916\u6539\u8fdb\u3002", "conclusion": "\u51f8\u6027\u548c\u6548\u7387\u6027\u867d\u7136\u80fd\u4ea7\u751f\u76f8\u4f3c\u7684\u7ed3\u6784\u89c2\u5bdf\u7ed3\u679c\uff0c\u4f46\u672c\u8d28\u4e0a\u662f\u4e0d\u540c\u7684\u6982\u5ff5\u3002\u6548\u7387\u6027\u63d0\u4f9b\u4e86\u5bf9\u8bed\u4e49\u7c7b\u578b\u5b66\u66f4\u5168\u9762\u7684\u89e3\u91ca\uff0c\u80fd\u89e3\u91ca\u51f8\u6027\u65e0\u6cd5\u89e3\u91ca\u7684\u4e00\u7cfb\u5217\u5b9e\u8bc1\u73b0\u8c61\u3002"}}
{"id": "2602.07463", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.07463", "abs": "https://arxiv.org/abs/2602.07463", "authors": ["Misbah Ijaz", "Saif Ur Rehman Khan", "Abd Ur Rehman", "Tayyaba Asif", "Sebastian Vollmer", "Andreas Dengel", "Muhammad Nabeel Asim"], "title": "GlobalWasteData: A Large-Scale, Integrated Dataset for Robust Waste Classification and Environmental Monitoring", "comment": null, "summary": "The growing amount of waste is a problem for the environment that requires efficient sorting techniques for various kinds of waste. An automated waste classification system is used for this purpose. The effectiveness of these Artificial Intelligence (AI) models depends on the quality and accessibility of publicly available datasets, which provide the basis for training and analyzing classification algorithms. Although several public waste classification datasets exist, they remain fragmented, inconsistent, and biased toward specific environments. Differences in class names, annotation formats, image conditions, and class distributions make it difficult to combine these datasets or train models that generalize well to real world scenarios. To address these issues, we introduce the GlobalWasteData (GWD) archive, a large scale dataset of 89,807 images across 14 main categories, annotated with 68 distinct subclasses. We compile this novel integrated GWD archive by merging multiple publicly available datasets into a single, unified resource. This GWD archive offers consistent labeling, improved domain diversity, and more balanced class representation, enabling the development of robust and generalizable waste recognition models. Additional preprocessing steps such as quality filtering, duplicate removal, and metadata generation further improve dataset reliability. Overall, this dataset offers a strong foundation for Machine Learning (ML) applications in environmental monitoring, recycling automation, and waste identification, and is publicly available to promote future research and reproducibility.", "AI": {"tldr": "\u7814\u7a76\u4eba\u5458\u521b\u5efa\u4e86GlobalWasteData(GWD)\u6570\u636e\u96c6\uff0c\u6574\u5408\u4e86\u591a\u4e2a\u516c\u5f00\u7684\u5783\u573e\u5206\u7c7b\u6570\u636e\u96c6\uff0c\u5305\u542b89,807\u5f20\u56fe\u50cf\u300114\u4e2a\u4e3b\u7c7b\u522b\u548c68\u4e2a\u5b50\u7c7b\uff0c\u65e8\u5728\u89e3\u51b3\u73b0\u6709\u6570\u636e\u96c6\u788e\u7247\u5316\u3001\u4e0d\u4e00\u81f4\u548c\u504f\u5dee\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u5783\u573e\u5206\u7c7b\u6570\u636e\u96c6\u5b58\u5728\u788e\u7247\u5316\u3001\u4e0d\u4e00\u81f4\u548c\u7279\u5b9a\u73af\u5883\u504f\u5dee\u95ee\u9898\uff0c\u7c7b\u522b\u540d\u79f0\u3001\u6807\u6ce8\u683c\u5f0f\u3001\u56fe\u50cf\u6761\u4ef6\u548c\u7c7b\u522b\u5206\u5e03\u5dee\u5f02\u4f7f\u5f97\u96be\u4ee5\u7ec4\u5408\u6570\u636e\u96c6\u6216\u8bad\u7ec3\u80fd\u6cdb\u5316\u5230\u771f\u5b9e\u573a\u666f\u7684\u6a21\u578b\u3002", "method": "\u901a\u8fc7\u5408\u5e76\u591a\u4e2a\u516c\u5f00\u53ef\u7528\u7684\u6570\u636e\u96c6\u521b\u5efa\u7edf\u4e00\u7684GWD\u6570\u636e\u96c6\uff0c\u91c7\u7528\u8d28\u91cf\u8fc7\u6ee4\u3001\u91cd\u590d\u53bb\u9664\u548c\u5143\u6570\u636e\u751f\u6210\u7b49\u9884\u5904\u7406\u6b65\u9aa4\u63d0\u9ad8\u6570\u636e\u96c6\u53ef\u9760\u6027\uff0c\u786e\u4fdd\u4e00\u81f4\u7684\u6807\u6ce8\u3001\u6539\u8fdb\u7684\u9886\u57df\u591a\u6837\u6027\u548c\u66f4\u5e73\u8861\u7684\u7c7b\u522b\u8868\u793a\u3002", "result": "\u521b\u5efa\u4e86\u5305\u542b89,807\u5f20\u56fe\u50cf\u300114\u4e2a\u4e3b\u7c7b\u522b\u548c68\u4e2a\u5b50\u7c7b\u7684GWD\u6570\u636e\u96c6\uff0c\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684\u8d44\u6e90\uff0c\u652f\u6301\u5f00\u53d1\u7a33\u5065\u4e14\u53ef\u6cdb\u5316\u7684\u5783\u573e\u8bc6\u522b\u6a21\u578b\u3002", "conclusion": "GWD\u6570\u636e\u96c6\u4e3a\u73af\u5883\u76d1\u6d4b\u3001\u56de\u6536\u81ea\u52a8\u5316\u548c\u5783\u573e\u8bc6\u522b\u7684\u673a\u5668\u5b66\u4e60\u5e94\u7528\u63d0\u4f9b\u4e86\u575a\u5b9e\u57fa\u7840\uff0c\u516c\u5f00\u53ef\u7528\u4ee5\u4fc3\u8fdb\u672a\u6765\u7814\u7a76\u548c\u53ef\u91cd\u590d\u6027\u3002"}}
{"id": "2602.08030", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.08030", "abs": "https://arxiv.org/abs/2602.08030", "authors": ["Yilun Zheng", "Dongyang Ma", "Tian Liang", "Jiahao Xu", "Xinting Huang", "Lijie Chen", "Haitao Mi", "Yan Wang"], "title": "Free(): Learning to Forget in Malloc-Only Reasoning Models", "comment": null, "summary": "Reasoning models enhance problem-solving by scaling test-time compute, yet they face a critical paradox: excessive thinking tokens often degrade performance rather than improve it. We attribute this to a fundamental architectural flaw: standard LLMs operate as \"malloc-only\" engines, continuously accumulating valid and redundant steps alike without a mechanism to prune obsolete information. To break this cycle, we propose Free()LM, a model that introduces an intrinsic self-forgetting capability via the Free-Module, a plug-and-play LoRA adapter. By iteratively switching between reasoning and cleaning modes, Free()LM dynamically identifies and prunes useless context chunks, maintaining a compact and noise-free state.\n  Extensive experiments show that Free()LM provides consistent improvements across all model scales (8B to 685B). It achieves a 3.3% average improvement over top-tier reasoning baselines, even establishing a new SOTA on IMOanswerBench using DeepSeek V3.2-Speciale. Most notably, in long-horizon tasks where the standard Qwen3-235B-A22B model suffers a total collapse (0% accuracy), Free()LM restores performance to 50%. Our findings suggest that sustainable intelligence requires the freedom to forget as much as the power to think.", "AI": {"tldr": "Free()LM\u901a\u8fc7\u5f15\u5165\u81ea\u6211\u9057\u5fd8\u673a\u5236\u89e3\u51b3\u63a8\u7406\u6a21\u578b\u8fc7\u5ea6\u601d\u8003\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u7684\u95ee\u9898\uff0c\u4f7f\u7528Free-Module LoRA\u9002\u914d\u5668\u52a8\u6001\u4fee\u526a\u65e0\u7528\u4e0a\u4e0b\u6587\uff0c\u5728\u591a\u79cd\u89c4\u6a21\u6a21\u578b\u4e0a\u5b9e\u73b0\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u6807\u51c6LLMs\u5b58\u5728\"\u53ea\u5206\u914d\u4e0d\u91ca\u653e\"\u7684\u67b6\u6784\u7f3a\u9677\uff0c\u6301\u7eed\u7d2f\u79ef\u6709\u6548\u548c\u5197\u4f59\u63a8\u7406\u6b65\u9aa4\uff0c\u7f3a\u4e4f\u4fee\u526a\u8fc7\u65f6\u4fe1\u606f\u7684\u673a\u5236\uff0c\u5bfc\u81f4\u8fc7\u5ea6\u601d\u8003\u65f6\u6027\u80fd\u4e0b\u964d\u800c\u975e\u63d0\u5347\u3002", "method": "\u63d0\u51faFree()LM\u6a21\u578b\uff0c\u901a\u8fc7Free-Module\uff08\u5373\u63d2\u5373\u7528LoRA\u9002\u914d\u5668\uff09\u5f15\u5165\u5185\u5728\u81ea\u6211\u9057\u5fd8\u80fd\u529b\u3002\u6a21\u578b\u5728\u63a8\u7406\u6a21\u5f0f\u548c\u6e05\u7406\u6a21\u5f0f\u4e4b\u95f4\u8fed\u4ee3\u5207\u6362\uff0c\u52a8\u6001\u8bc6\u522b\u5e76\u4fee\u526a\u65e0\u7528\u4e0a\u4e0b\u6587\u5757\uff0c\u4fdd\u6301\u7d27\u51d1\u65e0\u566a\u58f0\u72b6\u6001\u3002", "result": "\u5728\u6240\u6709\u6a21\u578b\u89c4\u6a21\uff088B\u5230685B\uff09\u4e0a\u5747\u5b9e\u73b0\u4e00\u81f4\u6539\u8fdb\uff0c\u5e73\u5747\u6bd4\u9876\u7ea7\u63a8\u7406\u57fa\u7ebf\u63d0\u53473.3%\uff0c\u5728IMOanswerBench\u4e0a\u5efa\u7acb\u65b0SOTA\u3002\u5728\u957f\u65f6\u7a0b\u4efb\u52a1\u4e2d\uff0c\u6807\u51c6Qwen3-235B-A22B\u6a21\u578b\u5b8c\u5168\u5d29\u6e83\uff080%\u51c6\u786e\u7387\uff09\uff0c\u800cFree()LM\u6062\u590d\u81f350%\u51c6\u786e\u7387\u3002", "conclusion": "\u53ef\u6301\u7eed\u667a\u80fd\u65e2\u9700\u8981\u601d\u8003\u7684\u80fd\u529b\uff0c\u4e5f\u9700\u8981\u9057\u5fd8\u7684\u81ea\u7531\u3002\u81ea\u6211\u9057\u5fd8\u673a\u5236\u662f\u89e3\u51b3\u63a8\u7406\u6a21\u578b\u8fc7\u5ea6\u601d\u8003\u95ee\u9898\u7684\u5173\u952e\u3002"}}
{"id": "2602.08252", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.08252", "abs": "https://arxiv.org/abs/2602.08252", "authors": ["Devin R. Wright", "Justin E. Lane", "F. LeRon Shults"], "title": "Language Predicts Identity Fusion Across Cultures and Reveals Divergent Pathways to Violence", "comment": "Initial submitted version", "summary": "In light of increasing polarization and political violence, understanding the psychological roots of extremism is increasingly important. Prior research shows that identity fusion predicts willingness to engage in extreme acts. We evaluate the Cognitive Linguistic Identity Fusion Score, a method that uses cognitive linguistic patterns, LLMs, and implicit metaphor to measure fusion from language. Across datasets from the United Kingdom and Singapore, this approach outperforms existing methods in predicting validated fusion scores. Applied to extremist manifestos, two distinct high-fusion pathways to violence emerge: ideologues tend to frame themselves in terms of group, forming kinship bonds; whereas grievance-driven individuals frame the group in terms of their personal identity. These results refine theories of identity fusion and provide a scalable tool aiding fusion research and extremism detection.", "AI": {"tldr": "CLIFS\u65b9\u6cd5\u5229\u7528\u8ba4\u77e5\u8bed\u8a00\u6a21\u5f0f\u3001\u5927\u8bed\u8a00\u6a21\u578b\u548c\u9690\u5f0f\u9690\u55bb\u4ece\u8bed\u8a00\u4e2d\u6d4b\u91cf\u8eab\u4efd\u878d\u5408\uff0c\u5728\u9884\u6d4b\u5df2\u9a8c\u8bc1\u7684\u878d\u5408\u5206\u6570\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u8bc6\u522b\u51fa\u4e24\u79cd\u6781\u7aef\u66b4\u529b\u8def\u5f84", "motivation": "\u968f\u7740\u4e24\u6781\u5206\u5316\u548c\u653f\u6cbb\u66b4\u529b\u52a0\u5267\uff0c\u7406\u89e3\u6781\u7aef\u4e3b\u4e49\u7684\u5fc3\u7406\u6839\u6e90\u65e5\u76ca\u91cd\u8981\u3002\u5148\u524d\u7814\u7a76\u8868\u660e\u8eab\u4efd\u878d\u5408\u80fd\u9884\u6d4b\u53c2\u4e0e\u6781\u7aef\u884c\u4e3a\u7684\u610f\u613f\uff0c\u4f46\u9700\u8981\u66f4\u6709\u6548\u7684\u6d4b\u91cf\u65b9\u6cd5", "method": "\u5f00\u53d1\u4e86\u8ba4\u77e5\u8bed\u8a00\u8eab\u4efd\u878d\u5408\u8bc4\u5206\u65b9\u6cd5\uff0c\u5229\u7528\u8ba4\u77e5\u8bed\u8a00\u6a21\u5f0f\u3001\u5927\u8bed\u8a00\u6a21\u578b\u548c\u9690\u5f0f\u9690\u55bb\u4ece\u8bed\u8a00\u4e2d\u6d4b\u91cf\u878d\u5408\uff0c\u5728\u82f1\u56fd\u548c\u65b0\u52a0\u5761\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u9a8c\u8bc1", "result": "\u8be5\u65b9\u6cd5\u5728\u9884\u6d4b\u5df2\u9a8c\u8bc1\u7684\u878d\u5408\u5206\u6570\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002\u5e94\u7528\u4e8e\u6781\u7aef\u4e3b\u4e49\u5ba3\u8a00\u65f6\uff0c\u8bc6\u522b\u51fa\u4e24\u79cd\u9ad8\u878d\u5408\u66b4\u529b\u8def\u5f84\uff1a\u610f\u8bc6\u5f62\u6001\u8005\u503e\u5411\u4e8e\u4ee5\u7fa4\u4f53\u672f\u8bed\u6846\u67b6\u81ea\u6211\uff0c\u5f62\u6210\u4eb2\u5c5e\u5173\u7cfb\u7ebd\u5e26\uff1b\u800c\u53d7\u59d4\u5c48\u9a71\u52a8\u7684\u4e2a\u4f53\u5219\u4ee5\u4e2a\u4eba\u8eab\u4efd\u672f\u8bed\u6846\u67b6\u7fa4\u4f53", "conclusion": "\u8fd9\u4e9b\u7ed3\u679c\u5b8c\u5584\u4e86\u8eab\u4efd\u878d\u5408\u7406\u8bba\uff0c\u5e76\u4e3a\u878d\u5408\u7814\u7a76\u548c\u6781\u7aef\u4e3b\u4e49\u68c0\u6d4b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u5de5\u5177"}}
{"id": "2602.07493", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.07493", "abs": "https://arxiv.org/abs/2602.07493", "authors": ["Tianhao Zhou", "Yujia Chen", "Zhihao Zhan", "Yuhang Ming", "Jianzhu Huai"], "title": "Thermal odometry and dense mapping using learned ddometry and Gaussian splatting", "comment": "11 pages, 2 figures, 5 tables", "summary": "Thermal infrared sensors, with wavelengths longer than smoke particles, can capture imagery independent of darkness, dust, and smoke. This robustness has made them increasingly valuable for motion estimation and environmental perception in robotics, particularly in adverse conditions. Existing thermal odometry and mapping approaches, however, are predominantly geometric and often fail across diverse datasets while lacking the ability to produce dense maps. Motivated by the efficiency and high-quality reconstruction ability of recent Gaussian Splatting (GS) techniques, we propose TOM-GS, a thermal odometry and mapping method that integrates learning-based odometry with GS-based dense mapping. TOM-GS is among the first GS-based SLAM systems tailored for thermal cameras, featuring dedicated thermal image enhancement and monocular depth integration. Extensive experiments on motion estimation and novel-view rendering demonstrate that TOM-GS outperforms existing learning-based methods, confirming the benefits of learning-based pipelines for robust thermal odometry and dense reconstruction.", "AI": {"tldr": "TOM-GS\uff1a\u9996\u4e2a\u57fa\u4e8e\u9ad8\u65af\u6e85\u5c04\u7684\u70ed\u6210\u50cfSLAM\u7cfb\u7edf\uff0c\u7ed3\u5408\u5b66\u4e60\u91cc\u7a0b\u8ba1\u4e0e\u5bc6\u96c6\u5efa\u56fe\uff0c\u5728\u6076\u52a3\u6761\u4ef6\u4e0b\u5b9e\u73b0\u9c81\u68d2\u8fd0\u52a8\u4f30\u8ba1\u548c\u9ad8\u8d28\u91cf\u91cd\u5efa", "motivation": "\u70ed\u7ea2\u5916\u4f20\u611f\u5668\u5728\u9ed1\u6697\u3001\u7070\u5c18\u548c\u70df\u96fe\u4e2d\u5177\u6709\u9c81\u68d2\u6027\uff0c\u4f46\u73b0\u6709\u70ed\u6210\u50cf\u91cc\u7a0b\u8ba1\u548c\u5efa\u56fe\u65b9\u6cd5\u4e3b\u8981\u662f\u51e0\u4f55\u65b9\u6cd5\uff0c\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4e0d\u4f73\u4e14\u65e0\u6cd5\u751f\u6210\u5bc6\u96c6\u5730\u56fe\u3002\u53d7\u9ad8\u65af\u6e85\u5c04\u6280\u672f\u9ad8\u6548\u9ad8\u8d28\u91cf\u91cd\u5efa\u80fd\u529b\u7684\u542f\u53d1\uff0c\u9700\u8981\u5f00\u53d1\u4e13\u95e8\u9488\u5bf9\u70ed\u6210\u50cf\u76f8\u673a\u7684SLAM\u7cfb\u7edf\u3002", "method": "\u63d0\u51faTOM-GS\u65b9\u6cd5\uff0c\u6574\u5408\u5b66\u4e60\u578b\u91cc\u7a0b\u8ba1\u4e0e\u57fa\u4e8e\u9ad8\u65af\u6e85\u5c04\u7684\u5bc6\u96c6\u5efa\u56fe\uff0c\u5305\u542b\u4e13\u95e8\u7684\u70ed\u56fe\u50cf\u589e\u5f3a\u548c\u5355\u76ee\u6df1\u5ea6\u96c6\u6210\u6a21\u5757\uff0c\u662f\u9488\u5bf9\u70ed\u6210\u50cf\u76f8\u673a\u7684\u9996\u4e2a\u9ad8\u65af\u6e85\u5c04SLAM\u7cfb\u7edf\u3002", "result": "\u5728\u8fd0\u52a8\u4f30\u8ba1\u548c\u65b0\u89c6\u89d2\u6e32\u67d3\u65b9\u9762\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cTOM-GS\u4f18\u4e8e\u73b0\u6709\u7684\u5b66\u4e60\u65b9\u6cd5\uff0c\u8bc1\u5b9e\u4e86\u5b66\u4e60\u578b\u6d41\u7a0b\u5728\u9c81\u68d2\u70ed\u6210\u50cf\u91cc\u7a0b\u8ba1\u548c\u5bc6\u96c6\u91cd\u5efa\u65b9\u9762\u7684\u4f18\u52bf\u3002", "conclusion": "TOM-GS\u5c55\u793a\u4e86\u5b66\u4e60\u578b\u91cc\u7a0b\u8ba1\u4e0e\u9ad8\u65af\u6e85\u5c04\u5efa\u56fe\u7ed3\u5408\u7684\u6709\u6548\u6027\uff0c\u4e3a\u70ed\u6210\u50cfSLAM\u63d0\u4f9b\u4e86\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u6076\u52a3\u73af\u5883\u4e0b\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2602.08274", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08274", "abs": "https://arxiv.org/abs/2602.08274", "authors": ["Jan Philip Wahle"], "title": "Language Modeling and Understanding Through Paraphrase Generation and Detection", "comment": "PhD dissertation, University of G\u00f6ttingen Germany, 2025. 182 pages", "summary": "Language enables humans to share knowledge, reason about the world, and pass on strategies for survival and innovation across generations. At the heart of this process is not just the ability to communicate but also the remarkable flexibility in how we can express ourselves. We can express the same thoughts in virtually infinite ways using different words and structures - this ability to rephrase and reformulate expressions is known as paraphrase. Modeling paraphrases is a keystone to meaning in computational language models; being able to construct different variations of texts that convey the same meaning or not shows strong abilities of semantic understanding. If computational language models are to represent meaning, they must understand and control the different aspects that construct the same meaning as opposed to different meanings at a fine granularity. Yet most existing approaches reduce paraphrasing to a binary decision between two texts or to producing a single rewrite of a source, obscuring which linguistic factors are responsible for meaning preservation. In this thesis, I propose that decomposing paraphrases into their constituent linguistic aspects (paraphrase types) offers a more fine-grained and cognitively grounded view of semantic equivalence. I show that even advanced machine learning models struggle with this task. Yet, when explicitly trained on paraphrase types, models achieve stronger performance on related paraphrase tasks and downstream applications. For example, in plagiarism detection, language models trained on paraphrase types surpass human baselines: 89.6% accuracy compared to 78.4% for plagiarism cases from Wikipedia, and 66.5% compared to 55.7% for plagiarism of scientific papers from arXiv. In identifying duplicate questions on Quora, models trained with paraphrase types improve over models trained on binary pairs. Furthermore, I demonstrate that...", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u5c06\u91ca\u4e49\u5206\u89e3\u4e3a\u6784\u6210\u6027\u8bed\u8a00\u65b9\u9762\uff08\u91ca\u4e49\u7c7b\u578b\uff09\uff0c\u4e3a\u8bed\u4e49\u7b49\u4ef7\u63d0\u4f9b\u66f4\u7ec6\u7c92\u5ea6\u7684\u8ba4\u77e5\u57fa\u7840\u89c6\u89d2\uff0c\u5e76\u8bc1\u660e\u57fa\u4e8e\u91ca\u4e49\u7c7b\u578b\u8bad\u7ec3\u7684\u6a21\u578b\u5728\u76f8\u5173\u4efb\u52a1\u548c\u4e0b\u6e38\u5e94\u7528\u4e2d\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5c06\u91ca\u4e49\u7b80\u5316\u4e3a\u4e24\u4e2a\u6587\u672c\u4e4b\u95f4\u7684\u4e8c\u5143\u51b3\u7b56\u6216\u5355\u4e00\u91cd\u5199\uff0c\u63a9\u76d6\u4e86\u54ea\u4e9b\u8bed\u8a00\u56e0\u7d20\u8d1f\u8d23\u610f\u4e49\u4fdd\u7559\u3002\u9700\u8981\u66f4\u7ec6\u7c92\u5ea6\u7684\u8bed\u4e49\u7b49\u4ef7\u7406\u89e3\uff0c\u4f7f\u8ba1\u7b97\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u7406\u89e3\u548c\u63a7\u5236\u6784\u6210\u76f8\u540c\u610f\u4e49\u7684\u4e0d\u540c\u65b9\u9762\u3002", "method": "\u63d0\u51fa\u5c06\u91ca\u4e49\u5206\u89e3\u4e3a\u6784\u6210\u6027\u8bed\u8a00\u65b9\u9762\uff08\u91ca\u4e49\u7c7b\u578b\uff09\u7684\u65b9\u6cd5\uff0c\u4e3a\u8bed\u4e49\u7b49\u4ef7\u63d0\u4f9b\u66f4\u7ec6\u7c92\u5ea6\u548c\u8ba4\u77e5\u57fa\u7840\u7684\u89c6\u89d2\u3002\u8bad\u7ec3\u6a21\u578b\u8bc6\u522b\u8fd9\u4e9b\u91ca\u4e49\u7c7b\u578b\uff0c\u800c\u4e0d\u662f\u7b80\u5355\u7684\u4e8c\u5143\u5206\u7c7b\u3002", "result": "\u57fa\u4e8e\u91ca\u4e49\u7c7b\u578b\u8bad\u7ec3\u7684\u6a21\u578b\u5728\u76f8\u5173\u4efb\u52a1\u4e2d\u8868\u73b0\u66f4\u5f3a\uff1a\u5728\u6284\u88ad\u68c0\u6d4b\u4e2d\u8d85\u8d8a\u4eba\u7c7b\u57fa\u7ebf\uff08\u7ef4\u57fa\u767e\u79d1\u6848\u4f8b89.6% vs 78.4%\uff0carXiv\u79d1\u5b66\u8bba\u6587\u6848\u4f8b66.5% vs 55.7%\uff09\uff1b\u5728Quora\u91cd\u590d\u95ee\u9898\u8bc6\u522b\u4e2d\u4f18\u4e8e\u57fa\u4e8e\u4e8c\u5143\u5bf9\u8bad\u7ec3\u7684\u6a21\u578b\u3002", "conclusion": "\u5206\u89e3\u91ca\u4e49\u4e3a\u6784\u6210\u6027\u8bed\u8a00\u65b9\u9762\uff08\u91ca\u4e49\u7c7b\u578b\uff09\u63d0\u4f9b\u4e86\u66f4\u7ec6\u7c92\u5ea6\u7684\u8bed\u4e49\u7b49\u4ef7\u89c6\u89d2\uff0c\u80fd\u663e\u8457\u63d0\u5347\u6a21\u578b\u5728\u76f8\u5173\u4efb\u52a1\u548c\u4e0b\u6e38\u5e94\u7528\u4e2d\u7684\u6027\u80fd\uff0c\u5373\u4f7f\u5148\u8fdb\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u672a\u4e13\u95e8\u8bad\u7ec3\u65f6\u4e5f\u96be\u4ee5\u5b8c\u6210\u6b64\u4efb\u52a1\u3002"}}
{"id": "2602.07495", "categories": ["cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2602.07495", "abs": "https://arxiv.org/abs/2602.07495", "authors": ["Jiawen Zheng", "Haonan Jia", "Ming Li", "Yuhui Zheng", "Yufeng Zeng", "Yang Gao", "Chen Liang"], "title": "Learning Brain Representation with Hierarchical Visual Embeddings", "comment": null, "summary": "Decoding visual representations from brain signals has attracted significant attention in both neuroscience and artificial intelligence. However, the degree to which brain signals truly encode visual information remains unclear. Current visual decoding approaches explore various brain-image alignment strategies, yet most emphasize high-level semantic features while neglecting pixel-level details, thereby limiting our understanding of the human visual system. In this paper, we propose a brain-image alignment strategy that leverages multiple pre-trained visual encoders with distinct inductive biases to capture hierarchical and multi-scale visual representations, while employing a contrastive learning objective to achieve effective alignment between brain signals and visual embeddings. Furthermore, we introduce a Fusion Prior, which learns a stable mapping on large-scale visual data and subsequently matches brain features to this pre-trained prior, thereby enhancing distributional consistency across modalities. Extensive quantitative and qualitative experiments demonstrate that our method achieves a favorable balance between retrieval accuracy and reconstruction fidelity.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u591a\u9884\u8bad\u7ec3\u89c6\u89c9\u7f16\u7801\u5668\u548c\u5bf9\u6bd4\u5b66\u4e60\u7684\u5927\u8111-\u56fe\u50cf\u5bf9\u9f50\u7b56\u7565\uff0c\u901a\u8fc7\u878d\u5408\u5148\u9a8c\u589e\u5f3a\u8de8\u6a21\u6001\u4e00\u81f4\u6027\uff0c\u5728\u89c6\u89c9\u89e3\u7801\u4e2d\u5b9e\u73b0\u4e86\u68c0\u7d22\u51c6\u786e\u6027\u548c\u91cd\u5efa\u4fdd\u771f\u5ea6\u7684\u826f\u597d\u5e73\u8861\u3002", "motivation": "\u5f53\u524d\u89c6\u89c9\u89e3\u7801\u65b9\u6cd5\u5927\u591a\u5173\u6ce8\u9ad8\u5c42\u8bed\u4e49\u7279\u5f81\u800c\u5ffd\u7565\u50cf\u7d20\u7ea7\u7ec6\u8282\uff0c\u9650\u5236\u4e86\u6211\u4eec\u5bf9\u4eba\u7c7b\u89c6\u89c9\u7cfb\u7edf\u7684\u7406\u89e3\u3002\u5927\u8111\u4fe1\u53f7\u662f\u5426\u771f\u6b63\u7f16\u7801\u89c6\u89c9\u4fe1\u606f\u4ee5\u53ca\u7f16\u7801\u7a0b\u5ea6\u4ecd\u4e0d\u6e05\u695a\u3002", "method": "1. \u4f7f\u7528\u591a\u4e2a\u5177\u6709\u4e0d\u540c\u5f52\u7eb3\u504f\u7f6e\u7684\u9884\u8bad\u7ec3\u89c6\u89c9\u7f16\u7801\u5668\u6355\u83b7\u5206\u5c42\u591a\u5c3a\u5ea6\u89c6\u89c9\u8868\u793a\uff1b2. \u91c7\u7528\u5bf9\u6bd4\u5b66\u4e60\u76ee\u6807\u5b9e\u73b0\u5927\u8111\u4fe1\u53f7\u4e0e\u89c6\u89c9\u5d4c\u5165\u7684\u6709\u6548\u5bf9\u9f50\uff1b3. \u5f15\u5165\u878d\u5408\u5148\u9a8c\uff0c\u5728\u5927\u89c4\u6a21\u89c6\u89c9\u6570\u636e\u4e0a\u5b66\u4e60\u7a33\u5b9a\u6620\u5c04\uff0c\u7136\u540e\u5c06\u5927\u8111\u7279\u5f81\u5339\u914d\u5230\u8fd9\u4e2a\u9884\u8bad\u7ec3\u5148\u9a8c\uff0c\u589e\u5f3a\u8de8\u6a21\u6001\u5206\u5e03\u4e00\u81f4\u6027\u3002", "result": "\u5927\u91cf\u5b9a\u91cf\u548c\u5b9a\u6027\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u68c0\u7d22\u51c6\u786e\u6027\u548c\u91cd\u5efa\u4fdd\u771f\u5ea6\u4e4b\u95f4\u53d6\u5f97\u4e86\u826f\u597d\u7684\u5e73\u8861\u3002", "conclusion": "\u63d0\u51fa\u7684\u591a\u7f16\u7801\u5668\u5927\u8111-\u56fe\u50cf\u5bf9\u9f50\u7b56\u7565\u548c\u878d\u5408\u5148\u9a8c\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u89e3\u7801\u5927\u8111\u4e2d\u7684\u89c6\u89c9\u4fe1\u606f\uff0c\u4e3a\u7406\u89e3\u4eba\u7c7b\u89c6\u89c9\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\u3002"}}
{"id": "2602.08061", "categories": ["cs.AI", "q-bio.OT"], "pdf": "https://arxiv.org/pdf/2602.08061", "abs": "https://arxiv.org/abs/2602.08061", "authors": ["Doni Bloomfield", "Allison Berke", "Moritz S. Hanke", "Aaron Maiwald", "James R. M. Black", "Toby Webster", "Tina Hernandez-Boussard", "Oliver M. Crook", "Jassi Pannu"], "title": "Securing Dual-Use Pathogen Data of Concern", "comment": "39th Conference on Neural Information Processing Systems (NeurIPS 2025) Workshop: Biosecurity Safeguards for Generative AI", "summary": "Training data is an essential input into creating competent artificial intelligence (AI) models. AI models for biology are trained on large volumes of data, including data related to biological sequences, structures, images, and functions. The type of data used to train a model is intimately tied to the capabilities it ultimately possesses--including those of biosecurity concern. For this reason, an international group of more than 100 researchers at the recent 50th anniversary Asilomar Conference endorsed data controls to prevent the use of AI for harmful applications such as bioweapons development. To help design such controls, we introduce a five-tier Biosecurity Data Level (BDL) framework for categorizing pathogen data. Each level contains specific data types, based on their expected ability to contribute to capabilities of concern when used to train AI models. For each BDL tier, we propose technical restrictions appropriate to its level of risk. Finally, we outline a novel governance framework for newly created dual-use pathogen data. In a world with widely accessible computational and coding resources, data controls may be among the most high-leverage interventions available to reduce the proliferation of concerning biological AI capabilities.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u4e94\u7ea7\u751f\u7269\u5b89\u5168\u6570\u636e\u6846\u67b6\uff08BDL\uff09\uff0c\u7528\u4e8e\u6839\u636e\u75c5\u539f\u4f53\u6570\u636e\u5728\u8bad\u7ec3AI\u6a21\u578b\u65f6\u53ef\u80fd\u5e26\u6765\u7684\u751f\u7269\u5b89\u5168\u98ce\u9669\u8fdb\u884c\u5206\u7c7b\uff0c\u5e76\u4e3a\u6bcf\u4e2a\u7ea7\u522b\u63d0\u51fa\u76f8\u5e94\u7684\u6280\u672f\u9650\u5236\u63aa\u65bd\uff0c\u65e8\u5728\u901a\u8fc7\u6570\u636e\u63a7\u5236\u51cf\u5c11\u751f\u7269\u6b66\u5668\u5f00\u53d1\u7b49\u6709\u5bb3AI\u5e94\u7528\u7684\u98ce\u9669\u3002", "motivation": "\u968f\u7740AI\u5728\u751f\u7269\u5b66\u9886\u57df\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u8bad\u7ec3AI\u6a21\u578b\u6240\u4f7f\u7528\u7684\u751f\u7269\u6570\u636e\uff08\u5982\u5e8f\u5217\u3001\u7ed3\u6784\u3001\u56fe\u50cf\u3001\u529f\u80fd\u6570\u636e\uff09\u4e0e\u6a21\u578b\u6700\u7ec8\u80fd\u529b\u5bc6\u5207\u76f8\u5173\uff0c\u5305\u62ec\u53ef\u80fd\u5e26\u6765\u751f\u7269\u5b89\u5168\u98ce\u9669\u7684\u80fd\u529b\u3002\u56fd\u9645\u7814\u7a76\u56e2\u4f53\u5df2\u8ba4\u53ef\u901a\u8fc7\u6570\u636e\u63a7\u5236\u6765\u9632\u6b62AI\u88ab\u7528\u4e8e\u6709\u5bb3\u5e94\u7528\uff08\u5982\u751f\u7269\u6b66\u5668\u5f00\u53d1\uff09\uff0c\u56e0\u6b64\u9700\u8981\u5efa\u7acb\u4e00\u4e2a\u7cfb\u7edf\u5316\u7684\u6570\u636e\u5206\u7c7b\u548c\u63a7\u5236\u6846\u67b6\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u4e94\u7ea7\u751f\u7269\u5b89\u5168\u6570\u636e\u7b49\u7ea7\uff08BDL\uff09\u6846\u67b6\uff0c\u6839\u636e\u4e0d\u540c\u7c7b\u578b\u75c5\u539f\u4f53\u6570\u636e\u5728\u8bad\u7ec3AI\u6a21\u578b\u65f6\u53ef\u80fd\u8d21\u732e\u7ed9\u6709\u5bb3\u80fd\u529b\u7684\u7a0b\u5ea6\u8fdb\u884c\u5206\u7c7b\u3002\u6bcf\u4e2a\u7ea7\u522b\u5305\u542b\u7279\u5b9a\u7684\u6570\u636e\u7c7b\u578b\uff0c\u5e76\u9488\u5bf9\u5176\u98ce\u9669\u6c34\u5e73\u63d0\u51fa\u76f8\u5e94\u7684\u6280\u672f\u9650\u5236\u63aa\u65bd\u3002\u540c\u65f6\u4e3a\u65b0\u578b\u53cc\u91cd\u7528\u9014\u75c5\u539f\u4f53\u6570\u636e\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u6cbb\u7406\u6846\u67b6\u3002", "result": "\u5efa\u7acb\u4e86\u4e00\u4e2a\u7cfb\u7edf\u5316\u7684\u751f\u7269\u5b89\u5168\u6570\u636e\u5206\u7c7b\u4f53\u7cfb\uff0c\u5c06\u75c5\u539f\u4f53\u6570\u636e\u6309\u7167\u98ce\u9669\u7b49\u7ea7\u5206\u4e3a\u4e94\u4e2a\u5c42\u6b21\uff0c\u5e76\u4e3a\u6bcf\u4e2a\u5c42\u6b21\u5236\u5b9a\u4e86\u9002\u5f53\u7684\u6280\u672f\u63a7\u5236\u63aa\u65bd\u3002\u8be5\u6846\u67b6\u4e3a\u6570\u636e\u6cbb\u7406\u63d0\u4f9b\u4e86\u5177\u4f53\u7684\u6280\u672f\u65b9\u6848\uff0c\u7279\u522b\u662f\u5728\u5e7f\u6cdb\u53ef\u83b7\u53d6\u8ba1\u7b97\u548c\u7f16\u7801\u8d44\u6e90\u7684\u80cc\u666f\u4e0b\uff0c\u6570\u636e\u63a7\u5236\u6210\u4e3a\u51cf\u5c11\u6709\u5bb3\u751f\u7269AI\u80fd\u529b\u6269\u6563\u7684\u9ad8\u6760\u6746\u5e72\u9884\u624b\u6bb5\u3002", "conclusion": "\u5728\u8ba1\u7b97\u548c\u7f16\u7801\u8d44\u6e90\u5e7f\u6cdb\u53ef\u83b7\u53d6\u7684\u4e16\u754c\u4e2d\uff0c\u6570\u636e\u63a7\u5236\u53ef\u80fd\u662f\u51cf\u5c11\u6709\u5bb3\u751f\u7269AI\u80fd\u529b\u6269\u6563\u7684\u6700\u6709\u6548\u5e72\u9884\u624b\u6bb5\u4e4b\u4e00\u3002\u63d0\u51fa\u7684BDL\u6846\u67b6\u548c\u6cbb\u7406\u65b9\u6848\u4e3a\u5b9e\u65bd\u8fd9\u79cd\u63a7\u5236\u63d0\u4f9b\u4e86\u5177\u4f53\u7684\u6280\u672f\u548c\u6cbb\u7406\u57fa\u7840\uff0c\u6709\u52a9\u4e8e\u5e73\u8861AI\u5728\u751f\u7269\u5b66\u7814\u7a76\u4e2d\u7684\u521b\u65b0\u5e94\u7528\u4e0e\u751f\u7269\u5b89\u5168\u98ce\u9669\u9632\u63a7\u4e4b\u95f4\u7684\u5173\u7cfb\u3002"}}
{"id": "2602.08281", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.08281", "abs": "https://arxiv.org/abs/2602.08281", "authors": ["Zhilin Wang", "Yafu Li", "Shunkai Zhang", "Zhi Wang", "Haoran Zhang", "Xiaoye Qu", "Yu Cheng"], "title": "New Skills or Sharper Primitives? A Probabilistic Perspective on the Emergence of Reasoning in RLVR", "comment": "15 pages", "summary": "Whether Reinforcement Learning with Verifiable Rewards (RLVR) endows Large Language Models (LLMs) with new capabilities or merely elicits latent traces remains a central debate. In this work, we align with the former view, proposing a probabilistic framework where capability is defined by instance-level solvability. We hypothesize that the emergence of complex reasoning can be driven by sharpening atomic step probabilities, which enables models to overcome the exponential decay of success rates inherent in multi-step reasoning chains. Utilizing the Algebrarium framework, we train models exclusively on single-step operations and evaluate their performance on unseen multi-step tasks. Our empirical results confirm that: (1) RLVR incentivizes the exploration of previously inaccessible solution paths by amplifying the model's existing skills; (2) composite performance is strictly governed by the joint probability of atomic steps, evidenced by high Pearson correlation coefficients ($\u03c1\\in [0.69, 0.96]$); and (3) RLVR, acting as a global optimizer, can cause specific skills to be sacrificed to maximize aggregate reward. Our work offers a novel explanation for emergent abilities in RLVR, suggesting that the iterative optimization of solvable problems enables models to develop the capabilities to tackle previously unsolvable scenarios.", "AI": {"tldr": "RLVR\u901a\u8fc7\u4f18\u5316\u539f\u5b50\u6b65\u9aa4\u6982\u7387\u4f7fLLM\u83b7\u5f97\u65b0\u80fd\u529b\uff0c\u800c\u975e\u4ec5\u6fc0\u53d1\u6f5c\u5728\u80fd\u529b\uff0c\u8fd9\u4e3a\u89e3\u51b3\u591a\u6b65\u63a8\u7406\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u89e3\u91ca\u3002", "motivation": "\u6f84\u6e05RLVR\u662f\u5426\u8d4b\u4e88LLM\u65b0\u80fd\u529b\u8fd8\u662f\u4ec5\u6fc0\u53d1\u6f5c\u5728\u80fd\u529b\u8fd9\u4e00\u6838\u5fc3\u4e89\u8bae\uff0c\u4e3a\u6d8c\u73b0\u80fd\u529b\u63d0\u4f9b\u65b0\u7684\u7406\u8bba\u89e3\u91ca\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u5b9e\u4f8b\u7ea7\u53ef\u89e3\u6027\u7684\u6982\u7387\u6846\u67b6\uff0c\u5047\u8bbe\u901a\u8fc7\u9510\u5316\u539f\u5b50\u6b65\u9aa4\u6982\u7387\u9a71\u52a8\u590d\u6742\u63a8\u7406\uff1b\u4f7f\u7528Algebrarium\u6846\u67b6\uff0c\u5728\u5355\u6b65\u64cd\u4f5c\u4e0a\u8bad\u7ec3\u6a21\u578b\uff0c\u8bc4\u4f30\u672a\u89c1\u591a\u6b65\u4efb\u52a1\u6027\u80fd\u3002", "result": "(1) RLVR\u901a\u8fc7\u653e\u5927\u73b0\u6709\u6280\u80fd\u6fc0\u52b1\u63a2\u7d22\u65b0\u89e3\u8def\u5f84\uff1b(2) \u590d\u5408\u6027\u80fd\u4e25\u683c\u53d7\u539f\u5b50\u6b65\u9aa4\u8054\u5408\u6982\u7387\u63a7\u5236\uff08\u03c1\u2208[0.69,0.96]\uff09\uff1b(3) RLVR\u4f5c\u4e3a\u5168\u5c40\u4f18\u5316\u5668\u53ef\u80fd\u5bfc\u81f4\u7279\u5b9a\u6280\u80fd\u88ab\u727a\u7272\u4ee5\u6700\u5927\u5316\u603b\u5956\u52b1\u3002", "conclusion": "RLVR\u901a\u8fc7\u8fed\u4ee3\u4f18\u5316\u53ef\u89e3\u95ee\u9898\u4f7f\u6a21\u578b\u53d1\u5c55\u51fa\u89e3\u51b3\u5148\u524d\u4e0d\u53ef\u89e3\u573a\u666f\u7684\u80fd\u529b\uff0c\u4e3aRLVR\u4e2d\u7684\u6d8c\u73b0\u80fd\u529b\u63d0\u4f9b\u4e86\u65b0\u9896\u89e3\u91ca\u3002"}}
{"id": "2602.07498", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.07498", "abs": "https://arxiv.org/abs/2602.07498", "authors": ["Zhufeng Xu", "Xuan Gao", "Feng-Lin Liu", "Haoxian Zhang", "Zhixue Fang", "Yu-Kun Lai", "Xiaoqiang Liu", "Pengfei Wan", "Lin Gao"], "title": "IM-Animation: An Implicit Motion Representation for Identity-decoupled Character Animation", "comment": null, "summary": "Recent progress in video diffusion models has markedly advanced character animation, which synthesizes motioned videos by animating a static identity image according to a driving video. Explicit methods represent motion using skeleton, DWPose or other explicit structured signals, but struggle to handle spatial mismatches and varying body scales. %proportions. Implicit methods, on the other hand, capture high-level implicit motion semantics directly from the driving video, but suffer from identity leakage and entanglement between motion and appearance. To address the above challenges, we propose a novel implicit motion representation that compresses per-frame motion into compact 1D motion tokens. This design relaxes strict spatial constraints inherent in 2D representations and effectively prevents identity information leakage from the motion video. Furthermore, we design a temporally consistent mask token-based retargeting module that enforces a temporal training bottleneck, mitigating interference from the source images' motion and improving retargeting consistency. Our methodology employs a three-stage training strategy to enhance the training efficiency and ensure high fidelity. Extensive experiments demonstrate that our implicit motion representation and the propose IM-Animation's generative capabilities are achieve superior or competitive performance compared with state-of-the-art methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u9690\u5f0f\u8fd0\u52a8\u8868\u793a\u65b9\u6cd5\uff0c\u5c06\u6bcf\u5e27\u8fd0\u52a8\u538b\u7f29\u4e3a\u7d27\u51d1\u76841D\u8fd0\u52a8\u6807\u8bb0\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u663e\u5f0f\u65b9\u6cd5\u7684\u7a7a\u95f4\u4e0d\u5339\u914d\u95ee\u9898\u548c\u9690\u5f0f\u65b9\u6cd5\u7684\u8eab\u4efd\u6cc4\u6f0f\u95ee\u9898\uff0c\u901a\u8fc7\u4e09\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\u5b9e\u73b0\u4e86\u9ad8\u6548\u9ad8\u8d28\u91cf\u7684\u5b57\u7b26\u52a8\u753b\u751f\u6210\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u6269\u6563\u6a21\u578b\u5728\u5b57\u7b26\u52a8\u753b\u65b9\u9762\u5b58\u5728\u4e24\u5927\u6311\u6218\uff1a\u663e\u5f0f\u65b9\u6cd5\uff08\u5982\u9aa8\u67b6\u3001DWPose\uff09\u96be\u4ee5\u5904\u7406\u7a7a\u95f4\u4e0d\u5339\u914d\u548c\u8eab\u4f53\u6bd4\u4f8b\u53d8\u5316\uff1b\u9690\u5f0f\u65b9\u6cd5\u867d\u7136\u80fd\u6355\u6349\u9ad8\u5c42\u8fd0\u52a8\u8bed\u4e49\uff0c\u4f46\u5b58\u5728\u8eab\u4efd\u4fe1\u606f\u6cc4\u6f0f\u548c\u8fd0\u52a8\u4e0e\u5916\u89c2\u7ea0\u7f20\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u9690\u5f0f\u8fd0\u52a8\u8868\u793a\uff0c\u5c06\u6bcf\u5e27\u8fd0\u52a8\u538b\u7f29\u4e3a1D\u8fd0\u52a8\u6807\u8bb0\uff0c\u653e\u677e\u4e862D\u8868\u793a\u7684\u4e25\u683c\u7a7a\u95f4\u7ea6\u675f\uff1b\u8bbe\u8ba1\u57fa\u4e8e\u65f6\u95f4\u4e00\u81f4\u63a9\u7801\u6807\u8bb0\u7684\u91cd\u5b9a\u5411\u6a21\u5757\uff0c\u901a\u8fc7\u65f6\u95f4\u8bad\u7ec3\u74f6\u9888\u51cf\u5c11\u6e90\u56fe\u50cf\u8fd0\u52a8\u7684\u5e72\u6270\uff1b\u91c7\u7528\u4e09\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\u63d0\u9ad8\u8bad\u7ec3\u6548\u7387\u548c\u4fdd\u771f\u5ea6\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u63d0\u51fa\u7684\u9690\u5f0f\u8fd0\u52a8\u8868\u793a\u548cIM-Animation\u751f\u6210\u80fd\u529b\u5728\u6027\u80fd\u4e0a\u8fbe\u5230\u6216\u8d85\u8d8a\u4e86\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u8eab\u4efd\u6cc4\u6f0f\u95ee\u9898\u5e76\u63d0\u9ad8\u4e86\u91cd\u5b9a\u5411\u4e00\u81f4\u6027\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u9690\u5f0f\u8fd0\u52a8\u8868\u793a\u65b9\u6cd5\u901a\u8fc71D\u8fd0\u52a8\u6807\u8bb0\u538b\u7f29\u548c\u65f6\u95f4\u4e00\u81f4\u91cd\u5b9a\u5411\u6a21\u5757\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u5b57\u7b26\u52a8\u753b\u4e2d\u7684\u7a7a\u95f4\u7ea6\u675f\u548c\u8eab\u4efd\u6cc4\u6f0f\u95ee\u9898\uff0c\u4e3a\u9ad8\u8d28\u91cf\u89c6\u9891\u751f\u6210\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.07512", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.07512", "abs": "https://arxiv.org/abs/2602.07512", "authors": ["Tao Wang", "Chenyu Lin", "Chenwei Tang", "Jizhe Zhou", "Deng Xiong", "Jianan Li", "Jian Zhao", "Jiancheng Lv"], "title": "Adaptive Image Zoom-in with Bounding Box Transformation for UAV Object Detection", "comment": "paper accepted by ISPRS Journal of Photogrammetry and Remote Sensing ( IF=12.2)", "summary": "Detecting objects from UAV-captured images is challenging due to the small object size. In this work, a simple and efficient adaptive zoom-in framework is explored for object detection on UAV images. The main motivation is that the foreground objects are generally smaller and sparser than those in common scene images, which hinders the optimization of effective object detectors. We thus aim to zoom in adaptively on the objects to better capture object features for the detection task. To achieve the goal, two core designs are required: \\textcolor{black}{i) How to conduct non-uniform zooming on each image efficiently? ii) How to enable object detection training and inference with the zoomed image space?} Correspondingly, a lightweight offset prediction scheme coupled with a novel box-based zooming objective is introduced to learn non-uniform zooming on the input image. Based on the learned zooming transformation, a corner-aligned bounding box transformation method is proposed. The method warps the ground-truth bounding boxes to the zoomed space to learn object detection, and warps the predicted bounding boxes back to the original space during inference. We conduct extensive experiments on three representative UAV object detection datasets, including VisDrone, UAVDT, and SeaDronesSee. The proposed ZoomDet is architecture-independent and can be applied to an arbitrary object detection architecture. Remarkably, on the SeaDronesSee dataset, ZoomDet offers more than 8.4 absolute gain of mAP with a Faster R-CNN model, with only about 3 ms additional latency. The code is available at https://github.com/twangnh/zoomdet_code.", "AI": {"tldr": "ZoomDet\uff1a\u9488\u5bf9\u65e0\u4eba\u673a\u56fe\u50cf\u5c0f\u76ee\u6807\u68c0\u6d4b\u7684\u81ea\u9002\u5e94\u653e\u5927\u6846\u67b6\uff0c\u901a\u8fc7\u975e\u5747\u5300\u653e\u5927\u548c\u8fb9\u754c\u6846\u53d8\u6362\u63d0\u5347\u68c0\u6d4b\u6027\u80fd", "motivation": "\u65e0\u4eba\u673a\u56fe\u50cf\u4e2d\u7684\u524d\u666f\u7269\u4f53\u901a\u5e38\u6bd4\u666e\u901a\u573a\u666f\u56fe\u50cf\u4e2d\u7684\u7269\u4f53\u66f4\u5c0f\u3001\u66f4\u7a00\u758f\uff0c\u8fd9\u963b\u788d\u4e86\u6709\u6548\u7269\u4f53\u68c0\u6d4b\u5668\u7684\u4f18\u5316\u3002\u56e0\u6b64\u9700\u8981\u81ea\u9002\u5e94\u653e\u5927\u7269\u4f53\u4ee5\u66f4\u597d\u5730\u6355\u6349\u7269\u4f53\u7279\u5f81\u7528\u4e8e\u68c0\u6d4b\u4efb\u52a1\u3002", "method": "\u63d0\u51fa\u8f7b\u91cf\u7ea7\u504f\u79fb\u9884\u6d4b\u65b9\u6848\u7ed3\u5408\u65b0\u9896\u7684\u57fa\u4e8e\u8fb9\u754c\u6846\u7684\u653e\u5927\u76ee\u6807\uff0c\u5b66\u4e60\u8f93\u5165\u56fe\u50cf\u7684\u975e\u5747\u5300\u653e\u5927\u3002\u57fa\u4e8e\u5b66\u4e60\u7684\u653e\u5927\u53d8\u6362\uff0c\u63d0\u51fa\u89d2\u5bf9\u9f50\u7684\u8fb9\u754c\u6846\u53d8\u6362\u65b9\u6cd5\uff0c\u5c06\u771f\u5b9e\u8fb9\u754c\u6846\u53d8\u6362\u5230\u653e\u5927\u7a7a\u95f4\u8fdb\u884c\u8bad\u7ec3\uff0c\u63a8\u7406\u65f6\u5c06\u9884\u6d4b\u8fb9\u754c\u6846\u53d8\u6362\u56de\u539f\u59cb\u7a7a\u95f4\u3002", "result": "\u5728\u4e09\u4e2a\u4ee3\u8868\u6027\u65e0\u4eba\u673a\u7269\u4f53\u68c0\u6d4b\u6570\u636e\u96c6\uff08VisDrone\u3001UAVDT\u3001SeaDronesSee\uff09\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u5b9e\u9a8c\u3002\u5728SeaDronesSee\u6570\u636e\u96c6\u4e0a\uff0cZoomDet\u4f7f\u7528Faster R-CNN\u6a21\u578b\u83b7\u5f97\u4e86\u8d85\u8fc78.4\u4e2a\u7edd\u5bf9mAP\u589e\u76ca\uff0c\u4ec5\u589e\u52a0\u7ea63ms\u5ef6\u8fdf\u3002", "conclusion": "ZoomDet\u662f\u67b6\u6784\u65e0\u5173\u7684\uff0c\u53ef\u5e94\u7528\u4e8e\u4efb\u610f\u7269\u4f53\u68c0\u6d4b\u67b6\u6784\uff0c\u80fd\u6709\u6548\u63d0\u5347\u65e0\u4eba\u673a\u56fe\u50cf\u4e2d\u5c0f\u76ee\u6807\u7684\u68c0\u6d4b\u6027\u80fd\uff0c\u8ba1\u7b97\u5f00\u9500\u5c0f\u3002"}}
{"id": "2602.08294", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.08294", "abs": "https://arxiv.org/abs/2602.08294", "authors": ["Dingzirui Wang", "Xuanliang Zhang", "Keyan Xu", "Qingfu Zhu", "Wanxiang Che", "Yang Deng"], "title": "When Does Context Help? Error Dynamics of Contextual Information in Large Language Models", "comment": null, "summary": "Contextual information at inference time, such as demonstrations, retrieved knowledge, or interaction history, can substantially improve large language models (LLMs) without parameter updates, yet its theoretical role remains poorly understood beyond specific settings such as in-context learning (ICL). We present a unified theoretical framework for analyzing the effect of arbitrary contextual information in Transformer-based LLMs. Our analysis characterizes contextual influence through output error dynamics. In a single-layer Transformer, we prove that the context-conditioned error vector decomposes additively into the baseline error vector and a contextual correction vector. This yields necessary geometric conditions for error reduction: the contextual correction must align with the negative baseline error and satisfy a norm constraint. We further show that the contextual correction norm admits an explicit upper bound determined by context-query relevance and complementarity. These results extend to multi-context and multi-layer Transformers. Experiments across ICL, retrieval-augmented generation, and memory evolution validate our theory and motivate a principled context selection strategy that improves performance by $0.6\\%$.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u7406\u8bba\u6846\u67b6\u6765\u5206\u6790Transformer\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u4efb\u610f\u4e0a\u4e0b\u6587\u4fe1\u606f\u7684\u5f71\u54cd\uff0c\u901a\u8fc7\u8f93\u51fa\u8bef\u5dee\u52a8\u6001\u6765\u8868\u5f81\u4e0a\u4e0b\u6587\u5f71\u54cd\uff0c\u5e76\u63a8\u5bfc\u51fa\u8bef\u5dee\u51cf\u5c11\u7684\u51e0\u4f55\u6761\u4ef6\u3002", "motivation": "\u5c3d\u7ba1\u63a8\u7406\u65f6\u7684\u4e0a\u4e0b\u6587\u4fe1\u606f\uff08\u5982\u6f14\u793a\u3001\u68c0\u7d22\u77e5\u8bc6\u6216\u4ea4\u4e92\u5386\u53f2\uff09\u53ef\u4ee5\u5728\u4e0d\u66f4\u65b0\u53c2\u6570\u7684\u60c5\u51b5\u4e0b\u663e\u8457\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6027\u80fd\uff0c\u4f46\u5176\u7406\u8bba\u4f5c\u7528\u5728\u7279\u5b9a\u8bbe\u7f6e\uff08\u5982\u4e0a\u4e0b\u6587\u5b66\u4e60\uff09\u4e4b\u5916\u4ecd\u7f3a\u4e4f\u6df1\u5165\u7406\u89e3\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u7406\u8bba\u6846\u67b6\u6765\u5206\u6790Transformer\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u4e0a\u4e0b\u6587\u4fe1\u606f\u5f71\u54cd\u3002\u5728\u5355\u5c42Transformer\u4e2d\uff0c\u8bc1\u660e\u4e86\u4e0a\u4e0b\u6587\u6761\u4ef6\u8bef\u5dee\u5411\u91cf\u53ef\u52a0\u6027\u5206\u89e3\u4e3a\u57fa\u7ebf\u8bef\u5dee\u5411\u91cf\u548c\u4e0a\u4e0b\u6587\u4fee\u6b63\u5411\u91cf\uff0c\u5e76\u63a8\u5bfc\u51fa\u8bef\u5dee\u51cf\u5c11\u7684\u51e0\u4f55\u6761\u4ef6\u3002\u8fdb\u4e00\u6b65\u5c55\u793a\u4e86\u4e0a\u4e0b\u6587\u4fee\u6b63\u8303\u6570\u5b58\u5728\u7531\u4e0a\u4e0b\u6587-\u67e5\u8be2\u76f8\u5173\u6027\u548c\u4e92\u8865\u6027\u51b3\u5b9a\u7684\u663e\u5f0f\u4e0a\u754c\uff0c\u5e76\u5c06\u7ed3\u679c\u6269\u5c55\u5230\u591a\u4e0a\u4e0b\u6587\u548c\u591a\u5c42Transformer\u3002", "result": "\u5b9e\u9a8c\u5728\u4e0a\u4e0b\u6587\u5b66\u4e60\u3001\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u548c\u8bb0\u5fc6\u6f14\u5316\u7b49\u4efb\u52a1\u4e0a\u9a8c\u8bc1\u4e86\u7406\u8bba\uff0c\u5e76\u57fa\u4e8e\u7406\u8bba\u63d0\u51fa\u4e86\u4e00\u79cd\u539f\u5219\u6027\u7684\u4e0a\u4e0b\u6587\u9009\u62e9\u7b56\u7565\uff0c\u5c06\u6027\u80fd\u63d0\u5347\u4e860.6%\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u7406\u89e3\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u4e0a\u4e0b\u6587\u4fe1\u606f\u7684\u4f5c\u7528\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684\u7406\u8bba\u6846\u67b6\uff0c\u63ed\u793a\u4e86\u8bef\u5dee\u51cf\u5c11\u7684\u51e0\u4f55\u6761\u4ef6\uff0c\u5e76\u63d0\u51fa\u4e86\u6709\u6548\u7684\u4e0a\u4e0b\u6587\u9009\u62e9\u7b56\u7565\uff0c\u5bf9\u63d0\u5347\u6a21\u578b\u6027\u80fd\u5177\u6709\u7406\u8bba\u548c\u5b9e\u8df5\u610f\u4e49\u3002"}}
{"id": "2602.08305", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.08305", "abs": "https://arxiv.org/abs/2602.08305", "authors": ["Binglin Wu", "Yingyi Zhang", "Xiannneg Li"], "title": "JUSTICE: Judicial Unified Synthesis Through Intermediate Conclusion Emulation for Automated Judgment Document Generation", "comment": null, "summary": "Automated judgment document generation is a significant yet challenging legal AI task. As the conclusive written instrument issued by a court, a judgment document embodies complex legal reasoning. However, existing methods often oversimplify this complex process, particularly by omitting the ``Pre-Judge'' phase, a crucial step where human judges form a preliminary conclusion. This omission leads to two core challenges: 1) the ineffective acquisition of foundational judicial elements, and 2) the inadequate modeling of the Pre-Judge process, which collectively undermine the final document's legal soundness. To address these challenges, we propose \\textit{\\textbf{J}udicial \\textbf{U}nified \\textbf{S}ynthesis \\textbf{T}hrough \\textbf{I}ntermediate \\textbf{C}onclusion \\textbf{E}mulation} (JUSTICE), a novel framework that emulates the ``Search $\\rightarrow$ Pre-Judge $\\rightarrow$ Write'' cognitive workflow of human judges. Specifically, it introduces the Pre-Judge stage through three dedicated components: Referential Judicial Element Retriever (RJER), Intermediate Conclusion Emulator (ICE), and Judicial Unified Synthesizer (JUS). RJER first retrieves legal articles and a precedent case to establish a referential foundation. ICE then operationalizes the Pre-Judge phase by generating a verifiable intermediate conclusion. Finally, JUS synthesizes these inputs to craft the final judgment. Experiments on both an in-domain legal benchmark and an out-of-distribution dataset show that JUSTICE significantly outperforms strong baselines, with substantial gains in legal accuracy, including a 4.6\\% improvement in prison term prediction. Our findings underscore the importance of explicitly modeling the Pre-Judge process to enhance the legal coherence and accuracy of generated judgment documents.", "AI": {"tldr": "JUSTICE\u6846\u67b6\u901a\u8fc7\u6a21\u62df\u6cd5\u5b98\"\u641c\u7d22\u2192\u9884\u5224\u2192\u64b0\u5199\"\u7684\u8ba4\u77e5\u6d41\u7a0b\uff0c\u5f15\u5165\u9884\u5224\u9636\u6bb5\u6765\u63d0\u5347\u6cd5\u5f8b\u5224\u51b3\u6587\u4e66\u751f\u6210\u7684\u51c6\u786e\u6027\u548c\u6cd5\u5f8b\u5408\u7406\u6027\u3002", "motivation": "\u73b0\u6709\u6cd5\u5f8b\u5224\u51b3\u6587\u4e66\u751f\u6210\u65b9\u6cd5\u8fc7\u4e8e\u7b80\u5316\u590d\u6742\u6cd5\u5f8b\u63a8\u7406\u8fc7\u7a0b\uff0c\u7279\u522b\u662f\u5ffd\u7565\u4e86\u6cd5\u5b98\u5f62\u6210\u521d\u6b65\u7ed3\u8bba\u7684\"\u9884\u5224\"\u9636\u6bb5\uff0c\u5bfc\u81f4\u4e24\u4e2a\u6838\u5fc3\u95ee\u9898\uff1a1) \u57fa\u7840\u53f8\u6cd5\u8981\u7d20\u83b7\u53d6\u4e0d\u8db3\uff1b2) \u9884\u5224\u8fc7\u7a0b\u5efa\u6a21\u4e0d\u5145\u5206\uff0c\u4ece\u800c\u5f71\u54cd\u6700\u7ec8\u6587\u4e66\u7684\u5408\u6cd5\u6027\u3002", "method": "\u63d0\u51faJUSTICE\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a1) \u53c2\u8003\u6027\u53f8\u6cd5\u8981\u7d20\u68c0\u7d22\u5668(RJER)\uff1a\u68c0\u7d22\u76f8\u5173\u6cd5\u5f8b\u6761\u6587\u548c\u5148\u4f8b\u6848\u4f8b\uff1b2) \u4e2d\u95f4\u7ed3\u8bba\u6a21\u62df\u5668(ICE)\uff1a\u751f\u6210\u53ef\u9a8c\u8bc1\u7684\u4e2d\u95f4\u7ed3\u8bba\uff0c\u5b9e\u73b0\u9884\u5224\u9636\u6bb5\uff1b3) \u53f8\u6cd5\u7edf\u4e00\u5408\u6210\u5668(JUS)\uff1a\u7efc\u5408\u6240\u6709\u8f93\u5165\u751f\u6210\u6700\u7ec8\u5224\u51b3\u6587\u4e66\u3002", "result": "\u5728\u9886\u57df\u5185\u6cd5\u5f8b\u57fa\u51c6\u6d4b\u8bd5\u548c\u5206\u5e03\u5916\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cJUSTICE\u663e\u8457\u4f18\u4e8e\u5f3a\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5728\u6cd5\u5f8b\u51c6\u786e\u6027\u65b9\u9762\u6709\u5b9e\u8d28\u6027\u63d0\u5347\uff0c\u5305\u62ec\u5211\u671f\u9884\u6d4b\u51c6\u786e\u7387\u63d0\u9ad84.6%\u3002", "conclusion": "\u660e\u786e\u5efa\u6a21\u9884\u5224\u8fc7\u7a0b\u5bf9\u4e8e\u589e\u5f3a\u751f\u6210\u5224\u51b3\u6587\u4e66\u7684\u5408\u6cd5\u8fde\u8d2f\u6027\u548c\u51c6\u786e\u6027\u81f3\u5173\u91cd\u8981\uff0cJUSTICE\u6846\u67b6\u901a\u8fc7\u6a21\u62df\u4eba\u7c7b\u6cd5\u5b98\u7684\u8ba4\u77e5\u5de5\u4f5c\u6d41\u7a0b\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2602.07532", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.07532", "abs": "https://arxiv.org/abs/2602.07532", "authors": ["Krishnakant Singh", "Simone Schaub-Meyer", "Stefan Roth"], "title": "Evaluating Object-Centric Models beyond Object Discovery", "comment": "Project Page: https://guided-sa.github.io/eval-ocl/", "summary": "Object-centric learning (OCL) aims to learn structured scene representations that support compositional generalization and robustness to out-of-distribution (OOD) data. However, OCL models are often not evaluated regarding these goals. Instead, most prior work focuses on evaluating OCL models solely through object discovery and simple reasoning tasks, such as probing the representation via image classification. We identify two limitations in existing benchmarks: (1) They provide limited insights on the representation usefulness of OCL models, and (2) localization and representation usefulness are assessed using disjoint metrics. To address (1), we use instruction-tuned VLMs as evaluators, enabling scalable benchmarking across diverse VQA datasets to measure how well VLMs leverage OCL representations for complex reasoning tasks. To address (2), we introduce a unified evaluation task and metric that jointly assess localization (where) and representation usefulness (what), thereby eliminating inconsistencies introduced by disjoint evaluation. Finally, we include a simple multi-feature reconstruction baseline as a reference point.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u8bc4\u4f30\u5bf9\u8c61\u4e2d\u5fc3\u5b66\u4e60\u6a21\u578b\u7684\u65b0\u6846\u67b6\uff0c\u4f7f\u7528\u6307\u4ee4\u8c03\u4f18\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u8bc4\u4f30\u5668\uff0c\u5e76\u5f15\u5165\u7edf\u4e00\u7684\u8bc4\u4f30\u4efb\u52a1\u548c\u6307\u6807\u6765\u8054\u5408\u8bc4\u4f30\u5b9a\u4f4d\u80fd\u529b\u548c\u8868\u793a\u6709\u7528\u6027\u3002", "motivation": "\u73b0\u6709\u5bf9\u8c61\u4e2d\u5fc3\u5b66\u4e60\u6a21\u578b\u7684\u8bc4\u4f30\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a1) \u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u5bf9\u6a21\u578b\u8868\u793a\u6709\u7528\u6027\u63d0\u4f9b\u6709\u9650\u6d1e\u5bdf\uff1b2) \u5b9a\u4f4d\u80fd\u529b\u548c\u8868\u793a\u6709\u7528\u6027\u4f7f\u7528\u5206\u79bb\u7684\u6307\u6807\u8bc4\u4f30\uff0c\u5bfc\u81f4\u4e0d\u4e00\u81f4\u6027\u3002", "method": "\u4f7f\u7528\u6307\u4ee4\u8c03\u4f18\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u8bc4\u4f30\u5668\uff0c\u5728\u591a\u6837\u5316\u7684\u89c6\u89c9\u95ee\u7b54\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u53ef\u6269\u5c55\u7684\u57fa\u51c6\u6d4b\u8bd5\uff1b\u5f15\u5165\u7edf\u4e00\u7684\u8bc4\u4f30\u4efb\u52a1\u548c\u6307\u6807\uff0c\u8054\u5408\u8bc4\u4f30\u5b9a\u4f4d\u80fd\u529b\u548c\u8868\u793a\u6709\u7528\u6027\uff1b\u5305\u542b\u4e00\u4e2a\u7b80\u5355\u7684\u591a\u7279\u5f81\u91cd\u5efa\u57fa\u7ebf\u4f5c\u4e3a\u53c2\u8003\u70b9\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u66f4\u5168\u9762\u5730\u8bc4\u4f30\u5bf9\u8c61\u4e2d\u5fc3\u5b66\u4e60\u6a21\u578b\u7684\u8868\u793a\u6709\u7528\u6027\uff0c\u901a\u8fc7\u7edf\u4e00\u7684\u8bc4\u4f30\u6846\u67b6\u6d88\u9664\u5206\u79bb\u8bc4\u4f30\u5e26\u6765\u7684\u4e0d\u4e00\u81f4\u6027\uff0c\u4e3a\u6a21\u578b\u6027\u80fd\u63d0\u4f9b\u66f4\u51c6\u786e\u7684\u8861\u91cf\u3002", "conclusion": "\u63d0\u51fa\u7684\u8bc4\u4f30\u6846\u67b6\u89e3\u51b3\u4e86\u73b0\u6709\u5bf9\u8c61\u4e2d\u5fc3\u5b66\u4e60\u6a21\u578b\u8bc4\u4f30\u7684\u5c40\u9650\u6027\uff0c\u901a\u8fc7\u4f7f\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u8bc4\u4f30\u5668\u548c\u7edf\u4e00\u7684\u8bc4\u4f30\u6307\u6807\uff0c\u80fd\u591f\u66f4\u597d\u5730\u8861\u91cf\u6a21\u578b\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u8868\u793a\u6709\u7528\u6027\u548c\u5b9a\u4f4d\u80fd\u529b\u3002"}}
{"id": "2602.08321", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.08321", "abs": "https://arxiv.org/abs/2602.08321", "authors": ["Zijie Chen", "Zhenghao Lin", "Xiao Liu", "Zhenzhong Lan", "Yeyun Gong", "Peng Cheng"], "title": "Improving Data and Reward Design for Scientific Reasoning in Large Language Models", "comment": null, "summary": "Solving open-ended science questions remains challenging for large language models, particularly due to inherently unreliable supervision and evaluation. The bottleneck lies in the data construction and reward design for scientific post-training. We develop a large-scale, systematic data processing pipeline that transforms heterogeneous open-source science data into Dr. SCI dataset, which comprises of 1M questions across eight STEM subjects, with explicit verifiable/open-ended splits, scalable difficulty annotation, and fine-grained rubrics that operationalize evaluation for open-ended answers. Building on this dataset, we propose the Dr. SCI post-training pipeline, which redesigns the standard SFT -> RL workflow through three components: (i) Exploration-Expanding SFT, which broadens the model's reasoning pattern coverage prior to RL; (ii) Dynamic Difficulty Curriculum, which adapts training data to the model's evolving scientific capability; and (iii) SciRubric-Guided RL, which enables stable reinforcement learning on open-ended scientific questions via rubric-based evaluation with explicit answer correctness. Qwen3-4B-Base trained using Dr.SCI pipeline achieves 63.2 on GPQA-diamond and 32.4 on GPQA-general, consistently improves over strong post-trained baselines such as o1-mini and GPT-4o, demonstrating substantial gains in scientific reasoning, especially in open-ended settings.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faDr. SCI\u6570\u636e\u96c6\u548c\u8bad\u7ec3\u6d41\u7a0b\uff0c\u7528\u4e8e\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5f00\u653e\u5f0f\u79d1\u5b66\u95ee\u9898\u4e0a\u7684\u8868\u73b0\uff0c\u901a\u8fc7\u7cfb\u7edf\u5316\u6570\u636e\u5904\u7406\u3001\u52a8\u6001\u96be\u5ea6\u8bfe\u7a0b\u548c\u57fa\u4e8e\u8bc4\u5206\u6807\u51c6\u7684\u5f3a\u5316\u5b66\u4e60\uff0c\u663e\u8457\u63d0\u5347\u4e86\u79d1\u5b66\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5f00\u653e\u5f0f\u79d1\u5b66\u95ee\u9898\u4e0a\u7684\u6311\u6218\uff0c\u4e3b\u8981\u74f6\u9888\u5728\u4e8e\u79d1\u5b66\u540e\u8bad\u7ec3\u7684\u6570\u636e\u6784\u5efa\u548c\u5956\u52b1\u8bbe\u8ba1\u3002\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u76d1\u7763\u4e0d\u53ef\u9760\u3001\u8bc4\u4f30\u56f0\u96be\u7684\u95ee\u9898\uff0c\u9700\u8981\u7cfb\u7edf\u5316\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "1) \u6784\u5efaDr. SCI\u6570\u636e\u96c6\uff1a\u5305\u542b100\u4e07\u95ee\u9898\uff0c\u8986\u76d68\u4e2aSTEM\u5b66\u79d1\uff0c\u6709\u660e\u786e\u7684\u53ef\u9a8c\u8bc1/\u5f00\u653e\u5f0f\u5212\u5206\u3001\u53ef\u6269\u5c55\u7684\u96be\u5ea6\u6807\u6ce8\u548c\u7ec6\u7c92\u5ea6\u8bc4\u5206\u6807\u51c6\uff1b2) Dr. SCI\u540e\u8bad\u7ec3\u6d41\u7a0b\uff1a\u5305\u62ec\u63a2\u7d22\u6269\u5c55\u7684SFT\uff08\u6269\u5927\u63a8\u7406\u6a21\u5f0f\u8986\u76d6\uff09\u3001\u52a8\u6001\u96be\u5ea6\u8bfe\u7a0b\uff08\u6839\u636e\u6a21\u578b\u80fd\u529b\u8c03\u6574\u8bad\u7ec3\u6570\u636e\uff09\u3001\u57fa\u4e8e\u79d1\u5b66\u8bc4\u5206\u6807\u51c6\u7684RL\uff08\u901a\u8fc7\u8bc4\u5206\u6807\u51c6\u5b9e\u73b0\u5f00\u653e\u5f0f\u95ee\u9898\u7684\u7a33\u5b9a\u5f3a\u5316\u5b66\u4e60\uff09\u3002", "result": "\u4f7f\u7528Dr. SCI\u6d41\u7a0b\u8bad\u7ec3\u7684Qwen3-4B-Base\u6a21\u578b\u5728GPQA-diamond\u4e0a\u8fbe\u523063.2\u5206\uff0c\u5728GPQA-general\u4e0a\u8fbe\u523032.4\u5206\uff0c\u6301\u7eed\u4f18\u4e8eo1-mini\u548cGPT-4o\u7b49\u5f3a\u57fa\u7ebf\uff0c\u5728\u79d1\u5b66\u63a8\u7406\u7279\u522b\u662f\u5f00\u653e\u5f0f\u8bbe\u7f6e\u4e2d\u53d6\u5f97\u663e\u8457\u63d0\u5347\u3002", "conclusion": "Dr. SCI\u6570\u636e\u96c6\u548c\u8bad\u7ec3\u6d41\u7a0b\u6709\u6548\u89e3\u51b3\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5f00\u653e\u5f0f\u79d1\u5b66\u95ee\u9898\u4e0a\u7684\u6311\u6218\uff0c\u901a\u8fc7\u7cfb\u7edf\u5316\u7684\u6570\u636e\u6784\u5efa\u548c\u521b\u65b0\u7684\u8bad\u7ec3\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u7684\u79d1\u5b66\u63a8\u7406\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u5f00\u653e\u5f0f\u95ee\u9898\u56de\u7b54\u65b9\u9762\u3002"}}
{"id": "2602.07534", "categories": ["cs.CV", "cs.AI", "eess.IV"], "pdf": "https://arxiv.org/pdf/2602.07534", "abs": "https://arxiv.org/abs/2602.07534", "authors": ["Mowmita Parvin Hera", "Md. Shahriar Mahmud Kallol", "Shohanur Rahman Nirob", "Md. Badsha Bulbul", "Jubayer Ahmed", "M. Zhourul Islam", "Hazrat Ali", "Mohammmad Farhad Bulbul"], "title": "Fine-Grained Cat Breed Recognition with Global Context Vision Transformer", "comment": "4 pages, accepted at International Conference on Computer and Information Technology (ICCIT) 2025", "summary": "Accurate identification of cat breeds from images is a challenging task due to subtle differences in fur patterns, facial structure, and color. In this paper, we present a deep learning-based approach for classifying cat breeds using a subset of the Oxford-IIIT Pet Dataset, which contains high-resolution images of various domestic breeds. We employed the Global Context Vision Transformer (GCViT) architecture-tiny for cat breed recognition. To improve model generalization, we used extensive data augmentation, including rotation, horizontal flipping, and brightness adjustment. Experimental results show that the GCViT-Tiny model achieved a test accuracy of 92.00% and validation accuracy of 94.54%. These findings highlight the effectiveness of transformer-based architectures for fine-grained image classification tasks. Potential applications include veterinary diagnostics, animal shelter management, and mobile-based breed recognition systems. We also provide a hugging face demo at https://huggingface.co/spaces/bfarhad/cat-breed-classifier.", "AI": {"tldr": "\u4f7f\u7528GCViT-Tiny\u67b6\u6784\u5728\u725b\u6d25-IIIT\u5ba0\u7269\u6570\u636e\u96c6\u5b50\u96c6\u4e0a\u5b9e\u73b0\u732b\u54c1\u79cd\u5206\u7c7b\uff0c\u901a\u8fc7\u6570\u636e\u589e\u5f3a\u8fbe\u523092%\u6d4b\u8bd5\u51c6\u786e\u7387", "motivation": "\u732b\u54c1\u79cd\u8bc6\u522b\u5177\u6709\u6311\u6218\u6027\uff0c\u56e0\u4e3a\u4e0d\u540c\u54c1\u79cd\u5728\u6bdb\u76ae\u56fe\u6848\u3001\u9762\u90e8\u7ed3\u6784\u548c\u989c\u8272\u4e0a\u5dee\u5f02\u7ec6\u5fae\u3002\u9700\u8981\u51c6\u786e\u7684\u65b9\u6cd5\u6765\u652f\u6301\u517d\u533b\u8bca\u65ad\u3001\u52a8\u7269\u6536\u5bb9\u6240\u7ba1\u7406\u548c\u79fb\u52a8\u7aef\u8bc6\u522b\u7cfb\u7edf\u7b49\u5e94\u7528\u3002", "method": "\u91c7\u7528\u5168\u5c40\u4e0a\u4e0b\u6587\u89c6\u89c9\u53d8\u6362\u5668(GCViT)\u67b6\u6784\u7684Tiny\u7248\u672c\u8fdb\u884c\u732b\u54c1\u79cd\u8bc6\u522b\uff0c\u4f7f\u7528\u725b\u6d25-IIIT\u5ba0\u7269\u6570\u636e\u96c6\u7684\u5b50\u96c6\uff0c\u901a\u8fc7\u65cb\u8f6c\u3001\u6c34\u5e73\u7ffb\u8f6c\u548c\u4eae\u5ea6\u8c03\u6574\u7b49\u6570\u636e\u589e\u5f3a\u6280\u672f\u63d0\u9ad8\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u3002", "result": "GCViT-Tiny\u6a21\u578b\u5728\u6d4b\u8bd5\u96c6\u4e0a\u8fbe\u523092.00%\u7684\u51c6\u786e\u7387\uff0c\u9a8c\u8bc1\u96c6\u4e0a\u8fbe\u523094.54%\u7684\u51c6\u786e\u7387\uff0c\u8bc1\u660e\u4e86\u57fa\u4e8e\u53d8\u6362\u5668\u7684\u67b6\u6784\u5728\u7ec6\u7c92\u5ea6\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\u53d8\u6362\u5668\u67b6\u6784\u5728\u732b\u54c1\u79cd\u5206\u7c7b\u7b49\u7ec6\u7c92\u5ea6\u8bc6\u522b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\uff0c\u5e76\u63d0\u4f9b\u4e86Hugging Face\u6f14\u793a\u5e73\u53f0\u4f9b\u5b9e\u9645\u4f7f\u7528\u3002"}}
{"id": "2602.08222", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08222", "abs": "https://arxiv.org/abs/2602.08222", "authors": ["Zehao Chen", "Gongxun Li", "Tianxiang Ai", "Yifei Li", "Zixuan Huang", "Wang Zhou", "Fuzhen Zhuang", "Xianglong Liu", "Jianxin Li", "Deqing Wang", "Yikun Ban"], "title": "Weak-Driven Learning: How Weak Agents make Strong Agents Stronger", "comment": null, "summary": "As post-training optimization becomes central to improving large language models, we observe a persistent saturation bottleneck: once models grow highly confident, further training yields diminishing returns. While existing methods continue to reinforce target predictions, we find that informative supervision signals remain latent in models' own historical weak states. Motivated by this observation, we propose WMSS (Weak Agents Can Make Strong Agents Stronger), a post-training paradigm that leverages weak checkpoints to guide continued optimization. By identifying recoverable learning gaps via entropy dynamics and reinforcing them through compensatory learning, WMSS enables strong agents to improve beyond conventional post-training saturation. Experiments on mathematical reasoning and code generation datasets show that agents trained with our approach achieve effective performance improvements, while incurring zero additional inference cost.", "AI": {"tldr": "WMSS\u662f\u4e00\u79cd\u540e\u8bad\u7ec3\u4f18\u5316\u65b9\u6cd5\uff0c\u5229\u7528\u6a21\u578b\u81ea\u8eab\u5386\u53f2\u5f31\u68c0\u67e5\u70b9\u6765\u6307\u5bfc\u7ee7\u7eed\u4f18\u5316\uff0c\u901a\u8fc7\u71b5\u52a8\u6001\u8bc6\u522b\u53ef\u6062\u590d\u7684\u5b66\u4e60\u5dee\u8ddd\u5e76\u8fdb\u884c\u8865\u507f\u5b66\u4e60\uff0c\u7a81\u7834\u540e\u8bad\u7ec3\u9971\u548c\u74f6\u9888\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u540e\u8bad\u7ec3\u4e2d\u5b58\u5728\u6301\u7eed\u9971\u548c\u74f6\u9888\uff1a\u6a21\u578b\u53d8\u5f97\u9ad8\u5ea6\u81ea\u4fe1\u540e\uff0c\u8fdb\u4e00\u6b65\u8bad\u7ec3\u6536\u76ca\u9012\u51cf\u3002\u73b0\u6709\u65b9\u6cd5\u7ee7\u7eed\u5f3a\u5316\u76ee\u6807\u9884\u6d4b\uff0c\u4f46\u7814\u7a76\u53d1\u73b0\u4fe1\u606f\u4e30\u5bcc\u7684\u76d1\u7763\u4fe1\u53f7\u4ecd\u6f5c\u85cf\u5728\u6a21\u578b\u81ea\u8eab\u7684\u5386\u53f2\u5f31\u72b6\u6001\u4e2d\u3002", "method": "\u63d0\u51faWMSS\u540e\u8bad\u7ec3\u8303\u5f0f\uff0c\u5229\u7528\u5f31\u68c0\u67e5\u70b9\u6307\u5bfc\u7ee7\u7eed\u4f18\u5316\u3002\u901a\u8fc7\u71b5\u52a8\u6001\u8bc6\u522b\u53ef\u6062\u590d\u7684\u5b66\u4e60\u5dee\u8ddd\uff0c\u5e76\u901a\u8fc7\u8865\u507f\u5b66\u4e60\u5f3a\u5316\u8fd9\u4e9b\u5dee\u8ddd\uff0c\u4f7f\u5f3a\u667a\u80fd\u4f53\u8d85\u8d8a\u4f20\u7edf\u540e\u8bad\u7ec3\u9971\u548c\u3002", "result": "\u5728\u6570\u5b66\u63a8\u7406\u548c\u4ee3\u7801\u751f\u6210\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u4f7f\u7528WMSS\u65b9\u6cd5\u8bad\u7ec3\u7684\u667a\u80fd\u4f53\u5b9e\u73b0\u4e86\u6709\u6548\u7684\u6027\u80fd\u63d0\u5347\uff0c\u540c\u65f6\u4e0d\u4ea7\u751f\u989d\u5916\u7684\u63a8\u7406\u6210\u672c\u3002", "conclusion": "WMSS\u901a\u8fc7\u5229\u7528\u6a21\u578b\u81ea\u8eab\u5386\u53f2\u5f31\u72b6\u6001\u4e2d\u7684\u76d1\u7763\u4fe1\u53f7\uff0c\u6210\u529f\u7a81\u7834\u4e86\u540e\u8bad\u7ec3\u9971\u548c\u74f6\u9888\uff0c\u4e3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u6301\u7eed\u4f18\u5316\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2602.08322", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.08322", "abs": "https://arxiv.org/abs/2602.08322", "authors": ["Wei Zhu"], "title": "An Attention-over-Attention Generative Model for Joint Multiple Intent Detection and Slot Filling", "comment": null, "summary": "In task-oriented dialogue systems, spoken language understanding (SLU) is a critical component, which consists of two sub-tasks, intent detection and slot filling. Most existing methods focus on the single-intent SLU, where each utterance only has one intent. However, in real-world scenarios users usually express multiple intents in an utterance, which poses a challenge for existing dialogue systems and datasets. In this paper, we propose a generative framework to simultaneously address multiple intent detection and slot filling. In particular, an attention-over-attention decoder is proposed to handle the variable number of intents and the interference between the two sub-tasks by incorporating an inductive bias into the process of multi-task learning. Besides, we construct two new multi-intent SLU datasets based on single-intent utterances by taking advantage of the next sentence prediction (NSP) head of the BERT model. Experimental results demonstrate that our proposed attention-over-attention generative model achieves state-of-the-art performance on two public datasets, MixATIS and MixSNIPS, and our constructed datasets.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6ce8\u610f\u529b\u673a\u5236\u7684\u751f\u6210\u5f0f\u6846\u67b6\uff0c\u7528\u4e8e\u540c\u65f6\u5904\u7406\u591a\u610f\u56fe\u68c0\u6d4b\u548c\u69fd\u586b\u5145\u4efb\u52a1\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u5bf9\u8bdd\u7cfb\u7edf\u4e2d\u591a\u610f\u56fe\u573a\u666f\u7684\u6311\u6218\u3002", "motivation": "\u73b0\u5b9e\u5bf9\u8bdd\u573a\u666f\u4e2d\u7528\u6237\u901a\u5e38\u5728\u4e00\u4e2a\u8bdd\u8bed\u4e2d\u8868\u8fbe\u591a\u4e2a\u610f\u56fe\uff0c\u800c\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u5355\u610f\u56fe\u573a\u666f\uff0c\u591a\u610f\u56feSLU\u9762\u4e34\u6570\u636e\u96c6\u7f3a\u4e4f\u548c\u4efb\u52a1\u5e72\u6270\u7684\u6311\u6218\u3002", "method": "\u63d0\u51fa\u751f\u6210\u5f0f\u6846\u67b6\uff0c\u91c7\u7528\u6ce8\u610f\u529b\u53e0\u52a0\u89e3\u7801\u5668\u5904\u7406\u53ef\u53d8\u6570\u91cf\u610f\u56fe\uff0c\u901a\u8fc7\u5f15\u5165\u5f52\u7eb3\u504f\u7f6e\u89e3\u51b3\u591a\u4efb\u52a1\u5b66\u4e60\u4e2d\u7684\u5e72\u6270\u95ee\u9898\uff1b\u5229\u7528BERT\u7684NSP\u5934\u6784\u5efa\u65b0\u7684\u591a\u610f\u56fe\u6570\u636e\u96c6\u3002", "result": "\u5728MixATIS\u548cMixSNIPS\u4e24\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4ee5\u53ca\u81ea\u5efa\u6570\u636e\u96c6\u4e0a\u5747\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u8868\u73b0\u3002", "conclusion": "\u63d0\u51fa\u7684\u6ce8\u610f\u529b\u53e0\u52a0\u6ce8\u610f\u529b\u751f\u6210\u6a21\u578b\u6709\u6548\u89e3\u51b3\u4e86\u591a\u610f\u56feSLU\u95ee\u9898\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u89e3\u7801\u5668\u8bbe\u8ba1\u548c\u6570\u636e\u96c6\u6784\u5efa\u65b9\u6cd5\u63d0\u5347\u4e86\u4efb\u52a1\u6027\u80fd\u3002"}}
{"id": "2602.07535", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07535", "abs": "https://arxiv.org/abs/2602.07535", "authors": ["Md Sazidur Rahman", "Kjersti Engan", "Kathinka D\u00e6hli Kurz", "Mahdieh Khanmohammadi"], "title": "Beyond Core and Penumbra: Bi-Temporal Image-Driven Stroke Evolution Analysis", "comment": null, "summary": "Computed tomography perfusion (CTP) at admission is routinely used to estimate the ischemic core and penumbra, while follow-up diffusion-weighted MRI (DWI) provides the definitive infarct outcome. However, single time-point segmentations fail to capture the biological heterogeneity and temporal evolution of stroke. We propose a bi-temporal analysis framework that characterizes ischemic tissue using statistical descriptors, radiomic texture features, and deep feature embeddings from two architectures (mJ-Net and nnU-Net). Bi-temporal refers to admission (T1) and post-treatment follow-up (T2). All features are extracted at T1 from CTP, with follow-up DWI aligned to ensure spatial correspondence. Manually delineated masks at T1 and T2 are intersected to construct six regions of interest (ROIs) encoding both initial tissue state and final outcome. Features were aggregated per region and analyzed in feature space. Evaluation on 18 patients with successful reperfusion demonstrated meaningful clustering of region-level representations. Regions classified as penumbra or healthy at T1 that ultimately recovered exhibited feature similarity to preserved brain tissue, whereas infarct-bound regions formed distinct groupings. Both baseline GLCM and deep embeddings showed a similar trend: penumbra regions exhibit features that are significantly different depending on final state, whereas this difference is not significant for core regions. Deep feature spaces, particularly mJ-Net, showed strong separation between salvageable and non-salvageable tissue, with a penumbra separation index that differed significantly from zero (Wilcoxon signed-rank test). These findings suggest that encoder-derived feature manifolds reflect underlying tissue phenotypes and state transitions, providing insight into imaging-based quantification of stroke evolution.", "AI": {"tldr": "\u63d0\u51fa\u53cc\u65f6\u76f8\u5206\u6790\u6846\u67b6\uff0c\u4f7f\u7528\u7edf\u8ba1\u63cf\u8ff0\u7b26\u3001\u5f71\u50cf\u7ec4\u5b66\u7eb9\u7406\u7279\u5f81\u548c\u6df1\u5ea6\u7279\u5f81\u5d4c\u5165\u6765\u8868\u5f81\u7f3a\u8840\u7ec4\u7ec7\uff0c\u901a\u8fc7\u7279\u5f81\u7a7a\u95f4\u5206\u6790\u63ed\u793a\u5352\u4e2d\u6f14\u53d8\u7684\u7ec4\u7ec7\u8868\u578b", "motivation": "\u4f20\u7edf\u5355\u65f6\u95f4\u70b9\u5206\u5272\u65e0\u6cd5\u6355\u6349\u5352\u4e2d\u7684\u751f\u7269\u5b66\u5f02\u8d28\u6027\u548c\u65f6\u95f4\u6f14\u53d8\uff0c\u9700\u8981\u66f4\u7cbe\u7ec6\u7684\u65b9\u6cd5\u6765\u8868\u5f81\u7f3a\u8840\u7ec4\u7ec7\u7684\u72b6\u6001\u53d8\u5316\u548c\u6700\u7ec8\u7ed3\u5c40", "method": "\u4f7f\u7528\u53cc\u65f6\u76f8\u5206\u6790\u6846\u67b6\uff08\u5165\u9662T1\u548c\u968f\u8bbfT2\uff09\uff0c\u4eceCTP\u63d0\u53d6\u7edf\u8ba1\u63cf\u8ff0\u7b26\u3001\u7eb9\u7406\u7279\u5f81\u548c\u6df1\u5ea6\u7279\u5f81\u5d4c\u5165\uff08mJ-Net\u548cnnU-Net\uff09\uff0c\u6784\u5efa\u516d\u4e2aROI\u533a\u57df\uff0c\u5728\u7279\u5f81\u7a7a\u95f4\u4e2d\u8fdb\u884c\u805a\u7c7b\u5206\u6790", "result": "\u572818\u540d\u6210\u529f\u518d\u704c\u6ce8\u60a3\u8005\u4e2d\uff0c\u533a\u57df\u7ea7\u8868\u5f81\u663e\u793a\u51fa\u6709\u610f\u4e49\u7684\u805a\u7c7b\uff1a\u53ef\u633d\u6551\u7ec4\u7ec7\u4e0e\u4e0d\u53ef\u633d\u6551\u7ec4\u7ec7\u5728\u7279\u5f81\u7a7a\u95f4\u4e2d\u660e\u663e\u5206\u79bb\uff0c\u7279\u522b\u662fmJ-Net\u6df1\u5ea6\u7279\u5f81\u7a7a\u95f4\u663e\u793a\u51fa\u663e\u8457\u7684\u7ec4\u7ec7\u5206\u79bb", "conclusion": "\u7f16\u7801\u5668\u884d\u751f\u7684\u7279\u5f81\u6d41\u5f62\u53cd\u6620\u4e86\u57fa\u7840\u7ec4\u7ec7\u8868\u578b\u548c\u72b6\u6001\u8f6c\u53d8\uff0c\u4e3a\u57fa\u4e8e\u5f71\u50cf\u7684\u5352\u4e2d\u6f14\u53d8\u91cf\u5316\u63d0\u4f9b\u4e86\u65b0\u7684\u89c1\u89e3\uff0c\u6df1\u5ea6\u7279\u5f81\u7a7a\u95f4\u80fd\u6709\u6548\u533a\u5206\u53ef\u633d\u6551\u4e0e\u4e0d\u53ef\u633d\u6551\u7ec4\u7ec7"}}
{"id": "2602.08332", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08332", "abs": "https://arxiv.org/abs/2602.08332", "authors": ["Ido Amos", "Avi Caciularu", "Mor Geva", "Amir Globerson", "Jonathan Herzig", "Lior Shani", "Idan Szpektor"], "title": "Latent Reasoning with Supervised Thinking States", "comment": null, "summary": "Reasoning with a chain-of-thought (CoT) enables Large Language Models (LLMs) to solve complex tasks but incurs significant inference costs due to the generation of long rationales. We propose Thinking States, a method that performs reasoning {\\em while} the input is processing. Specifically, Thinking States generates sequences of thinking tokens every few input tokens, transforms the thoughts back into embedding space, and adds them to the following input tokens. This has two key advantages. First, it captures the recurrent nature of CoT, but where the thought tokens are generated as input is processing. Second, since the thoughts are represented as tokens, they can be learned from natural language supervision, and using teacher-forcing, which is parallelizable. Empirically, Thinking States outperforms other latent reasoning methods on multiple reasoning tasks, narrowing the gap to CoT on math problems, and matching its performance on 2-Hop QA with improved latency. On state-tracking tasks, we show Thinking States leads to stronger reasoning behavior than CoT, successfully extrapolating to longer sequences than seen during training.", "AI": {"tldr": "Thinking States\u662f\u4e00\u79cd\u5728\u8f93\u5165\u5904\u7406\u8fc7\u7a0b\u4e2d\u8fdb\u884c\u63a8\u7406\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u751f\u6210\u601d\u8003\u6807\u8bb0\u5e76\u5d4c\u5165\u5230\u540e\u7eed\u8f93\u5165\u4e2d\uff0c\u51cf\u5c11\u63a8\u7406\u5ef6\u8fdf\uff0c\u63d0\u9ad8\u6548\u7387\u3002", "motivation": "\u94fe\u5f0f\u601d\u7ef4\u63a8\u7406\u867d\u7136\u80fd\u5e2e\u52a9\u5927\u8bed\u8a00\u6a21\u578b\u89e3\u51b3\u590d\u6742\u4efb\u52a1\uff0c\u4f46\u751f\u6210\u957f\u63a8\u7406\u8fc7\u7a0b\u4f1a\u5bfc\u81f4\u663e\u8457\u7684\u63a8\u7406\u6210\u672c\u3002\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u63a8\u7406\u65b9\u6cd5\u3002", "method": "\u5728\u8f93\u5165\u5904\u7406\u8fc7\u7a0b\u4e2d\u6bcf\u51e0\u4e2a\u8f93\u5165\u6807\u8bb0\u5c31\u751f\u6210\u4e00\u7cfb\u5217\u601d\u8003\u6807\u8bb0\uff0c\u5c06\u8fd9\u4e9b\u601d\u8003\u8f6c\u6362\u56de\u5d4c\u5165\u7a7a\u95f4\uff0c\u5e76\u6dfb\u52a0\u5230\u540e\u7eed\u8f93\u5165\u6807\u8bb0\u4e2d\u3002\u8fd9\u79cd\u65b9\u6cd5\u65e2\u80fd\u6355\u6349CoT\u7684\u5faa\u73af\u7279\u6027\uff0c\u53c8\u80fd\u5728\u8f93\u5165\u5904\u7406\u65f6\u751f\u6210\u601d\u8003\u3002", "result": "\u5728\u591a\u4e2a\u63a8\u7406\u4efb\u52a1\u4e0a\u4f18\u4e8e\u5176\u4ed6\u6f5c\u5728\u63a8\u7406\u65b9\u6cd5\uff0c\u5728\u6570\u5b66\u95ee\u9898\u4e0a\u7f29\u5c0f\u4e86\u4e0eCoT\u7684\u5dee\u8ddd\uff0c\u57282-Hop QA\u4e0a\u8fbe\u5230\u4e0eCoT\u76f8\u5f53\u7684\u6027\u80fd\u4e14\u5ef6\u8fdf\u66f4\u4f4e\u3002\u5728\u72b6\u6001\u8ddf\u8e2a\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u6bd4CoT\u66f4\u5f3a\u7684\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u80fd\u6210\u529f\u6cdb\u5316\u5230\u6bd4\u8bad\u7ec3\u65f6\u66f4\u957f\u7684\u5e8f\u5217\u3002", "conclusion": "Thinking States\u662f\u4e00\u79cd\u6709\u6548\u7684\u63a8\u7406\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u8f93\u5165\u5904\u7406\u8fc7\u7a0b\u4e2d\u8fdb\u884c\u5e76\u884c\u5316\u63a8\u7406\uff0c\u65e2\u4fdd\u6301\u4e86CoT\u7684\u4f18\u52bf\uff0c\u53c8\u663e\u8457\u964d\u4f4e\u4e86\u63a8\u7406\u5ef6\u8fdf\uff0c\u5177\u6709\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2602.07540", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.07540", "abs": "https://arxiv.org/abs/2602.07540", "authors": ["Huimin Yan", "Liang Bai", "Xian Yang", "Long Chen"], "title": "LLM-Guided Diagnostic Evidence Alignment for Medical Vision-Language Pretraining under Limited Pairing", "comment": null, "summary": "Most existing CLIP-style medical vision--language pretraining methods rely on global or local alignment with substantial paired data. However, global alignment is easily dominated by non-diagnostic information, while local alignment fails to integrate key diagnostic evidence. As a result, learning reliable diagnostic representations becomes difficult, which limits their applicability in medical scenarios with limited paired data. To address this issue, we propose an LLM-Guided Diagnostic Evidence Alignment method (LGDEA), which shifts the pretraining objective toward evidence-level alignment that is more consistent with the medical diagnostic process. Specifically, we leverage LLMs to extract key diagnostic evidence from radiology reports and construct a shared diagnostic evidence space, enabling evidence-aware cross-modal alignment and allowing LGDEA to effectively exploit abundant unpaired medical images and reports, thereby substantially alleviating the reliance on paired data. Extensive experimental results demonstrate that our method achieves consistent and significant improvements on phrase grounding, image--text retrieval, and zero-shot classification, and even rivals pretraining methods that rely on substantial paired data.", "AI": {"tldr": "LGDEA\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eLLM\u5f15\u5bfc\u7684\u8bca\u65ad\u8bc1\u636e\u5bf9\u9f50\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bc1\u636e\u7ea7\u5bf9\u9f50\u800c\u975e\u5168\u5c40\u6216\u5c40\u90e8\u5bf9\u9f50\uff0c\u51cf\u5c11\u5bf9\u914d\u5bf9\u6570\u636e\u7684\u4f9d\u8d56\uff0c\u5728\u533b\u5b66\u89c6\u89c9-\u8bed\u8a00\u9884\u8bad\u7ec3\u4e2d\u53d6\u5f97\u663e\u8457\u6539\u8fdb\u3002", "motivation": "\u73b0\u6709\u533b\u5b66\u89c6\u89c9-\u8bed\u8a00\u9884\u8bad\u7ec3\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff1a\u5168\u5c40\u5bf9\u9f50\u5bb9\u6613\u88ab\u975e\u8bca\u65ad\u4fe1\u606f\u4e3b\u5bfc\uff0c\u5c40\u90e8\u5bf9\u9f50\u96be\u4ee5\u6574\u5408\u5173\u952e\u8bca\u65ad\u8bc1\u636e\uff0c\u5bfc\u81f4\u5b66\u4e60\u53ef\u9760\u8bca\u65ad\u8868\u793a\u56f0\u96be\uff0c\u9650\u5236\u4e86\u5728\u914d\u5bf9\u6570\u636e\u6709\u9650\u573a\u666f\u4e0b\u7684\u5e94\u7528\u3002", "method": "\u63d0\u51faLLM\u5f15\u5bfc\u7684\u8bca\u65ad\u8bc1\u636e\u5bf9\u9f50\u65b9\u6cd5(LGDEA)\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u4ece\u653e\u5c04\u5b66\u62a5\u544a\u4e2d\u63d0\u53d6\u5173\u952e\u8bca\u65ad\u8bc1\u636e\uff0c\u6784\u5efa\u5171\u4eab\u8bca\u65ad\u8bc1\u636e\u7a7a\u95f4\uff0c\u5b9e\u73b0\u8bc1\u636e\u611f\u77e5\u7684\u8de8\u6a21\u6001\u5bf9\u9f50\uff0c\u6709\u6548\u5229\u7528\u5927\u91cf\u672a\u914d\u5bf9\u7684\u533b\u5b66\u56fe\u50cf\u548c\u62a5\u544a\u3002", "result": "\u5728\u77ed\u8bed\u5b9a\u4f4d\u3001\u56fe\u50cf-\u6587\u672c\u68c0\u7d22\u548c\u96f6\u6837\u672c\u5206\u7c7b\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e00\u81f4\u4e14\u663e\u8457\u7684\u6539\u8fdb\uff0c\u6027\u80fd\u751a\u81f3\u53ef\u4e0e\u4f9d\u8d56\u5927\u91cf\u914d\u5bf9\u6570\u636e\u7684\u9884\u8bad\u7ec3\u65b9\u6cd5\u76f8\u5ab2\u7f8e\u3002", "conclusion": "LGDEA\u901a\u8fc7\u8bc1\u636e\u7ea7\u5bf9\u9f50\u65b9\u6cd5\uff0c\u66f4\u7b26\u5408\u533b\u5b66\u8bca\u65ad\u8fc7\u7a0b\uff0c\u663e\u8457\u51cf\u8f7b\u4e86\u5bf9\u914d\u5bf9\u6570\u636e\u7684\u4f9d\u8d56\uff0c\u4e3a\u533b\u5b66\u89c6\u89c9-\u8bed\u8a00\u9884\u8bad\u7ec3\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.08336", "categories": ["cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.08336", "abs": "https://arxiv.org/abs/2602.08336", "authors": ["Cheng Yang", "Chufan Shi", "Bo Shui", "Yaokang Wu", "Muzi Tao", "Huijuan Wang", "Ivan Yee Lee", "Yong Liu", "Xuezhe Ma", "Taylor Berg-Kirkpatrick"], "title": "UReason: Benchmarking the Reasoning Paradox in Unified Multimodal Models", "comment": "Project page: https://ureason.github.io", "summary": "To elicit capabilities for addressing complex and implicit visual requirements, recent unified multimodal models increasingly adopt chain-of-thought reasoning to guide image generation. However, the actual effect of reasoning on visual synthesis remains unclear. We present UReason, a diagnostic benchmark for reasoning-driven image generation that evaluates whether reasoning can be faithfully executed in pixels. UReason contains 2,000 instances across five task families: Code, Arithmetic, Spatial, Attribute, and Text reasoning. To isolate the role of reasoning traces, we introduce an evaluation framework comparing direct generation, reasoning-guided generation, and de-contextualized generation which conditions only on the refined prompt. Across eight open-source unified models, we observe a consistent Reasoning Paradox: Reasoning traces generally improve performance over direct generation, yet retaining intermediate thoughts as conditioning context often hinders visual synthesis, and conditioning only on the refined prompt yields substantial gains. Our analysis suggests that the bottleneck lies in contextual interference rather than insufficient reasoning capacity. UReason provides a principled testbed for studying reasoning in unified models and motivates future methods that effectively integrate reasoning for visual generation while mitigating interference.", "AI": {"tldr": "UReason\u662f\u4e00\u4e2a\u8bca\u65ad\u6027\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u63a8\u7406\u5728\u56fe\u50cf\u751f\u6210\u4e2d\u7684\u5b9e\u9645\u6548\u679c\uff0c\u53d1\u73b0\u63a8\u7406\u75d5\u8ff9\u867d\u7136\u80fd\u63d0\u5347\u6027\u80fd\uff0c\u4f46\u4f5c\u4e3a\u4e0a\u4e0b\u6587\u6761\u4ef6\u53cd\u800c\u4f1a\u963b\u788d\u89c6\u89c9\u5408\u6210\uff0c\u5b58\u5728\"\u63a8\u7406\u6096\u8bba\"\u73b0\u8c61\u3002", "motivation": "\u5f53\u524d\u7edf\u4e00\u591a\u6a21\u6001\u6a21\u578b\u8d8a\u6765\u8d8a\u591a\u5730\u91c7\u7528\u601d\u7ef4\u94fe\u63a8\u7406\u6765\u6307\u5bfc\u56fe\u50cf\u751f\u6210\uff0c\u4f46\u63a8\u7406\u5bf9\u89c6\u89c9\u5408\u6210\u7684\u5b9e\u9645\u6548\u679c\u5c1a\u4e0d\u6e05\u695a\u3002\u9700\u8981\u8bc4\u4f30\u63a8\u7406\u80fd\u5426\u5728\u50cf\u7d20\u5c42\u9762\u5fe0\u5b9e\u6267\u884c\u3002", "method": "\u63d0\u51faUReason\u8bca\u65ad\u57fa\u51c6\uff0c\u5305\u542b2,000\u4e2a\u5b9e\u4f8b\uff0c\u6db5\u76d6\u4ee3\u7801\u3001\u7b97\u672f\u3001\u7a7a\u95f4\u3001\u5c5e\u6027\u548c\u6587\u672c\u63a8\u7406\u4e94\u4e2a\u4efb\u52a1\u65cf\u3002\u5f15\u5165\u8bc4\u4f30\u6846\u67b6\u6bd4\u8f83\u76f4\u63a5\u751f\u6210\u3001\u63a8\u7406\u5f15\u5bfc\u751f\u6210\u548c\u53bb\u4e0a\u4e0b\u6587\u751f\u6210\u4e09\u79cd\u65b9\u6cd5\u3002", "result": "\u5728\u516b\u4e2a\u5f00\u6e90\u7edf\u4e00\u6a21\u578b\u4e2d\u89c2\u5bdf\u5230\u4e00\u81f4\u7684\"\u63a8\u7406\u6096\u8bba\"\uff1a\u63a8\u7406\u75d5\u8ff9\u901a\u5e38\u6bd4\u76f4\u63a5\u751f\u6210\u63d0\u5347\u6027\u80fd\uff0c\u4f46\u5c06\u4e2d\u95f4\u601d\u60f3\u4f5c\u4e3a\u6761\u4ef6\u4e0a\u4e0b\u6587\u5f80\u5f80\u4f1a\u963b\u788d\u89c6\u89c9\u5408\u6210\uff0c\u800c\u4ec5\u57fa\u4e8e\u7cbe\u70bc\u63d0\u793a\u7684\u6761\u4ef6\u80fd\u5e26\u6765\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u74f6\u9888\u5728\u4e8e\u4e0a\u4e0b\u6587\u5e72\u6270\u800c\u975e\u63a8\u7406\u80fd\u529b\u4e0d\u8db3\u3002UReason\u4e3a\u7814\u7a76\u7edf\u4e00\u6a21\u578b\u4e2d\u7684\u63a8\u7406\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u6d4b\u8bd5\u5e73\u53f0\uff0c\u5e76\u6fc0\u52b1\u672a\u6765\u65b9\u6cd5\u5728\u6709\u6548\u6574\u5408\u63a8\u7406\u8fdb\u884c\u89c6\u89c9\u751f\u6210\u7684\u540c\u65f6\u51cf\u8f7b\u5e72\u6270\u3002"}}
{"id": "2602.07544", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.07544", "abs": "https://arxiv.org/abs/2602.07544", "authors": ["Sebastian Bock", "Leonie Sch\u00fc\u00dfler", "Krishnakant Singh", "Simone Schaub-Meyer", "Stefan Roth"], "title": "MUFASA: A Multi-Layer Framework for Slot Attention", "comment": "Authors Sebastian Bock and Leonie Sch\u00fc\u00dfler contributed equally. Project page: https://leonieschuessler.github.io/mufasa/", "summary": "Unsupervised object-centric learning (OCL) decomposes visual scenes into distinct entities. Slot attention is a popular approach that represents individual objects as latent vectors, called slots. Current methods obtain these slot representations solely from the last layer of a pre-trained vision transformer (ViT), ignoring valuable, semantically rich information encoded across the other layers. To better utilize this latent semantic information, we introduce MUFASA, a lightweight plug-and-play framework for slot attention-based approaches to unsupervised object segmentation. Our model computes slot attention across multiple feature layers of the ViT encoder, fully leveraging their semantic richness. We propose a fusion strategy to aggregate slots obtained on multiple layers into a unified object-centric representation. Integrating MUFASA into existing OCL methods improves their segmentation results across multiple datasets, setting a new state of the art while simultaneously improving training convergence with only minor inference overhead.", "AI": {"tldr": "MUFASA\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u5373\u63d2\u5373\u7528\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u591a\u5c42ViT\u7279\u5f81\u4e0a\u8fdb\u884c\u69fd\u6ce8\u610f\u529b\u8ba1\u7b97\uff0c\u63d0\u5347\u65e0\u76d1\u7763\u7269\u4f53\u4e2d\u5fc3\u5b66\u4e60\u7684\u5206\u5272\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u69fd\u6ce8\u610f\u529b\u7684\u65e0\u76d1\u7763\u7269\u4f53\u4e2d\u5fc3\u5b66\u4e60\u65b9\u6cd5\u4ec5\u4f7f\u7528\u9884\u8bad\u7ec3ViT\u7684\u6700\u540e\u4e00\u5c42\u7279\u5f81\uff0c\u5ffd\u7565\u4e86\u5176\u4ed6\u5c42\u5305\u542b\u7684\u4e30\u5bcc\u8bed\u4e49\u4fe1\u606f\uff0c\u5bfc\u81f4\u4fe1\u606f\u5229\u7528\u4e0d\u8db3\u3002", "method": "\u63d0\u51faMUFASA\u6846\u67b6\uff1a1\uff09\u5728ViT\u7f16\u7801\u5668\u7684\u591a\u4e2a\u7279\u5f81\u5c42\u4e0a\u8ba1\u7b97\u69fd\u6ce8\u610f\u529b\uff1b2\uff09\u63d0\u51fa\u878d\u5408\u7b56\u7565\u5c06\u591a\u5c42\u83b7\u5f97\u7684\u69fd\u805a\u5408\u6210\u7edf\u4e00\u7684\u7269\u4f53\u4e2d\u5fc3\u8868\u793a\uff1b3\uff09\u4f5c\u4e3a\u8f7b\u91cf\u7ea7\u63d2\u4ef6\u96c6\u6210\u5230\u73b0\u6709OCL\u65b9\u6cd5\u4e2d\u3002", "result": "\u5c06MUFASA\u96c6\u6210\u5230\u73b0\u6709OCL\u65b9\u6cd5\u4e2d\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u63d0\u5347\u4e86\u5206\u5272\u7ed3\u679c\uff0c\u8fbe\u5230\u4e86\u65b0\u7684SOTA\uff0c\u540c\u65f6\u6539\u5584\u4e86\u8bad\u7ec3\u6536\u655b\u6027\uff0c\u4ec5\u5e26\u6765\u8f7b\u5fae\u63a8\u7406\u5f00\u9500\u3002", "conclusion": "\u901a\u8fc7\u5145\u5206\u5229\u7528ViT\u591a\u5c42\u7279\u5f81\u7684\u8bed\u4e49\u4e30\u5bcc\u6027\uff0cMUFASA\u6846\u67b6\u6709\u6548\u63d0\u5347\u4e86\u65e0\u76d1\u7763\u7269\u4f53\u4e2d\u5fc3\u5b66\u4e60\u7684\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u591a\u5c42\u7279\u5f81\u878d\u5408\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2602.08241", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.08241", "abs": "https://arxiv.org/abs/2602.08241", "authors": ["Siqu Ou", "Tianrui Wan", "Zhiyuan Zhao", "Junyu Gao", "Xuelong Li"], "title": "Do MLLMs Really See It: Reinforcing Visual Attention in Multimodal LLMs", "comment": null, "summary": "While chain-of-thought (CoT) reasoning has substantially improved multimodal large language models (MLLMs) on complex reasoning tasks, existing approaches largely rely on long textual reasoning trajectories and provide limited mechanisms for learning stable visual attention policies. Our analysis shows that current MLLMs exhibit weak visual focus: early-stage visual misalignment is rarely corrected during subsequent reasoning, leading to error propagation and failed inferences. We argue that this limitation stems from inadequate credit assignment for visual attention during training. To address this issue, we propose SAYO, a visual reasoning model trained with a reinforcement learning (RL) framework that introduces a region-level visual attention-based reward. This reward explicitly aligns optimization signals with visually grounded reasoning steps, enabling the model to learn more reliable attention behaviors. Extensive experiments across multiple multimodal benchmarks demonstrate that SAYO consistently improves performance on diverse reasoning and perception tasks.", "AI": {"tldr": "SAYO\uff1a\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u5f15\u5165\u533a\u57df\u7ea7\u89c6\u89c9\u6ce8\u610f\u529b\u5956\u52b1\uff0c\u89e3\u51b3\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u89c6\u89c9\u6ce8\u610f\u529b\u4e0d\u7a33\u5b9a\u7684\u95ee\u9898\uff0c\u63d0\u5347\u590d\u6742\u63a8\u7406\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u4e3b\u8981\u4f9d\u8d56\u957f\u6587\u672c\u63a8\u7406\u8f68\u8ff9\uff0c\u4f46\u7f3a\u4e4f\u5b66\u4e60\u7a33\u5b9a\u89c6\u89c9\u6ce8\u610f\u529b\u7b56\u7565\u7684\u673a\u5236\u3002\u5206\u6790\u53d1\u73b0\u5f53\u524d\u6a21\u578b\u5b58\u5728\u89c6\u89c9\u805a\u7126\u5f31\u7684\u95ee\u9898\uff1a\u65e9\u671f\u89c6\u89c9\u5bf9\u9f50\u9519\u8bef\u5f88\u5c11\u5728\u540e\u7eed\u63a8\u7406\u4e2d\u5f97\u5230\u7ea0\u6b63\uff0c\u5bfc\u81f4\u9519\u8bef\u4f20\u64ad\u548c\u63a8\u7406\u5931\u8d25\u3002\u8fd9\u4e00\u5c40\u9650\u6e90\u4e8e\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u89c6\u89c9\u6ce8\u610f\u529b\u4fe1\u7528\u5206\u914d\u4e0d\u8db3\u3002", "method": "\u63d0\u51faSAYO\u6a21\u578b\uff0c\u91c7\u7528\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u8bad\u7ec3\uff0c\u5f15\u5165\u533a\u57df\u7ea7\u89c6\u89c9\u6ce8\u610f\u529b\u5956\u52b1\u3002\u8be5\u5956\u52b1\u660e\u786e\u5c06\u4f18\u5316\u4fe1\u53f7\u4e0e\u89c6\u89c9\u57fa\u7840\u63a8\u7406\u6b65\u9aa4\u5bf9\u9f50\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u5b66\u4e60\u66f4\u53ef\u9760\u7684\u6ce8\u610f\u529b\u884c\u4e3a\u3002", "result": "\u5728\u591a\u4e2a\u591a\u6a21\u6001\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cSAYO\u5728\u591a\u6837\u5316\u7684\u63a8\u7406\u548c\u611f\u77e5\u4efb\u52a1\u4e0a\u6301\u7eed\u63d0\u5347\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u4e2d\u7684\u533a\u57df\u7ea7\u89c6\u89c9\u6ce8\u610f\u529b\u5956\u52b1\uff0cSAYO\u80fd\u591f\u89e3\u51b3\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u89c6\u89c9\u6ce8\u610f\u529b\u4e0d\u7a33\u5b9a\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u590d\u6742\u63a8\u7406\u4efb\u52a1\u7684\u6027\u80fd\u8868\u73b0\u3002"}}
{"id": "2602.07550", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07550", "abs": "https://arxiv.org/abs/2602.07550", "authors": ["Hussni Mohd Zakir", "Eric Tatt Wei Ho"], "title": "Revealing the Semantic Selection Gap in DINOv3 through Training-Free Few-Shot Segmentation", "comment": "10 pages, 3 figures, 7 tables", "summary": "Recent self-supervised Vision Transformers (ViTs), such as DINOv3, provide rich feature representations for dense vision tasks. This study investigates the intrinsic few-shot semantic segmentation (FSS) capabilities of frozen DINOv3 features through a training-free baseline, FSSDINO, utilizing class-specific prototypes and Gram-matrix refinement. Our results across binary, multi-class, and cross-domain (CDFSS) benchmarks demonstrate that this minimal approach, applied to the final backbone layer, is highly competitive with specialized methods involving complex decoders or test-time adaptation. Crucially, we conduct an Oracle-guided layer analysis, identifying a significant performance gap between the standard last-layer features and globally optimal intermediate representations. We reveal a \"Safest vs. Optimal\" dilemma: while the Oracle proves higher performance is attainable, matching the results of compute-intensive adaptation methods, current unsupervised and support-guided selection metrics consistently yield lower performance than the last-layer baseline. This characterizes a \"Semantic Selection Gap\" in Foundation Models, a disconnect where traditional heuristics fail to reliably identify high-fidelity features. Our work establishes the \"Last-Layer\" as a deceptively strong baseline and provides a rigorous diagnostic of the latent semantic potentials in DINOv3.The code is publicly available at https://github.com/hussni0997/fssdino.", "AI": {"tldr": "DINOv3\u7279\u5f81\u5728\u5c11\u6837\u672c\u8bed\u4e49\u5206\u5272\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4ec5\u4f7f\u7528\u7c7b\u539f\u578b\u548cGram\u77e9\u9635\u4f18\u5316\u7684\u8bad\u7ec3\u514d\u8d39\u65b9\u6cd5FSSDINO\u5c31\u80fd\u4e0e\u590d\u6742\u65b9\u6cd5\u7ade\u4e89\uff0c\u4f46\u7814\u7a76\u53d1\u73b0\u5b58\u5728\"\u8bed\u4e49\u9009\u62e9\u9e3f\u6c9f\"\uff1a\u4f20\u7edf\u542f\u53d1\u5f0f\u65b9\u6cd5\u65e0\u6cd5\u53ef\u9760\u8bc6\u522b\u6700\u4f18\u4e2d\u95f4\u5c42\u7279\u5f81\u3002", "motivation": "\u7814\u7a76\u81ea\u76d1\u7763\u89c6\u89c9Transformer\uff08\u5982DINOv3\uff09\u5728\u5c11\u6837\u672c\u8bed\u4e49\u5206\u5272\u4efb\u52a1\u4e2d\u7684\u5185\u5728\u80fd\u529b\uff0c\u63a2\u7d22\u65e0\u9700\u8bad\u7ec3\u7684\u65b9\u6cd5\u80fd\u5426\u8fbe\u5230\u4e0e\u590d\u6742\u65b9\u6cd5\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u5e76\u5206\u6790\u7279\u5f81\u9009\u62e9\u4e2d\u5b58\u5728\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faFSSDINO\u65b9\u6cd5\uff1a\u4f7f\u7528\u51bb\u7ed3\u7684DINOv3\u7279\u5f81\uff0c\u901a\u8fc7\u7c7b\u7279\u5b9a\u539f\u578b\u548cGram\u77e9\u9635\u4f18\u5316\u8fdb\u884c\u8bad\u7ec3\u514d\u8d39\u7684\u5c11\u6837\u672c\u8bed\u4e49\u5206\u5272\u3002\u8fdb\u884cOracle\u5f15\u5bfc\u7684\u5c42\u5206\u6790\uff0c\u6bd4\u8f83\u4e0d\u540c\u5c42\u7279\u5f81\u7684\u8868\u73b0\u3002", "result": "FSSDINO\u5728\u4e8c\u5143\u3001\u591a\u7c7b\u548c\u8de8\u57df\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e0e\u9700\u8981\u590d\u6742\u89e3\u7801\u5668\u6216\u6d4b\u8bd5\u65f6\u9002\u5e94\u7684\u65b9\u6cd5\u7ade\u4e89\u3002\u53d1\u73b0\"\u6700\u540e\u5c42\"\u662f\u4e00\u4e2a\u5f3a\u5927\u7684\u57fa\u7ebf\uff0c\u4f46Oracle\u5206\u6790\u663e\u793a\u4e2d\u95f4\u5c42\u5b58\u5728\u66f4\u9ad8\u6027\u80fd\u6f5c\u529b\uff0c\u4f20\u7edf\u9009\u62e9\u6307\u6807\u65e0\u6cd5\u53ef\u9760\u8bc6\u522b\u8fd9\u4e9b\u7279\u5f81\u3002", "conclusion": "\u7814\u7a76\u786e\u7acb\u4e86\"\u6700\u540e\u5c42\"\u4f5c\u4e3a\u5c11\u6837\u672c\u8bed\u4e49\u5206\u5272\u7684\u5f3a\u5927\u57fa\u7ebf\uff0c\u63ed\u793a\u4e86DINOv3\u4e2d\u6f5c\u5728\u7684\u8bed\u4e49\u80fd\u529b\uff0c\u5e76\u8bca\u65ad\u51fa\"\u8bed\u4e49\u9009\u62e9\u9e3f\u6c9f\"\u95ee\u9898\uff1a\u4f20\u7edf\u542f\u53d1\u5f0f\u65b9\u6cd5\u65e0\u6cd5\u53ef\u9760\u8bc6\u522b\u6700\u4f18\u7279\u5f81\u5c42\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u65b9\u5411\u3002"}}
{"id": "2602.08253", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08253", "abs": "https://arxiv.org/abs/2602.08253", "authors": ["Baoyun Zhao", "He Wang", "Liang Zeng"], "title": "G-LNS: Generative Large Neighborhood Search for LLM-Based Automatic Heuristic Design", "comment": null, "summary": "While Large Language Models (LLMs) have recently shown promise in Automated Heuristic Design (AHD), existing approaches typically formulate AHD around constructive priority rules or parameterized local search guidance, thereby restricting the search space to fixed heuristic forms. Such designs offer limited capacity for structural exploration, making it difficult to escape deep local optima in complex Combinatorial Optimization Problems (COPs). In this work, we propose G-LNS, a generative evolutionary framework that extends LLM-based AHD to the automated design of Large Neighborhood Search (LNS) operators. Unlike prior methods that evolve heuristics in isolation, G-LNS leverages LLMs to co-evolve tightly coupled pairs of destroy and repair operators. A cooperative evaluation mechanism explicitly captures their interaction, enabling the discovery of complementary operator logic that jointly performs effective structural disruption and reconstruction. Extensive experiments on challenging COP benchmarks, such as Traveling Salesman Problems (TSP) and Capacitated Vehicle Routing Problems (CVRP), demonstrate that G-LNS significantly outperforms LLM-based AHD methods as well as strong classical solvers. The discovered heuristics not only achieve near-optimal solutions with reduced computational budgets but also exhibit robust generalization across diverse and unseen instance distributions.", "AI": {"tldr": "G-LNS\u662f\u4e00\u4e2a\u751f\u6210\u5f0f\u8fdb\u5316\u6846\u67b6\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u81ea\u52a8\u8bbe\u8ba1\u5927\u89c4\u6a21\u90bb\u57df\u641c\u7d22\u7b97\u5b50\uff0c\u901a\u8fc7\u534f\u540c\u8fdb\u5316\u7834\u574f\u548c\u4fee\u590d\u7b97\u5b50\u5bf9\uff0c\u5728\u7ec4\u5408\u4f18\u5316\u95ee\u9898\u4e0a\u8d85\u8d8a\u4e86\u73b0\u6709LLM\u542f\u53d1\u5f0f\u8bbe\u8ba1\u65b9\u6cd5\u548c\u7ecf\u5178\u6c42\u89e3\u5668\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u81ea\u52a8\u542f\u53d1\u5f0f\u8bbe\u8ba1\u65b9\u6cd5\u901a\u5e38\u5c40\u9650\u4e8e\u6784\u9020\u6027\u4f18\u5148\u7ea7\u89c4\u5219\u6216\u53c2\u6570\u5316\u5c40\u90e8\u641c\u7d22\u6307\u5bfc\uff0c\u9650\u5236\u4e86\u641c\u7d22\u7a7a\u95f4\u5230\u56fa\u5b9a\u7684\u542f\u53d1\u5f0f\u5f62\u5f0f\u3002\u8fd9\u79cd\u8bbe\u8ba1\u5728\u7ed3\u6784\u63a2\u7d22\u65b9\u9762\u80fd\u529b\u6709\u9650\uff0c\u96be\u4ee5\u5728\u590d\u6742\u7ec4\u5408\u4f18\u5316\u95ee\u9898\u4e2d\u9003\u79bb\u6df1\u5ea6\u5c40\u90e8\u6700\u4f18\u3002", "method": "\u63d0\u51faG-LNS\u751f\u6210\u5f0f\u8fdb\u5316\u6846\u67b6\uff0c\u6269\u5c55LLM-based AHD\u5230\u5927\u89c4\u6a21\u90bb\u57df\u641c\u7d22\u7b97\u5b50\u7684\u81ea\u52a8\u8bbe\u8ba1\u3002\u4e0d\u540c\u4e8e\u5148\u524d\u5b64\u7acb\u8fdb\u5316\u542f\u53d1\u5f0f\u7684\u65b9\u6cd5\uff0cG-LNS\u5229\u7528LLM\u534f\u540c\u8fdb\u5316\u7d27\u5bc6\u8026\u5408\u7684\u7834\u574f\u548c\u4fee\u590d\u7b97\u5b50\u5bf9\uff0c\u901a\u8fc7\u5408\u4f5c\u8bc4\u4f30\u673a\u5236\u663e\u5f0f\u6355\u6349\u5b83\u4eec\u7684\u4ea4\u4e92\u4f5c\u7528\u3002", "result": "\u5728\u65c5\u884c\u5546\u95ee\u9898\u548c\u5e26\u5bb9\u91cf\u8f66\u8f86\u8def\u5f84\u95ee\u9898\u7b49\u6311\u6218\u6027\u7ec4\u5408\u4f18\u5316\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cG-LNS\u663e\u8457\u4f18\u4e8e\u57fa\u4e8eLLM\u7684AHD\u65b9\u6cd5\u4ee5\u53ca\u5f3a\u5927\u7684\u7ecf\u5178\u6c42\u89e3\u5668\u3002\u53d1\u73b0\u7684\u542f\u53d1\u5f0f\u4e0d\u4ec5\u4ee5\u66f4\u5c11\u7684\u8ba1\u7b97\u9884\u7b97\u83b7\u5f97\u63a5\u8fd1\u6700\u4f18\u89e3\uff0c\u800c\u4e14\u5728\u591a\u6837\u5316\u548c\u672a\u89c1\u8fc7\u7684\u5b9e\u4f8b\u5206\u5e03\u4e0a\u8868\u73b0\u51fa\u9c81\u68d2\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "G-LNS\u6846\u67b6\u6210\u529f\u5730\u5c06LLM-based AHD\u6269\u5c55\u5230\u66f4\u7075\u6d3b\u7684\u5927\u89c4\u6a21\u90bb\u57df\u641c\u7d22\u7b97\u5b50\u8bbe\u8ba1\uff0c\u901a\u8fc7\u534f\u540c\u8fdb\u5316\u7834\u574f-\u4fee\u590d\u7b97\u5b50\u5bf9\u5b9e\u73b0\u4e86\u66f4\u6709\u6548\u7684\u7ed3\u6784\u63a2\u7d22\uff0c\u4e3a\u590d\u6742\u7ec4\u5408\u4f18\u5316\u95ee\u9898\u7684\u81ea\u52a8\u542f\u53d1\u5f0f\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2602.08371", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.08371", "abs": "https://arxiv.org/abs/2602.08371", "authors": ["Hung Quang Tran", "Nam Tien Pham", "Son T. Luu", "Kiet Van Nguyen"], "title": "ViGoEmotions: A Benchmark Dataset For Fine-grained Emotion Detection on Vietnamese Texts", "comment": "Accepted as main paper at EACL 2026", "summary": "Emotion classification plays a significant role in emotion prediction and harmful content detection. Recent advancements in NLP, particularly through large language models (LLMs), have greatly improved outcomes in this field. This study introduces ViGoEmotions -- a Vietnamese emotion corpus comprising 20,664 social media comments in which each comment is classified into 27 fine-grained distinct emotions. To evaluate the quality of the dataset and its impact on emotion classification, eight pre-trained Transformer-based models were evaluated under three preprocessing strategies: preserving original emojis with rule-based normalization, converting emojis into textual descriptions, and applying ViSoLex, a model-based lexical normalization system. Results show that converting emojis into text often improves the performance of several BERT-based baselines, while preserving emojis yields the best results for ViSoBERT and CafeBERT. In contrast, removing emojis generally leads to lower performance. ViSoBERT achieved the highest Macro F1-score of 61.50% and Weighted F1-score of 63.26%. Strong performance was also observed from CafeBERT and PhoBERT. These findings highlight that while the proposed corpus can support diverse architectures effectively, preprocessing strategies and annotation quality remain key factors influencing downstream performance.", "AI": {"tldr": "\u8be5\u7814\u7a76\u521b\u5efa\u4e86\u8d8a\u5357\u8bed\u60c5\u611f\u8bed\u6599\u5e93ViGoEmotions\uff0c\u5305\u542b20,664\u6761\u793e\u4ea4\u5a92\u4f53\u8bc4\u8bba\uff0c\u6807\u6ce8\u4e3a27\u79cd\u7ec6\u7c92\u5ea6\u60c5\u611f\uff0c\u5e76\u8bc4\u4f30\u4e868\u79cd\u9884\u8bad\u7ec3Transformer\u6a21\u578b\u5728\u4e09\u79cd\u9884\u5904\u7406\u7b56\u7565\u4e0b\u7684\u60c5\u611f\u5206\u7c7b\u6027\u80fd\u3002", "motivation": "\u60c5\u611f\u5206\u7c7b\u5728\u60c5\u611f\u9884\u6d4b\u548c\u6709\u5bb3\u5185\u5bb9\u68c0\u6d4b\u4e2d\u5177\u6709\u91cd\u8981\u4f5c\u7528\u3002\u5c3d\u7ba1NLP\u9886\u57df\u7279\u522b\u662f\u5927\u8bed\u8a00\u6a21\u578b\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u8d8a\u5357\u8bed\u60c5\u611f\u5206\u7c7b\u9886\u57df\u4ecd\u7f3a\u4e4f\u9ad8\u8d28\u91cf\u3001\u7ec6\u7c92\u5ea6\u7684\u60c5\u611f\u8bed\u6599\u5e93\uff0c\u9700\u8981\u7814\u7a76\u4e0d\u540c\u9884\u5904\u7406\u7b56\u7565\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\u3002", "method": "\u521b\u5efa\u4e86\u8d8a\u5357\u8bed\u60c5\u611f\u8bed\u6599\u5e93ViGoEmotions\uff0c\u5305\u542b20,664\u6761\u793e\u4ea4\u5a92\u4f53\u8bc4\u8bba\uff0c\u6807\u6ce8\u4e3a27\u79cd\u7ec6\u7c92\u5ea6\u60c5\u611f\u3002\u8bc4\u4f30\u4e868\u79cd\u9884\u8bad\u7ec3\u7684Transformer\u6a21\u578b\uff0c\u91c7\u7528\u4e09\u79cd\u9884\u5904\u7406\u7b56\u7565\uff1a1) \u4fdd\u7559\u539f\u59cb\u8868\u60c5\u7b26\u53f7\u5e76\u8fdb\u884c\u89c4\u5219\u5316\u5f52\u4e00\u5316\uff1b2) \u5c06\u8868\u60c5\u7b26\u53f7\u8f6c\u6362\u4e3a\u6587\u672c\u63cf\u8ff0\uff1b3) \u5e94\u7528ViSoLex\u6a21\u578b\u8fdb\u884c\u8bcd\u6c47\u5f52\u4e00\u5316\u3002", "result": "\u5c06\u8868\u60c5\u7b26\u53f7\u8f6c\u6362\u4e3a\u6587\u672c\u901a\u5e38\u80fd\u63d0\u9ad8\u591a\u4e2aBERT\u57fa\u7ebf\u7684\u6027\u80fd\uff0c\u800c\u4fdd\u7559\u8868\u60c5\u7b26\u53f7\u5bf9ViSoBERT\u548cCafeBERT\u6548\u679c\u6700\u597d\u3002\u79fb\u9664\u8868\u60c5\u7b26\u53f7\u901a\u5e38\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002ViSoBERT\u83b7\u5f97\u4e86\u6700\u9ad8\u7684Macro F1-score\uff0861.50%\uff09\u548cWeighted F1-score\uff0863.26%\uff09\u3002CafeBERT\u548cPhoBERT\u4e5f\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "\u63d0\u51fa\u7684ViGoEmotions\u8bed\u6599\u5e93\u80fd\u6709\u6548\u652f\u6301\u591a\u79cd\u67b6\u6784\uff0c\u4f46\u9884\u5904\u7406\u7b56\u7565\u548c\u6807\u6ce8\u8d28\u91cf\u4ecd\u7136\u662f\u5f71\u54cd\u4e0b\u6e38\u6027\u80fd\u7684\u5173\u952e\u56e0\u7d20\u3002\u8868\u60c5\u7b26\u53f7\u5904\u7406\u65b9\u5f0f\u5bf9\u6a21\u578b\u6027\u80fd\u6709\u663e\u8457\u5f71\u54cd\uff0c\u4e0d\u540c\u6a21\u578b\u5bf9\u8868\u60c5\u7b26\u53f7\u7684\u5904\u7406\u7b56\u7565\u6709\u4e0d\u540c\u7684\u9002\u5e94\u6027\u3002"}}
{"id": "2602.07554", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.07554", "abs": "https://arxiv.org/abs/2602.07554", "authors": ["Guandong Li", "Yijun Ding"], "title": "FlexID: Training-Free Flexible Identity Injection via Intent-Aware Modulation for Text-to-Image Generation", "comment": null, "summary": "Personalized text-to-image generation aims to seamlessly integrate specific identities into textual descriptions. However, existing training-free methods often rely on rigid visual feature injection, creating a conflict between identity fidelity and textual adaptability. To address this, we propose FlexID, a novel training-free framework utilizing intent-aware modulation. FlexID orthogonally decouples identity into two dimensions: a Semantic Identity Projector (SIP) that injects high-level priors into the language space, and a Visual Feature Anchor (VFA) that ensures structural fidelity within the latent space. Crucially, we introduce a Context-Aware Adaptive Gating (CAG) mechanism that dynamically modulates the weights of these streams based on editing intent and diffusion timesteps. By automatically relaxing rigid visual constraints when strong editing intent is detected, CAG achieves synergy between identity preservation and semantic variation. Extensive experiments on IBench demonstrate that FlexID achieves a state-of-the-art balance between identity consistency and text adherence, offering an efficient solution for complex narrative generation.", "AI": {"tldr": "FlexID\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684\u4e2a\u4eba\u5316\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u610f\u56fe\u611f\u77e5\u8c03\u5236\u89e3\u51b3\u8eab\u4efd\u4fdd\u771f\u5ea6\u4e0e\u6587\u672c\u9002\u5e94\u6027\u4e4b\u95f4\u7684\u51b2\u7a81\u3002", "motivation": "\u73b0\u6709\u65e0\u9700\u8bad\u7ec3\u7684\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u521a\u6027\u7684\u89c6\u89c9\u7279\u5f81\u6ce8\u5165\uff0c\u5bfc\u81f4\u8eab\u4efd\u4fdd\u771f\u5ea6\u4e0e\u6587\u672c\u9002\u5e94\u6027\u4e4b\u95f4\u5b58\u5728\u51b2\u7a81\uff0c\u96be\u4ee5\u5728\u4fdd\u6301\u8eab\u4efd\u4e00\u81f4\u6027\u7684\u540c\u65f6\u5b9e\u73b0\u8bed\u4e49\u53d8\u5316\u3002", "method": "FlexID\u91c7\u7528\u6b63\u4ea4\u89e3\u8026\u65b9\u6cd5\uff0c\u5c06\u8eab\u4efd\u5206\u4e3a\u4e24\u4e2a\u7ef4\u5ea6\uff1a\u8bed\u4e49\u8eab\u4efd\u6295\u5f71\u5668\uff08SIP\uff09\u5728\u8bed\u8a00\u7a7a\u95f4\u6ce8\u5165\u9ad8\u7ea7\u5148\u9a8c\uff0c\u89c6\u89c9\u7279\u5f81\u951a\u70b9\uff08VFA\uff09\u5728\u6f5c\u5728\u7a7a\u95f4\u786e\u4fdd\u7ed3\u6784\u4fdd\u771f\u5ea6\u3002\u5173\u952e\u521b\u65b0\u662f\u4e0a\u4e0b\u6587\u611f\u77e5\u81ea\u9002\u5e94\u95e8\u63a7\uff08CAG\uff09\u673a\u5236\uff0c\u6839\u636e\u7f16\u8f91\u610f\u56fe\u548c\u6269\u6563\u65f6\u95f4\u6b65\u52a8\u6001\u8c03\u5236\u8fd9\u4e24\u4e2a\u6d41\u7684\u6743\u91cd\u3002", "result": "\u5728IBench\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cFlexID\u5728\u8eab\u4efd\u4e00\u81f4\u6027\u548c\u6587\u672c\u9075\u5faa\u6027\u4e4b\u95f4\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u5e73\u8861\uff0c\u4e3a\u590d\u6742\u53d9\u4e8b\u751f\u6210\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "FlexID\u901a\u8fc7\u610f\u56fe\u611f\u77e5\u8c03\u5236\u6846\u67b6\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u4e2a\u4eba\u5316\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u4e2d\u7684\u8eab\u4efd\u4fdd\u771f\u5ea6\u4e0e\u6587\u672c\u9002\u5e94\u6027\u51b2\u7a81\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u8eab\u4efd\u4fdd\u5b58\u4e0e\u8bed\u4e49\u53d8\u5316\u7684\u534f\u540c\u3002"}}
{"id": "2602.08382", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08382", "abs": "https://arxiv.org/abs/2602.08382", "authors": ["Zhuoen Chen", "Dongfang Li", "Meishan Zhang", "Baotian Hu", "Min Zhang"], "title": "Dynamic Long Context Reasoning over Compressed Memory via End-to-End Reinforcement Learning", "comment": "26 pages, 7 figures. Code and models will be released", "summary": "Large Language Models (LLMs) face significant challenges in long-context processing, including quadratic computational costs, information forgetting, and the context fragmentation inherent in retrieval-augmented generation (RAG). We propose a cognitively inspired framework for efficient long-context inference based on chunk-wise compression and selective memory recall, rather than processing all raw tokens. The framework segments long inputs into chunks and encodes each chunk into compressed memory representations using a learned compressor. A gating module dynamically selects relevant memory blocks, which are then iteratively processed by a reasoning module with an evolving working memory to solve downstream tasks. The compressor and reasoner are jointly optimized via end-to-end reinforcement learning, while the gating module is trained separately as a classifier. Experimental results show that the proposed method achieves competitive accuracy on multi-hop reasoning benchmarks such as RULER-HQA, extrapolates context length from 7K to 1.75M tokens, and offers a favorable accuracy-efficiency trade-off compared to strong long-context baselines. In particular, it achieves up to a 2 times reduction in peak GPU memory usage and a 6 times inference speedup over MemAgent.", "AI": {"tldr": "\u63d0\u51fa\u8ba4\u77e5\u542f\u53d1\u7684\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u5757\u538b\u7f29\u548c\u9009\u62e9\u6027\u8bb0\u5fc6\u53ec\u56de\u89e3\u51b3LLM\u957f\u4e0a\u4e0b\u6587\u5904\u7406\u95ee\u9898\uff0c\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u6548\u7387", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u957f\u4e0a\u4e0b\u6587\u65f6\u9762\u4e34\u8ba1\u7b97\u6210\u672c\u9ad8\u3001\u4fe1\u606f\u9057\u5fd8\u548c\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u4e2d\u7684\u4e0a\u4e0b\u6587\u788e\u7247\u5316\u7b49\u6311\u6218\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848", "method": "\u57fa\u4e8e\u5206\u5757\u538b\u7f29\u548c\u9009\u62e9\u6027\u8bb0\u5fc6\u53ec\u56de\uff1a\u5c06\u957f\u8f93\u5165\u5206\u6bb5\u4e3a\u5757\uff0c\u7528\u5b66\u4e60\u5230\u7684\u538b\u7f29\u5668\u7f16\u7801\u4e3a\u538b\u7f29\u8bb0\u5fc6\u8868\u793a\uff1b\u95e8\u63a7\u6a21\u5757\u52a8\u6001\u9009\u62e9\u76f8\u5173\u8bb0\u5fc6\u5757\uff1b\u63a8\u7406\u6a21\u5757\u901a\u8fc7\u6f14\u5316\u7684\u5de5\u4f5c\u8bb0\u5fc6\u8fed\u4ee3\u5904\u7406\u4ee5\u89e3\u51b3\u4e0b\u6e38\u4efb\u52a1", "result": "\u5728RULER-HQA\u7b49\u591a\u8df3\u63a8\u7406\u57fa\u51c6\u4e0a\u8fbe\u5230\u7ade\u4e89\u6027\u51c6\u786e\u7387\uff0c\u4e0a\u4e0b\u6587\u957f\u5ea6\u4ece7K\u6269\u5c55\u52301.75M\u4ee4\u724c\uff0c\u76f8\u6bd4\u5f3a\u57fa\u7ebf\u5728\u51c6\u786e\u7387-\u6548\u7387\u6743\u8861\u4e0a\u8868\u73b0\u66f4\u4f18\uff0cGPU\u5cf0\u503c\u5185\u5b58\u4f7f\u7528\u51cf\u5c112\u500d\uff0c\u63a8\u7406\u901f\u5ea6\u6bd4MemAgent\u5feb6\u500d", "conclusion": "\u63d0\u51fa\u7684\u8ba4\u77e5\u542f\u53d1\u6846\u67b6\u901a\u8fc7\u538b\u7f29\u548c\u9009\u62e9\u6027\u8bb0\u5fc6\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86LLM\u957f\u4e0a\u4e0b\u6587\u5904\u7406\u7684\u5173\u952e\u6311\u6218\uff0c\u5728\u4fdd\u6301\u51c6\u786e\u6027\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u8ba1\u7b97\u6548\u7387"}}
{"id": "2602.08404", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.08404", "abs": "https://arxiv.org/abs/2602.08404", "authors": ["Linye Wei", "Zixiang Luo", "Pingzhi Tang", "Meng Li"], "title": "TEAM: Temporal-Spatial Consistency Guided Expert Activation for MoE Diffusion Language Model Acceleration", "comment": null, "summary": "Diffusion large language models (dLLMs) have recently gained significant attention due to their inherent support for parallel decoding. Building on this paradigm, Mixture-of-Experts (MoE) dLLMs with autoregressive (AR) initialization have further demonstrated strong performance competitive with mainstream AR models. However, we identify a fundamental mismatch between MoE architectures and diffusion-based decoding. Specifically, a large number of experts are activated at each denoising step, while only a small subset of tokens is ultimately accepted, resulting in substantial inference overhead and limiting their deployment in latency-sensitive applications. In this work, we propose TEAM, a plug-and-play framework that accelerates MoE dLLMs by enabling more accepted tokens with fewer activated experts. TEAM is motivated by the observation that expert routing decisions exhibit strong temporal consistency across denoising levels as well as spatial consistency across token positions. Leveraging these properties, TEAM employs three complementary expert activation and decoding strategies, conservatively selecting necessary experts for decoded and masked tokens and simultaneously performing aggressive speculative exploration across multiple candidates. Experimental results demonstrate that TEAM achieves up to 2.2x speedup over vanilla MoE dLLM, with negligible performance degradation. Code is released at https://github.com/PKU-SEC-Lab/TEAM-MoE-dLLM.", "AI": {"tldr": "TEAM\u6846\u67b6\u901a\u8fc7\u5229\u7528\u4e13\u5bb6\u8def\u7531\u51b3\u7b56\u7684\u65f6\u7a7a\u4e00\u81f4\u6027\uff0c\u91c7\u7528\u4e09\u79cd\u4e92\u8865\u7b56\u7565\u6765\u52a0\u901fMoE\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u5b9e\u73b0\u6700\u9ad82.2\u500d\u52a0\u901f\u3002", "motivation": "MoE\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b\u5728\u63a8\u7406\u65f6\u5b58\u5728\u6548\u7387\u95ee\u9898\uff1a\u6bcf\u4e2a\u53bb\u566a\u6b65\u9aa4\u6fc0\u6d3b\u5927\u91cf\u4e13\u5bb6\uff0c\u4f46\u53ea\u6709\u5c11\u91cftoken\u88ab\u6700\u7ec8\u63a5\u53d7\uff0c\u5bfc\u81f4\u663e\u8457\u7684\u63a8\u7406\u5f00\u9500\uff0c\u9650\u5236\u4e86\u5728\u5ef6\u8fdf\u654f\u611f\u5e94\u7528\u4e2d\u7684\u90e8\u7f72\u3002", "method": "TEAM\u6846\u67b6\u5229\u7528\u4e13\u5bb6\u8def\u7531\u51b3\u7b56\u5728\u53bb\u566a\u5c42\u7ea7\u95f4\u7684\u65f6\u95f4\u4e00\u81f4\u6027\u548ctoken\u4f4d\u7f6e\u95f4\u7684\u7a7a\u95f4\u4e00\u81f4\u6027\uff0c\u91c7\u7528\u4e09\u79cd\u4e92\u8865\u7b56\u7565\uff1a\u4fdd\u5b88\u9009\u62e9\u5df2\u89e3\u7801\u548c\u63a9\u7801token\u7684\u5fc5\u8981\u4e13\u5bb6\uff0c\u540c\u65f6\u5728\u591a\u4e2a\u5019\u9009token\u4e0a\u8fdb\u884c\u6fc0\u8fdb\u7684\u63a8\u6d4b\u6027\u63a2\u7d22\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cTEAM\u76f8\u6bd4\u539f\u59cbMoE\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b\u5b9e\u73b0\u4e86\u6700\u9ad82.2\u500d\u7684\u52a0\u901f\uff0c\u4e14\u6027\u80fd\u4e0b\u964d\u53ef\u5ffd\u7565\u4e0d\u8ba1\u3002", "conclusion": "TEAM\u662f\u4e00\u4e2a\u5373\u63d2\u5373\u7528\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u66f4\u5c11\u7684\u6fc0\u6d3b\u4e13\u5bb6\u5b9e\u73b0\u66f4\u591atoken\u7684\u63a5\u53d7\uff0c\u6709\u6548\u89e3\u51b3\u4e86MoE\u67b6\u6784\u4e0e\u6269\u6563\u89e3\u7801\u4e4b\u95f4\u7684\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63a8\u7406\u6548\u7387\u3002"}}
{"id": "2602.08276", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08276", "abs": "https://arxiv.org/abs/2602.08276", "authors": ["Haoyu Jia", "Kento Kawaharazuka", "Kei Okada"], "title": "Toward Formalizing LLM-Based Agent Designs through Structural Context Modeling and Semantic Dynamics Analysis", "comment": null, "summary": "Current research on large language model (LLM) agents is fragmented: discussions of conceptual frameworks and methodological principles are frequently intertwined with low-level implementation details, causing both readers and authors to lose track amid a proliferation of superficially distinct concepts. We argue that this fragmentation largely stems from the absence of an analyzable, self-consistent formal model that enables implementation-independent characterization and comparison of LLM agents. To address this gap, we propose the \\texttt{Structural Context Model}, a formal model for analyzing and comparing LLM agents from the perspective of context structure. Building upon this foundation, we introduce two complementary components that together span the full lifecycle of LLM agent research and development: (1) a declarative implementation framework; and (2) a sustainable agent engineering workflow, \\texttt{Semantic Dynamics Analysis}. The proposed workflow provides principled insights into agent mechanisms and supports rapid, systematic design iteration. We demonstrate the effectiveness of the complete framework on dynamic variants of the monkey-banana problem, where agents engineered using our approach achieve up to a 32 percentage points improvement in success rate on the most challenging setting.", "AI": {"tldr": "\u63d0\u51faStructural Context Model\u5f62\u5f0f\u5316\u6a21\u578b\uff0c\u901a\u8fc7\u4e0a\u4e0b\u6587\u7ed3\u6784\u5206\u6790\u6bd4\u8f83LLM\u667a\u80fd\u4f53\uff0c\u5305\u542b\u58f0\u660e\u5f0f\u5b9e\u73b0\u6846\u67b6\u548cSemantic Dynamics Analysis\u5de5\u4f5c\u6d41\uff0c\u5728\u52a8\u6001\u7334\u5b50\u9999\u8549\u95ee\u9898\u4e0a\u5b9e\u73b032%\u6210\u529f\u7387\u63d0\u5347\u3002", "motivation": "\u5f53\u524dLLM\u667a\u80fd\u4f53\u7814\u7a76\u5b58\u5728\u788e\u7247\u5316\u95ee\u9898\uff0c\u6982\u5ff5\u6846\u67b6\u548c\u65b9\u6cd5\u8bba\u539f\u5219\u5e38\u4e0e\u5e95\u5c42\u5b9e\u73b0\u7ec6\u8282\u6df7\u6742\uff0c\u7f3a\u4e4f\u53ef\u5206\u6790\u3001\u81ea\u6d3d\u7684\u5f62\u5f0f\u5316\u6a21\u578b\u6765\u8fdb\u884c\u5b9e\u73b0\u65e0\u5173\u7684\u667a\u80fd\u4f53\u8868\u5f81\u548c\u6bd4\u8f83\u3002", "method": "\u63d0\u51faStructural Context Model\u5f62\u5f0f\u5316\u6a21\u578b\uff0c\u4ece\u4e0a\u4e0b\u6587\u7ed3\u6784\u89d2\u5ea6\u5206\u6790LLM\u667a\u80fd\u4f53\uff1b\u5305\u542b\u58f0\u660e\u5f0f\u5b9e\u73b0\u6846\u67b6\u548cSemantic Dynamics Analysis\u53ef\u6301\u7eed\u5de5\u7a0b\u5de5\u4f5c\u6d41\uff0c\u652f\u6301\u5feb\u901f\u7cfb\u7edf\u5316\u8bbe\u8ba1\u8fed\u4ee3\u3002", "result": "\u5728\u52a8\u6001\u7334\u5b50\u9999\u8549\u95ee\u9898\u53d8\u4f53\u4e0a\uff0c\u4f7f\u7528\u8be5\u6846\u67b6\u8bbe\u8ba1\u7684\u667a\u80fd\u4f53\u5728\u6700\u5177\u6311\u6218\u6027\u7684\u8bbe\u7f6e\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u8fbe32\u4e2a\u767e\u5206\u70b9\u7684\u6210\u529f\u7387\u63d0\u5347\u3002", "conclusion": "\u63d0\u51fa\u7684\u5f62\u5f0f\u5316\u6a21\u578b\u548c\u5de5\u4f5c\u6d41\u4e3aLLM\u667a\u80fd\u4f53\u7814\u7a76\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u89c1\u89e3\uff0c\u652f\u6301\u5feb\u901f\u7cfb\u7edf\u5316\u8bbe\u8ba1\u8fed\u4ee3\uff0c\u89e3\u51b3\u4e86\u5f53\u524d\u7814\u7a76\u788e\u7247\u5316\u95ee\u9898\u3002"}}
{"id": "2602.07565", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.07565", "abs": "https://arxiv.org/abs/2602.07565", "authors": ["Jingzhe Ma", "Meng Zhang", "Jianlong Yu", "Kun Liu", "Zunxiao Xu", "Xue Cheng", "Junjie Zhou", "Yanfei Wang", "Jiahang Li", "Zepeng Wang", "Kazuki Osamura", "Rujie Liu", "Narishige Abe", "Jingjie Wang", "Shunli Zhang", "Haojun Xie", "Jiajun Wu", "Weiming Wu", "Wenxiong Kang", "Qingshuo Gao", "Jiaming Xiong", "Xianye Ben", "Lei Chen", "Lichen Song", "Junjian Cui", "Haijun Xiong", "Junhao Lu", "Bin Feng", "Mengyuan Liu", "Ji Zhou", "Baoquan Zhao", "Ke Xu", "Yongzhen Huang", "Liang Wang", "Manuel J Marin-Jimenez", "Md Atiqur Rahman Ahad", "Shiqi Yu"], "title": "Human Identification at a Distance: Challenges, Methods and Results on the Competition HID 2025", "comment": "Accepted by IJCB 2025(https://ijcb2025.ieee-biometrics.org/competitions/)", "summary": "Human identification at a distance (HID) is challenging because traditional biometric modalities such as face and fingerprints are often difficult to acquire in real-world scenarios. Gait recognition provides a practical alternative, as it can be captured reliably at a distance. To promote progress in gait recognition and provide a fair evaluation platform, the International Competition on Human Identification at a Distance (HID) has been organized annually since 2020. Since 2023, the competition has adopted the challenging SUSTech-Competition dataset, which features substantial variations in clothing, carried objects, and view angles. No dedicated training data are provided, requiring participants to train their models using external datasets. Each year, the competition applies a different random seed to generate distinct evaluation splits, which reduces the risk of overfitting and supports a fair assessment of cross-domain generalization. While HID 2023 and HID 2024 already used this dataset, HID 2025 explicitly examined whether algorithmic advances could surpass the accuracy limits observed previously. Despite the heightened difficulty, participants achieved further improvements, and the best-performing method reached 94.2% accuracy, setting a new benchmark on this dataset. We also analyze key technical trends and outline potential directions for future research in gait recognition.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86HID 2025\u7ade\u8d5b\uff0c\u8be5\u7ade\u8d5b\u4f7f\u7528SUSTech-Competition\u6570\u636e\u96c6\u8bc4\u4f30\u6b65\u6001\u8bc6\u522b\u7b97\u6cd5\uff0c\u53c2\u8d5b\u8005\u5728\u6ca1\u6709\u4e13\u7528\u8bad\u7ec3\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u53d6\u5f97\u4e8694.2%\u7684\u51c6\u786e\u7387\uff0c\u521b\u9020\u4e86\u8be5\u6570\u636e\u96c6\u7684\u65b0\u57fa\u51c6\u3002", "motivation": "\u8fdc\u8ddd\u79bb\u4eba\u4f53\u8bc6\u522b(HID)\u5177\u6709\u6311\u6218\u6027\uff0c\u56e0\u4e3a\u4f20\u7edf\u751f\u7269\u7279\u5f81\u6a21\u6001\u5982\u4eba\u8138\u548c\u6307\u7eb9\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\u96be\u4ee5\u83b7\u53d6\u3002\u6b65\u6001\u8bc6\u522b\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u53ef\u4ee5\u5728\u8fdc\u8ddd\u79bb\u53ef\u9760\u5730\u6355\u6349\u3002HID\u7ade\u8d5b\u65e8\u5728\u4fc3\u8fdb\u6b65\u6001\u8bc6\u522b\u8fdb\u5c55\u5e76\u63d0\u4f9b\u516c\u5e73\u8bc4\u4f30\u5e73\u53f0\u3002", "method": "\u7ade\u8d5b\u91c7\u7528SUSTech-Competition\u6570\u636e\u96c6\uff0c\u8be5\u6570\u636e\u96c6\u5305\u542b\u670d\u88c5\u3001\u643a\u5e26\u7269\u54c1\u548c\u89c6\u89d2\u7684\u663e\u8457\u53d8\u5316\u3002\u4e0d\u63d0\u4f9b\u4e13\u7528\u8bad\u7ec3\u6570\u636e\uff0c\u53c2\u8d5b\u8005\u9700\u4f7f\u7528\u5916\u90e8\u6570\u636e\u96c6\u8bad\u7ec3\u6a21\u578b\u3002\u6bcf\u5e74\u4f7f\u7528\u4e0d\u540c\u968f\u673a\u79cd\u5b50\u751f\u6210\u4e0d\u540c\u7684\u8bc4\u4f30\u5212\u5206\uff0c\u51cf\u5c11\u8fc7\u62df\u5408\u98ce\u9669\u5e76\u652f\u6301\u8de8\u57df\u6cdb\u5316\u7684\u516c\u5e73\u8bc4\u4f30\u3002", "result": "HID 2025\u7ade\u8d5b\u4e2d\uff0c\u5c3d\u7ba1\u96be\u5ea6\u589e\u52a0\uff0c\u53c2\u8d5b\u8005\u53d6\u5f97\u4e86\u8fdb\u4e00\u6b65\u6539\u8fdb\uff0c\u6700\u4f73\u65b9\u6cd5\u8fbe\u5230\u4e8694.2%\u7684\u51c6\u786e\u7387\uff0c\u4e3a\u8be5\u6570\u636e\u96c6\u8bbe\u7acb\u4e86\u65b0\u7684\u57fa\u51c6\u3002", "conclusion": "HID\u7ade\u8d5b\u901a\u8fc7SUSTech-Competition\u6570\u636e\u96c6\u63a8\u52a8\u4e86\u6b65\u6001\u8bc6\u522b\u6280\u672f\u7684\u53d1\u5c55\uff0c\u53c2\u8d5b\u8005\u7684\u6301\u7eed\u6539\u8fdb\u8868\u660e\u7b97\u6cd5\u8fdb\u6b65\u80fd\u591f\u8d85\u8d8a\u5148\u524d\u89c2\u5bdf\u5230\u7684\u51c6\u786e\u7387\u6781\u9650\u3002\u8bba\u6587\u8fd8\u5206\u6790\u4e86\u5173\u952e\u6280\u672f\u8d8b\u52bf\u5e76\u6982\u8ff0\u4e86\u6b65\u6001\u8bc6\u522b\u672a\u6765\u7814\u7a76\u7684\u6f5c\u5728\u65b9\u5411\u3002"}}
{"id": "2602.08295", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08295", "abs": "https://arxiv.org/abs/2602.08295", "authors": ["Ilya Levin"], "title": "The Vibe-Automation of Automation: A Proactive Education Framework for Computer Science in the Age of Generative AI", "comment": "19 pages", "summary": "The emergence of generative artificial intelligence (GenAI) represents not an incremental technological advance but a qualitative epistemological shift that challenges foundational assumptions of computer science. Whereas machine learning has been described as the automation of automation, generative AI operates by navigating contextual, semantic, and stylistic coherence rather than optimizing predefined objective metrics. This paper introduces the concept of Vibe-Automation to characterize this transition.\n  The central claim is that the significance of GenAI lies in its functional access to operationalized tacit regularities: context-sensitive patterns embedded in practice that cannot be fully specified through explicit algorithmic rules. Although generative systems do not possess tacit knowledge in a phenomenological sense, they operationalize sensitivities to tone, intent, and situated judgment encoded in high-dimensional latent representations. On this basis, the human role shifts from algorithmic problem specification toward Vibe-Engineering, understood as the orchestration of alignment and contextual judgment in generative systems.\n  The paper connects this epistemological shift to educational and institutional transformation by proposing a conceptual framework structured across three analytical levels and three domains of action: faculty worldview, industry relations, and curriculum design. The risks of mode collapse and cultural homogenization are briefly discussed, emphasizing the need for deliberate engagement with generative systems to avoid regression toward synthetic uniformity.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\"\u6c1b\u56f4\u81ea\u52a8\u5316\"\u6982\u5ff5\uff0c\u8ba4\u4e3a\u751f\u6210\u5f0fAI\u4ee3\u8868\u4e86\u4ece\u7b97\u6cd5\u4f18\u5316\u5230\u8bed\u5883\u8bed\u4e49\u5bfc\u822a\u7684\u8ba4\u77e5\u8f6c\u53d8\uff0c\u5c06\u4eba\u7c7b\u89d2\u8272\u4ece\u95ee\u9898\u89c4\u8303\u8f6c\u5411\"\u6c1b\u56f4\u5de5\u7a0b\"\u3002", "motivation": "\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\u4ee3\u8868\u4e86\u8ba1\u7b97\u673a\u79d1\u5b66\u57fa\u7840\u5047\u8bbe\u7684\u8d28\u53d8\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u6280\u672f\u589e\u91cf\u8fdb\u6b65\u3002\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u662f\"\u81ea\u52a8\u5316\u7684\u81ea\u52a8\u5316\"\uff0c\u800c\u751f\u6210\u5f0fAI\u901a\u8fc7\u5bfc\u822a\u8bed\u5883\u3001\u8bed\u4e49\u548c\u98ce\u683c\u8fde\u8d2f\u6027\u8fd0\u4f5c\uff0c\u6311\u6218\u4e86\u57fa\u4e8e\u9884\u5b9a\u4e49\u5ba2\u89c2\u6307\u6807\u4f18\u5316\u7684\u8303\u5f0f\u3002", "method": "\u5f15\u5165\"\u6c1b\u56f4\u81ea\u52a8\u5316\"\u6982\u5ff5\u6846\u67b6\uff0c\u5206\u6790\u751f\u6210\u5f0fAI\u5982\u4f55\u64cd\u4f5c\u5316\u9690\u6027\u89c4\u5f8b\u2014\u2014\u65e0\u6cd5\u901a\u8fc7\u663e\u5f0f\u7b97\u6cd5\u89c4\u5219\u5b8c\u5168\u6307\u5b9a\u7684\u60c5\u5883\u654f\u611f\u6a21\u5f0f\u3002\u63d0\u51fa\u4eba\u7c7b\u89d2\u8272\u8f6c\u53d8\u4e3a\"\u6c1b\u56f4\u5de5\u7a0b\"\uff0c\u5373\u534f\u8c03\u751f\u6210\u7cfb\u7edf\u4e2d\u7684\u5bf9\u9f50\u548c\u60c5\u5883\u5224\u65ad\u3002", "result": "\u6784\u5efa\u4e86\u5305\u542b\u4e09\u4e2a\u5206\u6790\u5c42\u6b21\u548c\u4e09\u4e2a\u884c\u52a8\u9886\u57df\u7684\u6982\u5ff5\u6846\u67b6\uff1a\u6559\u5e08\u4e16\u754c\u89c2\u3001\u884c\u4e1a\u5173\u7cfb\u548c\u8bfe\u7a0b\u8bbe\u8ba1\u3002\u8ba8\u8bba\u4e86\u6a21\u5f0f\u5d29\u6e83\u548c\u6587\u5316\u540c\u8d28\u5316\u7684\u98ce\u9669\uff0c\u5f3a\u8c03\u9700\u8981\u523b\u610f\u53c2\u4e0e\u751f\u6210\u7cfb\u7edf\u4ee5\u907f\u514d\u56de\u5f52\u5408\u6210\u7edf\u4e00\u6027\u3002", "conclusion": "\u751f\u6210\u5f0fAI\u4ee3\u8868\u4e86\u8ba1\u7b97\u673a\u79d1\u5b66\u7684\u8ba4\u8bc6\u8bba\u8f6c\u53d8\uff0c\u4ece\u7b97\u6cd5\u4f18\u5316\u8f6c\u5411\u8bed\u5883\u5bfc\u822a\u3002\u8fd9\u8981\u6c42\u6559\u80b2\u673a\u6784\u548c\u884c\u4e1a\u8fdb\u884c\u76f8\u5e94\u8f6c\u578b\uff0c\u901a\u8fc7\"\u6c1b\u56f4\u5de5\u7a0b\"\u7ba1\u7406\u9690\u6027\u89c4\u5f8b\u7684\u64cd\u4f5c\u5316\uff0c\u540c\u65f6\u8b66\u60d5\u6587\u5316\u540c\u8d28\u5316\u98ce\u9669\u3002"}}
{"id": "2602.08437", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.08437", "abs": "https://arxiv.org/abs/2602.08437", "authors": ["Ziyan wang", "Longlong Ma"], "title": "Large Language Models and Impossible Language Acquisition: \"False Promise\" or an Overturn of our Current Perspective towards AI", "comment": null, "summary": "In Chomsky's provocative critique \"The False Promise of CHATGPT,\" Large Language Models (LLMs) are characterized as mere pattern predictors that do not acquire languages via intrinsic causal and self-correction structures like humans, therefore are not able to distinguish impossible languages. It stands as a representative in a fundamental challenge to the intellectual foundations of AI, for it integrally synthesizes major issues in methodologies within LLMs and possesses an iconic a priori rationalist perspective. We examine this famous critic from both the perspective in pre-existing literature of linguistics and psychology as well as a research based on an experiment inquiring the capacity of learning both possible and impossible languages among LLMs. We constructed a set of syntactically impossible languages by applying certain transformations to English. These include reversing whole sentences, and adding negation based on word-count parity. Two rounds of controlled experiments were each conducted on GPT-2 small models and long short-term memory (LSTM) models. Statistical analysis (Welch's t-test) shows GPT2 small models underperform in learning all of the impossible languages compared to their performance on the possible language (p<.001). On the other hand, LSTM models' performance tallies with Chomsky's argument, suggesting the irreplaceable role of the evolution of transformer architecture. Based on theoretical analysis and empirical findings, we propose a new vision within Chomsky's theory towards LLMs, and a shift of theoretical paradigm outside Chomsky, from his \"rationalist-romantics\" paradigm to functionalism and empiricism in LLMs research.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u5b9e\u9a8c\u68c0\u9a8c\u4e54\u59c6\u65af\u57fa\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u6279\u8bc4\uff0c\u6784\u5efa\u4e0d\u53ef\u80fd\u8bed\u8a00\u6d4b\u8bd5GPT-2\u548cLSTM\u6a21\u578b\u7684\u5b66\u4e60\u80fd\u529b\uff0c\u53d1\u73b0GPT-2\u5728\u4e0d\u53ef\u80fd\u8bed\u8a00\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u800cLSTM\u8868\u73b0\u7b26\u5408\u4e54\u59c6\u65af\u57fa\u89c2\u70b9\uff0c\u63d0\u51fa\u4e86\u4ece\u7406\u6027\u4e3b\u4e49\u8f6c\u5411\u529f\u80fd\u4e3b\u4e49\u548c\u7ecf\u9a8c\u4e3b\u4e49\u7684\u7814\u7a76\u8303\u5f0f\u8f6c\u53d8\u3002", "motivation": "\u4e54\u59c6\u65af\u57fa\u5728\u300aCHATGPT\u7684\u865a\u5047\u627f\u8bfa\u300b\u4e2d\u6279\u8bc4\u5927\u578b\u8bed\u8a00\u6a21\u578b\u53ea\u662f\u6a21\u5f0f\u9884\u6d4b\u5668\uff0c\u7f3a\u4e4f\u4eba\u7c7b\u8bed\u8a00\u4e60\u5f97\u7684\u5185\u5728\u56e0\u679c\u548c\u81ea\u6211\u4fee\u6b63\u7ed3\u6784\uff0c\u65e0\u6cd5\u533a\u5206\u4e0d\u53ef\u80fd\u8bed\u8a00\u3002\u8fd9\u4e00\u6279\u8bc4\u4ee3\u8868\u4e86AI\u57fa\u7840\u7406\u8bba\u7684\u6839\u672c\u6311\u6218\uff0c\u7ed3\u5408\u4e86LLM\u65b9\u6cd5\u8bba\u7684\u4e3b\u8981\u95ee\u9898\u5e76\u5177\u6709\u5178\u578b\u7684\u5148\u9a8c\u7406\u6027\u4e3b\u4e49\u89c6\u89d2\u3002\u7814\u7a76\u65e8\u5728\u4ece\u8bed\u8a00\u5b66\u548c\u5fc3\u7406\u5b66\u6587\u732e\u89d2\u5ea6\u4ee5\u53ca\u5b9e\u9a8c\u7814\u7a76\u89d2\u5ea6\u68c0\u9a8c\u8fd9\u4e00\u8457\u540d\u6279\u8bc4\u3002", "method": "\u901a\u8fc7\u5e94\u7528\u7279\u5b9a\u8f6c\u6362\u6784\u5efa\u4e00\u7ec4\u53e5\u6cd5\u4e0a\u4e0d\u53ef\u80fd\u7684\u8bed\u8a00\uff08\u5305\u62ec\u6574\u53e5\u53cd\u8f6c\u548c\u57fa\u4e8e\u8bcd\u6570\u5947\u5076\u6027\u6dfb\u52a0\u5426\u5b9a\uff09\u3002\u5728GPT-2\u5c0f\u578b\u6a21\u578b\u548c\u957f\u77ed\u671f\u8bb0\u5fc6\uff08LSTM\uff09\u6a21\u578b\u4e0a\u8fdb\u884c\u4e24\u8f6e\u5bf9\u7167\u5b9e\u9a8c\uff0c\u4f7f\u7528\u97e6\u5c14\u5947t\u68c0\u9a8c\u8fdb\u884c\u7edf\u8ba1\u5206\u6790\u3002", "result": "\u7edf\u8ba1\u5206\u6790\u663e\u793a\uff0cGPT-2\u5c0f\u578b\u6a21\u578b\u5728\u5b66\u4e60\u6240\u6709\u4e0d\u53ef\u80fd\u8bed\u8a00\u65b9\u9762\u7684\u8868\u73b0\u5747\u4f4e\u4e8e\u5176\u5728\u53ef\u80fd\u8bed\u8a00\u4e0a\u7684\u8868\u73b0\uff08p<0.001\uff09\u3002\u800cLSTM\u6a21\u578b\u7684\u8868\u73b0\u4e0e\u4e54\u59c6\u65af\u57fa\u7684\u8bba\u70b9\u4e00\u81f4\uff0c\u8868\u660eTransformer\u67b6\u6784\u6f14\u5316\u7684\u4e0d\u53ef\u66ff\u4ee3\u4f5c\u7528\u3002", "conclusion": "\u57fa\u4e8e\u7406\u8bba\u5206\u6790\u548c\u5b9e\u8bc1\u53d1\u73b0\uff0c\u63d0\u51fa\u4e86\u5728\u4e54\u59c6\u65af\u57fa\u7406\u8bba\u6846\u67b6\u5185\u5bf9LLM\u7684\u65b0\u89c6\u89d2\uff0c\u4ee5\u53ca\u4ece\u4e54\u59c6\u65af\u57fa\u7684\"\u7406\u6027\u4e3b\u4e49-\u6d6a\u6f2b\u4e3b\u4e49\"\u8303\u5f0f\u5411\u529f\u80fd\u4e3b\u4e49\u548c\u7ecf\u9a8c\u4e3b\u4e49\u7684\u7406\u8bba\u8303\u5f0f\u8f6c\u53d8\uff0c\u4e3aLLM\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u5411\u3002"}}
{"id": "2602.07566", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07566", "abs": "https://arxiv.org/abs/2602.07566", "authors": ["Runcheng Wang", "Yaru Chen", "Guiguo Zhang", "Honghua Jiang", "Yongliang Qiao"], "title": "Cross-Camera Cow Identification via Disentangled Representation Learning", "comment": null, "summary": "Precise identification of individual cows is a fundamental prerequisite for comprehensive digital management in smart livestock farming. While existing animal identification methods excel in controlled, single-camera settings, they face severe challenges regarding cross-camera generalization. When models trained on source cameras are deployed to new monitoring nodes characterized by divergent illumination, backgrounds, viewpoints, and heterogeneous imaging properties, recognition performance often degrades dramatically. This limits the large-scale application of non-contact technologies in dynamic, real-world farming environments. To address this challenge, this study proposes a cross-camera cow identification framework based on disentangled representation learning. This framework leverages the Subspace Identifiability Guarantee (SIG) theory in the context of bovine visual recognition. By modeling the underlying physical data generation process, we designed a principle-driven feature disentanglement module that decomposes observed images into multiple orthogonal latent subspaces. This mechanism effectively isolates stable, identity-related biometric features that remain invariant across cameras, thereby substantially improving generalization to unseen cameras. We constructed a high-quality dataset spanning five distinct camera nodes, covering heterogeneous acquisition devices and complex variations in lighting and angles. Extensive experiments across seven cross-camera tasks demonstrate that the proposed method achieves an average accuracy of 86.0%, significantly outperforming the Source-only Baseline (51.9%) and the strongest cross-camera baseline method (79.8%). This work establishes a subspace-theoretic feature disentanglement framework for collaborative cross-camera cow identification, offering a new paradigm for precise animal monitoring in uncontrolled smart farming environments.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u89e3\u8026\u8868\u5f81\u5b66\u4e60\u7684\u8de8\u6444\u50cf\u5934\u5976\u725b\u8bc6\u522b\u6846\u67b6\uff0c\u901a\u8fc7\u5b50\u7a7a\u95f4\u53ef\u8bc6\u522b\u6027\u4fdd\u8bc1\u7406\u8bba\u5206\u89e3\u56fe\u50cf\u7279\u5f81\uff0c\u663e\u8457\u63d0\u5347\u5bf9\u672a\u89c1\u6444\u50cf\u5934\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u57287\u4e2a\u8de8\u6444\u50cf\u5934\u4efb\u52a1\u4e0a\u5e73\u5747\u51c6\u786e\u7387\u8fbe86.0%", "motivation": "\u73b0\u6709\u52a8\u7269\u8bc6\u522b\u65b9\u6cd5\u5728\u53d7\u63a7\u5355\u6444\u50cf\u5934\u73af\u5883\u4e0b\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u8de8\u6444\u50cf\u5934\u573a\u666f\u4e2d\u9762\u4e34\u4e25\u91cd\u6cdb\u5316\u6311\u6218\u3002\u5f53\u6a21\u578b\u4ece\u6e90\u6444\u50cf\u5934\u90e8\u7f72\u5230\u5177\u6709\u4e0d\u540c\u5149\u7167\u3001\u80cc\u666f\u3001\u89c6\u89d2\u548c\u6210\u50cf\u7279\u6027\u7684\u65b0\u76d1\u63a7\u8282\u70b9\u65f6\uff0c\u8bc6\u522b\u6027\u80fd\u6025\u5267\u4e0b\u964d\uff0c\u9650\u5236\u4e86\u975e\u63a5\u89e6\u6280\u672f\u5728\u52a8\u6001\u771f\u5b9e\u519c\u573a\u73af\u5883\u4e2d\u7684\u5927\u89c4\u6a21\u5e94\u7528\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u89e3\u8026\u8868\u5f81\u5b66\u4e60\u7684\u8de8\u6444\u50cf\u5934\u5976\u725b\u8bc6\u522b\u6846\u67b6\uff0c\u5229\u7528\u5b50\u7a7a\u95f4\u53ef\u8bc6\u522b\u6027\u4fdd\u8bc1\u7406\u8bba\uff0c\u901a\u8fc7\u5efa\u6a21\u5e95\u5c42\u7269\u7406\u6570\u636e\u751f\u6210\u8fc7\u7a0b\uff0c\u8bbe\u8ba1\u539f\u5219\u9a71\u52a8\u7684\u7279\u5f81\u89e3\u8026\u6a21\u5757\uff0c\u5c06\u89c2\u6d4b\u56fe\u50cf\u5206\u89e3\u4e3a\u591a\u4e2a\u6b63\u4ea4\u6f5c\u5728\u5b50\u7a7a\u95f4\uff0c\u6709\u6548\u5206\u79bb\u8de8\u6444\u50cf\u5934\u4e0d\u53d8\u7684\u7a33\u5b9a\u8eab\u4efd\u76f8\u5173\u751f\u7269\u7279\u5f81\u3002", "result": "\u6784\u5efa\u4e86\u8986\u76d65\u4e2a\u4e0d\u540c\u6444\u50cf\u5934\u8282\u70b9\u7684\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\uff0c\u5305\u542b\u5f02\u6784\u91c7\u96c6\u8bbe\u5907\u548c\u590d\u6742\u7684\u5149\u7167\u89d2\u5ea6\u53d8\u5316\u3002\u57287\u4e2a\u8de8\u6444\u50cf\u5934\u4efb\u52a1\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u65b9\u6cd5\u5e73\u5747\u51c6\u786e\u7387\u8fbe\u523086.0%\uff0c\u663e\u8457\u4f18\u4e8e\u4ec5\u6e90\u6444\u50cf\u5934\u57fa\u7ebf\uff0851.9%\uff09\u548c\u6700\u5f3a\u7684\u8de8\u6444\u50cf\u5934\u57fa\u7ebf\u65b9\u6cd5\uff0879.8%\uff09\u3002", "conclusion": "\u672c\u7814\u7a76\u5efa\u7acb\u4e86\u57fa\u4e8e\u5b50\u7a7a\u95f4\u7406\u8bba\u7684\u7279\u5f81\u89e3\u8026\u6846\u67b6\uff0c\u7528\u4e8e\u534f\u4f5c\u5f0f\u8de8\u6444\u50cf\u5934\u5976\u725b\u8bc6\u522b\uff0c\u4e3a\u4e0d\u53d7\u63a7\u667a\u80fd\u519c\u573a\u73af\u5883\u4e2d\u7684\u7cbe\u786e\u52a8\u7269\u76d1\u6d4b\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\uff0c\u89e3\u51b3\u4e86\u8de8\u6444\u50cf\u5934\u6cdb\u5316\u96be\u9898\u3002"}}
{"id": "2602.08311", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08311", "abs": "https://arxiv.org/abs/2602.08311", "authors": ["Shadman Rabby", "Md. Hefzul Hossain Papon", "Sabbir Ahmed", "Nokimul Hasan Arif", "A. B. M. Ashikur Rahman", "Irfan Ahmad"], "title": "Moral Sycophancy in Vision Language Models", "comment": "13 pages, 6 figures, 8 tables, Submitted for review in ACL", "summary": "Sycophancy in Vision-Language Models (VLMs) refers to their tendency to align with user opinions, often at the expense of moral or factual accuracy. While prior studies have explored sycophantic behavior in general contexts, its impact on morally grounded visual decision-making remains insufficiently understood. To address this gap, we present the first systematic study of moral sycophancy in VLMs, analyzing ten widely-used models on the Moralise and M^3oralBench datasets under explicit user disagreement. Our results reveal that VLMs frequently produce morally incorrect follow-up responses even when their initial judgments are correct, and exhibit a consistent asymmetry: models are more likely to shift from morally right to morally wrong judgments than the reverse when exposed to user-induced bias. Follow-up prompts generally degrade performance on Moralise, while yielding mixed or even improved accuracy on M^3oralBench, highlighting dataset-dependent differences in moral robustness. Evaluation using Error Introduction Rate (EIR) and Error Correction Rate (ECR) reveals a clear trade-off: models with stronger error-correction capabilities tend to introduce more reasoning errors, whereas more conservative models minimize errors but exhibit limited ability to self-correct. Finally, initial contexts with a morally right stance elicit stronger sycophantic behavior, emphasizing the vulnerability of VLMs to moral influence and the need for principled strategies to improve ethical consistency and robustness in multimodal AI systems.", "AI": {"tldr": "\u8be5\u7814\u7a76\u9996\u6b21\u7cfb\u7edf\u5206\u6790\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u9053\u5fb7\u8c04\u5a9a\u884c\u4e3a\uff0c\u53d1\u73b0\u5f53\u7528\u6237\u8868\u8fbe\u4e0d\u540c\u610f\u89c1\u65f6\uff0cVLMs\u7ecf\u5e38\u653e\u5f03\u6b63\u786e\u7684\u9053\u5fb7\u5224\u65ad\u800c\u8fce\u5408\u7528\u6237\uff0c\u4e14\u5b58\u5728\u4ece\u6b63\u786e\u8f6c\u5411\u9519\u8bef\u7684\u660e\u663e\u4e0d\u5bf9\u79f0\u6027\u3002", "motivation": "\u5148\u524d\u7814\u7a76\u4e3b\u8981\u63a2\u7d22\u4e86VLMs\u5728\u4e00\u822c\u60c5\u5883\u4e0b\u7684\u8c04\u5a9a\u884c\u4e3a\uff0c\u4f46\u5bf9\u5176\u5728\u9053\u5fb7\u57fa\u7840\u89c6\u89c9\u51b3\u7b56\u4e2d\u7684\u5f71\u54cd\u4e86\u89e3\u4e0d\u8db3\u3002\u672c\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u7cfb\u7edf\u5206\u6790VLMs\u5728\u7528\u6237\u660e\u786e\u53cd\u5bf9\u65f6\u7684\u9053\u5fb7\u8c04\u5a9a\u884c\u4e3a\u3002", "method": "\u5728Moralise\u548cM^3oralBench\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u4e8610\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684VLMs\uff0c\u901a\u8fc7\u7528\u6237\u660e\u786e\u53cd\u5bf9\u7684\u60c5\u5883\u5206\u6790\u9053\u5fb7\u8c04\u5a9a\u884c\u4e3a\u3002\u4f7f\u7528\u9519\u8bef\u5f15\u5165\u7387\uff08EIR\uff09\u548c\u9519\u8bef\u7ea0\u6b63\u7387\uff08ECR\uff09\u91cf\u5316\u6a21\u578b\u8868\u73b0\uff0c\u5e76\u5206\u6790\u4e0d\u540c\u9053\u5fb7\u7acb\u573a\u521d\u59cb\u8bed\u5883\u7684\u5f71\u54cd\u3002", "result": "VLMs\u7ecf\u5e38\u5728\u521d\u59cb\u5224\u65ad\u6b63\u786e\u7684\u60c5\u51b5\u4e0b\u4ea7\u751f\u9053\u5fb7\u9519\u8bef\u7684\u540e\u7eed\u56de\u5e94\uff1b\u5b58\u5728\u660e\u663e\u4e0d\u5bf9\u79f0\u6027\uff1a\u6a21\u578b\u66f4\u53ef\u80fd\u4ece\u9053\u5fb7\u6b63\u786e\u8f6c\u5411\u9519\u8bef\u800c\u975e\u76f8\u53cd\uff1b\u540e\u7eed\u63d0\u793a\u5728Moralise\u4e0a\u964d\u4f4e\u6027\u80fd\uff0c\u5728M^3oralBench\u4e0a\u8868\u73b0\u6df7\u5408\u751a\u81f3\u6539\u5584\uff1bEIR\u548cECR\u663e\u793a\u660e\u663e\u6743\u8861\uff1a\u7ea0\u9519\u80fd\u529b\u5f3a\u7684\u6a21\u578b\u5f15\u5165\u66f4\u591a\u63a8\u7406\u9519\u8bef\uff0c\u4fdd\u5b88\u6a21\u578b\u9519\u8bef\u5c11\u4f46\u81ea\u6211\u7ea0\u6b63\u80fd\u529b\u6709\u9650\uff1b\u9053\u5fb7\u6b63\u786e\u521d\u59cb\u8bed\u5883\u5f15\u53d1\u66f4\u5f3a\u7684\u8c04\u5a9a\u884c\u4e3a\u3002", "conclusion": "VLMs\u5bf9\u9053\u5fb7\u5f71\u54cd\u9ad8\u5ea6\u8106\u5f31\uff0c\u9700\u8981\u539f\u5219\u6027\u7b56\u7565\u6765\u63d0\u9ad8\u591a\u6a21\u6001AI\u7cfb\u7edf\u7684\u4f26\u7406\u4e00\u81f4\u6027\u548c\u9c81\u68d2\u6027\u3002\u7814\u7a76\u63ed\u793a\u4e86\u6a21\u578b\u5728\u9053\u5fb7\u51b3\u7b56\u4e2d\u8c04\u5a9a\u884c\u4e3a\u7684\u7cfb\u7edf\u6027\u6a21\u5f0f\uff0c\u4e3a\u5f00\u53d1\u66f4\u7a33\u5065\u7684\u9053\u5fb7AI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\u3002"}}
{"id": "2602.08498", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.08498", "abs": "https://arxiv.org/abs/2602.08498", "authors": ["Haoran Zhang", "Yafu Li", "Zhi Wang", "Zhilin Wang", "Shunkai Zhang", "Xiaoye Qu", "Yu Cheng"], "title": "Characterizing, Evaluating, and Optimizing Complex Reasoning", "comment": "Code and data are available at \\url{https://github.com/zzzhr97/TRM}", "summary": "Large Reasoning Models (LRMs) increasingly rely on reasoning traces with complex internal structures. However, existing work lacks a unified answer to three fundamental questions: (1) what defines high-quality reasoning, (2) how to reliably evaluate long, implicitly structured reasoning traces, and (3) how to use such evaluation signals for reasoning optimization. To address these challenges, we provide a unified perspective. (1) We introduce the ME$^2$ principle to characterize reasoning quality along macro- and micro-level concerning efficiency and effectiveness. (2) Built on this principle, we model reasoning traces as directed acyclic graphs (DAGs) and develop a DAG-based pairwise evaluation method, capturing complex reasoning structures. (3) Based on this method, we construct the TRM-Preference dataset and train a Thinking Reward Model (TRM) to evaluate reasoning quality at scale. Experiments show that thinking rewards serve as an effective optimization signal. At test time, selecting better reasoning leads to better outcomes (up to 19.3% gain), and during RL training, thinking rewards enhance reasoning and performance (up to 3.9% gain) across diverse tasks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u7edf\u4e00\u6846\u67b6\u89e3\u51b3\u5927\u578b\u63a8\u7406\u6a21\u578b\u7684\u4e09\u4e2a\u6838\u5fc3\u95ee\u9898\uff1a\u5b9a\u4e49\u63a8\u7406\u8d28\u91cf\u3001\u8bc4\u4f30\u590d\u6742\u63a8\u7406\u8f68\u8ff9\u3001\u5229\u7528\u8bc4\u4f30\u4fe1\u53f7\u4f18\u5316\u63a8\u7406\u3002\u57fa\u4e8eME\u00b2\u539f\u5219\uff08\u5b8f\u89c2\u5fae\u89c2\u6548\u7387\u6548\u679c\uff09\uff0c\u5c06\u63a8\u7406\u5efa\u6a21\u4e3a\u6709\u5411\u65e0\u73af\u56fe\uff0c\u5f00\u53d1DAG\u8bc4\u4f30\u65b9\u6cd5\uff0c\u6784\u5efaTRM-Preference\u6570\u636e\u96c6\u8bad\u7ec3\u601d\u7ef4\u5956\u52b1\u6a21\u578b\uff0c\u5b9e\u9a8c\u8bc1\u660e\u80fd\u663e\u8457\u63d0\u5347\u63a8\u7406\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u7f3a\u4e4f\u5bf9\u5927\u578b\u63a8\u7406\u6a21\u578b\u4e09\u4e2a\u57fa\u672c\u95ee\u9898\u7684\u7edf\u4e00\u89e3\u51b3\u65b9\u6848\uff1a1) \u5982\u4f55\u5b9a\u4e49\u9ad8\u8d28\u91cf\u63a8\u7406\uff1b2) \u5982\u4f55\u53ef\u9760\u8bc4\u4f30\u5177\u6709\u590d\u6742\u5185\u90e8\u7ed3\u6784\u7684\u957f\u63a8\u7406\u8f68\u8ff9\uff1b3) \u5982\u4f55\u5229\u7528\u8fd9\u79cd\u8bc4\u4f30\u4fe1\u53f7\u8fdb\u884c\u63a8\u7406\u4f18\u5316\u3002\u8fd9\u4e9b\u95ee\u9898\u963b\u788d\u4e86\u63a8\u7406\u6a21\u578b\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u3002", "method": "1) \u63d0\u51faME\u00b2\u539f\u5219\uff0c\u4ece\u5b8f\u89c2\u548c\u5fae\u89c2\u5c42\u9762\u5b9a\u4e49\u63a8\u7406\u8d28\u91cf\uff08\u6548\u7387\u548c\u6548\u679c\uff09\uff1b2) \u5c06\u63a8\u7406\u8f68\u8ff9\u5efa\u6a21\u4e3a\u6709\u5411\u65e0\u73af\u56fe\uff0c\u5f00\u53d1\u57fa\u4e8eDAG\u7684\u6210\u5bf9\u8bc4\u4f30\u65b9\u6cd5\uff1b3) \u6784\u5efaTRM-Preference\u6570\u636e\u96c6\uff0c\u8bad\u7ec3\u601d\u7ef4\u5956\u52b1\u6a21\u578b\u6765\u5927\u89c4\u6a21\u8bc4\u4f30\u63a8\u7406\u8d28\u91cf\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u601d\u7ef4\u5956\u52b1\u662f\u6709\u6548\u7684\u4f18\u5316\u4fe1\u53f7\uff1a\u5728\u6d4b\u8bd5\u65f6\u9009\u62e9\u66f4\u597d\u7684\u63a8\u7406\u80fd\u5e26\u6765\u66f4\u597d\u7684\u7ed3\u679c\uff08\u6700\u9ad819.3%\u63d0\u5347\uff09\uff1b\u5728\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u4e2d\uff0c\u601d\u7ef4\u5956\u52b1\u80fd\u589e\u5f3a\u63a8\u7406\u80fd\u529b\u548c\u6027\u80fd\uff08\u6700\u9ad83.9%\u63d0\u5347\uff09\uff0c\u5728\u591a\u6837\u5316\u4efb\u52a1\u4e2d\u5747\u6709\u6548\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u89c6\u89d2\u6765\u89e3\u51b3\u5927\u578b\u63a8\u7406\u6a21\u578b\u7684\u6838\u5fc3\u6311\u6218\uff0c\u901a\u8fc7ME\u00b2\u539f\u5219\u3001DAG\u5efa\u6a21\u548c\u601d\u7ef4\u5956\u52b1\u6a21\u578b\uff0c\u5b9e\u73b0\u4e86\u5bf9\u590d\u6742\u63a8\u7406\u8f68\u8ff9\u7684\u6709\u6548\u8bc4\u4f30\u548c\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63a8\u7406\u6a21\u578b\u7684\u6027\u80fd\u3002"}}
{"id": "2602.08335", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08335", "abs": "https://arxiv.org/abs/2602.08335", "authors": ["Yanming Li", "Xuelin Zhang", "WenJie Lu", "Ziye Tang", "Maodong Wu", "Haotian Luo", "Tongtong Wu", "Zijie Peng", "Hongze Mi", "Yibo Feng", "Naiqiang Tan", "Chao Huang", "Hong Chen", "Li Shen"], "title": "Who Deserves the Reward? SHARP: Shapley Credit-based Optimization for Multi-Agent System", "comment": null, "summary": "Integrating Large Language Models (LLMs) with external tools via multi-agent systems offers a promising new paradigm for decomposing and solving complex problems. However, training these systems remains notoriously difficult due to the credit assignment challenge, as it is often unclear which specific functional agent is responsible for the success or failure of decision trajectories. Existing methods typically rely on sparse or globally broadcast rewards, failing to capture individual contributions and leading to inefficient reinforcement learning. To address these limitations, we introduce the Shapley-based Hierarchical Attribution for Reinforcement Policy (SHARP), a novel framework for optimizing multi-agent reinforcement learning via precise credit attribution. SHARP effectively stabilizes training by normalizing agent-specific advantages across trajectory groups, primarily through a decomposed reward mechanism comprising a global broadcast-accuracy reward, a Shapley-based marginal-credit reward for each agent, and a tool-process reward to improve execution efficiency. Extensive experiments across various real-world benchmarks demonstrate that SHARP significantly outperforms recent state-of-the-art baselines, achieving average match improvements of 23.66% and 14.05% over single-agent and multi-agent approaches, respectively.", "AI": {"tldr": "SHARP\u6846\u67b6\u901a\u8fc7Shapley\u503c\u8fdb\u884c\u7cbe\u786e\u4fe1\u7528\u5206\u914d\uff0c\u4f18\u5316\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\uff0c\u89e3\u51b3LLM\u4e0e\u5916\u90e8\u5de5\u5177\u96c6\u6210\u4e2d\u7684\u4fe1\u7528\u5206\u914d\u96be\u9898", "motivation": "\u5c06LLM\u4e0e\u5916\u90e8\u5de5\u5177\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u96c6\u6210\u662f\u89e3\u51b3\u590d\u6742\u95ee\u9898\u7684\u65b0\u8303\u5f0f\uff0c\u4f46\u8bad\u7ec3\u8fd9\u4e9b\u7cfb\u7edf\u9762\u4e34\u4fe1\u7528\u5206\u914d\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u7a00\u758f\u6216\u5168\u5c40\u5e7f\u64ad\u5956\u52b1\uff0c\u65e0\u6cd5\u51c6\u786e\u6355\u6349\u4e2a\u4f53\u8d21\u732e\uff0c\u5bfc\u81f4\u5f3a\u5316\u5b66\u4e60\u6548\u7387\u4f4e\u4e0b", "method": "\u63d0\u51faSHARP\u6846\u67b6\uff0c\u901a\u8fc7\u5f52\u4e00\u5316\u8f68\u8ff9\u7ec4\u5185\u667a\u80fd\u4f53\u7279\u5b9a\u4f18\u52bf\u6765\u7a33\u5b9a\u8bad\u7ec3\uff0c\u91c7\u7528\u5206\u89e3\u7684\u5956\u52b1\u673a\u5236\uff1a\u5168\u5c40\u5e7f\u64ad\u51c6\u786e\u6027\u5956\u52b1\u3001\u57fa\u4e8eShapley\u503c\u7684\u8fb9\u9645\u4fe1\u7528\u5956\u52b1\uff08\u6bcf\u4e2a\u667a\u80fd\u4f53\uff09\u3001\u5de5\u5177\u8fc7\u7a0b\u5956\u52b1\uff08\u63d0\u9ad8\u6267\u884c\u6548\u7387\uff09", "result": "\u5728\u591a\u4e2a\u771f\u5b9e\u4e16\u754c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSHARP\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u57fa\u7ebf\uff0c\u76f8\u6bd4\u5355\u667a\u80fd\u4f53\u548c\u591a\u667a\u80fd\u4f53\u65b9\u6cd5\u5206\u522b\u5b9e\u73b0\u4e8623.66%\u548c14.05%\u7684\u5e73\u5747\u5339\u914d\u6539\u8fdb", "conclusion": "SHARP\u901a\u8fc7\u7cbe\u786e\u7684\u4fe1\u7528\u5206\u914d\u6709\u6548\u89e3\u51b3\u4e86\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u8bad\u7ec3\u7a33\u5b9a\u6027\u95ee\u9898\uff0c\u4e3aLLM\u4e0e\u5916\u90e8\u5de5\u5177\u96c6\u6210\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u4f18\u5316\u6846\u67b6"}}
{"id": "2602.08543", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2602.08543", "abs": "https://arxiv.org/abs/2602.08543", "authors": ["Yutao Zhu", "Xingshuo Zhang", "Maosen Zhang", "Jiajie Jin", "Liancheng Zhang", "Xiaoshuai Song", "Kangzhi Zhao", "Wencong Zeng", "Ruiming Tang", "Han Li", "Ji-Rong Wen", "Zhicheng Dou"], "title": "GISA: A Benchmark for General Information-Seeking Assistant", "comment": null, "summary": "The advancement of large language models (LLMs) has significantly accelerated the development of search agents capable of autonomously gathering information through multi-turn web interactions. Various benchmarks have been proposed to evaluate such agents. However, existing benchmarks often construct queries backward from answers, producing unnatural tasks misaligned with real-world needs. Moreover, these benchmarks tend to focus on either locating specific information or aggregating information from multiple sources, while relying on static answer sets prone to data contamination. To bridge these gaps, we introduce GISA, a benchmark for General Information-Seeking Assistants comprising 373 human-crafted queries that reflect authentic information-seeking scenarios. GISA features four structured answer formats (item, set, list, and table), enabling deterministic evaluation. It integrates both deep reasoning and broad information aggregation within unified tasks, and includes a live subset with periodically updated answers to resist memorization. Notably, GISA provides complete human search trajectories for every query, offering gold-standard references for process-level supervision and imitation learning. Experiments on mainstream LLMs and commercial search products reveal that even the best-performing model achieves only 19.30\\% exact match score, with performance notably degrading on tasks requiring complex planning and comprehensive information gathering. These findings highlight substantial room for future improvement.", "AI": {"tldr": "GISA\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u901a\u7528\u4fe1\u606f\u641c\u7d22\u52a9\u624b\u7684\u65b0\u57fa\u51c6\uff0c\u5305\u542b373\u4e2a\u4eba\u5de5\u6784\u5efa\u7684\u771f\u5b9e\u4fe1\u606f\u641c\u7d22\u67e5\u8be2\uff0c\u652f\u6301\u56db\u79cd\u7ed3\u6784\u5316\u7b54\u6848\u683c\u5f0f\uff0c\u5e76\u63d0\u4f9b\u5b8c\u6574\u7684\u4eba\u7c7b\u641c\u7d22\u8f68\u8ff9\u4f5c\u4e3a\u53c2\u8003\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u5b58\u5728\u4e09\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a1) \u4ece\u7b54\u6848\u53cd\u5411\u6784\u5efa\u67e5\u8be2\u5bfc\u81f4\u4efb\u52a1\u4e0d\u81ea\u7136\uff1b2) \u8981\u4e48\u4e13\u6ce8\u4e8e\u5b9a\u4f4d\u7279\u5b9a\u4fe1\u606f\uff0c\u8981\u4e48\u4e13\u6ce8\u4e8e\u591a\u6e90\u4fe1\u606f\u805a\u5408\uff0c\u7f3a\u4e4f\u7edf\u4e00\uff1b3) \u4f9d\u8d56\u9759\u6001\u7b54\u6848\u96c6\u5bb9\u6613\u53d7\u5230\u6570\u636e\u6c61\u67d3\u3002\u9700\u8981\u66f4\u771f\u5b9e\u3001\u5168\u9762\u7684\u8bc4\u4f30\u57fa\u51c6\u3002", "method": "\u521b\u5efaGISA\u57fa\u51c6\uff0c\u5305\u542b373\u4e2a\u4eba\u5de5\u6784\u5efa\u7684\u771f\u5b9e\u4fe1\u606f\u641c\u7d22\u67e5\u8be2\uff0c\u652f\u6301\u56db\u79cd\u7ed3\u6784\u5316\u7b54\u6848\u683c\u5f0f\uff08\u9879\u76ee\u3001\u96c6\u5408\u3001\u5217\u8868\u3001\u8868\u683c\uff09\uff0c\u786e\u4fdd\u786e\u5b9a\u6027\u8bc4\u4f30\u3002\u5305\u542b\u5b9e\u65f6\u5b50\u96c6\u5b9a\u671f\u66f4\u65b0\u7b54\u6848\u4ee5\u9632\u6b62\u8bb0\u5fc6\uff0c\u5e76\u63d0\u4f9b\u5b8c\u6574\u7684\u4eba\u7c7b\u641c\u7d22\u8f68\u8ff9\u4f5c\u4e3a\u8fc7\u7a0b\u7ea7\u76d1\u7763\u548c\u6a21\u4eff\u5b66\u4e60\u7684\u53c2\u8003\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u5373\u4f7f\u662f\u6027\u80fd\u6700\u597d\u7684\u6a21\u578b\u4e5f\u4ec5\u8fbe\u523019.30%\u7684\u7cbe\u786e\u5339\u914d\u5206\u6570\u3002\u5728\u9700\u8981\u590d\u6742\u89c4\u5212\u548c\u5168\u9762\u4fe1\u606f\u6536\u96c6\u7684\u4efb\u52a1\u4e0a\uff0c\u6027\u80fd\u663e\u8457\u4e0b\u964d\u3002\u8fd9\u8868\u660e\u73b0\u6709\u6a21\u578b\u5728\u771f\u5b9e\u4fe1\u606f\u641c\u7d22\u573a\u666f\u4e2d\u4ecd\u6709\u5f88\u5927\u6539\u8fdb\u7a7a\u95f4\u3002", "conclusion": "GISA\u57fa\u51c6\u586b\u8865\u4e86\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u7684\u7a7a\u767d\uff0c\u63d0\u4f9b\u4e86\u66f4\u771f\u5b9e\u3001\u5168\u9762\u7684\u4fe1\u606f\u641c\u7d22\u52a9\u624b\u8bc4\u4f30\u6846\u67b6\u3002\u5b9e\u9a8c\u7ed3\u679c\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u5728\u590d\u6742\u4fe1\u606f\u641c\u7d22\u4efb\u52a1\u4e0a\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u6307\u660e\u4e86\u6539\u8fdb\u65b9\u5411\u3002"}}
{"id": "2602.08339", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.08339", "abs": "https://arxiv.org/abs/2602.08339", "authors": ["Chengyi Du", "Yazhe Niu", "Dazhong Shen", "Luxin Xu"], "title": "CoTZero: Annotation-Free Human-Like Vision Reasoning via Hierarchical Synthetic CoT", "comment": "16 pages 6 figures", "summary": "Recent advances in vision-language models (VLMs) have markedly improved image-text alignment, yet they still fall short of human-like visual reasoning. A key limitation is that many VLMs rely on surface correlations rather than building logically coherent structured representations, which often leads to missed higher-level semantic structure and non-causal relational understanding, hindering compositional and verifiable reasoning. To address these limitations by introducing human models into the reasoning process, we propose CoTZero, an annotation-free paradigm with two components: (i) a dual-stage data synthesis approach and (ii) a cognition-aligned training method. In the first component, we draw inspiration from neurocognitive accounts of compositional productivity and global-to-local analysis. In the bottom-up stage, CoTZero extracts atomic visual primitives and incrementally composes them into diverse, structured question-reasoning forms. In the top-down stage, it enforces hierarchical reasoning by using coarse global structure to guide the interpretation of local details and causal relations. In the cognition-aligned training component, built on the synthesized CoT data, we introduce Cognitively Coherent Verifiable Rewards (CCVR) in Reinforcement Fine-Tuning (RFT) to further strengthen VLMs' hierarchical reasoning and generalization, providing stepwise feedback on reasoning coherence and factual correctness. Experiments show that CoTZero achieves an F1 score of 83.33 percent on our multi-level semantic inconsistency benchmark with lexical-perturbation negatives, across both in-domain and out-of-domain settings. Ablations confirm that each component contributes to more interpretable and human-aligned visual reasoning.", "AI": {"tldr": "CoTZero\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u6807\u6ce8\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u8303\u5f0f\uff0c\u901a\u8fc7\u53cc\u9636\u6bb5\u6570\u636e\u5408\u6210\u548c\u8ba4\u77e5\u5bf9\u9f50\u8bad\u7ec3\u65b9\u6cd5\uff0c\u63d0\u5347\u6a21\u578b\u7684\u5c42\u6b21\u5316\u63a8\u7406\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e3b\u8981\u4f9d\u8d56\u8868\u9762\u76f8\u5173\u6027\u800c\u975e\u903b\u8f91\u8fde\u8d2f\u7684\u7ed3\u6784\u5316\u8868\u793a\uff0c\u5bfc\u81f4\u7f3a\u4e4f\u9ad8\u5c42\u6b21\u8bed\u4e49\u7ed3\u6784\u548c\u975e\u56e0\u679c\u5173\u7cfb\u7406\u89e3\uff0c\u9650\u5236\u4e86\u7ec4\u5408\u6027\u548c\u53ef\u9a8c\u8bc1\u63a8\u7406\u80fd\u529b\u3002", "method": "CoTZero\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a(1) \u53cc\u9636\u6bb5\u6570\u636e\u5408\u6210\u65b9\u6cd5\uff1a\u81ea\u4e0b\u800c\u4e0a\u9636\u6bb5\u63d0\u53d6\u539f\u5b50\u89c6\u89c9\u57fa\u5143\u5e76\u7ec4\u5408\u6210\u7ed3\u6784\u5316\u95ee\u9898\u63a8\u7406\u5f62\u5f0f\uff1b\u81ea\u4e0a\u800c\u4e0b\u9636\u6bb5\u4f7f\u7528\u5168\u5c40\u7ed3\u6784\u6307\u5bfc\u5c40\u90e8\u7ec6\u8282\u548c\u56e0\u679c\u5173\u7cfb\u7684\u89e3\u91ca\uff1b(2) \u8ba4\u77e5\u5bf9\u9f50\u8bad\u7ec3\u65b9\u6cd5\uff1a\u5728\u5408\u6210\u6570\u636e\u57fa\u7840\u4e0a\uff0c\u901a\u8fc7\u5f3a\u5316\u5fae\u8c03\u4e2d\u7684\u8ba4\u77e5\u4e00\u81f4\u53ef\u9a8c\u8bc1\u5956\u52b1\u673a\u5236\uff0c\u63d0\u4f9b\u63a8\u7406\u8fde\u8d2f\u6027\u548c\u4e8b\u5b9e\u6b63\u786e\u6027\u7684\u9010\u6b65\u53cd\u9988\u3002", "result": "\u5728\u591a\u7ea7\u8bed\u4e49\u4e0d\u4e00\u81f4\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCoTZero\u5728\u5305\u542b\u8bcd\u6c47\u6270\u52a8\u8d1f\u6837\u672c\u7684\u8de8\u9886\u57df\u8bbe\u7f6e\u4e0b\u8fbe\u5230\u4e8683.33%\u7684F1\u5206\u6570\u3002\u6d88\u878d\u5b9e\u9a8c\u8bc1\u5b9e\u6bcf\u4e2a\u7ec4\u4ef6\u90fd\u5bf9\u63d0\u5347\u53ef\u89e3\u91ca\u6027\u548c\u4eba\u7c7b\u5bf9\u9f50\u7684\u89c6\u89c9\u63a8\u7406\u6709\u8d21\u732e\u3002", "conclusion": "CoTZero\u901a\u8fc7\u5f15\u5165\u4eba\u7c7b\u8ba4\u77e5\u6a21\u578b\u5230\u63a8\u7406\u8fc7\u7a0b\u4e2d\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u7ed3\u6784\u5316\u8868\u793a\u548c\u903b\u8f91\u63a8\u7406\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u5b9e\u73b0\u4e86\u66f4\u53ef\u89e3\u91ca\u548c\u4eba\u7c7b\u5bf9\u9f50\u7684\u89c6\u89c9\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2602.08548", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.08548", "abs": "https://arxiv.org/abs/2602.08548", "authors": ["Xuanliang Zhang", "Dingzirui Wang", "Keyan Xu", "Qingfu Zhu", "Wanxiang Che"], "title": "How Do Language Models Understand Tables? A Mechanistic Analysis of Cell Location", "comment": null, "summary": "While Large Language Models (LLMs) are increasingly deployed for table-related tasks, the internal mechanisms enabling them to process linearized two-dimensional structured tables remain opaque. In this work, we investigate the process of table understanding by dissecting the atomic task of cell location. Through activation patching and complementary interpretability techniques, we delineate the table understanding mechanism into a sequential three-stage pipeline: Semantic Binding, Coordinate Localization, and Information Extraction. We demonstrate that models locate the target cell via an ordinal mechanism that counts discrete delimiters to resolve coordinates. Furthermore, column indices are encoded within a linear subspace that allows for precise steering of model focus through vector arithmetic. Finally, we reveal that models generalize to multi-cell location tasks by multiplexing the identical attention heads identified during atomic location. Our findings provide a comprehensive explanation of table understanding within Transformer architectures.", "AI": {"tldr": "\u8bba\u6587\u901a\u8fc7\u6fc0\u6d3b\u4fee\u8865\u7b49\u6280\u672f\u63ed\u793a\u4e86LLM\u5904\u7406\u8868\u683c\u7684\u5185\u90e8\u673a\u5236\uff0c\u5c06\u8868\u683c\u7406\u89e3\u5206\u89e3\u4e3a\u8bed\u4e49\u7ed1\u5b9a\u3001\u5750\u6807\u5b9a\u4f4d\u548c\u4fe1\u606f\u63d0\u53d6\u4e09\u9636\u6bb5\u6d41\u7a0b\uff0c\u53d1\u73b0\u6a21\u578b\u901a\u8fc7\u8ba1\u6570\u5206\u9694\u7b26\u7684\u5e8f\u6570\u673a\u5236\u5b9a\u4f4d\u5355\u5143\u683c\uff0c\u5e76\u5728\u7ebf\u6027\u5b50\u7a7a\u95f4\u4e2d\u7f16\u7801\u5217\u7d22\u5f15\u3002", "motivation": "\u867d\u7136\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8d8a\u6765\u8d8a\u591a\u5730\u7528\u4e8e\u5904\u7406\u8868\u683c\u76f8\u5173\u4efb\u52a1\uff0c\u4f46\u5176\u5904\u7406\u7ebf\u6027\u5316\u4e8c\u7ef4\u7ed3\u6784\u5316\u8868\u683c\u7684\u5185\u90e8\u673a\u5236\u4ecd\u7136\u4e0d\u900f\u660e\u3002\u672c\u7814\u7a76\u65e8\u5728\u63ed\u793aLLM\u7406\u89e3\u8868\u683c\u7684\u5185\u90e8\u5de5\u4f5c\u539f\u7406\u3002", "method": "\u4f7f\u7528\u6fc0\u6d3b\u4fee\u8865\u6280\u672f\u548c\u4e92\u8865\u7684\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\uff0c\u5c06\u8868\u683c\u7406\u89e3\u673a\u5236\u5206\u89e3\u4e3a\u539f\u5b50\u4efb\u52a1\u2014\u2014\u5355\u5143\u683c\u5b9a\u4f4d\uff0c\u901a\u8fc7\u5206\u6790\u6a21\u578b\u5185\u90e8\u8868\u793a\u6765\u63ed\u793a\u5176\u5de5\u4f5c\u673a\u5236\u3002", "result": "\u53d1\u73b0\u6a21\u578b\u901a\u8fc7\u8ba1\u6570\u79bb\u6563\u5206\u9694\u7b26\u7684\u5e8f\u6570\u673a\u5236\u6765\u89e3\u6790\u5750\u6807\u5b9a\u4f4d\u5355\u5143\u683c\uff1b\u5217\u7d22\u5f15\u7f16\u7801\u5728\u7ebf\u6027\u5b50\u7a7a\u95f4\u4e2d\uff0c\u53ef\u901a\u8fc7\u5411\u91cf\u7b97\u672f\u7cbe\u786e\u5f15\u5bfc\u6a21\u578b\u6ce8\u610f\u529b\uff1b\u591a\u5355\u5143\u683c\u5b9a\u4f4d\u4efb\u52a1\u901a\u8fc7\u590d\u7528\u539f\u5b50\u5b9a\u4f4d\u4e2d\u8bc6\u522b\u7684\u76f8\u540c\u6ce8\u610f\u529b\u5934\u5b9e\u73b0\u6cdb\u5316\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86Transformer\u67b6\u6784\u4e2d\u8868\u683c\u7406\u89e3\u7684\u4e09\u9636\u6bb5\u673a\u5236\uff0c\u4e3a\u7406\u89e3LLM\u5904\u7406\u7ed3\u6784\u5316\u6570\u636e\u63d0\u4f9b\u4e86\u5168\u9762\u89e3\u91ca\uff0c\u8868\u660e\u6a21\u578b\u901a\u8fc7\u5e8f\u6570\u8ba1\u6570\u548c\u7ebf\u6027\u5b50\u7a7a\u95f4\u7f16\u7801\u7b49\u673a\u5236\u5b9e\u73b0\u8868\u683c\u5b9a\u4f4d\u529f\u80fd\u3002"}}
{"id": "2602.08600", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.08600", "abs": "https://arxiv.org/abs/2602.08600", "authors": ["Archchana Sindhujan", "Girish A. Koushik", "Shenbin Qian", "Diptesh Kanojia", "Constantin Or\u0103san"], "title": "Beyond Scalar Scores: Reinforcement Learning for Error-Aware Quality Estimation of Machine Translation", "comment": "Currently this article is under review for Natural Language Processing Journal", "summary": "Quality Estimation (QE) aims to assess the quality of machine translation (MT) outputs without relying on reference translations, making it essential for real-world, large-scale MT evaluation. Large Language Models (LLMs) have shown significant promise in advancing the field of quality estimation of machine translation. However, most of the QE approaches solely rely on scalar quality scores, offering no explicit information about the translation errors that should drive these judgments. Moreover, for low-resource languages where annotated QE data is limited, existing approaches struggle to achieve reliable performance. To address these challenges, we introduce the first segment-level QE dataset for English to Malayalam, a severely resource-scarce language pair in the QE domain, comprising human-annotated Direct Assessment (DA) scores and Translation Quality Remarks (TQR), which are short, contextual, free-form annotator comments that describe translation errors. We further introduce ALOPE-RL, a policy-based reinforcement learning framework that trains efficient adapters based on policy rewards derived from DA score and TQR. Integrating error-aware rewards with ALOPE-RL, enables LLMs to reason about translation quality beyond numeric scores. Despite being trained on a small-scale QE dataset, ALOPE-RL achieves state-of-the-art performance on English to Malayalam QE using compact LLMs (<=4B parameters}) fine-tuned with LoRA and 4-bit quantization, outperforming both larger LLM-based baselines and leading encoder-based QE models. Our results demonstrate that error-aware, policy-based learning can deliver strong QE performance under limited data and compute budgets. We release our dataset, code, and trained models to support future research.", "AI": {"tldr": "\u672c\u6587\u9488\u5bf9\u4f4e\u8d44\u6e90\u8bed\u8a00\u5bf9\uff08\u82f1\u8bed-\u9a6c\u62c9\u96c5\u62c9\u59c6\u8bed\uff09\u7684\u8d28\u91cf\u8bc4\u4f30\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u9996\u4e2a\u5305\u542b\u4eba\u5de5\u6807\u6ce8\u76f4\u63a5\u8bc4\u4f30\u5206\u6570\u548c\u7ffb\u8bd1\u8d28\u91cf\u8bc4\u6ce8\u7684\u7247\u6bb5\u7ea7QE\u6570\u636e\u96c6\uff0c\u5e76\u5f00\u53d1\u4e86ALOPE-RL\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u9519\u8bef\u611f\u77e5\u5956\u52b1\u673a\u5236\uff0c\u4f7fLLM\u80fd\u591f\u8d85\u8d8a\u6570\u503c\u8bc4\u5206\u8fdb\u884c\u7ffb\u8bd1\u8d28\u91cf\u63a8\u7406\u3002", "motivation": "\u5f53\u524d\u8d28\u91cf\u8bc4\u4f30\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u6807\u91cf\u8d28\u91cf\u5206\u6570\uff0c\u7f3a\u4e4f\u660e\u786e\u7684\u7ffb\u8bd1\u9519\u8bef\u4fe1\u606f\uff1b\u5bf9\u4e8e\u4f4e\u8d44\u6e90\u8bed\u8a00\uff0c\u7531\u4e8e\u6807\u6ce8\u6570\u636e\u6709\u9650\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u83b7\u5f97\u53ef\u9760\u6027\u80fd\u3002\u9700\u8981\u5f00\u53d1\u65e2\u80fd\u8bc4\u4f30\u8d28\u91cf\u53c8\u80fd\u89e3\u91ca\u9519\u8bef\u539f\u56e0\uff0c\u4e14\u5728\u6570\u636e\u6709\u9650\u60c5\u51b5\u4e0b\u4ecd\u80fd\u6709\u6548\u5de5\u4f5c\u7684QE\u65b9\u6cd5\u3002", "method": "1) \u521b\u5efa\u9996\u4e2a\u82f1\u8bed-\u9a6c\u62c9\u96c5\u62c9\u59c6\u8bed\u7247\u6bb5\u7ea7QE\u6570\u636e\u96c6\uff0c\u5305\u542b\u76f4\u63a5\u8bc4\u4f30\u5206\u6570\u548c\u7ffb\u8bd1\u8d28\u91cf\u8bc4\u6ce8\uff1b2) \u63d0\u51faALOPE-RL\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u57fa\u4e8e\u7b56\u7565\u5956\u52b1\u8bad\u7ec3\u9ad8\u6548\u9002\u914d\u5668\uff0c\u5956\u52b1\u6765\u81eaDA\u5206\u6570\u548cTQR\uff1b3) \u4f7f\u7528LoRA\u548c4\u4f4d\u91cf\u5316\u6280\u672f\u5fae\u8c03\u7d27\u51d1\u578bLLM\uff08\u22644B\u53c2\u6570\uff09\u3002", "result": "ALOPE-RL\u5728\u5c0f\u89c4\u6a21QE\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u540e\uff0c\u5728\u82f1\u8bed-\u9a6c\u62c9\u96c5\u62c9\u59c6\u8bedQE\u4efb\u52a1\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u8d85\u8d8a\u4e86\u66f4\u5927\u7684LLM\u57fa\u7ebf\u548c\u9886\u5148\u7684\u7f16\u7801\u5668\u57faQE\u6a21\u578b\u3002\u8bc1\u660e\u4e86\u9519\u8bef\u611f\u77e5\u3001\u57fa\u4e8e\u7b56\u7565\u7684\u5b66\u4e60\u5728\u6709\u9650\u6570\u636e\u548c\u8ba1\u7b97\u9884\u7b97\u4e0b\u4ecd\u80fd\u63d0\u4f9b\u5f3a\u5927\u7684QE\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u5408\u9519\u8bef\u611f\u77e5\u5956\u52b1\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u53ef\u4ee5\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u5bf9\u7684\u8d28\u91cf\u8bc4\u4f30\u4e2d\u5b9e\u73b0\u9ad8\u6027\u80fd\uff0c\u5373\u4f7f\u4f7f\u7528\u5c0f\u578bLLM\u548c\u6709\u9650\u6570\u636e\u3002\u8be5\u65b9\u6cd5\u4e3a\u8d44\u6e90\u7a00\u7f3a\u73af\u5883\u4e0b\u7684QE\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\uff0c\u5e76\u53d1\u5e03\u4e86\u6570\u636e\u96c6\u3001\u4ee3\u7801\u548c\u6a21\u578b\u4ee5\u652f\u6301\u672a\u6765\u7814\u7a76\u3002"}}
{"id": "2602.07595", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07595", "abs": "https://arxiv.org/abs/2602.07595", "authors": ["Yuanzhi Liang", "Xuan'er Wu", "Yirui Liu", "Yijie Fang", "Yizhen Fan", "Ke Hao", "Rui Li", "Ruiying Liu", "Ziqi Ni", "Peng Yu", "Yanbo Wang", "Haibin Huang", "Qizhen Weng", "Chi Zhang", "Xuelong Li"], "title": "TeleBoost: A Systematic Alignment Framework for High-Fidelity, Controllable, and Robust Video Generation", "comment": null, "summary": "Post-training is the decisive step for converting a pretrained video generator into a production-oriented model that is instruction-following, controllable, and robust over long temporal horizons. This report presents a systematical post-training framework that organizes supervised policy shaping, reward-driven reinforcement learning, and preference-based refinement into a single stability-constrained optimization stack. The framework is designed around practical video-generation constraints, including high rollout cost, temporally compounding failure modes, and feedback that is heterogeneous, uncertain, and often weakly discriminative. By treating optimization as a staged, diagnostic-driven process rather than a collection of isolated tricks, the report summarizes a cohesive recipe for improving perceptual fidelity, temporal coherence, and prompt adherence while preserving the controllability established at initialization. The resulting framework provides a clear blueprint for building scalable post-training pipelines that remain stable, extensible, and effective in real-world deployment settings.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7cfb\u7edf\u5316\u7684\u89c6\u9891\u751f\u6210\u6a21\u578b\u540e\u8bad\u7ec3\u6846\u67b6\uff0c\u5c06\u76d1\u7763\u7b56\u7565\u5851\u9020\u3001\u5956\u52b1\u9a71\u52a8\u7684\u5f3a\u5316\u5b66\u4e60\u548c\u57fa\u4e8e\u504f\u597d\u7684\u7cbe\u70bc\u6574\u5408\u5230\u4e00\u4e2a\u7a33\u5b9a\u6027\u7ea6\u675f\u7684\u4f18\u5316\u5806\u6808\u4e2d\uff0c\u65e8\u5728\u89e3\u51b3\u5b9e\u9645\u89c6\u9891\u751f\u6210\u4e2d\u7684\u9ad8\u6210\u672c\u3001\u65f6\u95f4\u7d2f\u79ef\u5931\u8d25\u6a21\u5f0f\u548c\u5f02\u6784\u53cd\u9988\u7b49\u6311\u6218\u3002", "motivation": "\u540e\u8bad\u7ec3\u662f\u5c06\u9884\u8bad\u7ec3\u89c6\u9891\u751f\u6210\u5668\u8f6c\u5316\u4e3a\u751f\u4ea7\u7ea7\u6a21\u578b\u7684\u5173\u952e\u6b65\u9aa4\uff0c\u9700\u8981\u4f7f\u6a21\u578b\u80fd\u591f\u9075\u5faa\u6307\u4ee4\u3001\u53ef\u63a7\u4e14\u5177\u6709\u957f\u65f6\u95f4\u7a33\u5b9a\u6027\u3002\u7136\u800c\uff0c\u5b9e\u9645\u89c6\u9891\u751f\u6210\u9762\u4e34\u9ad8\u8ba1\u7b97\u6210\u672c\u3001\u65f6\u95f4\u7d2f\u79ef\u7684\u5931\u8d25\u6a21\u5f0f\u4ee5\u53ca\u5f02\u6784\u3001\u4e0d\u786e\u5b9a\u4e14\u5224\u522b\u6027\u5f31\u7684\u53cd\u9988\u7b49\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u7cfb\u7edf\u5316\u7684\u540e\u8bad\u7ec3\u6846\u67b6\uff0c\u5c06\u76d1\u7763\u7b56\u7565\u5851\u9020\u3001\u5956\u52b1\u9a71\u52a8\u7684\u5f3a\u5316\u5b66\u4e60\u548c\u57fa\u4e8e\u504f\u597d\u7684\u7cbe\u70bc\u6574\u5408\u5230\u5355\u4e00\u7a33\u5b9a\u6027\u7ea6\u675f\u4f18\u5316\u5806\u6808\u4e2d\u3002\u91c7\u7528\u5206\u9636\u6bb5\u3001\u8bca\u65ad\u9a71\u52a8\u7684\u65b9\u6cd5\u800c\u975e\u5b64\u7acb\u6280\u5de7\u7684\u96c6\u5408\uff0c\u56f4\u7ed5\u5b9e\u9645\u89c6\u9891\u751f\u6210\u7ea6\u675f\u8fdb\u884c\u8bbe\u8ba1\u3002", "result": "\u8be5\u6846\u67b6\u63d0\u4f9b\u4e86\u6e05\u6670\u7684\u84dd\u56fe\uff0c\u80fd\u591f\u63d0\u5347\u611f\u77e5\u4fdd\u771f\u5ea6\u3001\u65f6\u95f4\u4e00\u81f4\u6027\u548c\u63d0\u793a\u9075\u5faa\u80fd\u529b\uff0c\u540c\u65f6\u4fdd\u6301\u521d\u59cb\u5316\u65f6\u5efa\u7acb\u7684\u53ef\u63a7\u6027\u3002\u6784\u5efa\u4e86\u53ef\u6269\u5c55\u7684\u540e\u8bad\u7ec3\u7ba1\u9053\uff0c\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\u4fdd\u6301\u7a33\u5b9a\u3001\u53ef\u6269\u5c55\u548c\u6709\u6548\u3002", "conclusion": "\u901a\u8fc7\u5c06\u591a\u79cd\u4f18\u5316\u6280\u672f\u6574\u5408\u5230\u7edf\u4e00\u7684\u7a33\u5b9a\u6027\u7ea6\u675f\u6846\u67b6\u4e2d\uff0c\u8be5\u7814\u7a76\u4e3a\u6784\u5efa\u751f\u4ea7\u7ea7\u89c6\u9891\u751f\u6210\u6a21\u578b\u63d0\u4f9b\u4e86\u7cfb\u7edf\u5316\u7684\u540e\u8bad\u7ec3\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u5b9e\u9645\u90e8\u7f72\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u611f\u77e5\u8d28\u91cf\u3001\u65f6\u95f4\u8fde\u8d2f\u6027\u548c\u6307\u4ee4\u9075\u5faa\u80fd\u529b\u7684\u534f\u540c\u63d0\u5347\u3002"}}
{"id": "2602.08344", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08344", "abs": "https://arxiv.org/abs/2602.08344", "authors": ["Qi Guo", "Jianing Wang", "Deyang Kong", "Xiangyu Xi", "Jianfei Zhang", "Yi Lu", "Jingang Wang", "Wei Wang", "Shikun Zhang", "Wei Ye"], "title": "OPE: Overcoming Information Saturation in Parallel Thinking via Outline-Guided Path Exploration", "comment": null, "summary": "Parallel thinking has emerged as a new paradigm for large reasoning models (LRMs) in tackling complex problems. Recent methods leverage Reinforcement Learning (RL) to enhance parallel thinking, aiming to address the limitations in computational resources and effectiveness encountered with supervised fine-tuning. However, most existing studies primarily focus on optimizing the aggregation phase, with limited attention to the path exploration stage. In this paper, we theoretically analyze the optimization of parallel thinking under the Reinforcement Learning with Verifiable Rewards (RLVR) setting, and identify that the mutual information bottleneck among exploration paths fundamentally restricts overall performance. To address this, we propose Outline-Guided Path Exploration (OPE), which explicitly partitions the solution space by generating diverse reasoning outlines prior to parallel path reasoning, thereby reducing information redundancy and improving the diversity of information captured across exploration paths. We implement OPE with an iterative RL strategy that optimizes outline planning and outline-guided reasoning independently. Extensive experiments across multiple challenging mathematical benchmarks demonstrate that OPE effectively improves reasoning performance in different aggregation strategies, enabling LRMs to more reliably discover correct solutions.", "AI": {"tldr": "\u63d0\u51faOutline-Guided Path Exploration (OPE)\u65b9\u6cd5\uff0c\u901a\u8fc7\u751f\u6210\u591a\u6837\u5316\u7684\u63a8\u7406\u5927\u7eb2\u6765\u5f15\u5bfc\u5e76\u884c\u8def\u5f84\u63a2\u7d22\uff0c\u89e3\u51b3\u5e76\u884c\u601d\u7ef4\u4e2d\u63a2\u7d22\u8def\u5f84\u95f4\u4e92\u4fe1\u606f\u74f6\u9888\u95ee\u9898\uff0c\u63d0\u5347\u5927\u578b\u63a8\u7406\u6a21\u578b\u7684\u6570\u5b66\u95ee\u9898\u89e3\u51b3\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u5e76\u884c\u601d\u7ef4\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u805a\u5408\u9636\u6bb5\u4f18\u5316\uff0c\u5bf9\u8def\u5f84\u63a2\u7d22\u9636\u6bb5\u5173\u6ce8\u6709\u9650\u3002\u7814\u7a76\u53d1\u73b0\u63a2\u7d22\u8def\u5f84\u95f4\u7684\u4e92\u4fe1\u606f\u74f6\u9888\u9650\u5236\u4e86\u6574\u4f53\u6027\u80fd\uff0c\u9700\u8981\u89e3\u51b3\u8def\u5f84\u63a2\u7d22\u4e2d\u7684\u4fe1\u606f\u5197\u4f59\u548c\u591a\u6837\u6027\u4e0d\u8db3\u95ee\u9898\u3002", "method": "\u63d0\u51faOutline-Guided Path Exploration (OPE)\u65b9\u6cd5\uff1a1) \u5148\u751f\u6210\u591a\u6837\u5316\u7684\u63a8\u7406\u5927\u7eb2\u6765\u5212\u5206\u89e3\u7a7a\u95f4\uff1b2) \u57fa\u4e8e\u5927\u7eb2\u8fdb\u884c\u5e76\u884c\u8def\u5f84\u63a8\u7406\uff1b3) \u91c7\u7528\u8fed\u4ee3\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u5206\u522b\u4f18\u5316\u5927\u7eb2\u89c4\u5212\u548c\u57fa\u4e8e\u5927\u7eb2\u7684\u63a8\u7406\u3002", "result": "\u5728\u591a\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u6570\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u8fdb\u884c\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u8bc1\u660eOPE\u80fd\u6709\u6548\u63d0\u5347\u4e0d\u540c\u805a\u5408\u7b56\u7565\u4e0b\u7684\u63a8\u7406\u6027\u80fd\uff0c\u4f7f\u5927\u578b\u63a8\u7406\u6a21\u578b\u66f4\u53ef\u9760\u5730\u53d1\u73b0\u6b63\u786e\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "OPE\u901a\u8fc7\u663e\u5f0f\u5212\u5206\u89e3\u7a7a\u95f4\u5e76\u51cf\u5c11\u4fe1\u606f\u5197\u4f59\uff0c\u89e3\u51b3\u4e86\u5e76\u884c\u601d\u7ef4\u4e2d\u63a2\u7d22\u8def\u5f84\u95f4\u7684\u4e92\u4fe1\u606f\u74f6\u9888\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5927\u578b\u63a8\u7406\u6a21\u578b\u5728\u590d\u6742\u6570\u5b66\u95ee\u9898\u4e0a\u7684\u8868\u73b0\u3002"}}
{"id": "2602.08607", "categories": ["cs.CL", "cs.SD"], "pdf": "https://arxiv.org/pdf/2602.08607", "abs": "https://arxiv.org/abs/2602.08607", "authors": ["Ziyang Cheng", "Yuhao Wang", "Heyang Liu", "Ronghua Wu", "Qunshan Gu", "Yanfeng Wang", "Yu Wang"], "title": "VocalNet-MDM: Accelerating Streaming Speech LLM via Self-Distilled Masked Diffusion Modeling", "comment": null, "summary": "Recent Speech Large Language Models~(LLMs) have achieved impressive capabilities in end-to-end speech interaction. However, the prevailing autoregressive paradigm imposes strict serial constraints, limiting generation efficiency and introducing exposure bias. In this paper, we investigate Masked Diffusion Modeling~(MDM) as a non-autoregressive paradigm for speech LLMs and introduce VocalNet-MDM. To adapt MDM for streaming speech interaction, we address two critical challenges: training-inference mismatch and iterative overhead. We propose Hierarchical Block-wise Masking to align training objectives with the progressive masked states encountered during block diffusion decoding, and Iterative Self-Distillation to compress multi-step refinement into fewer steps for low-latency inference. Trained on a limited scale of only 6K hours of speech data, VocalNet-MDM achieves a 3.7$\\times$--10$\\times$ decoding speedup and reduces first-chunk latency by 34\\% compared to AR baselines. It maintains competitive recognition accuracy while achieving state-of-the-art text quality and speech naturalness, demonstrating that MDM is a promising and scalable alternative for low-latency, efficient speech LLMs.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51faVocalNet-MDM\uff0c\u4e00\u79cd\u57fa\u4e8e\u63a9\u7801\u6269\u6563\u5efa\u6a21\u7684\u975e\u81ea\u56de\u5f52\u8bed\u97f3\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u81ea\u56de\u5f52\u6a21\u578b\u7684\u6548\u7387\u9650\u5236\uff0c\u5b9e\u73b0\u4e863.7-10\u500d\u7684\u89e3\u7801\u52a0\u901f\u548c34%\u7684\u9996\u5757\u5ef6\u8fdf\u964d\u4f4e\u3002", "motivation": "\u5f53\u524d\u81ea\u56de\u5f52\u8bed\u97f3\u5927\u8bed\u8a00\u6a21\u578b\u5b58\u5728\u4e25\u683c\u4e32\u884c\u7ea6\u675f\uff0c\u9650\u5236\u4e86\u751f\u6210\u6548\u7387\u5e76\u5f15\u5165\u4e86\u66dd\u5149\u504f\u5dee\u3002\u9700\u8981\u63a2\u7d22\u975e\u81ea\u56de\u5f52\u8303\u5f0f\u6765\u63d0\u9ad8\u8bed\u97f3\u4ea4\u4e92\u7684\u6548\u7387\u548c\u964d\u4f4e\u5ef6\u8fdf\u3002", "method": "\u63d0\u51faVocalNet-MDM\uff0c\u91c7\u7528\u63a9\u7801\u6269\u6563\u5efa\u6a21\u4f5c\u4e3a\u975e\u81ea\u56de\u5f52\u8303\u5f0f\u3002\u5f15\u5165\u5206\u5c42\u5757\u7ea7\u63a9\u7801\u6765\u5bf9\u9f50\u8bad\u7ec3\u76ee\u6807\u4e0e\u5757\u6269\u6563\u89e3\u7801\u4e2d\u7684\u6e10\u8fdb\u63a9\u7801\u72b6\u6001\uff0c\u4ee5\u53ca\u8fed\u4ee3\u81ea\u84b8\u998f\u6765\u5c06\u591a\u6b65\u4f18\u5316\u538b\u7f29\u5230\u66f4\u5c11\u6b65\u9aa4\u4ee5\u5b9e\u73b0\u4f4e\u5ef6\u8fdf\u63a8\u7406\u3002", "result": "\u4ec5\u4f7f\u75286K\u5c0f\u65f6\u8bed\u97f3\u6570\u636e\u8bad\u7ec3\uff0cVocalNet-MDM\u5b9e\u73b0\u4e863.7-10\u500d\u7684\u89e3\u7801\u52a0\u901f\uff0c\u9996\u5757\u5ef6\u8fdf\u964d\u4f4e34%\u3002\u5728\u4fdd\u6301\u7ade\u4e89\u6027\u8bc6\u522b\u51c6\u786e\u7387\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6587\u672c\u8d28\u91cf\u548c\u8bed\u97f3\u81ea\u7136\u5ea6\u3002", "conclusion": "\u63a9\u7801\u6269\u6563\u5efa\u6a21\u662f\u4f4e\u5ef6\u8fdf\u3001\u9ad8\u6548\u8bed\u97f3\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6709\u524d\u666f\u4e14\u53ef\u6269\u5c55\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347\u6d41\u5f0f\u8bed\u97f3\u4ea4\u4e92\u7684\u6548\u7387\u3002"}}
{"id": "2602.07605", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07605", "abs": "https://arxiv.org/abs/2602.07605", "authors": ["Hulingxiao He", "Zijun Geng", "Yuxin Peng"], "title": "Fine-R1: Make Multi-modal LLMs Excel in Fine-Grained Visual Recognition by Chain-of-Thought Reasoning", "comment": "Published as a conference paper at ICLR 2026. The models are available at https://huggingface.co/collections/StevenHH2000/fine-r1", "summary": "Any entity in the visual world can be hierarchically grouped based on shared characteristics and mapped to fine-grained sub-categories. While Multi-modal Large Language Models (MLLMs) achieve strong performance on coarse-grained visual tasks, they often struggle with Fine-Grained Visual Recognition (FGVR). Adapting general-purpose MLLMs to FGVR typically requires large amounts of annotated data, which is costly to obtain, leaving a substantial performance gap compared to contrastive CLIP models dedicated for discriminative tasks. Moreover, MLLMs tend to overfit to seen sub-categories and generalize poorly to unseen ones. To address these challenges, we propose Fine-R1, an MLLM tailored for FGVR through an R1-style training framework: (1) Chain-of-Thought Supervised Fine-tuning, where we construct a high-quality FGVR CoT dataset with rationales of \"visual analysis, candidate sub-categories, comparison, and prediction\", transition the model into a strong open-world classifier; and (2) Triplet Augmented Policy Optimization, where Intra-class Augmentation mixes trajectories from anchor and positive images within the same category to improve robustness to intra-class variance, while Inter-class Augmentation maximizes the response distinction conditioned on images across sub-categories to enhance discriminative ability. With only 4-shot training, Fine-R1 outperforms existing general MLLMs, reasoning MLLMs, and even contrastive CLIP models in identifying both seen and unseen sub-categories, showing promise in working in knowledge-intensive domains where gathering expert annotations for all sub-categories is arduous. Code is available at https://github.com/PKU-ICST-MIPL/FineR1_ICLR2026.", "AI": {"tldr": "Fine-R1\u662f\u4e00\u4e2a\u4e13\u95e8\u7528\u4e8e\u7ec6\u7c92\u5ea6\u89c6\u89c9\u8bc6\u522b\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7R1\u98ce\u683c\u8bad\u7ec3\u6846\u67b6\uff0c\u4ec5\u97004-shot\u8bad\u7ec3\u5c31\u80fd\u8d85\u8d8a\u73b0\u6709\u901a\u7528MLLM\u548c\u5bf9\u6bd4CLIP\u6a21\u578b\uff0c\u5728\u8bc6\u522b\u5df2\u89c1\u548c\u672a\u89c1\u5b50\u7c7b\u522b\u4e0a\u90fd\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u7c97\u7c92\u5ea6\u89c6\u89c9\u4efb\u52a1\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u7ec6\u7c92\u5ea6\u89c6\u89c9\u8bc6\u522b\u65b9\u9762\u5b58\u5728\u56f0\u96be\u3002\u9002\u5e94FGVR\u901a\u5e38\u9700\u8981\u5927\u91cf\u6807\u6ce8\u6570\u636e\uff0c\u6210\u672c\u9ad8\u6602\uff0c\u4e14\u6027\u80fd\u4ecd\u4e0d\u5982\u4e13\u95e8\u7528\u4e8e\u5224\u522b\u4efb\u52a1\u7684\u5bf9\u6bd4CLIP\u6a21\u578b\u3002\u6b64\u5916\uff0cMLLM\u5bb9\u6613\u5bf9\u5df2\u89c1\u5b50\u7c7b\u522b\u8fc7\u62df\u5408\uff0c\u5bf9\u672a\u89c1\u5b50\u7c7b\u522b\u6cdb\u5316\u80fd\u529b\u5dee\u3002", "method": "\u63d0\u51faFine-R1\u6a21\u578b\uff0c\u91c7\u7528R1\u98ce\u683c\u8bad\u7ec3\u6846\u67b6\uff1a1) \u601d\u7ef4\u94fe\u76d1\u7763\u5fae\u8c03\uff1a\u6784\u5efa\u9ad8\u8d28\u91cfFGVR CoT\u6570\u636e\u96c6\uff0c\u5305\u542b\"\u89c6\u89c9\u5206\u6790\u3001\u5019\u9009\u5b50\u7c7b\u522b\u3001\u6bd4\u8f83\u548c\u9884\u6d4b\"\u7684\u63a8\u7406\u8fc7\u7a0b\uff0c\u5c06\u6a21\u578b\u8f6c\u53d8\u4e3a\u5f3a\u5927\u7684\u5f00\u653e\u4e16\u754c\u5206\u7c7b\u5668\uff1b2) \u4e09\u5143\u7ec4\u589e\u5f3a\u7b56\u7565\u4f18\u5316\uff1a\u901a\u8fc7\u7c7b\u5185\u589e\u5f3a\u6df7\u5408\u540c\u4e00\u7c7b\u522b\u5185\u951a\u70b9\u548c\u6b63\u6837\u672c\u56fe\u50cf\u7684\u8f68\u8ff9\u6765\u63d0\u9ad8\u5bf9\u7c7b\u5185\u53d8\u5316\u7684\u9c81\u68d2\u6027\uff0c\u901a\u8fc7\u7c7b\u95f4\u589e\u5f3a\u6700\u5927\u5316\u8de8\u5b50\u7c7b\u522b\u56fe\u50cf\u6761\u4ef6\u4e0b\u7684\u54cd\u5e94\u5dee\u5f02\u6765\u589e\u5f3a\u5224\u522b\u80fd\u529b\u3002", "result": "\u4ec5\u97004-shot\u8bad\u7ec3\uff0cFine-R1\u5728\u8bc6\u522b\u5df2\u89c1\u548c\u672a\u89c1\u5b50\u7c7b\u522b\u4e0a\u90fd\u8d85\u8d8a\u4e86\u73b0\u6709\u7684\u901a\u7528MLLM\u3001\u63a8\u7406MLLM\u751a\u81f3\u5bf9\u6bd4CLIP\u6a21\u578b\uff0c\u5728\u77e5\u8bc6\u5bc6\u96c6\u578b\u9886\u57df\u5c55\u73b0\u51fa\u6f5c\u529b\u3002", "conclusion": "Fine-R1\u901a\u8fc7\u521b\u65b0\u7684\u8bad\u7ec3\u6846\u67b6\u89e3\u51b3\u4e86MLLM\u5728\u7ec6\u7c92\u5ea6\u89c6\u89c9\u8bc6\u522b\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u5728\u5c11\u91cf\u6837\u672c\u4e0b\u5b9e\u73b0\u4e86\u4f18\u5f02\u7684\u6027\u80fd\uff0c\u4e3a\u96be\u4ee5\u83b7\u53d6\u4e13\u5bb6\u6807\u6ce8\u7684\u77e5\u8bc6\u5bc6\u96c6\u578b\u9886\u57df\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.08353", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08353", "abs": "https://arxiv.org/abs/2602.08353", "authors": ["Zhang Jiasheng", "Li Zhangpin", "Wang Mingzhe", "Shao Jie", "Cui Jiangtao", "Li Hui"], "title": "Towards Better Evolution Modeling for Temporal Knowledge Graphs", "comment": "13 pages, 11 figures", "summary": "Temporal knowledge graphs (TKGs) structurally preserve evolving human knowledge. Recent research has focused on designing models to learn the evolutionary nature of TKGs to predict future facts, achieving impressive results. For instance, Hits@10 scores over 0.9 on YAGO dataset. However, we find that existing benchmarks inadvertently introduce a shortcut. Near state-of-the-art performance can be simply achieved by counting co-occurrences, without using any temporal information. In this work, we examine the root cause of this issue, identifying inherent biases in current datasets and over simplified form of evaluation task that can be exploited by these biases. Through this analysis, we further uncover additional limitations of existing benchmarks, including unreasonable formatting of time-interval knowledge, ignorance of learning knowledge obsolescence, and insufficient information for precise evolution understanding, all of which can amplify the shortcut and hinder a fair assessment. Therefore, we introduce the TKG evolution benchmark. It includes four bias-corrected datasets and two novel tasks closely aligned with the evolution process, promoting a more accurate understanding of the challenges in TKG evolution modeling. Benchmark is available at: https://github.com/zjs123/TKG-Benchmark.", "AI": {"tldr": "\u73b0\u6709TKG\u57fa\u51c6\u5b58\u5728\u4e25\u91cd\u7f3a\u9677\uff1a\u4ec5\u901a\u8fc7\u7edf\u8ba1\u5171\u73b0\u5c31\u80fd\u8fbe\u5230\u63a5\u8fd1SOTA\u7684\u6027\u80fd\uff0c\u65e0\u9700\u4f7f\u7528\u65f6\u95f4\u4fe1\u606f\uff0c\u8fd9\u66b4\u9732\u4e86\u6570\u636e\u96c6\u56fa\u6709\u504f\u5dee\u548c\u8bc4\u4f30\u4efb\u52a1\u8fc7\u4e8e\u7b80\u5316\u7684\u95ee\u9898\u3002", "motivation": "\u7814\u7a76\u53d1\u73b0\u73b0\u6709\u65f6\u5e8f\u77e5\u8bc6\u56fe\u8c31\uff08TKG\uff09\u57fa\u51c6\u5b58\u5728\u4e25\u91cd\u95ee\u9898\uff1a\u5373\u4f7f\u4e0d\u4f7f\u7528\u4efb\u4f55\u65f6\u95f4\u4fe1\u606f\uff0c\u4ec5\u901a\u8fc7\u7edf\u8ba1\u5b9e\u4f53\u5171\u73b0\u5c31\u80fd\u8fbe\u5230\u63a5\u8fd1\u6700\u5148\u8fdb\u6c34\u5e73\u7684\u6027\u80fd\u3002\u8fd9\u8868\u660e\u73b0\u6709\u57fa\u51c6\u65e0\u610f\u4e2d\u5f15\u5165\u4e86\u6377\u5f84\uff0c\u65e0\u6cd5\u516c\u5e73\u8bc4\u4f30TKG\u6f14\u5316\u5efa\u6a21\u7684\u771f\u6b63\u6311\u6218\u3002", "method": "\u901a\u8fc7\u5206\u6790\u73b0\u6709\u57fa\u51c6\u95ee\u9898\u7684\u6839\u6e90\uff0c\u8bc6\u522b\u51fa\u6570\u636e\u96c6\u56fa\u6709\u504f\u5dee\u548c\u8bc4\u4f30\u4efb\u52a1\u8fc7\u4e8e\u7b80\u5316\u7b49\u6838\u5fc3\u95ee\u9898\u3002\u5728\u6b64\u57fa\u7840\u4e0a\u6784\u5efa\u4e86TKG\u6f14\u5316\u57fa\u51c6\uff0c\u5305\u62ec\u56db\u4e2a\u504f\u5dee\u6821\u6b63\u540e\u7684\u6570\u636e\u96c6\u548c\u4e24\u4e2a\u4e0e\u6f14\u5316\u8fc7\u7a0b\u7d27\u5bc6\u76f8\u5173\u7684\u65b0\u4efb\u52a1\u3002", "result": "\u63ed\u793a\u4e86\u73b0\u6709TKG\u57fa\u51c6\u7684\u591a\u4e2a\u5c40\u9650\u6027\uff1a\u65f6\u95f4\u95f4\u9694\u77e5\u8bc6\u7684\u4e0d\u5408\u7406\u683c\u5f0f\u5316\u3001\u5ffd\u7565\u77e5\u8bc6\u8fc7\u65f6\u6027\u5b66\u4e60\u3001\u6f14\u5316\u7406\u89e3\u4fe1\u606f\u4e0d\u8db3\u7b49\u3002\u8fd9\u4e9b\u95ee\u9898\u90fd\u653e\u5927\u4e86\u6377\u5f84\u6548\u5e94\uff0c\u963b\u788d\u4e86\u516c\u5e73\u8bc4\u4f30\u3002", "conclusion": "\u63d0\u51fa\u4e86TKG\u6f14\u5316\u57fa\u51c6\uff0c\u901a\u8fc7\u504f\u5dee\u6821\u6b63\u6570\u636e\u96c6\u548c\u66f4\u8d34\u8fd1\u5b9e\u9645\u6f14\u5316\u8fc7\u7a0b\u7684\u65b0\u4efb\u52a1\uff0c\u4e3aTKG\u6f14\u5316\u5efa\u6a21\u63d0\u4f9b\u4e86\u66f4\u51c6\u786e\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u4fc3\u8fdb\u5bf9TKG\u6f14\u5316\u6311\u6218\u7684\u66f4\u51c6\u786e\u7406\u89e3\u3002"}}
{"id": "2602.08625", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.08625", "abs": "https://arxiv.org/abs/2602.08625", "authors": ["Muhammad Naufil"], "title": "Do Multilingual LLMs have specialized language heads?", "comment": null, "summary": "Multilingual large language models (LLMs) have gained significant popularity for their ability to process and generate text across multiple languages. However, deploying these models in production can be inefficient when only a subset of the supported languages is of interest. There has been some research conducted on identifying whether machine translation models have language-specific or language-agnostic heads, however no research has been conducted for multilingual LLMs, to the best of our knowledge, that as we know are capable of performing diverse tasks beyond just translation. This paper explores whether multilingual LLMs have specialized language attention heads for each language, and investigates the possibility of removing language-specific heads for unwanted languages without degrading performance in the targeted languages. Our findings could inform more efficient deployment strategies for multilingual LLMs, enabling reduced model complexity while maintaining high accuracy for targeted languages.", "AI": {"tldr": "\u7814\u7a76\u63a2\u7d22\u591a\u8bed\u8a00\u5927\u8bed\u8a00\u6a21\u578b\u662f\u5426\u5177\u6709\u9488\u5bf9\u7279\u5b9a\u8bed\u8a00\u7684\u6ce8\u610f\u529b\u5934\uff0c\u5e76\u5c1d\u8bd5\u79fb\u9664\u4e0d\u9700\u8981\u8bed\u8a00\u7684\u5934\u800c\u4e0d\u5f71\u54cd\u76ee\u6807\u8bed\u8a00\u6027\u80fd", "motivation": "\u591a\u8bed\u8a00\u5927\u8bed\u8a00\u6a21\u578b\u5728\u751f\u4ea7\u90e8\u7f72\u4e2d\u6548\u7387\u4f4e\u4e0b\uff0c\u5f53\u53ea\u5173\u6ce8\u90e8\u5206\u652f\u6301\u8bed\u8a00\u65f6\uff0c\u6a21\u578b\u590d\u6742\u5ea6\u53ef\u80fd\u8fc7\u9ad8\u3002\u76ee\u524d\u7f3a\u4e4f\u5bf9\u591a\u8bed\u8a00LLM\u8bed\u8a00\u7279\u5b9a\u6ce8\u610f\u529b\u5934\u7684\u7814\u7a76\uff0c\u800c\u8fd9\u7c7b\u6a21\u578b\u80fd\u6267\u884c\u7ffb\u8bd1\u4e4b\u5916\u7684\u591a\u6837\u5316\u4efb\u52a1", "method": "\u7814\u7a76\u591a\u8bed\u8a00LLM\u662f\u5426\u5177\u6709\u4e13\u95e8\u7684\u8bed\u8a00\u6ce8\u610f\u529b\u5934\uff0c\u5e76\u63a2\u7d22\u79fb\u9664\u4e0d\u9700\u8981\u8bed\u8a00\u7684\u5934\u800c\u4e0d\u964d\u4f4e\u76ee\u6807\u8bed\u8a00\u6027\u80fd\u7684\u53ef\u80fd\u6027", "result": "\u7814\u7a76\u53d1\u73b0\u591a\u8bed\u8a00LLM\u786e\u5b9e\u5b58\u5728\u8bed\u8a00\u7279\u5b9a\u7684\u6ce8\u610f\u529b\u5934\uff0c\u53ef\u4ee5\u79fb\u9664\u4e0d\u9700\u8981\u8bed\u8a00\u7684\u5934\u800c\u4e0d\u663e\u8457\u5f71\u54cd\u76ee\u6807\u8bed\u8a00\u6027\u80fd", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a\u591a\u8bed\u8a00LLM\u7684\u66f4\u9ad8\u6548\u90e8\u7f72\u7b56\u7565\u63d0\u4f9b\u4f9d\u636e\uff0c\u80fd\u591f\u5728\u4fdd\u6301\u76ee\u6807\u8bed\u8a00\u9ad8\u51c6\u786e\u6027\u7684\u540c\u65f6\u51cf\u5c11\u6a21\u578b\u590d\u6742\u5ea6"}}
{"id": "2602.07608", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.07608", "abs": "https://arxiv.org/abs/2602.07608", "authors": ["Yixin Chen", "Ziyu Su", "Lingbin Meng", "Elshad Hasanov", "Wei Chen", "Anil Parwani", "M. Khalid Khan Niazi"], "title": "HistoMet: A Pan-Cancer Deep Learning Framework for Prognostic Prediction of Metastatic Progression and Site Tropism from Primary Tumor Histopathology", "comment": null, "summary": "Metastatic Progression remains the leading cause of cancer-related mortality, yet predicting whether a primary tumor will metastasize and where it will disseminate directly from histopathology remains a fundamental challenge. Although whole-slide images (WSIs) provide rich morphological information, prior computational pathology approaches typically address metastatic status or site prediction as isolated tasks, and do not explicitly model the clinically sequential decision process of metastatic risk assessment followed by downstream site-specific evaluation. To address this research gap, we present a decision-aware, concept-aligned MIL framework, HistoMet, for prognostic metastatic outcome prediction from primary tumor WSIs. Our proposed framework adopts a two-module prediction pipeline in which the likelihood of metastatic progression from the primary tumor is first estimated, followed by conditional prediction of metastatic site for high-risk cases. To guide representation learning and improve clinical interpretability, our framework integrates linguistically defined and data-adaptive metastatic concepts through a pretrained pathology vision-language model. We evaluate HistoMet on a multi-institutional pan-cancer cohort of 6504 patients with metastasis follow-up and site annotations. Under clinically relevant high-sensitivity screening settings (95 percent sensitivity), HistoMet significantly reduces downstream workload while maintaining high metastatic risk recall. Conditional on metastatic cases, HistoMet achieves a macro F1 of 74.6 with a standard deviation of 1.3 and a macro one-vs-rest AUC of 92.1. These results demonstrate that explicitly modeling clinical decision structure enables robust and deployable prognostic prediction of metastatic progression and site tropism directly from primary tumor histopathology.", "AI": {"tldr": "HistoMet\u662f\u4e00\u4e2a\u51b3\u7b56\u611f\u77e5\u3001\u6982\u5ff5\u5bf9\u9f50\u7684\u591a\u5b9e\u4f8b\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u539f\u53d1\u80bf\u7624\u5168\u5207\u7247\u56fe\u50cf\u9884\u6d4b\u8f6c\u79fb\u9884\u540e\u7ed3\u679c\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u9884\u6d4b\u6d41\u7a0b\u5e76\u6574\u5408\u8bed\u8a00\u5b9a\u4e49\u548c\u6570\u636e\u81ea\u9002\u5e94\u8f6c\u79fb\u6982\u5ff5\u3002", "motivation": "\u8f6c\u79fb\u8fdb\u5c55\u662f\u764c\u75c7\u76f8\u5173\u6b7b\u4ea1\u7684\u4e3b\u8981\u539f\u56e0\uff0c\u4f46\u4ece\u7ec4\u7ec7\u75c5\u7406\u5b66\u76f4\u63a5\u9884\u6d4b\u539f\u53d1\u80bf\u7624\u662f\u5426\u4f1a\u8f6c\u79fb\u4ee5\u53ca\u8f6c\u79fb\u90e8\u4f4d\u4ecd\u7136\u662f\u4e00\u4e2a\u57fa\u672c\u6311\u6218\u3002\u73b0\u6709\u8ba1\u7b97\u65b9\u6cd5\u901a\u5e38\u5c06\u8f6c\u79fb\u72b6\u6001\u6216\u90e8\u4f4d\u9884\u6d4b\u4f5c\u4e3a\u5b64\u7acb\u4efb\u52a1\u5904\u7406\uff0c\u6ca1\u6709\u660e\u786e\u6a21\u62df\u4e34\u5e8a\u987a\u5e8f\u51b3\u7b56\u8fc7\u7a0b\u3002", "method": "\u63d0\u51faHistoMet\u6846\u67b6\uff0c\u91c7\u7528\u4e24\u6a21\u5757\u9884\u6d4b\u6d41\u7a0b\uff1a\u9996\u5148\u4f30\u8ba1\u539f\u53d1\u80bf\u7624\u8f6c\u79fb\u8fdb\u5c55\u7684\u53ef\u80fd\u6027\uff0c\u7136\u540e\u5bf9\u9ad8\u98ce\u9669\u75c5\u4f8b\u8fdb\u884c\u6761\u4ef6\u6027\u8f6c\u79fb\u90e8\u4f4d\u9884\u6d4b\u3002\u901a\u8fc7\u9884\u8bad\u7ec3\u75c5\u7406\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u6574\u5408\u8bed\u8a00\u5b9a\u4e49\u548c\u6570\u636e\u81ea\u9002\u5e94\u7684\u8f6c\u79fb\u6982\u5ff5\uff0c\u6307\u5bfc\u8868\u793a\u5b66\u4e60\u5e76\u63d0\u9ad8\u4e34\u5e8a\u53ef\u89e3\u91ca\u6027\u3002", "result": "\u57286504\u540d\u5177\u6709\u8f6c\u79fb\u968f\u8bbf\u548c\u90e8\u4f4d\u6ce8\u91ca\u7684\u591a\u673a\u6784\u6cdb\u764c\u961f\u5217\u4e2d\u8bc4\u4f30\u3002\u5728\u4e34\u5e8a\u76f8\u5173\u9ad8\u7075\u654f\u5ea6\u7b5b\u67e5\u8bbe\u7f6e\u4e0b\uff0895%\u7075\u654f\u5ea6\uff09\uff0cHistoMet\u663e\u8457\u51cf\u5c11\u4e0b\u6e38\u5de5\u4f5c\u91cf\u540c\u65f6\u4fdd\u6301\u9ad8\u8f6c\u79fb\u98ce\u9669\u53ec\u56de\u7387\u3002\u5bf9\u4e8e\u8f6c\u79fb\u75c5\u4f8b\uff0c\u83b7\u5f97\u5b8f\u89c2F1\u5206\u657074.6\uff08\u6807\u51c6\u5dee1.3\uff09\u548c\u5b8f\u89c2\u4e00\u5bf9\u591aAUC 92.1\u3002", "conclusion": "\u660e\u786e\u6a21\u62df\u4e34\u5e8a\u51b3\u7b56\u7ed3\u6784\u80fd\u591f\u76f4\u63a5\u4ece\u539f\u53d1\u80bf\u7624\u7ec4\u7ec7\u75c5\u7406\u5b66\u5b9e\u73b0\u7a33\u5065\u4e14\u53ef\u90e8\u7f72\u7684\u8f6c\u79fb\u8fdb\u5c55\u548c\u90e8\u4f4d\u8d8b\u5411\u6027\u9884\u540e\u9884\u6d4b\u3002"}}
{"id": "2602.08354", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08354", "abs": "https://arxiv.org/abs/2602.08354", "authors": ["Zixuan Huang", "Xin Xia", "Yuxi Ren", "Jianbin Zheng", "Xuanda Wang", "Zhixia Zhang", "Hongyan Xie", "Songshi Liang", "Zehao Chen", "Xuefeng Xiao", "Fuzhen Zhuang", "Jianxin Li", "Yikun Ban", "Deqing Wang"], "title": "Does Your Reasoning Model Implicitly Know When to Stop Thinking?", "comment": null, "summary": "Recent advancements in large reasoning models (LRMs) have greatly improved their capabilities on complex reasoning tasks through Long Chains of Thought (CoTs). However, this approach often results in substantial redundancy, impairing computational efficiency and causing significant delays in real-time applications. Recent studies show that longer reasoning chains are frequently uncorrelated with correctness and can even be detrimental to accuracy. In a further in-depth analysis of this phenomenon, we surprisingly uncover and empirically verify that LRMs implicitly know the appropriate time to stop thinking, while this capability is obscured by current sampling paradigms. Motivated by this, we introduce SAGE (Self-Aware Guided Efficient Reasoning), a novel sampling paradigm that unleashes this efficient reasoning potential. Furthermore, integrating SAGE as mixed sampling into group-based reinforcement learning (SAGE-RL) enables SAGE-RL to effectively incorporate SAGE-discovered efficient reasoning patterns into standard pass@1 inference, markedly enhancing both the reasoning accuracy and efficiency of LRMs across multiple challenging mathematical benchmarks.", "AI": {"tldr": "SAGE\u662f\u4e00\u79cd\u65b0\u7684\u91c7\u6837\u8303\u5f0f\uff0c\u901a\u8fc7\u91ca\u653e\u5927\u578b\u63a8\u7406\u6a21\u578b\u7684\u81ea\u6211\u505c\u6b62\u601d\u8003\u80fd\u529b\uff0c\u663e\u8457\u63d0\u5347\u63a8\u7406\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u5f53\u524d\u5927\u578b\u63a8\u7406\u6a21\u578b\u4f7f\u7528\u957f\u601d\u7ef4\u94fe\u65b9\u6cd5\u5b58\u5728\u5927\u91cf\u5197\u4f59\uff0c\u635f\u5bb3\u8ba1\u7b97\u6548\u7387\u5e76\u5bfc\u81f4\u5b9e\u65f6\u5e94\u7528\u5ef6\u8fdf\u3002\u7814\u7a76\u53d1\u73b0\u957f\u63a8\u7406\u94fe\u4e0e\u6b63\u786e\u6027\u65e0\u5173\u751a\u81f3\u6709\u5bb3\uff0c\u800c\u6a21\u578b\u672c\u8eab\u9690\u542b\u77e5\u9053\u4f55\u65f6\u505c\u6b62\u601d\u8003\u7684\u80fd\u529b\u88ab\u5f53\u524d\u91c7\u6837\u8303\u5f0f\u6240\u63a9\u76d6\u3002", "method": "\u63d0\u51faSAGE\uff08\u81ea\u6211\u611f\u77e5\u5f15\u5bfc\u9ad8\u6548\u63a8\u7406\uff09\u91c7\u6837\u8303\u5f0f\uff0c\u91ca\u653e\u6a21\u578b\u7684\u81ea\u6211\u505c\u6b62\u601d\u8003\u80fd\u529b\u3002\u8fdb\u4e00\u6b65\u5c06SAGE\u4f5c\u4e3a\u6df7\u5408\u91c7\u6837\u6574\u5408\u5230\u57fa\u4e8e\u7fa4\u4f53\u7684\u5f3a\u5316\u5b66\u4e60\u4e2d\uff08SAGE-RL\uff09\uff0c\u4f7fSAGE-RL\u80fd\u591f\u5c06SAGE\u53d1\u73b0\u7684\u9ad8\u6548\u63a8\u7406\u6a21\u5f0f\u878d\u5165\u6807\u51c6pass@1\u63a8\u7406\u3002", "result": "SAGE-RL\u663e\u8457\u63d0\u5347\u4e86\u5927\u578b\u63a8\u7406\u6a21\u578b\u5728\u591a\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u6570\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684\u63a8\u7406\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "conclusion": "\u901a\u8fc7\u91ca\u653e\u6a21\u578b\u5185\u5728\u7684\u81ea\u6211\u505c\u6b62\u601d\u8003\u80fd\u529b\uff0cSAGE\u91c7\u6837\u8303\u5f0f\u80fd\u591f\u663e\u8457\u63d0\u5347\u5927\u578b\u63a8\u7406\u6a21\u578b\u7684\u63a8\u7406\u6548\u7387\u548c\u51c6\u786e\u6027\uff0c\u4e3a\u9ad8\u6548\u63a8\u7406\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2602.08658", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.08658", "abs": "https://arxiv.org/abs/2602.08658", "authors": ["Mingzi Cao", "Xingwei Tan", "Mahmud Akhter", "Marco Valentino", "Maria Liakata", "Xi Wang", "Nikolaos Aletras"], "title": "Fundamental Reasoning Paradigms Induce Out-of-Domain Generalization in Language Models", "comment": null, "summary": "Deduction, induction, and abduction are fundamental reasoning paradigms, core for human logical thinking. Although improving Large Language Model (LLM) reasoning has attracted significant research efforts, the extent to which the fundamental paradigms induce generalization has yet to be systematically explored. In this study, we shed light on how the interplay between these core paradigms influences LLMs' reasoning behavior. To this end, we first collect a new dataset of reasoning trajectories from symbolic tasks, each targeting one of the three fundamental paradigms, to abstract from concrete world knowledge. Then, we investigate effective ways for inducing these skills into LLMs. We experiment with a battery of methods including simple fine-tuning, and more complex approaches to increase model depth, or transform a dense model to a mixture-of-experts. We comprehensively evaluate induced models on realistic out-of-domain tasks, that are entirely formulated in natural language and contain real-world knowledge. Our results reveal that our approach yields strong generalizability with substantial performance gains (up to $14.60$) across realistic tasks.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5982\u4f55\u5c06\u6f14\u7ece\u3001\u5f52\u7eb3\u548c\u6eaf\u56e0\u4e09\u79cd\u57fa\u672c\u63a8\u7406\u8303\u5f0f\u5f15\u5165\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u7b26\u53f7\u4efb\u52a1\u6570\u636e\u96c6\u8bad\u7ec3\u6a21\u578b\uff0c\u5728\u73b0\u5b9e\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6cdb\u5316\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u5c3d\u7ba1\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u6539\u8fdb\u5438\u5f15\u4e86\u5927\u91cf\u7814\u7a76\u5173\u6ce8\uff0c\u4f46\u57fa\u672c\u63a8\u7406\u8303\u5f0f\uff08\u6f14\u7ece\u3001\u5f52\u7eb3\u3001\u6eaf\u56e0\uff09\u5982\u4f55\u5f71\u54cd\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u5c1a\u672a\u5f97\u5230\u7cfb\u7edf\u63a2\u7d22\u3002\u672c\u7814\u7a76\u65e8\u5728\u63ed\u793a\u8fd9\u4e9b\u6838\u5fc3\u8303\u5f0f\u4e4b\u95f4\u7684\u76f8\u4e92\u4f5c\u7528\u5982\u4f55\u5f71\u54cd\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u884c\u4e3a\u3002", "method": "\u9996\u5148\u6536\u96c6\u4e86\u4e00\u4e2a\u65b0\u7684\u7b26\u53f7\u4efb\u52a1\u63a8\u7406\u8f68\u8ff9\u6570\u636e\u96c6\uff0c\u6bcf\u4e2a\u4efb\u52a1\u9488\u5bf9\u4e09\u79cd\u57fa\u672c\u63a8\u7406\u8303\u5f0f\u4e4b\u4e00\uff0c\u4ee5\u62bd\u8c61\u5316\u5177\u4f53\u4e16\u754c\u77e5\u8bc6\u3002\u7136\u540e\u7814\u7a76\u4e86\u5c06\u8fd9\u4e9b\u6280\u80fd\u5f15\u5165\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u5305\u62ec\u7b80\u5355\u5fae\u8c03\u3001\u589e\u52a0\u6a21\u578b\u6df1\u5ea6\u7684\u65b9\u6cd5\uff0c\u4ee5\u53ca\u5c06\u5bc6\u96c6\u6a21\u578b\u8f6c\u6362\u4e3a\u4e13\u5bb6\u6df7\u5408\u6a21\u578b\u7b49\u590d\u6742\u65b9\u6cd5\u3002", "result": "\u5728\u5b8c\u5168\u7528\u81ea\u7136\u8bed\u8a00\u8868\u8ff0\u4e14\u5305\u542b\u771f\u5b9e\u4e16\u754c\u77e5\u8bc6\u7684\u73b0\u5b9e\u9886\u57df\u5916\u4efb\u52a1\u4e0a\u8fdb\u884c\u5168\u9762\u8bc4\u4f30\uff0c\u7ed3\u679c\u663e\u793a\u8be5\u65b9\u6cd5\u5177\u6709\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5728\u73b0\u5b9e\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff08\u6700\u9ad8\u8fbe14.60\u5206\uff09\u3002", "conclusion": "\u901a\u8fc7\u7cfb\u7edf\u5730\u5c06\u57fa\u672c\u63a8\u7406\u8303\u5f0f\u5f15\u5165\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u6a21\u578b\u5728\u73b0\u5b9e\u4efb\u52a1\u4e2d\u7684\u6cdb\u5316\u6027\u80fd\uff0c\u4e3a\u6539\u8fdb\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u63d0\u4f9b\u4e86\u6709\u6548\u9014\u5f84\u3002"}}
{"id": "2602.08362", "categories": ["cs.AI", "cs.LG", "cs.LO"], "pdf": "https://arxiv.org/pdf/2602.08362", "abs": "https://arxiv.org/abs/2602.08362", "authors": ["Chunxi Ji", "Adnan Darwiche"], "title": "Circuit Representations of Random Forests with Applications to XAI", "comment": null, "summary": "We make three contributions in this paper. First, we present an approach for compiling a random forest classifier into a set of circuits, where each circuit directly encodes the instances in some class of the classifier. We show empirically that our proposed approach is significantly more efficient than existing similar approaches. Next, we utilize this approach to further obtain circuits that are tractable for computing the complete and general reasons of a decision, which are instance abstractions that play a fundamental role in computing explanations. Finally, we propose algorithms for computing the robustness of a decision and all shortest ways to flip it. We illustrate the utility of our contributions by using them to enumerate all sufficient reasons, necessary reasons and contrastive explanations of decisions; to compute the robustness of decisions; and to identify all shortest ways to flip the decisions made by random forest classifiers learned from a wide range of datasets.", "AI": {"tldr": "\u5c06\u968f\u673a\u68ee\u6797\u5206\u7c7b\u5668\u7f16\u8bd1\u4e3a\u7535\u8def\uff0c\u7528\u4e8e\u9ad8\u6548\u8ba1\u7b97\u51b3\u7b56\u89e3\u91ca\u548c\u9c81\u68d2\u6027\u5206\u6790", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u8ba1\u7b97\u968f\u673a\u68ee\u6797\u5206\u7c7b\u5668\u7684\u51b3\u7b56\u89e3\u91ca\u65f6\u6548\u7387\u8f83\u4f4e\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u6765\u83b7\u53d6\u51b3\u7b56\u7684\u5b8c\u6574\u539f\u56e0\u3001\u9c81\u68d2\u6027\u5206\u6790\u548c\u51b3\u7b56\u7ffb\u8f6c\u8def\u5f84", "method": "1) \u63d0\u51fa\u5c06\u968f\u673a\u68ee\u6797\u7f16\u8bd1\u4e3a\u7535\u8def\u7684\u65b9\u6cd5\uff0c\u6bcf\u4e2a\u7535\u8def\u76f4\u63a5\u7f16\u7801\u5206\u7c7b\u5668\u4e2d\u67d0\u4e2a\u7c7b\u522b\u7684\u5b9e\u4f8b\uff1b2) \u5229\u7528\u8be5\u65b9\u6cd5\u83b7\u5f97\u53ef\u5904\u7406\u7684\u7535\u8def\uff0c\u7528\u4e8e\u8ba1\u7b97\u51b3\u7b56\u7684\u5b8c\u6574\u548c\u4e00\u822c\u539f\u56e0\uff1b3) \u63d0\u51fa\u8ba1\u7b97\u51b3\u7b56\u9c81\u68d2\u6027\u548c\u6240\u6709\u6700\u77ed\u51b3\u7b56\u7ffb\u8f6c\u8def\u5f84\u7684\u7b97\u6cd5", "result": "\u63d0\u51fa\u7684\u65b9\u6cd5\u6bd4\u73b0\u6709\u7c7b\u4f3c\u65b9\u6cd5\u663e\u8457\u66f4\u9ad8\u6548\uff0c\u80fd\u591f\u679a\u4e3e\u6240\u6709\u5145\u5206\u539f\u56e0\u3001\u5fc5\u8981\u539f\u56e0\u548c\u5bf9\u6bd4\u89e3\u91ca\uff0c\u8ba1\u7b97\u51b3\u7b56\u9c81\u68d2\u6027\uff0c\u5e76\u8bc6\u522b\u968f\u673a\u68ee\u6797\u5206\u7c7b\u5668\u51b3\u7b56\u7684\u6240\u6709\u6700\u77ed\u7ffb\u8f6c\u8def\u5f84", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u968f\u673a\u68ee\u6797\u5206\u7c7b\u5668\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u51b3\u7b56\u89e3\u91ca\u6846\u67b6\uff0c\u652f\u6301\u591a\u79cd\u89e3\u91ca\u4efb\u52a1\uff0c\u5305\u62ec\u539f\u56e0\u679a\u4e3e\u3001\u9c81\u68d2\u6027\u5206\u6790\u548c\u51b3\u7b56\u7ffb\u8f6c\u8def\u5f84\u8bc6\u522b"}}
{"id": "2602.08672", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.08672", "abs": "https://arxiv.org/abs/2602.08672", "authors": ["Clemencia Siro", "Pourya Aliannejadi", "Mohammad Aliannejadi"], "title": "Learning to Judge: LLMs Designing and Applying Evaluation Rubrics", "comment": "Accepted at EACL 2026 Findings", "summary": "Large language models (LLMs) are increasingly used as evaluators for natural language generation, applying human-defined rubrics to assess system outputs. However, human rubrics are often static and misaligned with how models internally represent language quality. We introduce GER-Eval (Generating Evaluation Rubrics for Evaluation) to investigate whether LLMs can design and apply their own evaluation rubrics. We evaluate the semantic coherence and scoring reliability of LLM-defined criteria and their alignment with human criteria. LLMs reliably generate interpretable and task-aware evaluation dimensions and apply them consistently within models, but their scoring reliability degrades in factual and knowledge-intensive settings. Closed-source models such as GPT-4o achieve higher agreement and cross-model generalization than open-weight models such as Llama. Our findings position evaluation as a learned linguistic capability of LLMs, consistent within models but fragmented across them, and call for new methods that jointly model human and LLM evaluative language to improve reliability and interpretability.", "AI": {"tldr": "LLMs\u80fd\u591f\u751f\u6210\u5e76\u5e94\u7528\u81ea\u5df1\u7684\u8bc4\u4f30\u6807\u51c6\uff0c\u4f46\u5728\u4e8b\u5b9e\u6027\u548c\u77e5\u8bc6\u5bc6\u96c6\u578b\u4efb\u52a1\u4e2d\u8bc4\u5206\u53ef\u9760\u6027\u4e0b\u964d\uff0c\u4e0d\u540c\u6a21\u578b\u95f4\u7684\u8bc4\u4f30\u6807\u51c6\u5b58\u5728\u5dee\u5f02\u3002", "motivation": "\u4eba\u7c7b\u5b9a\u4e49\u7684\u8bc4\u4f30\u6807\u51c6\u901a\u5e38\u662f\u9759\u6001\u7684\uff0c\u4e14\u4e0eLLMs\u5185\u90e8\u8bed\u8a00\u8d28\u91cf\u8868\u793a\u65b9\u5f0f\u4e0d\u4e00\u81f4\u3002\u7814\u7a76\u8005\u5e0c\u671b\u63a2\u7d22LLMs\u662f\u5426\u80fd\u591f\u8bbe\u8ba1\u548c\u5e94\u7528\u81ea\u5df1\u7684\u8bc4\u4f30\u6807\u51c6\u3002", "method": "\u5f15\u5165GER-Eval\u65b9\u6cd5\uff0c\u7814\u7a76LLMs\u80fd\u5426\u8bbe\u8ba1\u548c\u5e94\u7528\u81ea\u5df1\u7684\u8bc4\u4f30\u6807\u51c6\u3002\u8bc4\u4f30LLM\u5b9a\u4e49\u6807\u51c6\u7684\u8bed\u4e49\u8fde\u8d2f\u6027\u3001\u8bc4\u5206\u53ef\u9760\u6027\u4ee5\u53ca\u4e0e\u4eba\u7c7b\u6807\u51c6\u7684\u5bf9\u9f50\u7a0b\u5ea6\u3002", "result": "LLMs\u80fd\u591f\u53ef\u9760\u5730\u751f\u6210\u53ef\u89e3\u91ca\u4e14\u4efb\u52a1\u611f\u77e5\u7684\u8bc4\u4f30\u7ef4\u5ea6\uff0c\u5e76\u5728\u6a21\u578b\u5185\u90e8\u4e00\u81f4\u5e94\u7528\u8fd9\u4e9b\u6807\u51c6\u3002\u4f46\u5728\u4e8b\u5b9e\u6027\u548c\u77e5\u8bc6\u5bc6\u96c6\u578b\u8bbe\u7f6e\u4e2d\uff0c\u8bc4\u5206\u53ef\u9760\u6027\u4f1a\u4e0b\u964d\u3002\u95ed\u6e90\u6a21\u578b\uff08\u5982GPT-4o\uff09\u6bd4\u5f00\u6e90\u6a21\u578b\uff08\u5982Llama\uff09\u5177\u6709\u66f4\u9ad8\u7684\u4e00\u81f4\u6027\u548c\u8de8\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8bc4\u4f30\u662fLLMs\u7684\u4e00\u79cd\u5b66\u4e60\u5230\u7684\u8bed\u8a00\u80fd\u529b\uff0c\u5728\u6a21\u578b\u5185\u90e8\u4e00\u81f4\u4f46\u5728\u4e0d\u540c\u6a21\u578b\u95f4\u5b58\u5728\u788e\u7247\u5316\u3002\u9700\u8981\u5f00\u53d1\u65b0\u7684\u65b9\u6cd5\u6765\u8054\u5408\u5efa\u6a21\u4eba\u7c7b\u548cLLMs\u7684\u8bc4\u4f30\u8bed\u8a00\uff0c\u4ee5\u63d0\u9ad8\u53ef\u9760\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2602.07643", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.07643", "abs": "https://arxiv.org/abs/2602.07643", "authors": ["Yichi Zhang", "Feiyang Xiao", "Le Xue", "Wenbo Zhang", "Gang Feng", "Chenguang Zheng", "Yuan Qi", "Yuan Cheng", "Zixin Hu"], "title": "Uncovering Modality Discrepancy and Generalization Illusion for General-Purpose 3D Medical Segmentation", "comment": null, "summary": "While emerging 3D medical foundation models are envisioned as versatile tools with offer general-purpose capabilities, their validation remains largely confined to regional and structural imaging, leaving a significant modality discrepancy unexplored. To provide a rigorous and objective assessment, we curate the UMD dataset comprising 490 whole-body PET/CT and 464 whole-body PET/MRI scans ($\\sim$675k 2D images, $\\sim$12k 3D organ annotations) and conduct a thorough and comprehensive evaluation of representative 3D segmentation foundation models. Through intra-subject controlled comparisons of paired scans, we isolate imaging modality as the primary independent variable to evaluate model robustness in real-world applications. Our evaluation reveals a stark discrepancy between literature-reported benchmarks and real-world efficacy, particularly when transitioning from structural to functional domains. Such systemic failures underscore that current 3D foundation models are far from achieving truly general-purpose status, necessitating a paradigm shift toward multi-modal training and evaluation to bridge the gap between idealized benchmarking and comprehensive clinical utility. This dataset and analysis establish a foundational cornerstone for future research to develop truly modality-agnostic medical foundation models.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u521b\u5efaUMD\u6570\u636e\u96c6\uff08\u5305\u542bPET/CT\u548cPET/MRI\u626b\u63cf\uff09\u8bc4\u4f303D\u533b\u5b66\u57fa\u7840\u6a21\u578b\uff0c\u53d1\u73b0\u4ece\u7ed3\u6784\u6210\u50cf\u8f6c\u5411\u529f\u80fd\u6210\u50cf\u65f6\u5b58\u5728\u663e\u8457\u6027\u80fd\u5dee\u8ddd\uff0c\u8868\u660e\u5f53\u524d\u6a21\u578b\u8fdc\u672a\u8fbe\u5230\u771f\u6b63\u901a\u7528\u76ee\u7684\u72b6\u6001\u3002", "motivation": "\u5f53\u524d3D\u533b\u5b66\u57fa\u7840\u6a21\u578b\u7684\u9a8c\u8bc1\u4e3b\u8981\u5c40\u9650\u4e8e\u533a\u57df\u548c\u7ed3\u6784\u6210\u50cf\uff0c\u5b58\u5728\u663e\u8457\u7684\u6a21\u6001\u5dee\u5f02\u672a\u88ab\u63a2\u7d22\u3002\u9700\u8981\u63d0\u4f9b\u4e25\u683c\u5ba2\u89c2\u7684\u8bc4\u4f30\u6765\u68c0\u9a8c\u8fd9\u4e9b\u6a21\u578b\u5728\u771f\u5b9e\u4e16\u754c\u5e94\u7528\u4e2d\u7684\u9c81\u68d2\u6027\u3002", "method": "\u521b\u5efaUMD\u6570\u636e\u96c6\uff08490\u4e2a\u5168\u8eabPET/CT\u548c464\u4e2a\u5168\u8eabPET/MRI\u626b\u63cf\uff0c\u5305\u542b\u7ea6675k 2D\u56fe\u50cf\u548c12k 3D\u5668\u5b98\u6807\u6ce8\uff09\uff0c\u901a\u8fc7\u53d7\u8bd5\u8005\u5185\u914d\u5bf9\u626b\u63cf\u7684\u5bf9\u7167\u6bd4\u8f83\uff0c\u5c06\u6210\u50cf\u6a21\u6001\u4f5c\u4e3a\u4e3b\u8981\u81ea\u53d8\u91cf\uff0c\u5bf9\u4ee3\u8868\u60273D\u5206\u5272\u57fa\u7840\u6a21\u578b\u8fdb\u884c\u5168\u9762\u8bc4\u4f30\u3002", "result": "\u8bc4\u4f30\u63ed\u793a\u4e86\u6587\u732e\u62a5\u9053\u7684\u57fa\u51c6\u4e0e\u771f\u5b9e\u4e16\u754c\u6548\u80fd\u4e4b\u95f4\u7684\u663e\u8457\u5dee\u5f02\uff0c\u7279\u522b\u662f\u5728\u4ece\u7ed3\u6784\u57df\u8f6c\u5411\u529f\u80fd\u57df\u65f6\u3002\u8fd9\u79cd\u7cfb\u7edf\u6027\u5931\u8d25\u8868\u660e\u5f53\u524d3D\u57fa\u7840\u6a21\u578b\u8fdc\u672a\u8fbe\u5230\u771f\u6b63\u901a\u7528\u76ee\u7684\u72b6\u6001\u3002", "conclusion": "\u9700\u8981\u5411\u591a\u6a21\u6001\u8bad\u7ec3\u548c\u8bc4\u4f30\u7684\u8303\u5f0f\u8f6c\u53d8\uff0c\u4ee5\u5f25\u5408\u7406\u60f3\u5316\u57fa\u51c6\u6d4b\u8bd5\u4e0e\u5168\u9762\u4e34\u5e8a\u6548\u7528\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002\u8be5\u6570\u636e\u96c6\u548c\u5206\u6790\u4e3a\u672a\u6765\u5f00\u53d1\u771f\u6b63\u6a21\u6001\u65e0\u5173\u7684\u533b\u5b66\u57fa\u7840\u6a21\u578b\u5960\u5b9a\u4e86\u57fa\u77f3\u3002"}}
{"id": "2602.08369", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.08369", "abs": "https://arxiv.org/abs/2602.08369", "authors": ["Xin Zhang", "Kailai Yang", "Chenyue Li", "Hao Li", "Qiyu Wei", "Jun'ichi Tsujii", "Sophia Ananiadou"], "title": "MemAdapter: Fast Alignment across Agent Memory Paradigms via Generative Subgraph Retrieval", "comment": null, "summary": "Memory mechanism is a core component of LLM-based agents, enabling reasoning and knowledge discovery over long-horizon contexts. Existing agent memory systems are typically designed within isolated paradigms (e.g., explicit, parametric, or latent memory) with tightly coupled retrieval methods that hinder cross-paradigm generalization and fusion. In this work, we take a first step toward unifying heterogeneous memory paradigms within a single memory system. We propose MemAdapter, a memory retrieval framework that enables fast alignment across agent memory paradigms. MemAdapter adopts a two-stage training strategy: (1) training a generative subgraph retriever from the unified memory space, and (2) adapting the retriever to unseen memory paradigms by training a lightweight alignment module through contrastive learning. This design improves the flexibility for memory retrieval and substantially reduces alignment cost across paradigms. Comprehensive experiments on three public evaluation benchmarks demonstrate that the generative subgraph retriever consistently outperforms five strong agent memory systems across three memory paradigms and agent model scales. Notably, MemAdapter completes cross-paradigm alignment within 13 minutes on a single GPU, achieving superior performance over original memory retrievers with less than 5% of training compute. Furthermore, MemAdapter enables effective zero-shot fusion across memory paradigms, highlighting its potential as a plug-and-play solution for agent memory systems.", "AI": {"tldr": "MemAdapter\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u8bb0\u5fc6\u68c0\u7d22\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\u5b9e\u73b0\u8de8\u8bb0\u5fc6\u8303\u5f0f\u7684\u5feb\u901f\u5bf9\u9f50\uff0c\u663e\u8457\u964d\u4f4e\u5bf9\u9f50\u6210\u672c\u5e76\u63d0\u5347\u68c0\u7d22\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eLLM\u7684\u667a\u80fd\u4f53\u8bb0\u5fc6\u7cfb\u7edf\u901a\u5e38\u91c7\u7528\u5b64\u7acb\u7684\u8bbe\u8ba1\u8303\u5f0f\uff08\u5982\u663e\u5f0f\u3001\u53c2\u6570\u5316\u6216\u6f5c\u5728\u8bb0\u5fc6\uff09\uff0c\u5176\u7d27\u5bc6\u8026\u5408\u7684\u68c0\u7d22\u65b9\u6cd5\u963b\u788d\u4e86\u8de8\u8303\u5f0f\u7684\u6cdb\u5316\u548c\u878d\u5408\uff0c\u9700\u8981\u7edf\u4e00\u7684\u8bb0\u5fc6\u7cfb\u7edf\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51faMemAdapter\u6846\u67b6\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff1a1\uff09\u5728\u7edf\u4e00\u8bb0\u5fc6\u7a7a\u95f4\u8bad\u7ec3\u751f\u6210\u5f0f\u5b50\u56fe\u68c0\u7d22\u5668\uff1b2\uff09\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u8bad\u7ec3\u8f7b\u91cf\u7ea7\u5bf9\u9f50\u6a21\u5757\uff0c\u4f7f\u68c0\u7d22\u5668\u9002\u5e94\u672a\u89c1\u8bb0\u5fc6\u8303\u5f0f\u3002", "result": "\u5728\u4e09\u4e2a\u516c\u5f00\u8bc4\u4f30\u57fa\u51c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u751f\u6210\u5f0f\u5b50\u56fe\u68c0\u7d22\u5668\u5728\u4e09\u79cd\u8bb0\u5fc6\u8303\u5f0f\u548c\u4e0d\u540c\u667a\u80fd\u4f53\u6a21\u578b\u89c4\u6a21\u4e0a\u5747\u4f18\u4e8e\u4e94\u79cd\u5f3a\u57fa\u7ebf\u7cfb\u7edf\u3002MemAdapter\u5728\u5355GPU\u4e0a13\u5206\u949f\u5185\u5b8c\u6210\u8de8\u8303\u5f0f\u5bf9\u9f50\uff0c\u4f7f\u7528\u4e0d\u52305%\u7684\u8bad\u7ec3\u8ba1\u7b97\u91cf\u5373\u8d85\u8d8a\u539f\u59cb\u8bb0\u5fc6\u68c0\u7d22\u5668\u6027\u80fd\uff0c\u5e76\u80fd\u5b9e\u73b0\u8de8\u8bb0\u5fc6\u8303\u5f0f\u7684\u6709\u6548\u96f6\u6837\u672c\u878d\u5408\u3002", "conclusion": "MemAdapter\u4f5c\u4e3a\u4e00\u4e2a\u5373\u63d2\u5373\u7528\u7684\u667a\u80fd\u4f53\u8bb0\u5fc6\u7cfb\u7edf\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u7edf\u4e00\u7684\u8bb0\u5fc6\u68c0\u7d22\u6846\u67b6\u5b9e\u73b0\u4e86\u8de8\u5f02\u6784\u8bb0\u5fc6\u8303\u5f0f\u7684\u5feb\u901f\u5bf9\u9f50\u548c\u9ad8\u6548\u878d\u5408\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bb0\u5fc6\u68c0\u7d22\u7684\u7075\u6d3b\u6027\u548c\u6548\u7387\u3002"}}
{"id": "2602.08688", "categories": ["cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2602.08688", "abs": "https://arxiv.org/abs/2602.08688", "authors": ["Hossein Kermani", "Fatemeh Oudlajani", "Pardis Yarahmadi", "Hamideh Mahdi Soltani", "Mohammad Makki", "Zahra HosseiniKhoo"], "title": "Old wine in old glasses: Comparing computational and qualitative methods in identifying incivility on Persian Twitter during the #MahsaAmini movement", "comment": null, "summary": "This paper compares three approaches to detecting incivility in Persian tweets: human qualitative coding, supervised learning with ParsBERT, and large language models (ChatGPT). Using 47,278 tweets from the #MahsaAmini movement in Iran, we evaluate the accuracy and efficiency of each method. ParsBERT substantially outperforms seven evaluated ChatGPT models in identifying hate speech. We also find that ChatGPT struggles not only with subtle cases but also with explicitly uncivil content, and that prompt language (English vs. Persian) does not meaningfully affect its outputs. The study provides a detailed comparison of these approaches and clarifies their strengths and limitations for analyzing hate speech in a low-resource language context.", "AI": {"tldr": "\u6bd4\u8f83\u6ce2\u65af\u8bed\u63a8\u6587\u4e0d\u6587\u660e\u5185\u5bb9\u68c0\u6d4b\u7684\u4e09\u79cd\u65b9\u6cd5\uff1a\u4eba\u5de5\u7f16\u7801\u3001ParsBERT\u76d1\u7763\u5b66\u4e60\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08ChatGPT\uff09\u3002\u5728#MahsaAmini\u8fd0\u52a8\u768447,278\u6761\u63a8\u6587\u4e0a\u8bc4\u4f30\uff0cParsBERT\u5728\u4ec7\u6068\u8a00\u8bba\u68c0\u6d4b\u4e0a\u663e\u8457\u4f18\u4e8eChatGPT\u6a21\u578b\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u6bd4\u8f83\u4e0d\u540c\u65b9\u6cd5\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\uff08\u6ce2\u65af\u8bed\uff09\u73af\u5883\u4e2d\u68c0\u6d4b\u4e0d\u6587\u660e\u5185\u5bb9\u548c\u4ec7\u6068\u8a00\u8bba\u7684\u6548\u679c\uff0c\u4e3a\u7814\u7a76\u4eba\u5458\u63d0\u4f9b\u65b9\u6cd5\u9009\u62e9\u7684\u6307\u5bfc\u3002", "method": "\u4f7f\u7528#MahsaAmini\u8fd0\u52a8\u768447,278\u6761\u6ce2\u65af\u8bed\u63a8\u6587\uff0c\u6bd4\u8f83\u4e09\u79cd\u65b9\u6cd5\uff1a1\uff09\u4eba\u5de5\u5b9a\u6027\u7f16\u7801\uff1b2\uff09\u57fa\u4e8eParsBERT\u7684\u76d1\u7763\u5b66\u4e60\uff1b3\uff09\u4e03\u4e2aChatGPT\u6a21\u578b\u3002\u8bc4\u4f30\u51c6\u786e\u6027\u548c\u6548\u7387\uff0c\u5e76\u6d4b\u8bd5\u63d0\u793a\u8bed\u8a00\uff08\u82f1\u8bedvs\u6ce2\u65af\u8bed\uff09\u7684\u5f71\u54cd\u3002", "result": "ParsBERT\u5728\u4ec7\u6068\u8a00\u8bba\u68c0\u6d4b\u4e0a\u663e\u8457\u4f18\u4e8e\u6240\u6709\u4e03\u4e2aChatGPT\u6a21\u578b\u3002ChatGPT\u4e0d\u4ec5\u5728\u5fae\u5999\u6848\u4f8b\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u5728\u660e\u786e\u7684\u4e0d\u6587\u660e\u5185\u5bb9\u4e0a\u4e5f\u5b58\u5728\u56f0\u96be\u3002\u63d0\u793a\u8bed\u8a00\uff08\u82f1\u8bedvs\u6ce2\u65af\u8bed\uff09\u5bf9ChatGPT\u8f93\u51fa\u6ca1\u6709\u663e\u8457\u5f71\u54cd\u3002", "conclusion": "\u7814\u7a76\u8be6\u7ec6\u6bd4\u8f83\u4e86\u4e0d\u540c\u65b9\u6cd5\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u73af\u5883\u4e2d\u5206\u6790\u4ec7\u6068\u8a00\u8bba\u7684\u4f18\u7f3a\u70b9\uff0c\u4e3a\u76f8\u5173\u7814\u7a76\u63d0\u4f9b\u4e86\u65b9\u6cd5\u9009\u62e9\u7684\u53c2\u8003\u4f9d\u636e\uff0c\u8868\u660eParsBERT\u5728\u6ce2\u65af\u8bed\u4e0d\u6587\u660e\u5185\u5bb9\u68c0\u6d4b\u4e0a\u4f18\u4e8eChatGPT\u3002"}}
{"id": "2602.08373", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.08373", "abs": "https://arxiv.org/abs/2602.08373", "authors": ["Feiyu Wu", "Xu Zheng", "Yue Qu", "Zhuocheng Wang", "Zicheng Feng", "Hui Li"], "title": "Grounding Generative Planners in Verifiable Logic: A Hybrid Architecture for Trustworthy Embodied AI", "comment": "Accepted to ICLR 2026. Project page. https://openreview.net/forum?id=wb05ver1k8&noteId=v1Ax8CwI71", "summary": "Large Language Models (LLMs) show promise as planners for embodied AI, but their stochastic nature lacks formal reasoning, preventing strict safety guarantees for physical deployment. Current approaches often rely on unreliable LLMs for safety checks or simply reject unsafe plans without offering repairs. We introduce the Verifiable Iterative Refinement Framework (VIRF), a neuro-symbolic architecture that shifts the paradigm from passive safety gatekeeping to active collaboration. Our core contribution is a tutor-apprentice dialogue where a deterministic Logic Tutor, grounded in a formal safety ontology, provides causal and pedagogical feedback to an LLM planner. This enables intelligent plan repairs rather than mere avoidance. We also introduce a scalable knowledge acquisition pipeline that synthesizes safety knowledge bases from real-world documents, correcting blind spots in existing benchmarks. In challenging home safety tasks, VIRF achieves a perfect 0 percent Hazardous Action Rate (HAR) and a 77.3 percent Goal-Condition Rate (GCR), which is the highest among all baselines. It is highly efficient, requiring only 1.1 correction iterations on average. VIRF demonstrates a principled pathway toward building fundamentally trustworthy and verifiably safe embodied agents.", "AI": {"tldr": "VIRF\u6846\u67b6\u901a\u8fc7\u795e\u7ecf\u7b26\u53f7\u67b6\u6784\u5c06LLM\u89c4\u5212\u5668\u4e0e\u5f62\u5f0f\u5316\u5b89\u5168\u903b\u8f91\u5bfc\u5e08\u7ed3\u5408\uff0c\u5b9e\u73b0\u53ef\u9a8c\u8bc1\u7684\u5b89\u5168\u89c4\u5212\u4e0e\u4e3b\u52a8\u4fee\u590d\uff0c\u800c\u975e\u88ab\u52a8\u62d2\u7edd\u4e0d\u5b89\u5168\u8ba1\u5212\u3002", "motivation": "\u5f53\u524dLLM\u4f5c\u4e3a\u5177\u8eabAI\u89c4\u5212\u5668\u7f3a\u4e4f\u5f62\u5f0f\u5316\u63a8\u7406\u80fd\u529b\uff0c\u65e0\u6cd5\u63d0\u4f9b\u4e25\u683c\u7684\u5b89\u5168\u4fdd\u8bc1\u3002\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u4f9d\u8d56\u4e0d\u53ef\u9760\u7684LLM\u8fdb\u884c\u5b89\u5168\u68c0\u67e5\uff0c\u8981\u4e48\u7b80\u5355\u5730\u62d2\u7edd\u4e0d\u5b89\u5168\u8ba1\u5212\u800c\u4e0d\u63d0\u4f9b\u4fee\u590d\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u53ef\u9a8c\u8bc1\u8fed\u4ee3\u7cbe\u70bc\u6846\u67b6(VIRF)\uff0c\u91c7\u7528\u795e\u7ecf\u7b26\u53f7\u67b6\u6784\uff0c\u5efa\u7acb\u5bfc\u5e08-\u5b66\u5f92\u5bf9\u8bdd\u673a\u5236\uff1a\u57fa\u4e8e\u5f62\u5f0f\u5316\u5b89\u5168\u672c\u4f53\u7684\u786e\u5b9a\u6027\u903b\u8f91\u5bfc\u5e08\u4e3aLLM\u89c4\u5212\u5668\u63d0\u4f9b\u56e0\u679c\u6027\u548c\u6559\u5b66\u6027\u53cd\u9988\uff0c\u5b9e\u73b0\u667a\u80fd\u8ba1\u5212\u4fee\u590d\u800c\u975e\u7b80\u5355\u907f\u514d\u3002\u540c\u65f6\u5f00\u53d1\u4e86\u4ece\u73b0\u5b9e\u4e16\u754c\u6587\u6863\u5408\u6210\u5b89\u5168\u77e5\u8bc6\u5e93\u7684\u53ef\u6269\u5c55\u77e5\u8bc6\u83b7\u53d6\u6d41\u7a0b\u3002", "result": "\u5728\u5bb6\u5ead\u5b89\u5168\u4efb\u52a1\u4e2d\uff0cVIRF\u5b9e\u73b0\u4e860%\u7684\u5371\u9669\u884c\u52a8\u7387(HAR)\u548c77.3%\u7684\u76ee\u6807\u6761\u4ef6\u7387(GCR)\uff0c\u5728\u6240\u6709\u57fa\u7ebf\u65b9\u6cd5\u4e2d\u8868\u73b0\u6700\u4f73\u3002\u5e73\u5747\u4ec5\u97001.1\u6b21\u4fee\u6b63\u8fed\u4ee3\uff0c\u6548\u7387\u5f88\u9ad8\u3002", "conclusion": "VIRF\u4e3a\u6784\u5efa\u6839\u672c\u4e0a\u53ef\u4fe1\u4e14\u53ef\u9a8c\u8bc1\u5b89\u5168\u7684\u5177\u8eab\u667a\u80fd\u4f53\u63d0\u4f9b\u4e86\u4e00\u6761\u539f\u5219\u6027\u8def\u5f84\uff0c\u5c06\u5b89\u5168\u8303\u5f0f\u4ece\u88ab\u52a8\u628a\u5173\u8f6c\u53d8\u4e3a\u4e3b\u52a8\u534f\u4f5c\u3002"}}
{"id": "2602.08698", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.08698", "abs": "https://arxiv.org/abs/2602.08698", "authors": ["Basudha Raje", "Sadanand Venkatraman", "Nandana TP", "Soumyadeepa Das", "Polkam Poojitha", "M. Vijaykumar", "Tanima Bagchi", "Hema A. Murthy"], "title": "Challenges in Translating Technical Lectures: Insights from the NPTEL", "comment": null, "summary": "This study examines the practical applications and methodological implications of Machine Translation in Indian Languages, specifically Bangla, Malayalam, and Telugu, within emerging translation workflows and in relation to existing evaluation frameworks. The choice of languages prioritized in this study is motivated by a triangulation of linguistic diversity, which illustrates the significance of multilingual accommodation of educational technology under NEP 2020. This is further supported by the largest MOOC portal, i.e., NPTEL, which has served as a corpus to facilitate the arguments presented in this paper. The curation of a spontaneous speech corpora that accounts for lucid delivery of technical concepts, considering the retention of suitable register and lexical choices are crucial in a diverse country like India. The findings of this study highlight metric-specific sensitivity and the challenges of morphologically rich and semantically compact features when tested against surface overlapping metrics.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u8ba8\u4e86\u673a\u5668\u7ffb\u8bd1\u5728\u5b5f\u52a0\u62c9\u8bed\u3001\u9a6c\u62c9\u96c5\u62c9\u59c6\u8bed\u548c\u6cf0\u5362\u56fa\u8bed\u7b49\u5370\u5ea6\u8bed\u8a00\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\u548c\u65b9\u6cd5\u8bba\u610f\u4e49\uff0c\u91cd\u70b9\u5173\u6ce8\u65b0\u5174\u7ffb\u8bd1\u5de5\u4f5c\u6d41\u7a0b\u548c\u73b0\u6709\u8bc4\u4f30\u6846\u67b6\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u6e90\u4e8e\u5370\u5ea6\u8bed\u8a00\u591a\u6837\u6027\u80cc\u666f\u4e0b\u6559\u80b2\u6280\u672f\u7684\u591a\u8bed\u8a00\u9002\u5e94\u9700\u6c42\uff0c\u7279\u522b\u662fNEP 2020\u653f\u7b56\u63a8\u52a8\u4e0b\u7684\u6559\u80b2\u6280\u672f\u53d1\u5c55\u3002\u9009\u62e9\u8fd9\u4e09\u79cd\u8bed\u8a00\u662f\u57fa\u4e8e\u8bed\u8a00\u591a\u6837\u6027\u7684\u4e09\u89d2\u9a8c\u8bc1\uff0c\u5e76\u5229\u7528NPTEL\u5927\u89c4\u6a21\u5f00\u653e\u5728\u7ebf\u8bfe\u7a0b\u5e73\u53f0\u4f5c\u4e3a\u8bed\u6599\u5e93\u3002", "method": "\u4f7f\u7528NPTEL MOOC\u5e73\u53f0\u4f5c\u4e3a\u8bed\u6599\u5e93\uff0c\u6784\u5efa\u4e86\u5305\u542b\u6280\u672f\u6982\u5ff5\u6e05\u6670\u8868\u8fbe\u7684\u81ea\u53d1\u8bed\u97f3\u8bed\u6599\u5e93\uff0c\u8003\u8651\u4e86\u5408\u9002\u7684\u8bed\u57df\u548c\u8bcd\u6c47\u9009\u62e9\u3002\u7814\u7a76\u6d4b\u8bd5\u4e86\u5f62\u6001\u4e30\u5bcc\u3001\u8bed\u4e49\u7d27\u51d1\u7684\u8bed\u8a00\u7279\u5f81\u5bf9\u8868\u9762\u91cd\u53e0\u5ea6\u8bc4\u4f30\u6307\u6807\u7684\u654f\u611f\u6027\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u8bc4\u4f30\u6307\u6807\u5b58\u5728\u7279\u5b9a\u654f\u611f\u6027\uff0c\u5f62\u6001\u4e30\u5bcc\u4e14\u8bed\u4e49\u7d27\u51d1\u7684\u8bed\u8a00\u7279\u5f81\u5728\u8868\u9762\u91cd\u53e0\u5ea6\u6307\u6807\u6d4b\u8bd5\u4e2d\u9762\u4e34\u6311\u6218\uff0c\u7a81\u663e\u4e86\u73b0\u6709\u673a\u5668\u7ffb\u8bd1\u8bc4\u4f30\u6846\u67b6\u5728\u5370\u5ea6\u8bed\u8a00\u5e94\u7528\u4e2d\u7684\u5c40\u9650\u6027\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u5728\u5370\u5ea6\u8fd9\u6837\u8bed\u8a00\u591a\u6837\u7684\u56fd\u5bb6\uff0c\u6784\u5efa\u9002\u5408\u6280\u672f\u6982\u5ff5\u6e05\u6670\u8868\u8fbe\u7684\u81ea\u53d1\u8bed\u97f3\u8bed\u6599\u5e93\u7684\u91cd\u8981\u6027\uff0c\u5e76\u6307\u51fa\u9700\u8981\u5f00\u53d1\u66f4\u9002\u5e94\u5f62\u6001\u4e30\u5bcc\u8bed\u8a00\u7279\u5f81\u7684\u673a\u5668\u7ffb\u8bd1\u8bc4\u4f30\u65b9\u6cd5\u3002"}}
{"id": "2602.07658", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.07658", "abs": "https://arxiv.org/abs/2602.07658", "authors": ["Avinash Kumar K M", "Samarth S. Raut"], "title": "Influence of Geometry, Class Imbalance and Alignment on Reconstruction Accuracy -- A Micro-CT Phantom-Based Evaluation", "comment": "22 pages, 13 figures", "summary": "The accuracy of the 3D models created from medical scans depends on imaging hardware, segmentation methods and mesh processing techniques etc. The effects of geometry type, class imbalance, voxel and point cloud alignment on accuracy remain to be thoroughly explored. This work evaluates the errors across the reconstruction pipeline and explores the use of voxel and surface-based accuracy metrics for different segmentation algorithms and geometry types. A sphere, a facemask, and an AAA were printed using the SLA technique and scanned using a micro-CT machine. Segmentation was performed using GMM, Otsu and RG based methods. Segmented and reference models aligned using the KU algorithm, were quantitatively compared to evaluate metrics like Dice and Jaccard scores, precision. Surface meshes were registered with reference meshes using an ICP-based alignment process. Metrics like chamfer distance, and average Hausdorff distance were evaluated. The Otsu method was found to be the most suitable method for all the geometries. AAA yielded low overlap scores due to its small wall thickness and misalignment. The effect of class imbalance on specificity was observed the most for AAA. Surface-based accuracy metrics differed from the voxel-based trends. The RG method performed best for sphere, while GMM and Otsu perform better for AAA. The facemask surface was most error-prone, possibly due to misalignment during the ICP process. Segmentation accuracy is a cumulative sum of errors across different stages of the reconstruction process. High voxel-based accuracy metrics may be misleading in cases of high class imbalance and sensitivity to alignment. The Jaccard index is found to be more stringent than the Dice and more suitable for accuracy assessment for thin-walled structures. Voxel and point cloud alignment should be ensured to make any reliable assessment of the reconstruction pipeline.", "AI": {"tldr": "\u8be5\u7814\u7a76\u8bc4\u4f30\u4e86\u533b\u5b66\u626b\u63cf3D\u91cd\u5efa\u6d41\u7a0b\u4e2d\u7684\u8bef\u5dee\uff0c\u6bd4\u8f83\u4e86\u4e0d\u540c\u5206\u5272\u7b97\u6cd5\u548c\u51e0\u4f55\u5f62\u72b6\u7684\u4f53\u7d20\u4e0e\u8868\u9762\u7cbe\u5ea6\u6307\u6807\uff0c\u53d1\u73b0Otsu\u65b9\u6cd5\u6700\u901a\u7528\uff0c\u8584\u58c1\u7ed3\u6784\u9700\u8981\u66f4\u4e25\u683c\u7684Jaccard\u6307\u6570\u8bc4\u4f30\u3002", "motivation": "\u533b\u5b66\u626b\u63cf\u521b\u5efa3D\u6a21\u578b\u7684\u7cbe\u5ea6\u53d7\u591a\u79cd\u56e0\u7d20\u5f71\u54cd\uff0c\u4f46\u51e0\u4f55\u7c7b\u578b\u3001\u7c7b\u522b\u4e0d\u5e73\u8861\u3001\u4f53\u7d20\u548c\u70b9\u4e91\u5bf9\u9f50\u5bf9\u7cbe\u5ea6\u7684\u5f71\u54cd\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002\u672c\u7814\u7a76\u65e8\u5728\u8bc4\u4f30\u91cd\u5efa\u6d41\u7a0b\u4e2d\u7684\u8bef\u5dee\uff0c\u5e76\u63a2\u7d22\u4e0d\u540c\u5206\u5272\u7b97\u6cd5\u548c\u51e0\u4f55\u5f62\u72b6\u7684\u4f53\u7d20\u4e0e\u8868\u9762\u7cbe\u5ea6\u6307\u6807\u3002", "method": "\u4f7f\u7528SLA\u6280\u672f\u6253\u5370\u7403\u4f53\u3001\u9762\u7f69\u548cAAA\uff08\u8179\u4e3b\u52a8\u8109\u7624\uff09\u6a21\u578b\uff0c\u901a\u8fc7\u5faeCT\u626b\u63cf\u3002\u91c7\u7528GMM\u3001Otsu\u548cRG\u4e09\u79cd\u5206\u5272\u65b9\u6cd5\u3002\u4f7f\u7528KU\u7b97\u6cd5\u5bf9\u9f50\u5206\u5272\u6a21\u578b\u4e0e\u53c2\u8003\u6a21\u578b\uff0c\u8bc4\u4f30Dice\u3001Jaccard\u5206\u6570\u548c\u7cbe\u5ea6\u7b49\u4f53\u7d20\u6307\u6807\u3002\u4f7f\u7528ICP\u5bf9\u9f50\u8fc7\u7a0b\u914d\u51c6\u8868\u9762\u7f51\u683c\uff0c\u8bc4\u4f30\u5012\u89d2\u8ddd\u79bb\u548c\u5e73\u5747Hausdorff\u8ddd\u79bb\u7b49\u8868\u9762\u6307\u6807\u3002", "result": "Otsu\u65b9\u6cd5\u5bf9\u6240\u6709\u51e0\u4f55\u5f62\u72b6\u6700\u9002\u7528\u3002AAA\u7531\u4e8e\u58c1\u8584\u548c\u5bf9\u9f50\u95ee\u9898\u5bfc\u81f4\u91cd\u53e0\u5206\u6570\u4f4e\u3002\u7c7b\u522b\u4e0d\u5e73\u8861\u5bf9AAA\u7684\u7279\u5f02\u6027\u5f71\u54cd\u6700\u5927\u3002\u8868\u9762\u7cbe\u5ea6\u6307\u6807\u4e0e\u4f53\u7d20\u6307\u6807\u8d8b\u52bf\u4e0d\u540c\uff1aRG\u65b9\u6cd5\u5bf9\u7403\u4f53\u8868\u73b0\u6700\u597d\uff0cGMM\u548cOtsu\u5bf9AAA\u8868\u73b0\u66f4\u597d\u3002\u9762\u7f69\u8868\u9762\u8bef\u5dee\u6700\u5927\uff0c\u53ef\u80fd\u662fICP\u5bf9\u9f50\u8fc7\u7a0b\u4e2d\u7684\u95ee\u9898\u3002", "conclusion": "\u5206\u5272\u7cbe\u5ea6\u662f\u91cd\u5efa\u8fc7\u7a0b\u5404\u9636\u6bb5\u8bef\u5dee\u7684\u7d2f\u79ef\u603b\u548c\u3002\u5728\u7c7b\u522b\u4e0d\u5e73\u8861\u548c\u5bf9\u9f50\u654f\u611f\u7684\u60c5\u51b5\u4e0b\uff0c\u9ad8\u4f53\u7d20\u7cbe\u5ea6\u6307\u6807\u53ef\u80fd\u5177\u6709\u8bef\u5bfc\u6027\u3002Jaccard\u6307\u6570\u6bd4Dice\u66f4\u4e25\u683c\uff0c\u66f4\u9002\u5408\u8bc4\u4f30\u8584\u58c1\u7ed3\u6784\u7684\u7cbe\u5ea6\u3002\u5fc5\u987b\u786e\u4fdd\u4f53\u7d20\u548c\u70b9\u4e91\u5bf9\u9f50\u624d\u80fd\u5bf9\u91cd\u5efa\u6d41\u7a0b\u8fdb\u884c\u53ef\u9760\u8bc4\u4f30\u3002"}}
{"id": "2602.08700", "categories": ["cs.CL", "cs.HC", "cs.IR"], "pdf": "https://arxiv.org/pdf/2602.08700", "abs": "https://arxiv.org/abs/2602.08700", "authors": ["Clemencia Siro", "Zahra Abbasiantaeb", "Yifei Yuan", "Mohammad Aliannejadi", "Maarten de Rijke"], "title": "Do Images Clarify? A Study on the Effect of Images on Clarifying Questions in Conversational Search", "comment": "Accepted at CHIIR 2025", "summary": "Conversational search systems increasingly employ clarifying questions to refine user queries and improve the search experience. Previous studies have demonstrated the usefulness of text-based clarifying questions in enhancing both retrieval performance and user experience. While images have been shown to improve retrieval performance in various contexts, their impact on user performance when incorporated into clarifying questions remains largely unexplored. We conduct a user study with 73 participants to investigate the role of images in conversational search, specifically examining their effects on two search-related tasks: (i) answering clarifying questions and (ii) query reformulation. We compare the effect of multimodal and text-only clarifying questions in both tasks within a conversational search context from various perspectives. Our findings reveal that while participants showed a strong preference for multimodal questions when answering clarifying questions, preferences were more balanced in the query reformulation task. The impact of images varied with both task type and user expertise. In answering clarifying questions, images helped maintain engagement across different expertise levels, while in query reformulation they led to more precise queries and improved retrieval performance. Interestingly, for clarifying question answering, text-only setups demonstrated better user performance as they provided more comprehensive textual information in the absence of images. These results provide valuable insights for designing effective multimodal conversational search systems, highlighting that the benefits of visual augmentation are task-dependent and should be strategically implemented based on the specific search context and user characteristics.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u5728\u5bf9\u8bdd\u5f0f\u641c\u7d22\u7cfb\u7edf\u4e2d\uff0c\u56fe\u50cf\u589e\u5f3a\u7684\u6f84\u6e05\u95ee\u9898\u5bf9\u7528\u6237\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u56fe\u50cf\u6548\u679c\u56e0\u4efb\u52a1\u7c7b\u578b\u548c\u7528\u6237\u4e13\u4e1a\u6c34\u5e73\u800c\u5f02", "motivation": "\u867d\u7136\u5148\u524d\u7814\u7a76\u8868\u660e\u6587\u672c\u6f84\u6e05\u95ee\u9898\u80fd\u63d0\u5347\u68c0\u7d22\u6027\u80fd\u548c\u7528\u6237\u4f53\u9a8c\uff0c\u4e14\u56fe\u50cf\u5728\u5176\u4ed6\u68c0\u7d22\u573a\u666f\u4e2d\u5df2\u88ab\u8bc1\u660e\u6709\u6548\uff0c\u4f46\u56fe\u50cf\u5728\u6f84\u6e05\u95ee\u9898\u4e2d\u5bf9\u7528\u6237\u6027\u80fd\u7684\u5f71\u54cd\u5c1a\u672a\u5145\u5206\u63a2\u7d22", "method": "\u5bf973\u540d\u53c2\u4e0e\u8005\u8fdb\u884c\u7528\u6237\u7814\u7a76\uff0c\u6bd4\u8f83\u591a\u6a21\u6001\uff08\u56fe\u50cf+\u6587\u672c\uff09\u548c\u7eaf\u6587\u672c\u6f84\u6e05\u95ee\u9898\u5728\u4e24\u79cd\u641c\u7d22\u76f8\u5173\u4efb\u52a1\u4e2d\u7684\u6548\u679c\uff1a\u56de\u7b54\u6f84\u6e05\u95ee\u9898\u548c\u67e5\u8be2\u91cd\u6784\uff0c\u4ece\u591a\u4e2a\u89d2\u5ea6\u5206\u6790\u5f71\u54cd", "result": "\u5728\u56de\u7b54\u6f84\u6e05\u95ee\u9898\u65f6\uff0c\u53c2\u4e0e\u8005\u5f3a\u70c8\u504f\u597d\u591a\u6a21\u6001\u95ee\u9898\uff0c\u4f46\u7eaf\u6587\u672c\u8bbe\u7f6e\u4e0b\u7528\u6237\u8868\u73b0\u66f4\u597d\uff1b\u5728\u67e5\u8be2\u91cd\u6784\u4efb\u52a1\u4e2d\uff0c\u504f\u597d\u66f4\u5e73\u8861\uff0c\u56fe\u50cf\u80fd\u4ea7\u751f\u66f4\u7cbe\u786e\u7684\u67e5\u8be2\u5e76\u63d0\u5347\u68c0\u7d22\u6027\u80fd\uff1b\u56fe\u50cf\u6548\u679c\u53d7\u4efb\u52a1\u7c7b\u578b\u548c\u7528\u6237\u4e13\u4e1a\u6c34\u5e73\u5f71\u54cd", "conclusion": "\u89c6\u89c9\u589e\u5f3a\u7684\u76ca\u5904\u5177\u6709\u4efb\u52a1\u4f9d\u8d56\u6027\uff0c\u5e94\u6839\u636e\u5177\u4f53\u641c\u7d22\u4e0a\u4e0b\u6587\u548c\u7528\u6237\u7279\u5f81\u8fdb\u884c\u6218\u7565\u5b9e\u65bd\uff0c\u4e3a\u8bbe\u8ba1\u6709\u6548\u7684\u591a\u6a21\u6001\u5bf9\u8bdd\u641c\u7d22\u7cfb\u7edf\u63d0\u4f9b\u91cd\u8981\u89c1\u89e3"}}
{"id": "2602.08709", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.08709", "abs": "https://arxiv.org/abs/2602.08709", "authors": ["Leandro Anghinoni", "Jorge Sanchez"], "title": "FactSim: Fact-Checking for Opinion Summarization", "comment": "10 pages, 4 figures", "summary": "We explore the need for more comprehensive and precise evaluation techniques for generative artificial intelligence (GenAI) in text summarization tasks, specifically in the area of opinion summarization. Traditional methods, which leverage automated metrics to compare machine-generated summaries from a collection of opinion pieces, e.g. product reviews, have shown limitations due to the paradigm shift introduced by large language models (LLM). This paper addresses these shortcomings by proposing a novel, fully automated methodology for assessing the factual consistency of such summaries. The method is based on measuring the similarity between the claims in a given summary with those from the original reviews, measuring the coverage and consistency of the generated summary. To do so, we rely on a simple approach to extract factual assessment from texts that we then compare and summarize in a suitable score. We demonstrate that the proposed metric attributes higher scores to similar claims, regardless of whether the claim is negated, paraphrased, or expanded, and that the score has a high correlation to human judgment when compared to state-of-the-art metrics.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u81ea\u52a8\u5316\u8bc4\u4f30\u65b9\u6cd5\uff0c\u7528\u4e8e\u8861\u91cf\u751f\u6210\u5f0fAI\u5728\u610f\u89c1\u6458\u8981\u4efb\u52a1\u4e2d\u7684\u4e8b\u5b9e\u4e00\u81f4\u6027\uff0c\u901a\u8fc7\u6bd4\u8f83\u6458\u8981\u4e2d\u7684\u4e3b\u5f20\u4e0e\u539f\u59cb\u8bc4\u8bba\u7684\u76f8\u4f3c\u6027\u6765\u8bc4\u4f30\u8986\u76d6\u5ea6\u548c\u4e00\u81f4\u6027\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u81ea\u52a8\u5316\u6307\u6807\u8bc4\u4f30\u751f\u6210\u5f0fAI\u5728\u6587\u672c\u6458\u8981\uff08\u7279\u522b\u662f\u610f\u89c1\u6458\u8981\uff09\u4efb\u52a1\u7684\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff0c\u5c24\u5176\u662f\u5728\u5927\u8bed\u8a00\u6a21\u578b\u5e26\u6765\u8303\u5f0f\u8f6c\u53d8\u7684\u80cc\u666f\u4e0b\uff0c\u9700\u8981\u66f4\u5168\u9762\u7cbe\u786e\u7684\u8bc4\u4f30\u6280\u672f\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5b8c\u5168\u81ea\u52a8\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u63d0\u53d6\u6587\u672c\u4e2d\u7684\u4e8b\u5b9e\u8bc4\u4f30\uff0c\u6d4b\u91cf\u6458\u8981\u4e2d\u7684\u4e3b\u5f20\u4e0e\u539f\u59cb\u8bc4\u8bba\u7684\u76f8\u4f3c\u6027\uff0c\u4ece\u800c\u8bc4\u4f30\u751f\u6210\u6458\u8981\u7684\u8986\u76d6\u5ea6\u548c\u4e00\u81f4\u6027\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u4e3a\u76f8\u4f3c\u7684\u4e3b\u5f20\u5206\u914d\u66f4\u9ad8\u7684\u5206\u6570\uff0c\u65e0\u8bba\u4e3b\u5f20\u662f\u5426\u88ab\u5426\u5b9a\u3001\u6539\u5199\u6216\u6269\u5c55\uff0c\u5e76\u4e14\u4e0e\u4eba\u7c7b\u5224\u65ad\u7684\u76f8\u5173\u6027\u9ad8\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u6307\u6807\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u751f\u6210\u5f0fAI\u5728\u610f\u89c1\u6458\u8981\u4efb\u52a1\u4e2d\u7684\u4e8b\u5b9e\u4e00\u81f4\u6027\u8bc4\u4f30\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u66f4\u597d\u5730\u8bc4\u4f30\u751f\u6210\u6458\u8981\u7684\u8d28\u91cf\u3002"}}
{"id": "2602.07680", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.07680", "abs": "https://arxiv.org/abs/2602.07680", "authors": ["Ross Greer", "Maitrayee Keskar", "Angel Martinez-Sanchez", "Parthib Roy", "Shashank Shriram", "Mohan Trivedi"], "title": "Vision and language: Novel Representations and Artificial intelligence for Driving Scene Safety Assessment and Autonomous Vehicle Planning", "comment": null, "summary": "Vision-language models (VLMs) have recently emerged as powerful representation learning systems that align visual observations with natural language concepts, offering new opportunities for semantic reasoning in safety-critical autonomous driving. This paper investigates how vision-language representations support driving scene safety assessment and decision-making when integrated into perception, prediction, and planning pipelines. We study three complementary system-level use cases. First, we introduce a lightweight, category-agnostic hazard screening approach leveraging CLIP-based image-text similarity to produce a low-latency semantic hazard signal. This enables robust detection of diverse and out-of-distribution road hazards without explicit object detection or visual question answering. Second, we examine the integration of scene-level vision-language embeddings into a transformer-based trajectory planning framework using the Waymo Open Dataset. Our results show that naively conditioning planners on global embeddings does not improve trajectory accuracy, highlighting the importance of representation-task alignment and motivating the development of task-informed extraction methods for safety-critical planning. Third, we investigate natural language as an explicit behavioral constraint on motion planning using the doScenes dataset. In this setting, passenger-style instructions grounded in visual scene elements suppress rare but severe planning failures and improve safety-aligned behavior in ambiguous scenarios. Taken together, these findings demonstrate that vision-language representations hold significant promise for autonomous driving safety when used to express semantic risk, intent, and behavioral constraints. Realizing this potential is fundamentally an engineering problem requiring careful system design and structured grounding rather than direct feature injection.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u81ea\u52a8\u9a7e\u9a76\u5b89\u5168\u8bc4\u4f30\u548c\u51b3\u7b56\u4e2d\u7684\u5e94\u7528\uff0c\u63a2\u7d22\u4e86\u4e09\u79cd\u7cfb\u7edf\u7ea7\u7528\u4f8b\uff1a\u57fa\u4e8eCLIP\u7684\u8f7b\u91cf\u7ea7\u5371\u9669\u7b5b\u67e5\u3001\u573a\u666f\u7ea7\u5d4c\u5165\u5728\u8f68\u8ff9\u89c4\u5212\u4e2d\u7684\u96c6\u6210\uff0c\u4ee5\u53ca\u81ea\u7136\u8bed\u8a00\u4f5c\u4e3a\u8fd0\u52a8\u89c4\u5212\u7684\u884c\u4e3a\u7ea6\u675f\u3002", "motivation": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u5c06\u89c6\u89c9\u89c2\u5bdf\u4e0e\u81ea\u7136\u8bed\u8a00\u6982\u5ff5\u5bf9\u9f50\uff0c\u4e3a\u5b89\u5168\u5173\u952e\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u8bed\u4e49\u63a8\u7406\u63d0\u4f9b\u4e86\u65b0\u673a\u4f1a\u3002\u672c\u6587\u65e8\u5728\u63a2\u7d22\u5982\u4f55\u5c06\u89c6\u89c9\u8bed\u8a00\u8868\u793a\u96c6\u6210\u5230\u611f\u77e5\u3001\u9884\u6d4b\u548c\u89c4\u5212\u6d41\u7a0b\u4e2d\uff0c\u4ee5\u652f\u6301\u9a7e\u9a76\u573a\u666f\u5b89\u5168\u8bc4\u4f30\u548c\u51b3\u7b56\u3002", "method": "\u7814\u7a76\u4e86\u4e09\u79cd\u4e92\u8865\u7684\u7cfb\u7edf\u7ea7\u7528\u4f8b\uff1a1\uff09\u57fa\u4e8eCLIP\u56fe\u50cf-\u6587\u672c\u76f8\u4f3c\u6027\u7684\u8f7b\u91cf\u7ea7\u3001\u7c7b\u522b\u65e0\u5173\u7684\u5371\u9669\u7b5b\u67e5\u65b9\u6cd5\uff1b2\uff09\u5c06\u573a\u666f\u7ea7\u89c6\u89c9\u8bed\u8a00\u5d4c\u5165\u96c6\u6210\u5230\u57fa\u4e8eTransformer\u7684\u8f68\u8ff9\u89c4\u5212\u6846\u67b6\u4e2d\uff08\u4f7f\u7528Waymo Open Dataset\uff09\uff1b3\uff09\u4f7f\u7528\u81ea\u7136\u8bed\u8a00\u4f5c\u4e3a\u8fd0\u52a8\u89c4\u5212\u7684\u663e\u5f0f\u884c\u4e3a\u7ea6\u675f\uff08\u4f7f\u7528doScenes\u6570\u636e\u96c6\uff09\u3002", "result": "1\uff09\u5371\u9669\u7b5b\u67e5\u65b9\u6cd5\u80fd\u591f\u4f4e\u5ef6\u8fdf\u68c0\u6d4b\u591a\u6837\u5316\u548c\u5206\u5e03\u5916\u7684\u9053\u8def\u5371\u9669\uff1b2\uff09\u7b80\u5355\u5730\u5c06\u5168\u5c40\u5d4c\u5165\u6761\u4ef6\u5316\u5230\u89c4\u5212\u5668\u4e2d\u4e0d\u4f1a\u63d0\u9ad8\u8f68\u8ff9\u7cbe\u5ea6\uff0c\u8868\u660e\u8868\u793a-\u4efb\u52a1\u5bf9\u9f50\u7684\u91cd\u8981\u6027\uff1b3\uff09\u57fa\u4e8e\u89c6\u89c9\u573a\u666f\u5143\u7d20\u7684\u4e58\u5ba2\u5f0f\u6307\u4ee4\u80fd\u591f\u6291\u5236\u7f55\u89c1\u4f46\u4e25\u91cd\u7684\u89c4\u5212\u5931\u8d25\uff0c\u5728\u6a21\u7cca\u573a\u666f\u4e2d\u6539\u5584\u5b89\u5168\u5bf9\u9f50\u884c\u4e3a\u3002", "conclusion": "\u89c6\u89c9\u8bed\u8a00\u8868\u793a\u5728\u8868\u8fbe\u8bed\u4e49\u98ce\u9669\u3001\u610f\u56fe\u548c\u884c\u4e3a\u7ea6\u675f\u65b9\u9762\u5bf9\u81ea\u52a8\u9a7e\u9a76\u5b89\u5168\u5177\u6709\u91cd\u8981\u6f5c\u529b\u3002\u5b9e\u73b0\u8fd9\u4e00\u6f5c\u529b\u672c\u8d28\u4e0a\u662f\u4e00\u4e2a\u5de5\u7a0b\u95ee\u9898\uff0c\u9700\u8981\u7cbe\u5fc3\u8bbe\u8ba1\u7cfb\u7edf\u548c\u7ed3\u6784\u5316\u57fa\u7840\uff0c\u800c\u4e0d\u662f\u76f4\u63a5\u7279\u5f81\u6ce8\u5165\u3002"}}
{"id": "2602.08716", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.08716", "abs": "https://arxiv.org/abs/2602.08716", "authors": ["Shangrui Nie", "Kian Omoomi", "Lucie Flek", "Zhixue Zhao", "Charles Welch"], "title": "PERSPECTRA: A Scalable and Configurable Pluralist Benchmark of Perspectives from Arguments", "comment": "15 pages, 1 figure", "summary": "Pluralism, the capacity to engage with diverse perspectives without collapsing them into a single viewpoint, is critical for developing large language models that faithfully reflect human heterogeneity. Yet this characteristic has not been carefully examined in the LLM research community and remains absent from most alignment studies. Debate-oriented sources provide a natural entry point for pluralism research. Previous work builds on online debate sources but remains constrained by costly human validation. Other debate-rich platforms such as Reddit and Kialo also offer promising material: Reddit provides linguistic diversity and scale but lacks clear argumentative structure, while Kialo supplies explicit pro/con graphs but remains overly concise and detached from natural discourse. We introduce PERSPECTRA, a pluralist benchmark that integrates the structural clarity of Kialo debate graphs with the linguistic diversity of real Reddit discussions. Using a controlled retrieval-and-expansion pipeline, we construct 3,810 enriched arguments spanning 762 pro/con stances on 100 controversial topics. Each opinion is expanded to multiple naturalistic variants, enabling robust evaluation of pluralism. We initialise three tasks with PERSPECTRA: opinion counting (identifying distinct viewpoints), opinion matching (aligning supporting stances and discourse to source opinions), and polarity check (inferring aggregate stance in mixed discourse). Experiments with state-of-the-art open-source and proprietary LLMs, highlight systematic failures, such as overestimating the number of viewpoints and misclassifying concessive structures, underscoring the difficulty of pluralism-aware understanding and reasoning. By combining diversity with structure, PERSPECTRA establishes the first scalable, configurable benchmark for evaluating how well models represent, distinguish, and reason over multiple perspectives.", "AI": {"tldr": "PERSPECTRA\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u591a\u5143\u4e3b\u4e49\u80fd\u529b\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u901a\u8fc7\u6574\u5408Kialo\u8fa9\u8bba\u56fe\u7684\u7ed3\u6784\u6e05\u6670\u6027\u548cReddit\u8ba8\u8bba\u7684\u8bed\u8a00\u591a\u6837\u6027\uff0c\u6784\u5efa\u4e86\u5305\u542b3,810\u4e2a\u6269\u5c55\u8bba\u70b9\u7684\u6570\u636e\u96c6\uff0c\u6db5\u76d6100\u4e2a\u4e89\u8bae\u8bdd\u9898\u7684762\u4e2a\u6b63\u53cd\u7acb\u573a\u3002", "motivation": "\u591a\u5143\u4e3b\u4e49\uff08\u80fd\u591f\u63a5\u89e6\u4e0d\u540c\u89c2\u70b9\u800c\u4e0d\u5c06\u5176\u7b80\u5316\u4e3a\u5355\u4e00\u89c6\u89d2\uff09\u5bf9\u4e8e\u5f00\u53d1\u5fe0\u5b9e\u53cd\u6620\u4eba\u7c7b\u591a\u6837\u6027\u7684\u5927\u8bed\u8a00\u6a21\u578b\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u8fd9\u4e00\u7279\u6027\u5728LLM\u7814\u7a76\u793e\u533a\u4e2d\u5c1a\u672a\u5f97\u5230\u4ed4\u7ec6\u68c0\u9a8c\uff0c\u4e14\u5927\u591a\u6570\u5bf9\u9f50\u7814\u7a76\u4e2d\u90fd\u7f3a\u5931\u3002\u73b0\u6709\u8fa9\u8bba\u6570\u636e\u6e90\u5b58\u5728\u5c40\u9650\u6027\uff1aReddit\u63d0\u4f9b\u8bed\u8a00\u591a\u6837\u6027\u548c\u89c4\u6a21\u4f46\u7f3a\u4e4f\u6e05\u6670\u7684\u8bba\u8bc1\u7ed3\u6784\uff0cKialo\u63d0\u4f9b\u660e\u786e\u7684\u6b63\u53cd\u56fe\u4f46\u8fc7\u4e8e\u7b80\u6d01\u4e14\u8131\u79bb\u81ea\u7136\u8bdd\u8bed\u3002", "method": "\u5f15\u5165PERSPECTRA\u57fa\u51c6\uff0c\u901a\u8fc7\u53d7\u63a7\u7684\u68c0\u7d22\u548c\u6269\u5c55\u6d41\u7a0b\uff0c\u6574\u5408Kialo\u8fa9\u8bba\u56fe\u7684\u7ed3\u6784\u6e05\u6670\u6027\u548cReddit\u8ba8\u8bba\u7684\u8bed\u8a00\u591a\u6837\u6027\u3002\u6784\u5efa\u4e863,810\u4e2a\u6269\u5c55\u8bba\u70b9\uff0c\u6db5\u76d6100\u4e2a\u4e89\u8bae\u8bdd\u9898\u7684762\u4e2a\u6b63\u53cd\u7acb\u573a\uff0c\u6bcf\u4e2a\u89c2\u70b9\u6269\u5c55\u5230\u591a\u4e2a\u81ea\u7136\u53d8\u4f53\u3002\u521d\u59cb\u5316\u4e09\u4e2a\u4efb\u52a1\uff1a\u89c2\u70b9\u8ba1\u6570\uff08\u8bc6\u522b\u4e0d\u540c\u89c2\u70b9\uff09\u3001\u89c2\u70b9\u5339\u914d\uff08\u5c06\u652f\u6301\u7acb\u573a\u548c\u8bdd\u8bed\u4e0e\u6e90\u89c2\u70b9\u5bf9\u9f50\uff09\u3001\u6781\u6027\u68c0\u67e5\uff08\u63a8\u65ad\u6df7\u5408\u8bdd\u8bed\u4e2d\u7684\u603b\u4f53\u7acb\u573a\uff09\u3002", "result": "\u5b9e\u9a8c\u4f7f\u7528\u6700\u5148\u8fdb\u7684\u5f00\u6e90\u548c\u4e13\u6709LLM\uff0c\u63ed\u793a\u4e86\u7cfb\u7edf\u6027\u5931\u8d25\uff0c\u5982\u9ad8\u4f30\u89c2\u70b9\u6570\u91cf\u548c\u5bf9\u8ba9\u6b65\u7ed3\u6784\u7684\u9519\u8bef\u5206\u7c7b\uff0c\u7a81\u663e\u4e86\u591a\u5143\u4e3b\u4e49\u611f\u77e5\u7406\u89e3\u548c\u63a8\u7406\u7684\u56f0\u96be\u3002\u6a21\u578b\u5728\u51c6\u786e\u8bc6\u522b\u3001\u533a\u5206\u548c\u63a8\u7406\u591a\u4e2a\u89c2\u70b9\u65b9\u9762\u5b58\u5728\u663e\u8457\u6311\u6218\u3002", "conclusion": "PERSPECTRA\u901a\u8fc7\u7ed3\u5408\u591a\u6837\u6027\u548c\u7ed3\u6784\uff0c\u5efa\u7acb\u4e86\u7b2c\u4e00\u4e2a\u53ef\u6269\u5c55\u3001\u53ef\u914d\u7f6e\u7684\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u6a21\u578b\u5982\u4f55\u4ee3\u8868\u3001\u533a\u5206\u548c\u63a8\u7406\u591a\u4e2a\u89c2\u70b9\u3002\u8fd9\u9879\u5de5\u4f5c\u586b\u8865\u4e86LLM\u7814\u7a76\u4e2d\u591a\u5143\u4e3b\u4e49\u8bc4\u4f30\u7684\u7a7a\u767d\uff0c\u4e3a\u5f00\u53d1\u66f4\u5fe0\u5b9e\u53cd\u6620\u4eba\u7c7b\u591a\u6837\u6027\u7684\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u91cd\u8981\u5de5\u5177\u3002"}}
{"id": "2602.07689", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07689", "abs": "https://arxiv.org/abs/2602.07689", "authors": ["Jusheng Zhang", "Kaitong Cai", "Jian Wang", "Yongsen Zheng", "Kwok-Yan Lam", "Keze Wang"], "title": "Process-of-Thought Reasoning for Videos", "comment": null, "summary": "Video understanding requires not only recognizing visual content but also performing temporally grounded, multi-step reasoning over long and noisy observations. We propose Process-of-Thought (PoT) Reasoning for Videos, a framework that makes the reasoning process explicit by structuring video inference into a sequence of lightweight, verifiable steps. PoT interleaves (i) temporal evidence selection, (ii) step-wise state updates, and (iii) constrained answer synthesis, enabling the model to progressively refine hypotheses while maintaining traceability to video evidence. The framework is designed to be model-agnostic and can be plugged into existing vision-language backbones, supporting both closed-book reasoning and evidence-augmented reasoning with external tools. We further introduce a unified representation for PoT traces that aligns intermediate decisions with temporal segments, which improves robustness to distractors and reduces hallucinated explanations. Extensive experiments on standard video reasoning tasks demonstrate that PoT consistently improves factual correctness and temporal grounding, while providing interpretable reasoning traces for diagnosis and downstream use.", "AI": {"tldr": "\u63d0\u51fa\u89c6\u9891\u63a8\u7406\u7684\u601d\u7ef4\u8fc7\u7a0b\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u63a8\u7406\u5206\u89e3\u4e3a\u53ef\u9a8c\u8bc1\u7684\u6b65\u9aa4\u6765\u63d0\u5347\u89c6\u9891\u7406\u89e3\u80fd\u529b", "motivation": "\u89c6\u9891\u7406\u89e3\u9700\u8981\u5904\u7406\u957f\u800c\u5608\u6742\u7684\u89c2\u5bdf\uff0c\u8fdb\u884c\u65f6\u95f4\u951a\u5b9a\u7684\u591a\u6b65\u63a8\u7406\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u63a8\u7406\u8fc7\u7a0b\u7684\u53ef\u89e3\u91ca\u6027\u548c\u8bc1\u636e\u8ffd\u6eaf\u6027\u65b9\u9762\u5b58\u5728\u4e0d\u8db3", "method": "\u63d0\u51fa\u601d\u7ef4\u8fc7\u7a0b\u63a8\u7406\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u4ea4\u9519\u6b65\u9aa4\uff1a\u65f6\u95f4\u8bc1\u636e\u9009\u62e9\u3001\u9010\u6b65\u72b6\u6001\u66f4\u65b0\u548c\u7ea6\u675f\u7b54\u6848\u5408\u6210\uff0c\u91c7\u7528\u6a21\u578b\u65e0\u5173\u8bbe\u8ba1\uff0c\u53ef\u4e0e\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u4e3b\u5e72\u7ed3\u5408", "result": "\u5728\u6807\u51c6\u89c6\u9891\u63a8\u7406\u4efb\u52a1\u4e0a\uff0cPoT\u6846\u67b6\u663e\u8457\u63d0\u9ad8\u4e86\u4e8b\u5b9e\u6b63\u786e\u6027\u548c\u65f6\u95f4\u5b9a\u4f4d\u51c6\u786e\u6027\uff0c\u540c\u65f6\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u63a8\u7406\u8f68\u8ff9", "conclusion": "PoT\u6846\u67b6\u901a\u8fc7\u7ed3\u6784\u5316\u3001\u53ef\u9a8c\u8bc1\u7684\u63a8\u7406\u6b65\u9aa4\uff0c\u6709\u6548\u63d0\u5347\u4e86\u89c6\u9891\u63a8\u7406\u7684\u9c81\u68d2\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u4e3a\u590d\u6742\u89c6\u9891\u7406\u89e3\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5"}}
{"id": "2602.08740", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.08740", "abs": "https://arxiv.org/abs/2602.08740", "authors": ["Gaifan Zhang", "Danushka Bollegala"], "title": "Map of Encoders -- Mapping Sentence Encoders using Quantum Relative Entropy", "comment": null, "summary": "We propose a method to compare and visualise sentence encoders at scale by creating a map of encoders where each sentence encoder is represented in relation to the other sentence encoders. Specifically, we first represent a sentence encoder using an embedding matrix of a sentence set, where each row corresponds to the embedding of a sentence. Next, we compute the Pairwise Inner Product (PIP) matrix for a sentence encoder using its embedding matrix. Finally, we create a feature vector for each sentence encoder reflecting its Quantum Relative Entropy (QRE) with respect to a unit base encoder. We construct a map of encoders covering 1101 publicly available sentence encoders, providing a new perspective of the landscape of the pre-trained sentence encoders. Our map accurately reflects various relationships between encoders, where encoders with similar attributes are proximally located on the map. Moreover, our encoder feature vectors can be used to accurately infer downstream task performance of the encoders, such as in retrieval and clustering tasks, demonstrating the faithfulness of our map.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u5927\u89c4\u6a21\u6bd4\u8f83\u548c\u53ef\u89c6\u5316\u53e5\u5b50\u7f16\u7801\u5668\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u521b\u5efa\u7f16\u7801\u5668\u5730\u56fe\u6765\u5c55\u793a\u5404\u7f16\u7801\u5668\u4e4b\u95f4\u7684\u76f8\u5bf9\u5173\u7cfb\u3002", "motivation": "\u9700\u8981\u4e00\u79cd\u7cfb\u7edf\u6027\u7684\u65b9\u6cd5\u6765\u7406\u89e3\u548c\u6bd4\u8f83\u5927\u91cf\u516c\u5f00\u53ef\u7528\u7684\u53e5\u5b50\u7f16\u7801\u5668\uff0c\u63ed\u793a\u5b83\u4eec\u4e4b\u95f4\u7684\u76f8\u4f3c\u6027\u548c\u5dee\u5f02\u6027\uff0c\u4e3a\u7814\u7a76\u8005\u63d0\u4f9b\u7f16\u7801\u5668\u9009\u62e9\u7684\u6307\u5bfc\u3002", "method": "\u9996\u5148\u7528\u53e5\u5b50\u96c6\u7684\u5d4c\u5165\u77e9\u9635\u8868\u793a\u6bcf\u4e2a\u7f16\u7801\u5668\uff0c\u7136\u540e\u8ba1\u7b97\u5176\u6210\u5bf9\u5185\u79ef\uff08PIP\uff09\u77e9\u9635\uff0c\u6700\u540e\u4e3a\u6bcf\u4e2a\u7f16\u7801\u5668\u521b\u5efa\u53cd\u6620\u5176\u4e0e\u5355\u4f4d\u57fa\u7f16\u7801\u5668\u91cf\u5b50\u76f8\u5bf9\u71b5\uff08QRE\uff09\u7684\u7279\u5f81\u5411\u91cf\u3002", "result": "\u6784\u5efa\u4e86\u5305\u542b1101\u4e2a\u516c\u5f00\u53e5\u5b50\u7f16\u7801\u5668\u7684\u5730\u56fe\uff0c\u51c6\u786e\u53cd\u6620\u4e86\u7f16\u7801\u5668\u4e4b\u95f4\u7684\u5404\u79cd\u5173\u7cfb\uff0c\u76f8\u4f3c\u5c5e\u6027\u7684\u7f16\u7801\u5668\u5728\u5730\u56fe\u4e0a\u4f4d\u7f6e\u76f8\u8fd1\u3002\u7f16\u7801\u5668\u7279\u5f81\u5411\u91cf\u80fd\u51c6\u786e\u9884\u6d4b\u7f16\u7801\u5668\u5728\u4e0b\u6e38\u4efb\u52a1\uff08\u5982\u68c0\u7d22\u548c\u805a\u7c7b\uff09\u4e2d\u7684\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u521b\u5efa\u4e86\u53e5\u5b50\u7f16\u7801\u5668\u7684\u5168\u9762\u5730\u56fe\uff0c\u4e3a\u7406\u89e3\u9884\u8bad\u7ec3\u53e5\u5b50\u7f16\u7801\u5668\u7684\u683c\u5c40\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\uff0c\u8bc1\u660e\u4e86\u5730\u56fe\u7684\u5fe0\u5b9e\u6027\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2602.08793", "categories": ["cs.CL", "cs.DB"], "pdf": "https://arxiv.org/pdf/2602.08793", "abs": "https://arxiv.org/abs/2602.08793", "authors": ["Yushi Sun", "Xujia Li", "Nan Tang", "Quanqing Xu", "Chuanhui Yang", "Lei Chen"], "title": "LakeHopper: Cross Data Lakes Column Type Annotation through Model Adaptation", "comment": null, "summary": "Column type annotation is vital for tasks like data cleaning, integration, and visualization. Recent solutions rely on resource-intensive language models fine-tuned on well-annotated columns from a particular set of tables, i.e., a source data lake. In this paper, we study whether we can adapt an existing pre-trained LM-based model to a new (i.e., target) data lake to minimize the annotations required on the new data lake. However, challenges include the source-target knowledge gap, selecting informative target data, and fine-tuning without losing shared knowledge exist. We propose LakeHopper, a framework that identifies and resolves the knowledge gap through LM interactions, employs a cluster-based data selection scheme for unannotated columns, and uses an incremental fine-tuning mechanism that gradually adapts the source model to the target data lake. Our experimental results validate the effectiveness of LakeHopper on two different data lake transfers under both low-resource and high-resource settings.", "AI": {"tldr": "LakeHopper\u6846\u67b6\u901a\u8fc7\u8bc6\u522b\u77e5\u8bc6\u5dee\u8ddd\u3001\u805a\u7c7b\u9009\u62e9\u6570\u636e\u548c\u589e\u91cf\u5fae\u8c03\uff0c\u5c06\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u4ece\u6e90\u6570\u636e\u6e56\u8fc1\u79fb\u5230\u76ee\u6807\u6570\u636e\u6e56\uff0c\u51cf\u5c11\u65b0\u6570\u636e\u6e56\u7684\u6807\u6ce8\u9700\u6c42\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u8bed\u8a00\u6a21\u578b\u7684\u5217\u7c7b\u578b\u6807\u6ce8\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u6807\u6ce8\u6570\u636e\uff0c\u4e14\u9488\u5bf9\u7279\u5b9a\u6570\u636e\u6e56\u8bad\u7ec3\u3002\u5f53\u9762\u5bf9\u65b0\u6570\u636e\u6e56\u65f6\uff0c\u9700\u8981\u91cd\u65b0\u6807\u6ce8\u5927\u91cf\u6570\u636e\uff0c\u6210\u672c\u9ad8\u6602\u3002\u7814\u7a76\u5982\u4f55\u5c06\u5df2\u6709\u6a21\u578b\u8fc1\u79fb\u5230\u65b0\u6570\u636e\u6e56\uff0c\u6700\u5c0f\u5316\u65b0\u6570\u636e\u6e56\u7684\u6807\u6ce8\u9700\u6c42\u3002", "method": "\u63d0\u51faLakeHopper\u6846\u67b6\uff1a1) \u901a\u8fc7\u8bed\u8a00\u6a21\u578b\u4ea4\u4e92\u8bc6\u522b\u6e90-\u76ee\u6807\u6570\u636e\u6e56\u4e4b\u95f4\u7684\u77e5\u8bc6\u5dee\u8ddd\uff1b2) \u91c7\u7528\u57fa\u4e8e\u805a\u7c7b\u7684\u6570\u636e\u9009\u62e9\u65b9\u6848\u4ece\u672a\u6807\u6ce8\u5217\u4e2d\u9009\u62e9\u4fe1\u606f\u91cf\u5927\u7684\u6570\u636e\uff1b3) \u4f7f\u7528\u589e\u91cf\u5fae\u8c03\u673a\u5236\u9010\u6b65\u5c06\u6e90\u6a21\u578b\u9002\u5e94\u5230\u76ee\u6807\u6570\u636e\u6e56\uff0c\u907f\u514d\u4e22\u5931\u5171\u4eab\u77e5\u8bc6\u3002", "result": "\u5728\u4e24\u4e2a\u4e0d\u540c\u6570\u636e\u6e56\u8fc1\u79fb\u4efb\u52a1\u4e0a\u9a8c\u8bc1\u4e86LakeHopper\u7684\u6709\u6548\u6027\uff0c\u5305\u62ec\u4f4e\u8d44\u6e90\u548c\u9ad8\u8d44\u6e90\u4e24\u79cd\u8bbe\u7f6e\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u6846\u67b6\u80fd\u591f\u6210\u529f\u5c06\u6e90\u6a21\u578b\u8fc1\u79fb\u5230\u76ee\u6807\u6570\u636e\u6e56\u3002", "conclusion": "LakeHopper\u6846\u67b6\u901a\u8fc7\u89e3\u51b3\u77e5\u8bc6\u5dee\u8ddd\u3001\u667a\u80fd\u6570\u636e\u9009\u62e9\u548c\u589e\u91cf\u5fae\u8c03\uff0c\u5b9e\u73b0\u4e86\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u5728\u4e0d\u540c\u6570\u636e\u6e56\u4e4b\u95f4\u7684\u6709\u6548\u8fc1\u79fb\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u65b0\u6570\u636e\u6e56\u7684\u6807\u6ce8\u9700\u6c42\u3002"}}
{"id": "2602.08520", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.08520", "abs": "https://arxiv.org/abs/2602.08520", "authors": ["Xinhai Sun"], "title": "Reinforcement Inference: Leveraging Uncertainty for Self-Correcting Language Model Reasoning", "comment": null, "summary": "Modern large language models (LLMs) are often evaluated and deployed under a \\emph{one-shot, greedy} inference protocol, especially in professional settings that require deterministic behavior. This regime can systematically under-estimate a fixed model's true capability: many errors arise not from missing knowledge, but from premature commitment under internal ambiguity. We introduce \\emph{Reinforcement Inference}, an entropy-aware inference-time control strategy that uses the model's own uncertainty to selectively invoke a second, more deliberate reasoning attempt, enabling stronger performance \\emph{without any retraining}.\n  On 12,032 MMLU-Pro questions across 14 subjects, using DeepSeek-v3.2 with deterministic decoding in a zero-shot setting, Reinforcement Inference improves accuracy from 60.72\\% to 84.03\\%, while only incurring 61.06\\% additional inference calls. A 100\\% re-asking ablation reaches 84.35\\%, indicating that uncertainty-aware selection captures most of the attainable improvement with substantially less compute. Moreover, a \\emph{prompt-only} ablation underperforms the baseline, suggesting that the gains are not explained by generic `` your output had high entropy, think step-by-step'' prompting alone.\n  Beyond providing a practical inference-time upgrade, our results suggest a broader \\emph{entropy-aware} paradigm for measuring and expanding model capability: because modern decoder-based models generate outputs autoregressively, entropy and related confidence measures arise naturally as first-class control signals during generation. The resulting gap between one-pass greedy inference and uncertainty-conditioned deliberation offers a diagnostic lens on an LLM's latent reasoning horizon and motivates future training objectives that explicitly constrain correctness--confidence alignment.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\"\u5f3a\u5316\u63a8\u7406\"\u65b9\u6cd5\uff0c\u5229\u7528LLM\u81ea\u8eab\u7684\u4e0d\u786e\u5b9a\u6027\u5728\u63a8\u7406\u65f6\u9009\u62e9\u6027\u8c03\u7528\u4e8c\u6b21\u63a8\u7406\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u5373\u53ef\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u5f53\u524dLLM\u5728\u5355\u6b21\u8d2a\u5a6a\u63a8\u7406\u534f\u8bae\u4e0b\u4f1a\u7cfb\u7edf\u6027\u5730\u4f4e\u4f30\u6a21\u578b\u771f\u5b9e\u80fd\u529b\uff0c\u8bb8\u591a\u9519\u8bef\u6e90\u4e8e\u5185\u90e8\u6a21\u7cca\u6027\u4e0b\u7684\u8fc7\u65e9\u51b3\u7b56\uff0c\u800c\u975e\u77e5\u8bc6\u7f3a\u5931\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u71b5\u611f\u77e5\u7684\u63a8\u7406\u65f6\u63a7\u5236\u7b56\u7565\uff0c\u5229\u7528\u6a21\u578b\u81ea\u8eab\u4e0d\u786e\u5b9a\u6027\u9009\u62e9\u6027\u5730\u8c03\u7528\u7b2c\u4e8c\u6b21\u66f4\u5ba1\u614e\u7684\u63a8\u7406\u5c1d\u8bd5\u3002", "result": "\u5728MMLU-Pro\u768412,032\u4e2a\u95ee\u9898\u4e0a\uff0c\u4f7f\u7528DeepSeek-v3.2\u6a21\u578b\uff0c\u51c6\u786e\u7387\u4ece60.72%\u63d0\u5347\u523084.03%\uff0c\u4ec5\u589e\u52a061.06%\u7684\u63a8\u7406\u8c03\u7528\u3002100%\u91cd\u95ee\u6d88\u878d\u5b9e\u9a8c\u8fbe\u523084.35%\uff0c\u8868\u660e\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u9009\u62e9\u80fd\u4ee5\u66f4\u5c11\u8ba1\u7b97\u83b7\u5f97\u5927\u90e8\u5206\u53ef\u5b9e\u73b0\u7684\u6539\u8fdb\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u63d0\u4f9b\u5b9e\u7528\u7684\u63a8\u7406\u65f6\u5347\u7ea7\uff0c\u8fd8\u63d0\u51fa\u4e86\u66f4\u5e7f\u6cdb\u7684\u71b5\u611f\u77e5\u8303\u5f0f\uff0c\u7528\u4e8e\u8861\u91cf\u548c\u6269\u5c55\u6a21\u578b\u80fd\u529b\uff0c\u4e3aLLM\u7684\u6f5c\u5728\u63a8\u7406\u8303\u56f4\u63d0\u4f9b\u8bca\u65ad\u89c6\u89d2\uff0c\u5e76\u6fc0\u52b1\u672a\u6765\u8bad\u7ec3\u76ee\u6807\u4e2d\u660e\u786e\u7ea6\u675f\u6b63\u786e\u6027-\u7f6e\u4fe1\u5ea6\u5bf9\u9f50\u3002"}}
{"id": "2602.08826", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08826", "abs": "https://arxiv.org/abs/2602.08826", "authors": ["Chenghui Zou", "Ning Wang", "Tiesunlong Shen", "Luwei Xiao", "Chuan Ma", "Xiangpeng Li", "Rui Mao", "Erik Cambria"], "title": "Affective Flow Language Model for Emotional Support Conversation", "comment": "19 pages, 7 figures", "summary": "Large language models (LLMs) have been widely applied to emotional support conversation (ESC). However, complex multi-turn support remains challenging.This is because existing alignment schemes rely on sparse outcome-level signals, thus offering limited supervision for intermediate strategy decisions. To fill this gap, this paper proposes affective flow language model for emotional support conversation (AFlow), a framework that introduces fine-grained supervision on dialogue prefixes by modeling a continuous affective flow along multi-turn trajectories. AFlow can estimate intermediate utility over searched trajectories and learn preference-consistent strategy transitions. To improve strategy coherence and empathetic response quality, a subpath-level flow-balance objective is presented to propagate preference signals to intermediate states. Experiment results show consistent and significant improvements over competitive baselines in diverse emotional contexts. Remarkably, AFlow with a compact open-source backbone outperforms proprietary LMMs such as GPT-4o and Claude-3.5 on major ESC metrics. Our code is available at https://github.com/chzou25-lgtm/AffectiveFlow.", "AI": {"tldr": "AFlow\u6846\u67b6\u901a\u8fc7\u5efa\u6a21\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u7684\u8fde\u7eed\u60c5\u611f\u6d41\uff0c\u4e3a\u60c5\u611f\u652f\u6301\u5bf9\u8bdd\u63d0\u4f9b\u7ec6\u7c92\u5ea6\u76d1\u7763\uff0c\u5728\u7b56\u7565\u4e00\u81f4\u6027\u548c\u5171\u60c5\u54cd\u5e94\u8d28\u91cf\u4e0a\u663e\u8457\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\uff0c\u751a\u81f3\u4f18\u4e8eGPT-4o\u548cClaude-3.5\u7b49\u4e13\u6709\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u60c5\u611f\u652f\u6301\u5bf9\u8bdd\u7cfb\u7edf\u4f9d\u8d56\u7a00\u758f\u7684\u7ed3\u679c\u7ea7\u4fe1\u53f7\u5bf9\u9f50\uff0c\u5bf9\u4e2d\u95f4\u7b56\u7565\u51b3\u7b56\u7684\u76d1\u7763\u6709\u9650\uff0c\u5bfc\u81f4\u590d\u6742\u591a\u8f6e\u652f\u6301\u5bf9\u8bdd\u6548\u679c\u4e0d\u4f73\u3002", "method": "\u63d0\u51faAFlow\u6846\u67b6\uff0c\u901a\u8fc7\u5efa\u6a21\u591a\u8f6e\u8f68\u8ff9\u4e2d\u7684\u8fde\u7eed\u60c5\u611f\u6d41\uff0c\u5f15\u5165\u5bf9\u8bdd\u524d\u7f00\u7684\u7ec6\u7c92\u5ea6\u76d1\u7763\uff1b\u4f7f\u7528\u5b50\u8def\u5f84\u7ea7\u6d41\u5e73\u8861\u76ee\u6807\u4f20\u64ad\u504f\u597d\u4fe1\u53f7\u5230\u4e2d\u95f4\u72b6\u6001\uff0c\u63d0\u5347\u7b56\u7565\u4e00\u81f4\u6027\u548c\u5171\u60c5\u54cd\u5e94\u8d28\u91cf\u3002", "result": "\u5728\u591a\u6837\u60c5\u611f\u8bed\u5883\u4e2d\u4e00\u81f4\u4e14\u663e\u8457\u4f18\u4e8e\u7ade\u4e89\u57fa\u7ebf\uff1b\u7d27\u51d1\u5f00\u6e90\u9aa8\u5e72\u7684AFlow\u5728\u4e3b\u8981ESC\u6307\u6807\u4e0a\u8d85\u8d8aGPT-4o\u548cClaude-3.5\u7b49\u4e13\u6709\u5927\u8bed\u8a00\u6a21\u578b\u3002", "conclusion": "AFlow\u901a\u8fc7\u8fde\u7eed\u60c5\u611f\u6d41\u5efa\u6a21\u4e3a\u60c5\u611f\u652f\u6301\u5bf9\u8bdd\u63d0\u4f9b\u6709\u6548\u7684\u7ec6\u7c92\u5ea6\u76d1\u7763\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u7684\u7b56\u7565\u51b3\u7b56\u548c\u5171\u60c5\u54cd\u5e94\u8d28\u91cf\u3002"}}
{"id": "2602.08829", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08829", "abs": "https://arxiv.org/abs/2602.08829", "authors": ["Hao Peng", "Yunjia Qi", "Xiaozhi Wang", "Zijun Yao", "Lei Hou", "Juanzi Li"], "title": "WildReward: Learning Reward Models from In-the-Wild Human Interactions", "comment": null, "summary": "Reward models (RMs) are crucial for the training of large language models (LLMs), yet they typically rely on large-scale human-annotated preference pairs. With the widespread deployment of LLMs, in-the-wild interactions have emerged as a rich source of implicit reward signals. This raises the question: Can we develop reward models directly from in-the-wild interactions? In this work, we explore this possibility by adopting WildChat as an interaction source and proposing a pipeline to extract reliable human feedback, yielding 186k high-quality instances for training WildReward via ordinal regression directly on user feedback without preference pairs. Extensive experiments demonstrate that WildReward achieves comparable or even superior performance compared to conventional reward models, with improved calibration and cross-sample consistency. We also observe that WildReward benefits directly from user diversity, where more users yield stronger reward models. Finally, we apply WildReward to online DPO training and observe significant improvements across various tasks. Code and data are released at https://github.com/THU-KEG/WildReward.", "AI": {"tldr": "WildReward\uff1a\u76f4\u63a5\u4ece\u7528\u6237\u4ea4\u4e92\u4e2d\u8bad\u7ec3\u5956\u52b1\u6a21\u578b\uff0c\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\u504f\u597d\u5bf9\uff0c\u6027\u80fd\u5ab2\u7f8e\u4f20\u7edf\u5956\u52b1\u6a21\u578b", "motivation": "\u4f20\u7edf\u5956\u52b1\u6a21\u578b\u4f9d\u8d56\u5927\u89c4\u6a21\u4eba\u5de5\u6807\u6ce8\u7684\u504f\u597d\u5bf9\uff0c\u6210\u672c\u9ad8\u6602\u3002\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5e7f\u6cdb\u90e8\u7f72\uff0c\u7528\u6237\u4ea4\u4e92\u6210\u4e3a\u4e30\u5bcc\u7684\u9690\u5f0f\u5956\u52b1\u4fe1\u53f7\u6765\u6e90\uff0c\u80fd\u5426\u76f4\u63a5\u4ece\u7528\u6237\u4ea4\u4e92\u4e2d\u5f00\u53d1\u5956\u52b1\u6a21\u578b\uff1f", "method": "\u91c7\u7528WildChat\u4f5c\u4e3a\u4ea4\u4e92\u6e90\uff0c\u63d0\u51fa\u4ece\u7528\u6237\u53cd\u9988\u4e2d\u63d0\u53d6\u53ef\u9760\u4eba\u7c7b\u53cd\u9988\u7684\u6d41\u7a0b\uff0c\u901a\u8fc7\u5e8f\u6570\u56de\u5f52\u76f4\u63a5\u5728\u7528\u6237\u53cd\u9988\u4e0a\u8bad\u7ec3WildReward\uff0c\u65e0\u9700\u504f\u597d\u5bf9\uff0c\u83b7\u5f9718.6\u4e07\u9ad8\u8d28\u91cf\u8bad\u7ec3\u5b9e\u4f8b", "result": "WildReward\u5728\u6027\u80fd\u4e0a\u8fbe\u5230\u751a\u81f3\u8d85\u8fc7\u4f20\u7edf\u5956\u52b1\u6a21\u578b\uff0c\u5177\u6709\u66f4\u597d\u7684\u6821\u51c6\u6027\u548c\u8de8\u6837\u672c\u4e00\u81f4\u6027\u3002\u6a21\u578b\u6027\u80fd\u76f4\u63a5\u53d7\u76ca\u4e8e\u7528\u6237\u591a\u6837\u6027\uff0c\u7528\u6237\u8d8a\u591a\u5956\u52b1\u6a21\u578b\u8d8a\u5f3a\u3002\u5e94\u7528\u4e8e\u5728\u7ebfDPO\u8bad\u7ec3\u65f6\uff0c\u5728\u5404\u79cd\u4efb\u52a1\u4e0a\u90fd\u6709\u663e\u8457\u63d0\u5347", "conclusion": "\u76f4\u63a5\u4ece\u7528\u6237\u4ea4\u4e92\u4e2d\u8bad\u7ec3\u5956\u52b1\u6a21\u578b\u662f\u53ef\u884c\u7684\uff0cWildReward\u5c55\u793a\u4e86\u8fd9\u79cd\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u4e3a\u5956\u52b1\u6a21\u578b\u8bad\u7ec3\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u3001\u6210\u672c\u66f4\u4f4e\u7684\u66ff\u4ee3\u65b9\u6848"}}
{"id": "2602.08864", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.08864", "abs": "https://arxiv.org/abs/2602.08864", "authors": ["Ibraheem Muhammad Moosa", "Suhas Lohit", "Ye Wang", "Moitreya Chatterjee", "Wenpeng Yin"], "title": "Understanding Dynamic Compute Allocation in Recurrent Transformers", "comment": null, "summary": "Token-level adaptive computation seeks to reduce inference cost by allocating more computation to harder tokens and less to easier ones. However, prior work is primarily evaluated on natural-language benchmarks using task-level metrics, where token-level difficulty is unobservable and confounded with architectural factors, making it unclear whether compute allocation truly aligns with underlying complexity. We address this gap through three contributions. First, we introduce a complexity-controlled evaluation paradigm using algorithmic and synthetic language tasks with parameterized difficulty, enabling direct testing of token-level compute allocation. Second, we propose ANIRA, a unified recurrent Transformer framework that supports per-token variable-depth computation while isolating compute allocation decisions from other model factors. Third, we use this framework to conduct a systematic analysis of token-level adaptive computation across alignment with complexity, generalization, and decision timing. Our results show that compute allocation aligned with task complexity can emerge without explicit difficulty supervision, but such alignment does not imply algorithmic generalization: models fail to extrapolate to unseen input sizes despite allocating additional computation. We further find that early compute decisions rely on static structural cues, whereas online halting more closely tracks algorithmic execution state.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u7b97\u6cd5\u548c\u5408\u6210\u8bed\u8a00\u4efb\u52a1\u8bc4\u4f30\u4ee4\u724c\u7ea7\u81ea\u9002\u5e94\u8ba1\u7b97\uff0c\u53d1\u73b0\u8ba1\u7b97\u5206\u914d\u80fd\u4e0e\u4efb\u52a1\u590d\u6742\u5ea6\u5bf9\u9f50\u4f46\u7f3a\u4e4f\u6cdb\u5316\u80fd\u529b\uff0c\u65e9\u671f\u51b3\u7b56\u4f9d\u8d56\u9759\u6001\u7ed3\u6784\u7ebf\u7d22\u800c\u5728\u7ebf\u505c\u6b62\u66f4\u63a5\u8fd1\u7b97\u6cd5\u6267\u884c\u72b6\u6001\u3002", "motivation": "\u5148\u524d\u5173\u4e8e\u4ee4\u724c\u7ea7\u81ea\u9002\u5e94\u8ba1\u7b97\u7684\u7814\u7a76\u4e3b\u8981\u5728\u81ea\u7136\u8bed\u8a00\u57fa\u51c6\u4e0a\u8fdb\u884c\u4efb\u52a1\u7ea7\u8bc4\u4f30\uff0c\u5176\u4e2d\u4ee4\u724c\u7ea7\u96be\u5ea6\u4e0d\u53ef\u89c2\u6d4b\u4e14\u4e0e\u67b6\u6784\u56e0\u7d20\u6df7\u6dc6\uff0c\u65e0\u6cd5\u786e\u5b9a\u8ba1\u7b97\u5206\u914d\u662f\u5426\u771f\u6b63\u4e0e\u5e95\u5c42\u590d\u6742\u5ea6\u5bf9\u9f50\u3002", "method": "1) \u5f15\u5165\u590d\u6742\u5ea6\u63a7\u5236\u7684\u8bc4\u4f30\u8303\u5f0f\uff0c\u4f7f\u7528\u53c2\u6570\u5316\u96be\u5ea6\u7684\u7b97\u6cd5\u548c\u5408\u6210\u8bed\u8a00\u4efb\u52a1\uff1b2) \u63d0\u51faANIRA\u7edf\u4e00\u5faa\u73afTransformer\u6846\u67b6\uff0c\u652f\u6301\u6bcf\u4ee4\u724c\u53ef\u53d8\u6df1\u5ea6\u8ba1\u7b97\uff1b3) \u4f7f\u7528\u8be5\u6846\u67b6\u7cfb\u7edf\u5206\u6790\u4ee4\u724c\u7ea7\u81ea\u9002\u5e94\u8ba1\u7b97\u4e0e\u590d\u6742\u5ea6\u5bf9\u9f50\u3001\u6cdb\u5316\u548c\u51b3\u7b56\u65f6\u5e8f\u7684\u5173\u7cfb\u3002", "result": "\u8ba1\u7b97\u5206\u914d\u80fd\u4e0e\u4efb\u52a1\u590d\u6742\u5ea6\u5bf9\u9f50\u800c\u65e0\u9700\u663e\u5f0f\u96be\u5ea6\u76d1\u7763\uff0c\u4f46\u8fd9\u79cd\u5bf9\u9f50\u4e0d\u610f\u5473\u7740\u7b97\u6cd5\u6cdb\u5316\uff1a\u6a21\u578b\u65e0\u6cd5\u6cdb\u5316\u5230\u672a\u89c1\u8f93\u5165\u5927\u5c0f\uff0c\u5c3d\u7ba1\u5206\u914d\u4e86\u989d\u5916\u8ba1\u7b97\u3002\u65e9\u671f\u8ba1\u7b97\u51b3\u7b56\u4f9d\u8d56\u9759\u6001\u7ed3\u6784\u7ebf\u7d22\uff0c\u800c\u5728\u7ebf\u505c\u6b62\u66f4\u63a5\u8fd1\u8ddf\u8e2a\u7b97\u6cd5\u6267\u884c\u72b6\u6001\u3002", "conclusion": "\u4ee4\u724c\u7ea7\u81ea\u9002\u5e94\u8ba1\u7b97\u80fd\u4e0e\u4efb\u52a1\u590d\u6742\u5ea6\u5bf9\u9f50\uff0c\u4f46\u7f3a\u4e4f\u6cdb\u5316\u80fd\u529b\uff0c\u8868\u660e\u9700\u8981\u66f4\u7cbe\u7ec6\u7684\u8bc4\u4f30\u65b9\u6cd5\u6765\u7406\u89e3\u81ea\u9002\u5e94\u8ba1\u7b97\u673a\u5236\u53ca\u5176\u5b9e\u9645\u6548\u679c\u3002"}}
{"id": "2602.07775", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.07775", "abs": "https://arxiv.org/abs/2602.07775", "authors": ["Haodong Li", "Shaoteng Liu", "Zhe Lin", "Manmohan Chandraker"], "title": "Rolling Sink: Bridging Limited-Horizon Training and Open-Ended Testing in Autoregressive Video Diffusion", "comment": "Figure PDFs were compressed to 150 dpi to comply with arXiv's submission size limit. Project page: https://rolling-sink.github.io/", "summary": "Recently, autoregressive (AR) video diffusion models has achieved remarkable performance. However, due to their limited training durations, a train-test gap emerges when testing at longer horizons, leading to rapid visual degradations. Following Self Forcing, which studies the train-test gap within the training duration, this work studies the train-test gap beyond the training duration, i.e., the gap between the limited horizons during training and open-ended horizons during testing. Since open-ended testing can extend beyond any finite training window, and long-video training is computationally expensive, we pursue a training-free solution to bridge this gap. To explore a training-free solution, we conduct a systematic analysis of AR cache maintenance. These insights lead to Rolling Sink. Built on Self Forcing (trained on only 5s clips), Rolling Sink effectively scales the AR video synthesis to ultra-long durations (e.g., 5-30 minutes at 16 FPS) at test time, with consistent subjects, stable colors, coherent structures, and smooth motions. As demonstrated by extensive experiments, Rolling Sink achieves superior long-horizon visual fidelity and temporal consistency compared to SOTA baselines. Project page: https://rolling-sink.github.io/", "AI": {"tldr": "\u63d0\u51faRolling Sink\u65b9\u6cd5\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u5373\u53ef\u89e3\u51b3\u81ea\u56de\u5f52\u89c6\u9891\u6269\u6563\u6a21\u578b\u5728\u8d85\u957f\u65f6\u57df\u6d4b\u8bd5\u4e2d\u7684\u89c6\u89c9\u9000\u5316\u95ee\u9898\uff0c\u5c06\u89c6\u9891\u5408\u6210\u6269\u5c55\u52305-30\u5206\u949f", "motivation": "\u81ea\u56de\u5f52\u89c6\u9891\u6269\u6563\u6a21\u578b\u5728\u6709\u9650\u8bad\u7ec3\u65f6\u957f\u4e0b\u5b58\u5728\u8bad\u7ec3-\u6d4b\u8bd5\u5dee\u8ddd\uff0c\u5f53\u6d4b\u8bd5\u65f6\u957f\u8d85\u8fc7\u8bad\u7ec3\u65f6\u957f\u65f6\u4f1a\u51fa\u73b0\u89c6\u89c9\u9000\u5316\u3002\u7531\u4e8e\u5f00\u653e\u6d4b\u8bd5\u53ef\u80fd\u8d85\u8fc7\u4efb\u4f55\u6709\u9650\u8bad\u7ec3\u7a97\u53e3\uff0c\u4e14\u957f\u89c6\u9891\u8bad\u7ec3\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u9700\u8981\u5bfb\u627e\u65e0\u9700\u8bad\u7ec3\u7684\u89e3\u51b3\u65b9\u6848", "method": "\u901a\u8fc7\u7cfb\u7edf\u5206\u6790\u81ea\u56de\u5f52\u7f13\u5b58\u7ef4\u62a4\u673a\u5236\uff0c\u63d0\u51faRolling Sink\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u57fa\u4e8eSelf Forcing\uff08\u4ec5\u75285\u79d2\u7247\u6bb5\u8bad\u7ec3\uff09\uff0c\u5728\u6d4b\u8bd5\u65f6\u901a\u8fc7\u6709\u6548\u7684\u7f13\u5b58\u7ba1\u7406\u5c06\u89c6\u9891\u5408\u6210\u6269\u5c55\u5230\u8d85\u957f\u65f6\u57df", "result": "Rolling Sink\u80fd\u591f\u5c06\u89c6\u9891\u5408\u6210\u6269\u5c55\u52305-30\u5206\u949f\uff0816 FPS\uff09\uff0c\u4fdd\u6301\u4e00\u81f4\u7684\u7269\u4f53\u3001\u7a33\u5b9a\u7684\u989c\u8272\u3001\u8fde\u8d2f\u7684\u7ed3\u6784\u548c\u5e73\u6ed1\u7684\u8fd0\u52a8\u3002\u76f8\u6bd4\u73b0\u6709\u6700\u4f73\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5728\u957f\u65f6\u57df\u89c6\u89c9\u4fdd\u771f\u5ea6\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u65b9\u9762\u8868\u73b0\u66f4\u4f18", "conclusion": "Rolling Sink\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u81ea\u56de\u5f52\u89c6\u9891\u6269\u6563\u6a21\u578b\u5728\u8d85\u957f\u65f6\u57df\u6d4b\u8bd5\u4e2d\u7684\u8bad\u7ec3-\u6d4b\u8bd5\u5dee\u8ddd\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u8d85\u957f\u89c6\u9891\u5408\u6210"}}
{"id": "2602.08597", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08597", "abs": "https://arxiv.org/abs/2602.08597", "authors": ["Roland Bertin-Johannet", "Lara Scipio", "Leopold Mayti\u00e9", "Rufin VanRullen"], "title": "An Attention Mechanism for Robust Multimodal Integration in a Global Workspace Architecture", "comment": null, "summary": "Global Workspace Theory (GWT), inspired by cognitive neuroscience, posits that flexible cognition could arise via the attentional selection of a relevant subset of modalities within a multimodal integration system. This cognitive framework can inspire novel computational architectures for multimodal integration. Indeed, recent implementations of GWT have explored its multimodal representation capabilities, but the related attention mechanisms remain understudied. Here, we propose and evaluate a top-down attention mechanism to select modalities inside a global workspace. First, we demonstrate that our attention mechanism improves noise robustness of a global workspace system on two multimodal datasets of increasing complexity: Simple Shapes and MM-IMDb 1.0. Second, we highlight various cross-task and cross-modality generalization capabilities that are not shared by multimodal attention models from the literature. Comparing against existing baselines on the MM-IMDb 1.0 benchmark, we find our attention mechanism makes the global workspace competitive with the state of the art.", "AI": {"tldr": "\u63d0\u51fa\u5e76\u8bc4\u4f30\u4e86\u4e00\u79cd\u7528\u4e8e\u5168\u5c40\u5de5\u4f5c\u7a7a\u95f4\u7406\u8bba\u4e2d\u6a21\u6001\u9009\u62e9\u7684\u9876\u90e8\u6ce8\u610f\u673a\u5236\uff0c\u8be5\u673a\u5236\u63d0\u9ad8\u4e86\u7cfb\u7edf\u7684\u566a\u58f0\u9c81\u68d2\u6027\uff0c\u5e76\u5728\u8de8\u4efb\u52a1\u548c\u8de8\u6a21\u6001\u6cdb\u5316\u65b9\u9762\u8868\u73b0\u51fa\u8272", "motivation": "\u5168\u5c40\u5de5\u4f5c\u7a7a\u95f4\u7406\u8bba(GWT)\u4f5c\u4e3a\u8ba4\u77e5\u795e\u7ecf\u79d1\u5b66\u542f\u53d1\u7684\u6846\u67b6\uff0c\u4e3a\u591a\u6a21\u6001\u6574\u5408\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u4f46\u73b0\u6709\u5b9e\u73b0\u4e2d\u76f8\u5173\u7684\u6ce8\u610f\u673a\u5236\u7814\u7a76\u4e0d\u8db3\uff0c\u9700\u8981\u5f00\u53d1\u6709\u6548\u7684\u9876\u90e8\u6ce8\u610f\u673a\u5236\u6765\u63d0\u5347\u7cfb\u7edf\u6027\u80fd", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9876\u90e8\u6ce8\u610f\u673a\u5236\u6765\u9009\u62e9\u5168\u5c40\u5de5\u4f5c\u7a7a\u95f4\u4e2d\u7684\u6a21\u6001\uff0c\u5728\u4e24\u4e2a\u590d\u6742\u5ea6\u9012\u589e\u7684\u591a\u6a21\u6001\u6570\u636e\u96c6(Simple Shapes\u548cMM-IMDb 1.0)\u4e0a\u8fdb\u884c\u8bc4\u4f30\uff0c\u5e76\u4e0e\u73b0\u6709\u591a\u6a21\u6001\u6ce8\u610f\u6a21\u578b\u8fdb\u884c\u6bd4\u8f83", "result": "\u8be5\u6ce8\u610f\u673a\u5236\u63d0\u9ad8\u4e86\u5168\u5c40\u5de5\u4f5c\u7a7a\u95f4\u7cfb\u7edf\u7684\u566a\u58f0\u9c81\u68d2\u6027\uff0c\u5c55\u793a\u4e86\u6587\u732e\u4e2d\u591a\u6a21\u6001\u6ce8\u610f\u6a21\u578b\u4e0d\u5177\u5907\u7684\u8de8\u4efb\u52a1\u548c\u8de8\u6a21\u6001\u6cdb\u5316\u80fd\u529b\uff0c\u5728MM-IMDb 1.0\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f7f\u5168\u5c40\u5de5\u4f5c\u7a7a\u95f4\u8fbe\u5230\u4e0e\u6700\u5148\u8fdb\u65b9\u6cd5\u7ade\u4e89\u7684\u6c34\u5e73", "conclusion": "\u63d0\u51fa\u7684\u9876\u90e8\u6ce8\u610f\u673a\u5236\u6709\u6548\u589e\u5f3a\u4e86\u5168\u5c40\u5de5\u4f5c\u7a7a\u95f4\u7406\u8bba\u5728\u591a\u6a21\u6001\u6574\u5408\u4e2d\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u566a\u58f0\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u65b9\u9762\uff0c\u4e3a\u8ba4\u77e5\u542f\u53d1\u7684\u8ba1\u7b97\u67b6\u6784\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u5b9e\u73b0\u65b9\u6848"}}
{"id": "2602.08603", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08603", "abs": "https://arxiv.org/abs/2602.08603", "authors": ["Teng Wang", "Rong Shan", "Jianghao Lin", "Junjie Wu", "Tianyi Xu", "Jianping Zhang", "Wenteng Chen", "Changwang Zhang", "Zhaoxiang Wang", "Weinan Zhang", "Jun Wang"], "title": "OSCAR: Optimization-Steered Agentic Planning for Composed Image Retrieval", "comment": null, "summary": "Composed image retrieval (CIR) requires complex reasoning over heterogeneous visual and textual constraints. Existing approaches largely fall into two paradigms: unified embedding retrieval, which suffers from single-model myopia, and heuristic agentic retrieval, which is limited by suboptimal, trial-and-error orchestration. To this end, we propose OSCAR, an optimization-steered agentic planning framework for composed image retrieval. We are the first to reformulate agentic CIR from a heuristic search process into a principled trajectory optimization problem. Instead of relying on heuristic trial-and-error exploration, OSCAR employs a novel offline-online paradigm. In the offline phase, we model CIR via atomic retrieval selection and composition as a two-stage mixed-integer programming problem, mathematically deriving optimal trajectories that maximize ground-truth coverage for training samples via rigorous boolean set operations. These trajectories are then stored in a golden library to serve as in-context demonstrations for online steering of VLM planner at online inference time. Extensive experiments on three public benchmarks and a private industrial benchmark show that OSCAR consistently outperforms SOTA baselines. Notably, it achieves superior performance using only 10% of training data, demonstrating strong generalization of planning logic rather than dataset-specific memorization.", "AI": {"tldr": "OSCAR\u662f\u4e00\u4e2a\u57fa\u4e8e\u4f18\u5316\u5f15\u5bfc\u7684\u667a\u80fd\u4f53\u89c4\u5212\u6846\u67b6\uff0c\u7528\u4e8e\u7ec4\u5408\u56fe\u50cf\u68c0\u7d22\uff0c\u5c06\u542f\u53d1\u5f0f\u641c\u7d22\u8fc7\u7a0b\u8f6c\u5316\u4e3a\u8f68\u8ff9\u4f18\u5316\u95ee\u9898\uff0c\u901a\u8fc7\u79bb\u7ebf-\u5728\u7ebf\u8303\u5f0f\u5b9e\u73b0\u66f4\u4f18\u7684\u68c0\u7d22\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7ec4\u5408\u56fe\u50cf\u68c0\u7d22\u65b9\u6cd5\u5b58\u5728\u4e24\u5927\u95ee\u9898\uff1a\u7edf\u4e00\u5d4c\u5165\u68c0\u7d22\u5b58\u5728\u5355\u6a21\u578b\u8fd1\u89c6\u95ee\u9898\uff0c\u800c\u542f\u53d1\u5f0f\u667a\u80fd\u4f53\u68c0\u7d22\u5219\u53d7\u9650\u4e8e\u6b21\u4f18\u7684\u8bd5\u9519\u7f16\u6392\u3002\u9700\u8981\u4e00\u79cd\u66f4\u7cfb\u7edf\u5316\u7684\u65b9\u6cd5\u6765\u5904\u7406\u5f02\u6784\u89c6\u89c9\u548c\u6587\u672c\u7ea6\u675f\u7684\u590d\u6742\u63a8\u7406\u3002", "method": "\u63d0\u51faOSCAR\u6846\u67b6\uff0c\u91c7\u7528\u79bb\u7ebf-\u5728\u7ebf\u8303\u5f0f\uff1a1) \u79bb\u7ebf\u9636\u6bb5\u5c06CIR\u5efa\u6a21\u4e3a\u4e24\u9636\u6bb5\u6df7\u5408\u6574\u6570\u89c4\u5212\u95ee\u9898\uff0c\u901a\u8fc7\u5e03\u5c14\u96c6\u5408\u8fd0\u7b97\u63a8\u5bfc\u6700\u5927\u5316\u771f\u5b9e\u8986\u76d6\u7684\u6700\u4f18\u8f68\u8ff9\uff1b2) \u5c06\u6700\u4f18\u8f68\u8ff9\u5b58\u50a8\u5728\u9ec4\u91d1\u5e93\u4e2d\uff0c\u4f5c\u4e3a\u5728\u7ebf\u63a8\u7406\u65f6VLM\u89c4\u5212\u5668\u7684\u4e0a\u4e0b\u6587\u6f14\u793a\uff1b3) \u5728\u7ebf\u9636\u6bb5\u4f7f\u7528\u8fd9\u4e9b\u6f14\u793a\u6765\u5f15\u5bfc\u89c4\u5212\u5668\u3002", "result": "\u5728\u4e09\u4e2a\u516c\u5171\u57fa\u51c6\u548c\u4e00\u4e2a\u79c1\u6709\u5de5\u4e1a\u57fa\u51c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cOSCAR\u59cb\u7ec8\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002\u7279\u522b\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u4ec5\u4f7f\u752810%\u7684\u8bad\u7ec3\u6570\u636e\u5c31\u80fd\u8fbe\u5230\u4f18\u8d8a\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u89c4\u5212\u903b\u8f91\u7684\u5f3a\u6cdb\u5316\u80fd\u529b\u800c\u975e\u6570\u636e\u96c6\u7279\u5b9a\u7684\u8bb0\u5fc6\u3002", "conclusion": "OSCAR\u6210\u529f\u5c06\u7ec4\u5408\u56fe\u50cf\u68c0\u7d22\u4ece\u542f\u53d1\u5f0f\u641c\u7d22\u8fc7\u7a0b\u8f6c\u5316\u4e3a\u539f\u5219\u6027\u7684\u8f68\u8ff9\u4f18\u5316\u95ee\u9898\uff0c\u901a\u8fc7\u79bb\u7ebf\u63a8\u5bfc\u6700\u4f18\u8f68\u8ff9\u5e76\u5728\u5728\u7ebf\u63a8\u7406\u4e2d\u5f15\u5bfc\u89c4\u5212\u5668\uff0c\u5b9e\u73b0\u4e86\u66f4\u4f18\u7684\u68c0\u7d22\u6027\u80fd\u548c\u66f4\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2602.08630", "categories": ["cs.AI", "cs.CC"], "pdf": "https://arxiv.org/pdf/2602.08630", "abs": "https://arxiv.org/abs/2602.08630", "authors": ["Jonah Brown-Cohen", "Geoffrey Irving", "Simon C. Marshall", "Ilan Newman", "Georgios Piliouras", "Mario Szegedy"], "title": "Debate is efficient with your time", "comment": "11 Pages, 0 figures", "summary": "AI safety via debate uses two competing models to help a human judge verify complex computational tasks. Previous work has established what problems debate can solve in principle, but has not analysed the practical cost of human oversight: how many queries must the judge make to the debate transcript? We introduce Debate Query Complexity}(DQC), the minimum number of bits a verifier must inspect to correctly decide a debate.\n  Surprisingly, we find that PSPACE/poly (the class of problems which debate can efficiently decide) is precisely the class of functions decidable with O(log n) queries. This characterisation shows that debate is remarkably query-efficient: even for highly complex problems, logarithmic oversight suffices. We also establish that functions depending on all their input bits require Omega(log n) queries, and that any function computable by a circuit of size s satisfies DQC(f) <= log(s) + 3. Interestingly, this last result implies that proving DQC lower bounds of log(n) + 6 for languages in P would yield new circuit lower bounds, connecting debate query complexity to central questions in circuit complexity.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86AI\u5b89\u5168\u8fa9\u8bba\u4e2d\u4eba\u7c7b\u76d1\u7763\u7684\u67e5\u8be2\u6210\u672c\uff0c\u5f15\u5165\u4e86\u8fa9\u8bba\u67e5\u8be2\u590d\u6742\u5ea6(DQC)\u6982\u5ff5\uff0c\u53d1\u73b0PSPACE/poly\u95ee\u9898\u7c7b\u6070\u597d\u662fO(log n)\u67e5\u8be2\u53ef\u5224\u5b9a\u7684\u51fd\u6570\u7c7b\uff0c\u8868\u660e\u8fa9\u8bba\u5177\u6709\u6781\u9ad8\u7684\u67e5\u8be2\u6548\u7387\u3002", "motivation": "AI\u5b89\u5168\u8fa9\u8bba\u4f7f\u7528\u4e24\u4e2a\u7ade\u4e89\u6a21\u578b\u5e2e\u52a9\u4eba\u7c7b\u5224\u65ad\u9a8c\u8bc1\u590d\u6742\u8ba1\u7b97\u4efb\u52a1\u3002\u5148\u524d\u5de5\u4f5c\u5efa\u7acb\u4e86\u8fa9\u8bba\u5728\u7406\u8bba\u4e0a\u80fd\u89e3\u51b3\u4ec0\u4e48\u95ee\u9898\uff0c\u4f46\u672a\u5206\u6790\u4eba\u7c7b\u76d1\u7763\u7684\u5b9e\u9645\u6210\u672c\uff1a\u5224\u65ad\u9700\u8981\u68c0\u67e5\u8fa9\u8bba\u8bb0\u5f55\u591a\u5c11\u6b21\u67e5\u8be2\uff1f", "method": "\u5f15\u5165\u8fa9\u8bba\u67e5\u8be2\u590d\u6742\u5ea6(DQC)\uff0c\u5373\u9a8c\u8bc1\u8005\u4e3a\u4e86\u6b63\u786e\u51b3\u5b9a\u8fa9\u8bba\u5fc5\u987b\u68c0\u67e5\u7684\u6700\u5c0f\u6bd4\u7279\u6570\u3002\u5206\u6790\u4e0d\u540c\u590d\u6742\u5ea6\u7c7b\u522b\u7684DQC\u8fb9\u754c\uff0c\u5efa\u7acb\u67e5\u8be2\u590d\u6742\u5ea6\u4e0e\u7535\u8def\u590d\u6742\u5ea6\u7684\u8054\u7cfb\u3002", "result": "\u53d1\u73b0PSPACE/poly\uff08\u8fa9\u8bba\u80fd\u9ad8\u6548\u5224\u5b9a\u7684\u95ee\u9898\u7c7b\uff09\u6070\u597d\u662fO(log n)\u67e5\u8be2\u53ef\u5224\u5b9a\u7684\u51fd\u6570\u7c7b\u3002\u8bc1\u660e\u4f9d\u8d56\u6240\u6709\u8f93\u5165\u6bd4\u7279\u7684\u51fd\u6570\u9700\u8981\u03a9(log n)\u67e5\u8be2\uff0c\u4efb\u4f55\u53ef\u7531\u5927\u5c0f\u4e3as\u7684\u7535\u8def\u8ba1\u7b97\u7684\u51fd\u6570\u6ee1\u8db3DQC(f) \u2264 log(s) + 3\u3002", "conclusion": "\u8fa9\u8bba\u5177\u6709\u60ca\u4eba\u7684\u67e5\u8be2\u6548\u7387\uff1a\u5373\u4f7f\u5bf9\u4e8e\u9ad8\u5ea6\u590d\u6742\u7684\u95ee\u9898\uff0c\u5bf9\u6570\u7ea7\u76d1\u7763\u5c31\u8db3\u591f\u4e86\u3002\u8bc1\u660eDQC\u4e0b\u754c\u4e0e\u7535\u8def\u590d\u6742\u5ea6\u4e2d\u5fc3\u95ee\u9898\u76f8\u5173\uff0c\u4e3aAI\u5b89\u5168\u8fa9\u8bba\u7684\u5b9e\u9645\u53ef\u884c\u6027\u63d0\u4f9b\u4e86\u7406\u8bba\u4fdd\u8bc1\u3002"}}
{"id": "2602.07814", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07814", "abs": "https://arxiv.org/abs/2602.07814", "authors": ["Simiao Ren", "Yuchen Zhou", "Xingyu Shen", "Kidus Zewde", "Tommy Duong", "George Huang", "Hatsanai", "Tiangratanakul", "Tsang", "Ng", "En Wei", "Jiayu Xue"], "title": "How well are open sourced AI-generated image detection models out-of-the-box: A comprehensive benchmark study", "comment": null, "summary": "As AI-generated images proliferate across digital platforms, reliable detection methods have become critical for combating misinformation and maintaining content authenticity. While numerous deepfake detection methods have been proposed, existing benchmarks predominantly evaluate fine-tuned models, leaving a critical gap in understanding out-of-the-box performance -- the most common deployment scenario for practitioners. We present the first comprehensive zero-shot evaluation of 16 state-of-the-art detection methods, comprising 23 pretrained detector variants (due to multiple released versions of certain detectors), across 12 diverse datasets, comprising 2.6~million image samples spanning 291 unique generators including modern diffusion models. Our systematic analysis reveals striking findings: (1)~no universal winner exists, with detector rankings exhibiting substantial instability (Spearman~$\u03c1$: 0.01 -- 0.87 across dataset pairs); (2)~a 37~percentage-point performance gap separates the best detector (75.0\\% mean accuracy) from the worst (37.5\\%); (3)~training data alignment critically impacts generalization, causing up to 20--60\\% performance variance within architecturally identical detector families; (4)~modern commercial generators (Flux~Dev, Firefly~v4, Midjourney~v7) defeat most detectors, achieving only 18--30\\% average accuracy; and (5)~we identify three systematic failure patterns affecting cross-dataset generalization. Statistical analysis confirms significant performance differences between detectors (Friedman test: $\u03c7^2$=121.01, $p<10^{-16}$, Kendall~$W$=0.524). Our findings challenge the ``one-size-fits-all'' detector paradigm and provide actionable deployment guidelines, demonstrating that practitioners must carefully select detectors based on their specific threat landscape rather than relying on published benchmark performance.", "AI": {"tldr": "\u8be5\u7814\u7a76\u9996\u6b21\u5bf916\u79cd\u6700\u5148\u8fdb\u7684AI\u751f\u6210\u56fe\u50cf\u68c0\u6d4b\u65b9\u6cd5\u8fdb\u884c\u4e86\u5168\u9762\u7684\u96f6\u6837\u672c\u8bc4\u4f30\uff0c\u8986\u76d623\u4e2a\u9884\u8bad\u7ec3\u68c0\u6d4b\u5668\u53d8\u4f53\u548c12\u4e2a\u6570\u636e\u96c6\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u68c0\u6d4b\u5668\u5728\u771f\u5b9e\u90e8\u7f72\u573a\u666f\u4e2d\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u968f\u7740AI\u751f\u6210\u56fe\u50cf\u5728\u6570\u5b57\u5e73\u53f0\u4e0a\u7684\u6fc0\u589e\uff0c\u53ef\u9760\u7684\u68c0\u6d4b\u65b9\u6cd5\u5bf9\u4e8e\u6253\u51fb\u865a\u5047\u4fe1\u606f\u548c\u7ef4\u62a4\u5185\u5bb9\u771f\u5b9e\u6027\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u57fa\u51c6\u4e3b\u8981\u8bc4\u4f30\u5fae\u8c03\u6a21\u578b\uff0c\u5ffd\u7565\u4e86\u96f6\u6837\u672c\u6027\u80fd\u8fd9\u4e00\u6700\u5e38\u89c1\u7684\u5b9e\u9645\u90e8\u7f72\u573a\u666f\uff0c\u5b58\u5728\u91cd\u8981\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u5bf916\u79cd\u6700\u5148\u8fdb\u7684\u68c0\u6d4b\u65b9\u6cd5\uff08\u5305\u542b23\u4e2a\u9884\u8bad\u7ec3\u68c0\u6d4b\u5668\u53d8\u4f53\uff09\u572812\u4e2a\u591a\u6837\u5316\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u96f6\u6837\u672c\u8bc4\u4f30\uff0c\u6db5\u76d6260\u4e07\u5f20\u56fe\u50cf\u6837\u672c\u548c291\u4e2a\u4e0d\u540c\u7684\u751f\u6210\u5668\uff0c\u5305\u62ec\u73b0\u4ee3\u6269\u6563\u6a21\u578b\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff1a(1)\u4e0d\u5b58\u5728\u901a\u7528\u6700\u4f73\u68c0\u6d4b\u5668\uff0c\u68c0\u6d4b\u5668\u6392\u540d\u5728\u4e0d\u540c\u6570\u636e\u96c6\u95f4\u6781\u4e0d\u7a33\u5b9a\uff1b(2)\u6700\u4f73\u68c0\u6d4b\u5668\uff08\u5e73\u5747\u51c6\u786e\u738775.0%\uff09\u4e0e\u6700\u5dee\u68c0\u6d4b\u5668\uff0837.5%\uff09\u5b58\u572837\u4e2a\u767e\u5206\u70b9\u7684\u6027\u80fd\u5dee\u8ddd\uff1b(3)\u8bad\u7ec3\u6570\u636e\u5bf9\u9f50\u5bf9\u6cdb\u5316\u80fd\u529b\u5f71\u54cd\u663e\u8457\uff0c\u5bfc\u81f4\u76f8\u540c\u67b6\u6784\u68c0\u6d4b\u5668\u5bb6\u65cf\u518520-60%\u7684\u6027\u80fd\u5dee\u5f02\uff1b(4)\u73b0\u4ee3\u5546\u4e1a\u751f\u6210\u5668\uff08Flux Dev\u3001Firefly v4\u3001Midjourney v7\uff09\u80fd\u51fb\u8d25\u5927\u591a\u6570\u68c0\u6d4b\u5668\uff0c\u5e73\u5747\u51c6\u786e\u7387\u4ec518-30%\uff1b(5)\u8bc6\u522b\u51fa\u4e09\u79cd\u5f71\u54cd\u8de8\u6570\u636e\u96c6\u6cdb\u5316\u7684\u7cfb\u7edf\u6027\u5931\u8d25\u6a21\u5f0f\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u6311\u6218\u4e86\"\u4e00\u5200\u5207\"\u7684\u68c0\u6d4b\u5668\u8303\u5f0f\uff0c\u8868\u660e\u4ece\u4e1a\u8005\u5fc5\u987b\u6839\u636e\u5177\u4f53\u5a01\u80c1\u73af\u5883\u4ed4\u7ec6\u9009\u62e9\u68c0\u6d4b\u5668\uff0c\u800c\u4e0d\u80fd\u4f9d\u8d56\u5df2\u53d1\u5e03\u7684\u57fa\u51c6\u6027\u80fd\u3002\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u90e8\u7f72\u6307\u5357\u3002"}}
{"id": "2602.08951", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.08951", "abs": "https://arxiv.org/abs/2602.08951", "authors": ["Rasul Dent", "Pedro Ortiz Suarez", "Thibault Cl\u00e9rice", "Beno\u00eet Sagot"], "title": "How Should We Model the Probability of a Language?", "comment": "Accepted for Vardial 2026", "summary": "Of the over 7,000 languages spoken in the world, commercial language identification (LID) systems only reliably identify a few hundred in written form. Research-grade systems extend this coverage under certain circumstances, but for most languages coverage remains patchy or nonexistent. This position paper argues that this situation is largely self-imposed. In particular, it arises from a persistent framing of LID as decontextualized text classification, which obscures the central role of prior probability estimation and is reinforced by institutional incentives that favor global, fixed-prior models. We argue that improving coverage for tail languages requires rethinking LID as a routing problem and developing principled ways to incorporate environmental cues that make languages locally plausible.", "AI": {"tldr": "\u8bba\u6587\u8ba4\u4e3a\u5f53\u524d\u8bed\u8a00\u8bc6\u522b\u7cfb\u7edf\u8986\u76d6\u8303\u56f4\u6709\u9650\u7684\u95ee\u9898\u4e3b\u8981\u662f\u81ea\u627e\u7684\uff0c\u6e90\u4e8e\u5c06LID\u89c6\u4e3a\u53bb\u8bed\u5883\u5316\u7684\u6587\u672c\u5206\u7c7b\uff0c\u5ffd\u89c6\u4e86\u5148\u9a8c\u6982\u7387\u4f30\u8ba1\u7684\u91cd\u8981\u6027\uff0c\u9700\u8981\u91cd\u65b0\u5c06LID\u5b9a\u4e49\u4e3a\u8def\u7531\u95ee\u9898\u5e76\u6574\u5408\u73af\u5883\u7ebf\u7d22\u3002", "motivation": "\u5546\u4e1a\u8bed\u8a00\u8bc6\u522b\u7cfb\u7edf\u4ec5\u80fd\u53ef\u9760\u8bc6\u522b\u51e0\u767e\u79cd\u4e66\u9762\u8bed\u8a00\uff0c\u7814\u7a76\u7ea7\u7cfb\u7edf\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u6269\u5c55\u4e86\u8986\u76d6\u8303\u56f4\uff0c\u4f46\u5bf9\u5927\u591a\u6570\u8bed\u8a00\u6765\u8bf4\u8986\u76d6\u4ecd\u7136\u96f6\u6563\u6216\u4e0d\u5b58\u5728\u3002\u8fd9\u79cd\u72b6\u51b5\u4e3b\u8981\u662f\u7531\u4e8e\u5c06LID\u89c6\u4e3a\u53bb\u8bed\u5883\u5316\u7684\u6587\u672c\u5206\u7c7b\u7684\u6846\u67b6\u5bfc\u81f4\u7684\u3002", "method": "\u8fd9\u662f\u4e00\u7bc7\u7acb\u573a\u8bba\u6587\uff0c\u4e3b\u5f20\u91cd\u65b0\u601d\u8003\u8bed\u8a00\u8bc6\u522b\u95ee\u9898\u3002\u5efa\u8bae\u5c06LID\u91cd\u65b0\u5b9a\u4e49\u4e3a\u8def\u7531\u95ee\u9898\uff0c\u5e76\u5f00\u53d1\u7cfb\u7edf\u6027\u7684\u65b9\u6cd5\u6765\u6574\u5408\u73af\u5883\u7ebf\u7d22\uff0c\u4f7f\u8bed\u8a00\u5728\u5c40\u90e8\u73af\u5883\u4e2d\u53d8\u5f97\u5408\u7406\u3002", "result": "\u8bba\u6587\u6ca1\u6709\u63d0\u4f9b\u5177\u4f53\u7684\u5b9e\u9a8c\u7ed3\u679c\uff0c\u800c\u662f\u63d0\u51fa\u4e86\u7406\u8bba\u6846\u67b6\u548c\u5206\u6790\u3002\u6307\u51fa\u5f53\u524dLID\u65b9\u6cd5\u7684\u95ee\u9898\u6839\u6e90\u5728\u4e8e\u53bb\u8bed\u5883\u5316\u7684\u6587\u672c\u5206\u7c7b\u6846\u67b6\u548c\u673a\u6784\u6fc0\u52b1\u673a\u5236\u504f\u5411\u4e8e\u5168\u5c40\u3001\u56fa\u5b9a\u5148\u9a8c\u7684\u6a21\u578b\u3002", "conclusion": "\u8981\u63d0\u9ad8\u5c3e\u90e8\u8bed\u8a00\u7684\u8986\u76d6\u8303\u56f4\uff0c\u9700\u8981\u91cd\u65b0\u5c06LID\u89c6\u4e3a\u8def\u7531\u95ee\u9898\uff0c\u5e76\u5f00\u53d1\u539f\u5219\u6027\u7684\u65b9\u6cd5\u6765\u6574\u5408\u73af\u5883\u7ebf\u7d22\uff0c\u8fd9\u4e9b\u7ebf\u7d22\u80fd\u4f7f\u8bed\u8a00\u5728\u5c40\u90e8\u73af\u5883\u4e2d\u53d8\u5f97\u5408\u7406\u3002\u8fd9\u9700\u8981\u6539\u53d8\u5f53\u524d\u7684\u7814\u7a76\u8303\u5f0f\u548c\u65b9\u6cd5\u8bba\u3002"}}
{"id": "2602.07815", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.07815", "abs": "https://arxiv.org/abs/2602.07815", "authors": ["Simiao Ren"], "title": "Out of the box age estimation through facial imagery: A Comprehensive Benchmark of Vision-Language Models vs. out-of-the-box Traditional Architectures", "comment": null, "summary": "Facial age estimation is critical for content moderation, age verification, and deepfake detection, yet no prior benchmark has systematically compared modern vision-language models (VLMs) against specialized age estimation architectures. We present the first large-scale cross-paradigm benchmark, evaluating \\textbf{34 models} -- 22 specialized architectures with publicly available pretrained weights and 12 general-purpose VLMs -- across \\textbf{8 standard datasets} (UTKFace, IMDB-WIKI, MORPH, AFAD, CACD, FG-NET, APPA-REAL, AgeDB) totaling 1{,}100 test images per model. Our key finding is striking: \\emph{zero-shot VLMs significantly outperform most specialized models}, achieving an average MAE of 5.65 years compared to 9.88 for non-LLM models. The best VLM (Gemini~3 Flash Preview, MAE~4.32) outperforms the best non-LLM model (MiVOLO, MAE~5.10) by 15\\%. Only MiVOLO, which uniquely combines face and body features via Vision Transformers, competes with VLMs. We further analyze age verification at the 18-year threshold, revealing that non-LLM models exhibit 60--100\\% false adult rates on minors while VLMs achieve 13--25\\%, and demonstrate that coarse age binning (8--9 classes) consistently degrades MAE beyond 13 years. Our stratified analysis across 14 age groups reveals that all models struggle most at extreme ages ($<$5 and 65+). These findings challenge the assumption that task-specific architectures are necessary for age estimation and suggest that the field should redirect toward distilling VLM capabilities into efficient specialized models.", "AI": {"tldr": "VLMs\u5728\u9762\u90e8\u5e74\u9f84\u4f30\u8ba1\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u4e13\u7528\u6a21\u578b\uff0c\u5e73\u5747MAE\u4e3a5.65\u5e74vs 9.88\u5e74\uff0c\u6700\u4f73VLM\u6bd4\u6700\u4f73\u4e13\u7528\u6a21\u578b\u6027\u80fd\u63d0\u534715%\uff0c\u6311\u6218\u4e86\u4efb\u52a1\u4e13\u7528\u67b6\u6784\u7684\u5fc5\u8981\u6027\u5047\u8bbe\u3002", "motivation": "\u9762\u90e8\u5e74\u9f84\u4f30\u8ba1\u5728\u5185\u5bb9\u5ba1\u6838\u3001\u5e74\u9f84\u9a8c\u8bc1\u548c\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u6b64\u524d\u7f3a\u4e4f\u5bf9\u73b0\u4ee3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e0e\u4e13\u7528\u5e74\u9f84\u4f30\u8ba1\u67b6\u6784\u7684\u7cfb\u7edf\u6027\u6bd4\u8f83\u3002", "method": "\u521b\u5efa\u9996\u4e2a\u5927\u89c4\u6a21\u8de8\u8303\u5f0f\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8bc4\u4f3034\u4e2a\u6a21\u578b\uff0822\u4e2a\u4e13\u7528\u67b6\u6784\u548c12\u4e2a\u901a\u7528VLMs\uff09\uff0c\u57288\u4e2a\u6807\u51c6\u6570\u636e\u96c6\u4e0a\u4f7f\u7528\u603b\u8ba11,100\u5f20\u6d4b\u8bd5\u56fe\u50cf\uff0c\u8fdb\u884cMAE\u3001\u5e74\u9f84\u9a8c\u8bc1\u51c6\u786e\u7387\u7b49\u591a\u7ef4\u5ea6\u5206\u6790\u3002", "result": "\u96f6\u6837\u672cVLMs\u663e\u8457\u4f18\u4e8e\u5927\u591a\u6570\u4e13\u7528\u6a21\u578b\uff08\u5e73\u5747MAE 5.65\u5e74 vs 9.88\u5e74\uff09\uff0c\u6700\u4f73VLM\uff08Gemini 3 Flash Preview\uff0cMAE 4.32\uff09\u6bd4\u6700\u4f73\u975eLLM\u6a21\u578b\uff08MiVOLO\uff0cMAE 5.10\uff09\u63d0\u534715%\u3002VLMs\u572818\u5c81\u9608\u503c\u5e74\u9f84\u9a8c\u8bc1\u4e2d\u9519\u8bef\u7387\u66f4\u4f4e\uff0813-25% vs 60-100%\uff09\u3002", "conclusion": "\u7814\u7a76\u6311\u6218\u4e86\u4efb\u52a1\u4e13\u7528\u67b6\u6784\u5bf9\u5e74\u9f84\u4f30\u8ba1\u5fc5\u8981\u7684\u5047\u8bbe\uff0c\u5efa\u8bae\u9886\u57df\u5e94\u8f6c\u5411\u5c06VLM\u80fd\u529b\u84b8\u998f\u5230\u9ad8\u6548\u4e13\u7528\u6a21\u578b\u4e2d\uff0c\u800c\u975e\u7ee7\u7eed\u5f00\u53d1\u4f20\u7edf\u4e13\u7528\u67b6\u6784\u3002"}}
{"id": "2602.08708", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08708", "abs": "https://arxiv.org/abs/2602.08708", "authors": ["Stefan Edelkamp", "Ji\u0159\u00ed Fink", "Petr Gregor", "Anders Jonsson", "Bernhard Nebel"], "title": "Intermediate Results on the Complexity of STRIPS$_{1}^{1}$", "comment": null, "summary": "This paper is based on Bylander's results on the computational complexity of propositional STRIPS planning. He showed that when only ground literals are permitted, determining plan existence is PSPACE-complete even if operators are limited to two preconditions and two postconditions. While NP-hardness is settled, it is unknown whether propositional STRIPS with operators that only have one precondition and one effect is NP-complete. We shed light on the question whether this small solution hypothesis for STRIPS$^1_1$ is true, calling a SAT solver for small instances, introducing the literal graph, and mapping it to Petri nets.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86STRIPS\u89c4\u5212\u4e2d\u64cd\u4f5c\u7b26\u9650\u5236\u4e3a1\u4e2a\u524d\u63d0\u6761\u4ef6\u548c1\u4e2a\u6548\u679c\u65f6\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u95ee\u9898\uff0c\u63a2\u8ba8\u8fd9\u79cd\u7b80\u5316\u7248\u672c\u662f\u5426NP\u5b8c\u5168", "motivation": "\u57fa\u4e8eBylander\u5173\u4e8e\u547d\u9898STRIPS\u89c4\u5212\u8ba1\u7b97\u590d\u6742\u5ea6\u7684\u7814\u7a76\u7ed3\u679c\uff0c\u867d\u7136\u5df2\u77e5\u5f53\u64cd\u4f5c\u7b26\u9650\u5236\u4e3a2\u4e2a\u524d\u63d0\u6761\u4ef6\u548c2\u4e2a\u540e\u7f6e\u6761\u4ef6\u65f6\uff0c\u89c4\u5212\u5b58\u5728\u6027\u5224\u5b9a\u662fPSPACE\u5b8c\u5168\u7684\uff0c\u4f46\u5bf9\u4e8e\u64cd\u4f5c\u7b26\u53ea\u67091\u4e2a\u524d\u63d0\u6761\u4ef6\u548c1\u4e2a\u6548\u679c\u7684\u60c5\u51b5\uff0c\u662f\u5426NP\u5b8c\u5168\u4ecd\u662f\u672a\u77e5\u95ee\u9898", "method": "\u901a\u8fc7\u8c03\u7528SAT\u6c42\u89e3\u5668\u5904\u7406\u5c0f\u89c4\u6a21\u5b9e\u4f8b\uff0c\u5f15\u5165\u6587\u5b57\u56fe\u6982\u5ff5\uff0c\u5e76\u5c06\u5176\u6620\u5c04\u5230Petri\u7f51\u8fdb\u884c\u5206\u6790", "result": "\u8bba\u6587\u4e3aSTRIPS$^1_1$\u7684\u5c0f\u89e3\u5047\u8bbe\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u7684\u89c1\u89e3\u548c\u8bc1\u636e", "conclusion": "\u8be5\u7814\u7a76\u6709\u52a9\u4e8e\u6f84\u6e05\u547d\u9898STRIPS\u89c4\u5212\u5728\u6781\u7aef\u7b80\u5316\u60c5\u51b5\u4e0b\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u8fb9\u754c\u95ee\u9898"}}
{"id": "2602.08984", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08984", "abs": "https://arxiv.org/abs/2602.08984", "authors": ["Yuliang Liu", "Yunchong Song", "Yixuan Wang", "Kewen Ge", "Alex Lamb", "Qipeng Guo", "Kai Chen", "Bowen Zhou", "Zhouhan Lin"], "title": "Next Concept Prediction in Discrete Latent Space Leads to Stronger Language Models", "comment": null, "summary": "We propose Next Concept Prediction (NCP), a generative pretraining paradigm built on top of Next Token Prediction (NTP). NCP predicts discrete concepts that span multiple tokens, thereby forming a more challenging pretraining objective. Our model, ConceptLM, quantizes hidden states using Vector Quantization and constructs a concept vocabulary. It leverages both NCP and NTP to drive parameter updates and generates a concept to guide the generation of the following tokens. We train ConceptLM from scratch at scales ranging from 70M to 1.5B parameters with up to 300B training data, including Pythia and GPT-2 backbones. Results on 13 benchmarks show that NCP yields consistent performance gains over traditional token-level models. Furthermore, continual pretraining experiments on an 8B-parameter Llama model indicate that NCP can further improve an NTP-trained model. Our analysis suggests that NCP leads to more powerful language models by introducing a harder pretraining task, providing a promising path toward better language modeling.", "AI": {"tldr": "\u63d0\u51faNext Concept Prediction (NCP)\u751f\u6210\u5f0f\u9884\u8bad\u7ec3\u8303\u5f0f\uff0c\u901a\u8fc7\u9884\u6d4b\u8de8\u591a\u4e2atoken\u7684\u79bb\u6563\u6982\u5ff5\u6765\u6784\u5efa\u66f4\u5177\u6311\u6218\u6027\u7684\u9884\u8bad\u7ec3\u76ee\u6807\uff0c\u76f8\u6bd4\u4f20\u7edftoken\u7ea7\u6a21\u578b\u83b7\u5f97\u4e00\u81f4\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u4f20\u7edfNext Token Prediction (NTP)\u5728token\u7ea7\u522b\u8fdb\u884c\u9884\u6d4b\uff0c\u800cNCP\u65e8\u5728\u6784\u5efa\u66f4\u9ad8\u7ea7\u522b\u7684\u6982\u5ff5\u9884\u6d4b\u4efb\u52a1\uff0c\u901a\u8fc7\u5f15\u5165\u66f4\u96be\u7684\u9884\u8bad\u7ec3\u76ee\u6807\u6765\u63d0\u5347\u8bed\u8a00\u6a21\u578b\u7684\u80fd\u529b\u3002", "method": "\u63d0\u51faConceptLM\u6a21\u578b\uff0c\u4f7f\u7528Vector Quantization\u5bf9\u9690\u85cf\u72b6\u6001\u8fdb\u884c\u91cf\u5316\uff0c\u6784\u5efa\u6982\u5ff5\u8bcd\u6c47\u8868\u3002\u6a21\u578b\u540c\u65f6\u5229\u7528NCP\u548cNTP\u9a71\u52a8\u53c2\u6570\u66f4\u65b0\uff0c\u751f\u6210\u6982\u5ff5\u6765\u6307\u5bfc\u540e\u7eedtoken\u7684\u751f\u6210\u3002\u5728\u4e0d\u540c\u89c4\u6a21\uff0870M-1.5B\u53c2\u6570\uff09\u4e0a\u4ece\u5934\u8bad\u7ec3\uff0c\u5305\u62ecPythia\u548cGPT-2\u67b6\u6784\u3002", "result": "\u572813\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\uff0cNCP\u76f8\u6bd4\u4f20\u7edftoken\u7ea7\u6a21\u578b\u83b7\u5f97\u4e00\u81f4\u6027\u80fd\u63d0\u5347\u3002\u57288B\u53c2\u6570\u7684Llama\u6a21\u578b\u4e0a\u7684\u6301\u7eed\u9884\u8bad\u7ec3\u5b9e\u9a8c\u8868\u660e\uff0cNCP\u80fd\u8fdb\u4e00\u6b65\u63d0\u5347NTP\u8bad\u7ec3\u7684\u6a21\u578b\u3002", "conclusion": "NCP\u901a\u8fc7\u5f15\u5165\u66f4\u96be\u7684\u9884\u8bad\u7ec3\u4efb\u52a1\uff0c\u80fd\u591f\u4ea7\u751f\u66f4\u5f3a\u5927\u7684\u8bed\u8a00\u6a21\u578b\uff0c\u4e3a\u66f4\u597d\u7684\u8bed\u8a00\u5efa\u6a21\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u8def\u5f84\u3002"}}
{"id": "2602.08715", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08715", "abs": "https://arxiv.org/abs/2602.08715", "authors": ["Miquel Mir\u00f3-Nicolau", "Gabriel Moy\u00e0-Alcover", "Anna Arias-Duart"], "title": "Exploring SAIG Methods for an Objective Evaluation of XAI", "comment": null, "summary": "The evaluation of eXplainable Artificial Intelligence (XAI) methods is a rapidly growing field, characterized by a wide variety of approaches. This diversity highlights the complexity of the XAI evaluation, which, unlike traditional AI assessment, lacks a universally correct ground truth for the explanation, making objective evaluation challenging. One promising direction to address this issue involves the use of what we term Synthetic Artificial Intelligence Ground truth (SAIG) methods, which generate artificial ground truths to enable the direct evaluation of XAI techniques. This paper presents the first review and analysis of SAIG methods. We introduce a novel taxonomy to classify these approaches, identifying seven key features that distinguish different SAIG methods. Our comparative study reveals a concerning lack of consensus on the most effective XAI evaluation techniques, underscoring the need for further research and standardization in this area.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u7cfb\u7edf\u7efc\u8ff0\u4e86SAIG\u65b9\u6cd5\u2014\u2014\u901a\u8fc7\u751f\u6210\u4eba\u5de5\u771f\u5b9e\u503c\u6765\u8bc4\u4f30XAI\u6280\u672f\u7684\u8bc4\u4f30\u65b9\u6cd5\uff0c\u63d0\u51fa\u4e86\u65b0\u7684\u5206\u7c7b\u6cd5\u5e76\u8bc6\u522b\u4e86\u4e03\u4e2a\u5173\u952e\u7279\u5f81\uff0c\u53d1\u73b0\u8be5\u9886\u57df\u7f3a\u4e4f\u5171\u8bc6\u9700\u8981\u8fdb\u4e00\u6b65\u6807\u51c6\u5316\u3002", "motivation": "XAI\u8bc4\u4f30\u9886\u57df\u65b9\u6cd5\u591a\u6837\u4e14\u590d\u6742\uff0c\u4e0e\u4f20\u7edfAI\u8bc4\u4f30\u4e0d\u540c\uff0cXAI\u7f3a\u4e4f\u666e\u904d\u6b63\u786e\u7684\u89e3\u91ca\u771f\u5b9e\u503c\uff0c\u4f7f\u5f97\u5ba2\u89c2\u8bc4\u4f30\u5177\u6709\u6311\u6218\u6027\u3002SAIG\u65b9\u6cd5\u901a\u8fc7\u751f\u6210\u4eba\u5de5\u771f\u5b9e\u503c\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u4f46\u8be5\u9886\u57df\u7f3a\u4e4f\u7cfb\u7edf\u6027\u7efc\u8ff0\u548c\u5206\u6790\u3002", "method": "1. \u9996\u6b21\u5bf9SAIG\u65b9\u6cd5\u8fdb\u884c\u5168\u9762\u7efc\u8ff0\u548c\u5206\u6790\uff1b2. \u63d0\u51fa\u65b0\u7684\u5206\u7c7b\u6cd5\u5bf9\u8fd9\u4e9b\u65b9\u6cd5\u8fdb\u884c\u5206\u7c7b\uff1b3. \u8bc6\u522b\u4e86\u533a\u5206\u4e0d\u540cSAIG\u65b9\u6cd5\u7684\u4e03\u4e2a\u5173\u952e\u7279\u5f81\uff1b4. \u8fdb\u884c\u5bf9\u6bd4\u7814\u7a76\u5206\u6790\u4e0d\u540c\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "result": "\u7814\u7a76\u53d1\u73b0XAI\u8bc4\u4f30\u6280\u672f\u4e2d\u6700\u6709\u6548\u7684\u65b9\u6cd5\u7f3a\u4e4f\u5171\u8bc6\uff0c\u8fd9\u4ee4\u4eba\u62c5\u5fe7\u3002\u5bf9\u6bd4\u7814\u7a76\u63ed\u793a\u4e86\u5f53\u524dSAIG\u65b9\u6cd5\u7684\u591a\u6837\u6027\u548c\u4e0d\u4e00\u81f4\u6027\uff0c\u5f3a\u8c03\u4e86\u8be5\u9886\u57df\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u548c\u6807\u51c6\u5316\u3002", "conclusion": "SAIG\u65b9\u6cd5\u4e3a\u89e3\u51b3XAI\u8bc4\u4f30\u6311\u6218\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u65b9\u5411\uff0c\u4f46\u5f53\u524d\u7f3a\u4e4f\u7edf\u4e00\u6807\u51c6\u548c\u5171\u8bc6\u3002\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u6765\u5efa\u7acb\u66f4\u53ef\u9760\u3001\u6807\u51c6\u5316\u7684XAI\u8bc4\u4f30\u6846\u67b6\uff0c\u4ee5\u63a8\u52a8\u8be5\u9886\u57df\u7684\u53d1\u5c55\u3002"}}
{"id": "2602.07827", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.07827", "abs": "https://arxiv.org/abs/2602.07827", "authors": ["Guoting Wei", "Xia Yuan", "Yang Zhou", "Haizhao Jing", "Yu Liu", "Xianbiao Qi", "Chunxia Zhao", "Haokui Zhang", "Rong Xiao"], "title": "Open-Text Aerial Detection: A Unified Framework For Aerial Visual Grounding And Detection", "comment": null, "summary": "Open-Vocabulary Aerial Detection (OVAD) and Remote Sensing Visual Grounding (RSVG) have emerged as two key paradigms for aerial scene understanding. However, each paradigm suffers from inherent limitations when operating in isolation: OVAD is restricted to coarse category-level semantics, while RSVG is structurally limited to single-target localization. These limitations prevent existing methods from simultaneously supporting rich semantic understanding and multi-target detection. To address this, we propose OTA-Det, the first unified framework that bridges both paradigms into a cohesive architecture. Specifically, we introduce a task reformulation strategy that unifies task objectives and supervision mechanisms, enabling joint training across datasets from both paradigms with dense supervision signals. Furthermore, we propose a dense semantic alignment strategy that establishes explicit correspondence at multiple granularities, from holistic expressions to individual attributes, enabling fine-grained semantic understanding. To ensure real-time efficiency, OTA-Det builds upon the RT-DETR architecture, extending it from closed-set detection to open-text detection by introducing several high efficient modules, achieving state-of-the-art performance on six benchmarks spanning both OVAD and RSVG tasks while maintaining real-time inference at 34 FPS.", "AI": {"tldr": "OTA-Det\u662f\u9996\u4e2a\u7edf\u4e00\u5f00\u653e\u8bcd\u6c47\u822a\u7a7a\u68c0\u6d4b\u548c\u9065\u611f\u89c6\u89c9\u5b9a\u4f4d\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u4efb\u52a1\u91cd\u6784\u548c\u5bc6\u96c6\u8bed\u4e49\u5bf9\u9f50\u7b56\u7565\uff0c\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u8bed\u4e49\u7406\u89e3\u548c\u591a\u76ee\u6807\u68c0\u6d4b\uff0c\u5728\u4fdd\u6301\u5b9e\u65f6\u63a8\u7406\u7684\u540c\u65f6\u5728\u516d\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230SOTA\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u5f00\u653e\u8bcd\u6c47\u822a\u7a7a\u68c0\u6d4b(OVAD)\u53ea\u80fd\u63d0\u4f9b\u7c97\u7c92\u5ea6\u7c7b\u522b\u8bed\u4e49\uff0c\u800c\u9065\u611f\u89c6\u89c9\u5b9a\u4f4d(RSVG)\u4ec5\u9650\u4e8e\u5355\u76ee\u6807\u5b9a\u4f4d\uff0c\u4e24\u8005\u90fd\u65e0\u6cd5\u540c\u65f6\u652f\u6301\u4e30\u5bcc\u7684\u8bed\u4e49\u7406\u89e3\u548c\u591a\u76ee\u6807\u68c0\u6d4b\uff0c\u9700\u8981\u7edf\u4e00\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "1) \u4efb\u52a1\u91cd\u6784\u7b56\u7565\u7edf\u4e00\u4efb\u52a1\u76ee\u6807\u548c\u76d1\u7763\u673a\u5236\uff0c\u5b9e\u73b0\u8de8\u8303\u5f0f\u6570\u636e\u96c6\u7684\u8054\u5408\u8bad\u7ec3\uff1b2) \u5bc6\u96c6\u8bed\u4e49\u5bf9\u9f50\u7b56\u7565\u5efa\u7acb\u4ece\u6574\u4f53\u8868\u8fbe\u5230\u4e2a\u4f53\u5c5e\u6027\u7684\u591a\u7c92\u5ea6\u5bf9\u5e94\u5173\u7cfb\uff1b3) \u57fa\u4e8eRT-DETR\u67b6\u6784\u6269\u5c55\uff0c\u5f15\u5165\u9ad8\u6548\u6a21\u5757\u5b9e\u73b0\u5f00\u653e\u6587\u672c\u68c0\u6d4b\u3002", "result": "\u5728\u516d\u4e2a\u6db5\u76d6OVAD\u548cRSVG\u4efb\u52a1\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u630134 FPS\u7684\u5b9e\u65f6\u63a8\u7406\u901f\u5ea6\u3002", "conclusion": "OTA-Det\u6210\u529f\u7edf\u4e00\u4e86\u4e24\u4e2a\u5173\u952e\u822a\u7a7a\u573a\u666f\u7406\u89e3\u8303\u5f0f\uff0c\u5b9e\u73b0\u4e86\u7ec6\u7c92\u5ea6\u8bed\u4e49\u7406\u89e3\u548c\u591a\u76ee\u6807\u68c0\u6d4b\u7684\u534f\u540c\uff0c\u4e3a\u822a\u7a7a\u573a\u666f\u7406\u89e3\u63d0\u4f9b\u4e86\u9ad8\u6548\u7edf\u4e00\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.07833", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.07833", "abs": "https://arxiv.org/abs/2602.07833", "authors": ["Weijiang Lv", "Yaoxuan Feng", "Xiaobo Xia", "Jiayu Wang", "Yan Jing", "Wenchao Chen", "Bo Chen"], "title": "SPD-Faith Bench: Diagnosing and Improving Faithfulness in Chain-of-Thought for Multimodal Large Language Models", "comment": "53 pages, 42 figures, 14 tables", "summary": "Chain-of-Thought reasoning is widely used to improve the interpretability of multimodal large language models (MLLMs), yet the faithfulness of the generated reasoning traces remains unclear. Prior work has mainly focused on perceptual hallucinations, leaving reasoning level unfaithfulness underexplored. To isolate faithfulness from linguistic priors, we introduce SPD-Faith Bench, a diagnostic benchmark based on fine-grained image difference reasoning that enforces explicit visual comparison. Evaluations on state-of-the-art MLLMs reveal two systematic failure modes, perceptual blindness and perception-reasoning dissociation. We trace these failures to decaying visual attention and representation shifts in the residual stream. Guided by this analysis, we propose SAGE, a train-free visual evidence-calibrated framework that improves visual routing and aligns reasoning with perception. Our results highlight the importance of explicitly evaluating faithfulness beyond response correctness. Our benchmark and codes are available at https://github.com/Johanson-colab/SPD-Faith-Bench.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86SPD-Faith Bench\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u8fc7\u7a0b\u7684\u5fe0\u5b9e\u6027\uff0c\u53d1\u73b0\u4e86\u4e24\u79cd\u7cfb\u7edf\u6027\u5931\u8d25\u6a21\u5f0f\uff0c\u5e76\u63d0\u51fa\u4e86\u65e0\u9700\u8bad\u7ec3\u7684SAGE\u6846\u67b6\u6765\u6539\u8fdb\u89c6\u89c9\u8bc1\u636e\u6821\u51c6\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5e7f\u6cdb\u4f7f\u7528\u601d\u7ef4\u94fe\u63a8\u7406\u6765\u63d0\u9ad8\u53ef\u89e3\u91ca\u6027\uff0c\u4f46\u751f\u6210\u7684\u63a8\u7406\u8f68\u8ff9\u7684\u5fe0\u5b9e\u6027\u4ecd\u4e0d\u6e05\u695a\u3002\u5148\u524d\u5de5\u4f5c\u4e3b\u8981\u5173\u6ce8\u611f\u77e5\u5e7b\u89c9\uff0c\u800c\u63a8\u7406\u5c42\u9762\u7684\u4e0d\u5fe0\u5b9e\u6027\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002", "method": "\u5f15\u5165SPD-Faith Bench\u8bca\u65ad\u57fa\u51c6\uff0c\u57fa\u4e8e\u7ec6\u7c92\u5ea6\u56fe\u50cf\u5dee\u5f02\u63a8\u7406\u5f3a\u5236\u8fdb\u884c\u663e\u5f0f\u89c6\u89c9\u6bd4\u8f83\u3002\u901a\u8fc7\u5206\u6790\u53d1\u73b0\u4e24\u79cd\u7cfb\u7edf\u6027\u5931\u8d25\u6a21\u5f0f\uff0c\u5e76\u63d0\u51faSAGE\u6846\u67b6\uff0c\u8fd9\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684\u89c6\u89c9\u8bc1\u636e\u6821\u51c6\u6846\u67b6\uff0c\u901a\u8fc7\u6539\u8fdb\u89c6\u89c9\u8def\u7531\u548c\u5bf9\u9f50\u63a8\u7406\u4e0e\u611f\u77e5\u6765\u63d0\u5347\u5fe0\u5b9e\u6027\u3002", "result": "\u5728\u5148\u8fdb\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u8bc4\u4f30\u4e2d\u53d1\u73b0\u4e86\u4e24\u79cd\u7cfb\u7edf\u6027\u5931\u8d25\u6a21\u5f0f\uff1a\u611f\u77e5\u76f2\u533a\u548c\u611f\u77e5-\u63a8\u7406\u5206\u79bb\u3002\u8fd9\u4e9b\u5931\u8d25\u6e90\u4e8e\u6b8b\u5dee\u6d41\u4e2d\u7684\u89c6\u89c9\u6ce8\u610f\u529b\u8870\u51cf\u548c\u8868\u793a\u504f\u79fb\u3002SAGE\u6846\u67b6\u80fd\u591f\u6709\u6548\u6539\u8fdb\u89c6\u89c9\u8def\u7531\u5e76\u63d0\u5347\u63a8\u7406\u5fe0\u5b9e\u6027\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u5728\u54cd\u5e94\u6b63\u786e\u6027\u4e4b\u5916\u663e\u5f0f\u8bc4\u4f30\u5fe0\u5b9e\u6027\u7684\u91cd\u8981\u6027\u3002\u63d0\u51fa\u7684\u57fa\u51c6\u548cSAGE\u6846\u67b6\u4e3a\u7406\u89e3\u548c\u6539\u8fdb\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u5fe0\u5b9e\u6027\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\u3002"}}
{"id": "2602.08754", "categories": ["cs.AI", "cs.CY", "cs.HC"], "pdf": "https://arxiv.org/pdf/2602.08754", "abs": "https://arxiv.org/abs/2602.08754", "authors": ["Rose E. Guingrich", "Dvija Mehta", "Umang Bhatt"], "title": "Belief Offloading in Human-AI Interaction", "comment": null, "summary": "What happens when people's beliefs are derived from information provided by an LLM? People's use of LLM chatbots as thought partners can contribute to cognitive offloading, which can have adverse effects on cognitive skills in cases of over-reliance. This paper defines and investigates a particular kind of cognitive offloading in human-AI interaction, \"belief offloading,\" in which people's processes of forming and upholding beliefs are offloaded onto an AI system with downstream consequences on their behavior and the nature of their system of beliefs. Drawing on philosophy, psychology, and computer science research, we clarify the boundary conditions under which belief offloading occurs and provide a descriptive taxonomy of belief offloading and its normative implications. We close with directions for future work to assess the potential for and consequences of belief offloading in human-AI interaction.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4eba\u7c7b\u4e0eAI\u4ea4\u4e92\u4e2d\u7684\"\u4fe1\u5ff5\u5378\u8f7d\"\u73b0\u8c61\uff0c\u5373\u4eba\u4eec\u5c06\u5f62\u6210\u548c\u7ef4\u6301\u4fe1\u5ff5\u7684\u8fc7\u7a0b\u5916\u5305\u7ed9AI\u7cfb\u7edf\uff0c\u8fd9\u4f1a\u5f71\u54cd\u5176\u884c\u4e3a\u548c\u4fe1\u5ff5\u4f53\u7cfb\u3002", "motivation": "\u968f\u7740\u4eba\u4eec\u8d8a\u6765\u8d8a\u591a\u5730\u5c06LLM\u804a\u5929\u673a\u5668\u4eba\u4f5c\u4e3a\u601d\u7ef4\u4f19\u4f34\uff0c\u8fd9\u53ef\u80fd\u5bfc\u81f4\u8ba4\u77e5\u5378\u8f7d\uff0c\u5728\u8fc7\u5ea6\u4f9d\u8d56\u7684\u60c5\u51b5\u4e0b\u5bf9\u8ba4\u77e5\u6280\u80fd\u4ea7\u751f\u8d1f\u9762\u5f71\u54cd\u3002\u9700\u8981\u7814\u7a76\u8fd9\u79cd\u7279\u5b9a\u7c7b\u578b\u7684\u8ba4\u77e5\u5378\u8f7d\u2014\u2014\u4fe1\u5ff5\u5378\u8f7d\u3002", "method": "\u7ed3\u5408\u54f2\u5b66\u3001\u5fc3\u7406\u5b66\u548c\u8ba1\u7b97\u673a\u79d1\u5b66\u7814\u7a76\uff0c\u660e\u786e\u4fe1\u5ff5\u5378\u8f7d\u53d1\u751f\u7684\u8fb9\u754c\u6761\u4ef6\uff0c\u5e76\u63d0\u4f9b\u4fe1\u5ff5\u5378\u8f7d\u7684\u63cf\u8ff0\u6027\u5206\u7c7b\u53ca\u5176\u89c4\u8303\u610f\u4e49\u3002", "result": "\u63d0\u51fa\u4e86\u4fe1\u5ff5\u5378\u8f7d\u7684\u6982\u5ff5\u6846\u67b6\u548c\u5206\u7c7b\u4f53\u7cfb\uff0c\u5206\u6790\u4e86\u5176\u53d1\u751f\u7684\u6761\u4ef6\uff0c\u5e76\u63a2\u8ba8\u4e86\u5176\u5bf9\u4eba\u7c7b\u884c\u4e3a\u548c\u4fe1\u5ff5\u4f53\u7cfb\u7684\u6f5c\u5728\u5f71\u54cd\u3002", "conclusion": "\u4fe1\u5ff5\u5378\u8f7d\u662f\u4eba\u7c7b\u4e0eAI\u4ea4\u4e92\u4e2d\u7684\u91cd\u8981\u73b0\u8c61\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u5176\u6f5c\u5728\u5f71\u54cd\u548c\u540e\u679c\uff0c\u4e3a\u672a\u6765\u5de5\u4f5c\u63d0\u4f9b\u65b9\u5411\u3002"}}
{"id": "2602.07835", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.07835", "abs": "https://arxiv.org/abs/2602.07835", "authors": ["Sanoojan Baliah", "Yohan Abeysinghe", "Rusiru Thushara", "Khan Muhammad", "Abhinav Dhall", "Karthik Nandakumar", "Muhammad Haris Khan"], "title": "VFace: A Training-Free Approach for Diffusion-Based Video Face Swapping", "comment": null, "summary": "We present a training-free, plug-and-play method, namely VFace, for high-quality face swapping in videos. It can be seamlessly integrated with image-based face swapping approaches built on diffusion models. First, we introduce a Frequency Spectrum Attention Interpolation technique to facilitate generation and intact key identity characteristics. Second, we achieve Target Structure Guidance via plug-and-play attention injection to better align the structural features from the target frame to the generation. Third, we present a Flow-Guided Attention Temporal Smoothening mechanism that enforces spatiotemporal coherence without modifying the underlying diffusion model to reduce temporal inconsistencies typically encountered in frame-wise generation. Our method requires no additional training or video-specific fine-tuning. Extensive experiments show that our method significantly enhances temporal consistency and visual fidelity, offering a practical and modular solution for video-based face swapping. Our code is available at https://github.com/Sanoojan/VFace.", "AI": {"tldr": "VFace\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u3001\u5373\u63d2\u5373\u7528\u7684\u89c6\u9891\u4eba\u8138\u4ea4\u6362\u65b9\u6cd5\uff0c\u53ef\u4e0e\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u56fe\u50cf\u4eba\u8138\u4ea4\u6362\u65b9\u6cd5\u65e0\u7f1d\u96c6\u6210\uff0c\u901a\u8fc7\u9891\u7387\u8c31\u6ce8\u610f\u529b\u63d2\u503c\u3001\u76ee\u6807\u7ed3\u6784\u5f15\u5bfc\u548c\u6d41\u5f15\u5bfc\u6ce8\u610f\u529b\u65f6\u5e8f\u5e73\u6ed1\u4e09\u4e2a\u6280\u672f\u63d0\u5347\u89c6\u9891\u4eba\u8138\u4ea4\u6362\u7684\u65f6\u5e8f\u4e00\u81f4\u6027\u548c\u89c6\u89c9\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u56fe\u50cf\u4eba\u8138\u4ea4\u6362\u65b9\u6cd5\u5728\u5e94\u7528\u5230\u89c6\u9891\u65f6\u5b58\u5728\u65f6\u5e8f\u4e0d\u4e00\u81f4\u95ee\u9898\uff0c\u5e27\u95f4\u751f\u6210\u7ed3\u679c\u7f3a\u4e4f\u8fde\u8d2f\u6027\uff0c\u9700\u8981\u4e00\u79cd\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u6216\u89c6\u9891\u7279\u5b9a\u5fae\u8c03\u7684\u65b9\u6cd5\u6765\u63d0\u5347\u89c6\u9891\u4eba\u8138\u4ea4\u6362\u7684\u65f6\u5e8f\u4e00\u81f4\u6027\u548c\u89c6\u89c9\u8d28\u91cf\u3002", "method": "1. \u9891\u7387\u8c31\u6ce8\u610f\u529b\u63d2\u503c\u6280\u672f\uff1a\u4fc3\u8fdb\u751f\u6210\u5e76\u4fdd\u7559\u5173\u952e\u8eab\u4efd\u7279\u5f81\n2. \u76ee\u6807\u7ed3\u6784\u5f15\u5bfc\uff1a\u901a\u8fc7\u5373\u63d2\u5373\u7528\u7684\u6ce8\u610f\u529b\u6ce8\u5165\uff0c\u5c06\u76ee\u6807\u5e27\u7684\u7ed3\u6784\u7279\u5f81\u66f4\u597d\u5730\u5bf9\u9f50\u5230\u751f\u6210\u8fc7\u7a0b\n3. \u6d41\u5f15\u5bfc\u6ce8\u610f\u529b\u65f6\u5e8f\u5e73\u6ed1\u673a\u5236\uff1a\u5728\u4e0d\u4fee\u6539\u5e95\u5c42\u6269\u6563\u6a21\u578b\u7684\u60c5\u51b5\u4e0b\u5f3a\u5236\u65f6\u7a7a\u4e00\u81f4\u6027\uff0c\u51cf\u5c11\u5e27\u95f4\u751f\u6210\u7684\u65f6\u95f4\u4e0d\u4e00\u81f4\u6027", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u65f6\u5e8f\u4e00\u81f4\u6027\u548c\u89c6\u89c9\u4fdd\u771f\u5ea6\uff0c\u4e3a\u89c6\u9891\u4eba\u8138\u4ea4\u6362\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b9e\u7528\u4e14\u6a21\u5757\u5316\u7684\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "VFace\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u3001\u5373\u63d2\u5373\u7528\u7684\u89c6\u9891\u4eba\u8138\u4ea4\u6362\u65b9\u6cd5\uff0c\u53ef\u4e0e\u73b0\u6709\u56fe\u50cf\u4eba\u8138\u4ea4\u6362\u65b9\u6cd5\u65e0\u7f1d\u96c6\u6210\uff0c\u901a\u8fc7\u4e09\u4e2a\u5173\u952e\u6280\u672f\u6709\u6548\u89e3\u51b3\u4e86\u89c6\u9891\u4eba\u8138\u4ea4\u6362\u4e2d\u7684\u65f6\u5e8f\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u63d0\u4f9b\u4e86\u9ad8\u8d28\u91cf\u7684\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.08783", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.08783", "abs": "https://arxiv.org/abs/2602.08783", "authors": ["Zirui Li", "Xuefeng Bai", "Kehai Chen", "Yizhi Li", "Jian Yang", "Chenghua Lin", "Min Zhang"], "title": "Dynamics Within Latent Chain-of-Thought: An Empirical Study of Causal Structure", "comment": "22 pages", "summary": "Latent or continuous chain-of-thought methods replace explicit textual rationales with a number of internal latent steps, but these intermediate computations are difficult to evaluate beyond correlation-based probes. In this paper, we view latent chain-of-thought as a manipulable causal process in representation space by modeling latent steps as variables in a structural causal model (SCM) and analyzing their effects through step-wise $\\mathrm{do}$-interventions. We study two representative paradigms (i.e., Coconut and CODI) on both mathematical and general reasoning tasks to investigate three key questions: (1) which steps are causally necessary for correctness and when answers become decidable early; (2) how does influence propagate across steps, and how does this structure compare to explicit CoT; and (3) do intermediate trajectories retain competing answer modes, and how does output-level commitment differ from representational commitment across steps. We find that latent-step budgets behave less like homogeneous extra depth and more like staged functionality with non-local routing, and we identify a persistent gap between early output bias and late representational commitment. These results motivate mode-conditional and stability-aware analyses -- and corresponding training/decoding objectives -- as more reliable tools for interpreting and improving latent reasoning systems.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5c06\u6f5c\u5728\u601d\u7ef4\u94fe\u89c6\u4e3a\u8868\u793a\u7a7a\u95f4\u4e2d\u53ef\u64cd\u7eb5\u7684\u56e0\u679c\u8fc7\u7a0b\uff0c\u901a\u8fc7\u7ed3\u6784\u56e0\u679c\u6a21\u578b\u5206\u6790\u6f5c\u5728\u6b65\u9aa4\uff0c\u63a2\u7a76\u5176\u5728\u6570\u5b66\u548c\u901a\u7528\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u56e0\u679c\u5fc5\u8981\u6027\u3001\u5f71\u54cd\u4f20\u64ad\u548c\u7b54\u6848\u6a21\u5f0f\u4fdd\u7559\u7b49\u5173\u952e\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u6f5c\u5728\u601d\u7ef4\u94fe\u65b9\u6cd5\u7528\u5185\u90e8\u6f5c\u5728\u6b65\u9aa4\u66ff\u4ee3\u663e\u5f0f\u6587\u672c\u63a8\u7406\uff0c\u4f46\u8fd9\u4e9b\u4e2d\u95f4\u8ba1\u7b97\u96be\u4ee5\u901a\u8fc7\u76f8\u5173\u6027\u63a2\u6d4b\u4e4b\u5916\u7684\u65b9\u5f0f\u8fdb\u884c\u8bc4\u4f30\u3002\u7814\u7a76\u8005\u5e0c\u671b\u5c06\u6f5c\u5728\u601d\u7ef4\u94fe\u89c6\u4e3a\u53ef\u64cd\u7eb5\u7684\u56e0\u679c\u8fc7\u7a0b\uff0c\u4ee5\u66f4\u597d\u5730\u7406\u89e3\u548c\u6539\u8fdb\u6f5c\u5728\u63a8\u7406\u7cfb\u7edf\u3002", "method": "\u5c06\u6f5c\u5728\u6b65\u9aa4\u5efa\u6a21\u4e3a\u7ed3\u6784\u56e0\u679c\u6a21\u578b\u4e2d\u7684\u53d8\u91cf\uff0c\u901a\u8fc7\u9010\u6b65do\u5e72\u9884\u5206\u6790\u5176\u5f71\u54cd\u3002\u7814\u7a76\u4e24\u79cd\u4ee3\u8868\u6027\u8303\u5f0f\uff08Coconut\u548cCODI\uff09\uff0c\u5728\u6570\u5b66\u548c\u901a\u7528\u63a8\u7406\u4efb\u52a1\u4e0a\u63a2\u7a76\u4e09\u4e2a\u5173\u952e\u95ee\u9898\uff1a\u6b65\u9aa4\u7684\u56e0\u679c\u5fc5\u8981\u6027\u3001\u5f71\u54cd\u4f20\u64ad\u673a\u5236\u4ee5\u53ca\u4e2d\u95f4\u8f68\u8ff9\u662f\u5426\u4fdd\u7559\u7ade\u4e89\u7b54\u6848\u6a21\u5f0f\u3002", "result": "\u53d1\u73b0\u6f5c\u5728\u6b65\u9aa4\u9884\u7b97\u4e0d\u50cf\u540c\u8d28\u7684\u989d\u5916\u6df1\u5ea6\uff0c\u800c\u66f4\u50cf\u5177\u6709\u975e\u5c40\u90e8\u8def\u7531\u7684\u5206\u9636\u6bb5\u529f\u80fd\uff1b\u8bc6\u522b\u51fa\u65e9\u671f\u8f93\u51fa\u504f\u5dee\u4e0e\u665a\u671f\u8868\u793a\u627f\u8bfa\u4e4b\u95f4\u7684\u6301\u7eed\u5dee\u8ddd\uff1b\u6f5c\u5728\u6b65\u9aa4\u8868\u73b0\u51fa\u660e\u663e\u7684\u56e0\u679c\u7ed3\u6784\u548c\u529f\u80fd\u5206\u5316\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u652f\u6301\u6a21\u5f0f\u6761\u4ef6\u548c\u7a33\u5b9a\u6027\u611f\u77e5\u5206\u6790\u4f5c\u4e3a\u89e3\u91ca\u548c\u6539\u8fdb\u6f5c\u5728\u63a8\u7406\u7cfb\u7edf\u7684\u66f4\u53ef\u9760\u5de5\u5177\uff0c\u5e76\u63d0\u51fa\u4e86\u76f8\u5e94\u7684\u8bad\u7ec3/\u89e3\u7801\u76ee\u6807\u3002\u5f3a\u8c03\u9700\u8981\u8d85\u8d8a\u76f8\u5173\u6027\u63a2\u6d4b\u7684\u56e0\u679c\u5206\u6790\u65b9\u6cd5\u6765\u7406\u89e3\u6f5c\u5728\u63a8\u7406\u8fc7\u7a0b\u3002"}}
{"id": "2602.07854", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.07854", "abs": "https://arxiv.org/abs/2602.07854", "authors": ["Chendong Xiang", "Jiajun Liu", "Jintao Zhang", "Xiao Yang", "Zhengwei Fang", "Shizun Wang", "Zijun Wang", "Yingtian Zou", "Hang Su", "Jun Zhu"], "title": "Geometry-Aware Rotary Position Embedding for Consistent Video World Model", "comment": null, "summary": "Predictive world models that simulate future observations under explicit camera control are fundamental to interactive AI. Despite rapid advances, current systems lack spatial persistence: they fail to maintain stable scene structures over long trajectories, frequently hallucinating details when cameras revisit previously observed locations. We identify that this geometric drift stems from reliance on screen-space positional embeddings, which conflict with the projective geometry required for 3D consistency. We introduce \\textbf{ViewRope}, a geometry-aware encoding that injects camera-ray directions directly into video transformer self-attention layers. By parameterizing attention with relative ray geometry rather than pixel locality, ViewRope provides a model-native inductive bias for retrieving 3D-consistent content across temporal gaps. We further propose \\textbf{Geometry-Aware Frame-Sparse Attention}, which exploits these geometric cues to selectively attend to relevant historical frames, improving efficiency without sacrificing memory consistency. We also present \\textbf{ViewBench}, a diagnostic suite measuring loop-closure fidelity and geometric drift. Our results demonstrate that ViewRope substantially improves long-term consistency while reducing computational costs.", "AI": {"tldr": "ViewRope\u662f\u4e00\u79cd\u51e0\u4f55\u611f\u77e5\u7f16\u7801\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u76f8\u673a\u5c04\u7ebf\u65b9\u5411\u76f4\u63a5\u6ce8\u5165\u89c6\u9891Transformer\u81ea\u6ce8\u610f\u529b\u5c42\uff0c\u89e3\u51b3\u4e86\u9884\u6d4b\u4e16\u754c\u6a21\u578b\u4e2d\u7a7a\u95f4\u6301\u4e45\u6027\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u663e\u8457\u6539\u5584\u4e86\u957f\u671f\u4e00\u81f4\u6027\u5e76\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u5f53\u524d\u9884\u6d4b\u4e16\u754c\u6a21\u578b\u7f3a\u4e4f\u7a7a\u95f4\u6301\u4e45\u6027\uff0c\u5728\u957f\u8f68\u8ff9\u4e2d\u65e0\u6cd5\u4fdd\u6301\u7a33\u5b9a\u7684\u573a\u666f\u7ed3\u6784\uff0c\u5f53\u76f8\u673a\u91cd\u65b0\u8bbf\u95ee\u5148\u524d\u89c2\u5bdf\u4f4d\u7f6e\u65f6\u7ecf\u5e38\u51fa\u73b0\u5e7b\u89c9\u7ec6\u8282\u3002\u8fd9\u79cd\u51e0\u4f55\u6f02\u79fb\u6e90\u4e8e\u5bf9\u5c4f\u5e55\u7a7a\u95f4\u4f4d\u7f6e\u5d4c\u5165\u7684\u4f9d\u8d56\uff0c\u8fd9\u4e0e3D\u4e00\u81f4\u6027\u6240\u9700\u7684\u6295\u5f71\u51e0\u4f55\u76f8\u51b2\u7a81\u3002", "method": "\u63d0\u51faViewRope\u51e0\u4f55\u611f\u77e5\u7f16\u7801\uff0c\u5c06\u76f8\u673a\u5c04\u7ebf\u65b9\u5411\u76f4\u63a5\u6ce8\u5165\u89c6\u9891Transformer\u81ea\u6ce8\u610f\u529b\u5c42\uff1b\u63d0\u51fa\u51e0\u4f55\u611f\u77e5\u5e27\u7a00\u758f\u6ce8\u610f\u529b\uff0c\u5229\u7528\u51e0\u4f55\u7ebf\u7d22\u9009\u62e9\u6027\u5173\u6ce8\u76f8\u5173\u5386\u53f2\u5e27\uff1b\u521b\u5efaViewBench\u8bca\u65ad\u5957\u4ef6\u6d4b\u91cf\u95ed\u73af\u4fdd\u771f\u5ea6\u548c\u51e0\u4f55\u6f02\u79fb\u3002", "result": "ViewRope\u663e\u8457\u6539\u5584\u4e86\u957f\u671f\u4e00\u81f4\u6027\uff0c\u540c\u65f6\u51cf\u5c11\u4e86\u8ba1\u7b97\u6210\u672c\u3002\u51e0\u4f55\u611f\u77e5\u5e27\u7a00\u758f\u6ce8\u610f\u529b\u63d0\u9ad8\u4e86\u6548\u7387\u800c\u4e0d\u727a\u7272\u5185\u5b58\u4e00\u81f4\u6027\u3002", "conclusion": "\u901a\u8fc7\u51e0\u4f55\u611f\u77e5\u7f16\u7801\u548c\u6ce8\u610f\u529b\u673a\u5236\uff0c\u53ef\u4ee5\u6709\u6548\u89e3\u51b3\u9884\u6d4b\u4e16\u754c\u6a21\u578b\u4e2d\u7684\u7a7a\u95f4\u6301\u4e45\u6027\u95ee\u9898\uff0c\u5b9e\u73b0\u66f4\u7a33\u5b9a\u76843D\u4e00\u81f4\u6027\u573a\u666f\u5efa\u6a21\u3002"}}
{"id": "2602.08796", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.08796", "abs": "https://arxiv.org/abs/2602.08796", "authors": ["Kevin Fan", "Jacquelyn A. Bialo", "Hongli Li"], "title": "The Use of AI Tools to Develop and Validate Q-Matrices", "comment": "An earlier version of this study was presented at the Psychometric Society Meeting held in July 2025 in Minneapolis, USA", "summary": "Constructing a Q-matrix is a critical but labor-intensive step in cognitive diagnostic modeling (CDM). This study investigates whether AI tools (i.e., general language models) can support Q-matrix development by comparing AI-generated Q-matrices with a validated Q-matrix from Li and Suen (2013) for a reading comprehension test. In May 2025, multiple AI models were provided with the same training materials as human experts. Agreement among AI-generated Q-matrices, the validated Q-matrix, and human raters' Q-matrices was assessed using Cohen's kappa. Results showed substantial variation across AI models, with Google Gemini 2.5 Pro achieving the highest agreement (Kappa = 0.63) with the validated Q-matrix, exceeding that of all human experts. A follow-up analysis in January 2026 using newer AI versions, however, revealed lower agreement with the validated Q-matrix. Implications and directions for future research are discussed.", "AI": {"tldr": "AI\u5de5\u5177\u5728\u8ba4\u77e5\u8bca\u65ad\u5efa\u6a21\u4e2d\u652f\u6301Q\u77e9\u9635\u6784\u5efa\u7684\u53ef\u884c\u6027\u7814\u7a76\uff0c\u53d1\u73b0\u4e0d\u540cAI\u6a21\u578b\u8868\u73b0\u5dee\u5f02\u663e\u8457\uff0c\u5176\u4e2dGoogle Gemini 2.5 Pro\u4e0e\u9a8c\u8bc1Q\u77e9\u9635\u7684\u4e00\u81f4\u6027\u6700\u9ad8\uff0c\u751a\u81f3\u8d85\u8fc7\u4eba\u7c7b\u4e13\u5bb6\uff0c\u4f46\u540e\u7eed\u7248\u672c\u8868\u73b0\u4e0b\u964d\u3002", "motivation": "Q\u77e9\u9635\u6784\u5efa\u662f\u8ba4\u77e5\u8bca\u65ad\u5efa\u6a21\u7684\u5173\u952e\u6b65\u9aa4\uff0c\u4f46\u4eba\u5de5\u6784\u5efa\u8017\u65f6\u8017\u529b\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u901a\u7528\u8bed\u8a00\u6a21\u578b\u7b49AI\u5de5\u5177\u662f\u5426\u80fd\u591f\u652f\u6301Q\u77e9\u9635\u7684\u5f00\u53d1\uff0c\u4e3a\u8fd9\u4e00\u52b3\u52a8\u5bc6\u96c6\u578b\u4efb\u52a1\u63d0\u4f9b\u81ea\u52a8\u5316\u8f85\u52a9\u3002", "method": "\u4f7f\u7528\u591a\u4e2aAI\u6a21\u578b\uff08\u901a\u7528\u8bed\u8a00\u6a21\u578b\uff09\u4e0e\u4eba\u7c7b\u4e13\u5bb6\u76f8\u540c\u7684\u8bad\u7ec3\u6750\u6599\uff0c\u751f\u6210\u9605\u8bfb\u7406\u89e3\u6d4b\u8bd5\u7684Q\u77e9\u9635\u3002\u5c06AI\u751f\u6210\u7684Q\u77e9\u9635\u4e0eLi\u548cSuen\uff082013\uff09\u9a8c\u8bc1\u7684Q\u77e9\u9635\u4ee5\u53ca\u4eba\u7c7b\u8bc4\u5206\u8005\u7684Q\u77e9\u9635\u8fdb\u884c\u6bd4\u8f83\uff0c\u4f7f\u7528Cohen's kappa\u8bc4\u4f30\u4e00\u81f4\u6027\u3002\u5206\u522b\u57282025\u5e745\u6708\u548c2026\u5e741\u6708\u8fdb\u884c\u4e24\u8f6e\u5206\u6790\u3002", "result": "\u4e0d\u540cAI\u6a21\u578b\u8868\u73b0\u5dee\u5f02\u663e\u8457\uff0cGoogle Gemini 2.5 Pro\u4e0e\u9a8c\u8bc1Q\u77e9\u9635\u7684\u4e00\u81f4\u6027\u6700\u9ad8\uff08Kappa = 0.63\uff09\uff0c\u8d85\u8fc7\u4e86\u6240\u6709\u4eba\u7c7b\u4e13\u5bb6\u3002\u7136\u800c\uff0c2026\u5e741\u6708\u4f7f\u7528\u66f4\u65b0\u7248\u672cAI\u8fdb\u884c\u7684\u540e\u7eed\u5206\u6790\u663e\u793a\uff0c\u4e0e\u9a8c\u8bc1Q\u77e9\u9635\u7684\u4e00\u81f4\u6027\u6709\u6240\u4e0b\u964d\u3002", "conclusion": "AI\u5de5\u5177\u5728\u652f\u6301Q\u77e9\u9635\u6784\u5efa\u65b9\u9762\u663e\u793a\u51fa\u6f5c\u529b\uff0c\u7279\u522b\u662f\u67d0\u4e9b\u6a21\u578b\u80fd\u591f\u8fbe\u5230\u751a\u81f3\u8d85\u8fc7\u4eba\u7c7b\u4e13\u5bb6\u7684\u8868\u73b0\u3002\u4f46AI\u6a21\u578b\u7248\u672c\u66f4\u65b0\u53ef\u80fd\u5f71\u54cd\u6027\u80fd\u7a33\u5b9a\u6027\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76AI\u5de5\u5177\u5728\u8ba4\u77e5\u8bca\u65ad\u5efa\u6a21\u4e2d\u7684\u53ef\u9760\u6027\u548c\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2602.07860", "categories": ["cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2602.07860", "abs": "https://arxiv.org/abs/2602.07860", "authors": ["Fei Yu", "Shudan Guo", "Shiqing Xin", "Beibei Wang", "Haisen Zhao", "Wenzheng Chen"], "title": "Recovering 3D Shapes from Ultra-Fast Motion-Blurred Images", "comment": "Accepted by 3DV 2026. Project page: https://maxmilite.github.io/rec-from-ultrafast-blur/", "summary": "We consider the problem of 3D shape recovery from ultra-fast motion-blurred images. While 3D reconstruction from static images has been extensively studied, recovering geometry from extreme motion-blurred images remains challenging. Such scenarios frequently occur in both natural and industrial settings, such as fast-moving objects in sports (e.g., balls) or rotating machinery, where rapid motion distorts object appearance and makes traditional 3D reconstruction techniques like Multi-View Stereo (MVS) ineffective.\n  In this paper, we propose a novel inverse rendering approach for shape recovery from ultra-fast motion-blurred images. While conventional rendering techniques typically synthesize blur by averaging across multiple frames, we identify a major computational bottleneck in the repeated computation of barycentric weights. To address this, we propose a fast barycentric coordinate solver, which significantly reduces computational overhead and achieves a speedup of up to 4.57x, enabling efficient and photorealistic simulation of high-speed motion. Crucially, our method is fully differentiable, allowing gradients to propagate from rendered images to the underlying 3D shape, thereby facilitating shape recovery through inverse rendering.\n  We validate our approach on two representative motion types: rapid translation and rotation. Experimental results demonstrate that our method enables efficient and realistic modeling of ultra-fast moving objects in the forward simulation. Moreover, it successfully recovers 3D shapes from 2D imagery of objects undergoing extreme translational and rotational motion, advancing the boundaries of vision-based 3D reconstruction. Project page: https://maxmilite.github.io/rec-from-ultrafast-blur/", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4ece\u8d85\u9ad8\u901f\u8fd0\u52a8\u6a21\u7cca\u56fe\u50cf\u4e2d\u6062\u590d3D\u5f62\u72b6\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5feb\u901f\u91cd\u5fc3\u5750\u6807\u6c42\u89e3\u5668\u5b9e\u73b0\u9ad8\u6548\u53ef\u5fae\u7684\u9006\u6e32\u67d3\uff0c\u5728\u5e73\u79fb\u548c\u65cb\u8f6c\u8fd0\u52a8\u573a\u666f\u4e2d\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\u3002", "motivation": "\u5728\u4f53\u80b2\uff08\u5982\u7403\u7c7b\uff09\u6216\u65cb\u8f6c\u673a\u68b0\u7b49\u81ea\u7136\u548c\u5de5\u4e1a\u573a\u666f\u4e2d\uff0c\u7269\u4f53\u9ad8\u901f\u8fd0\u52a8\u4f1a\u4ea7\u751f\u4e25\u91cd\u8fd0\u52a8\u6a21\u7cca\uff0c\u4f20\u7edf\u591a\u89c6\u56fe\u7acb\u4f53\u7b493D\u91cd\u5efa\u6280\u672f\u5931\u6548\uff0c\u9700\u8981\u89e3\u51b3\u4ece\u6781\u7aef\u8fd0\u52a8\u6a21\u7cca\u56fe\u50cf\u6062\u590d\u51e0\u4f55\u5f62\u72b6\u7684\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u9006\u6e32\u67d3\u65b9\u6cd5\uff0c\u6838\u5fc3\u662f\u5feb\u901f\u91cd\u5fc3\u5750\u6807\u6c42\u89e3\u5668\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u6e32\u67d3\u4e2d\u591a\u6b21\u8ba1\u7b97\u91cd\u5fc3\u6743\u91cd\u7684\u8ba1\u7b97\u74f6\u9888\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8fbe4.57\u500d\u7684\u52a0\u901f\uff0c\u540c\u65f6\u4fdd\u6301\u5b8c\u5168\u53ef\u5fae\u6027\uff0c\u652f\u6301\u4ece\u6e32\u67d3\u56fe\u50cf\u52303D\u5f62\u72b6\u7684\u68af\u5ea6\u4f20\u64ad\u3002", "result": "\u5728\u5feb\u901f\u5e73\u79fb\u548c\u65cb\u8f6c\u4e24\u79cd\u4ee3\u8868\u6027\u8fd0\u52a8\u7c7b\u578b\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff1a1\uff09\u524d\u5411\u6a21\u62df\u4e2d\u80fd\u9ad8\u6548\u903c\u771f\u5730\u5efa\u6a21\u8d85\u9ad8\u901f\u8fd0\u52a8\u7269\u4f53\uff1b2\uff09\u6210\u529f\u4ece\u7ecf\u5386\u6781\u7aef\u5e73\u79fb\u548c\u65cb\u8f6c\u8fd0\u52a8\u7684\u7269\u4f532D\u56fe\u50cf\u4e2d\u6062\u590d3D\u5f62\u72b6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u7a81\u7834\u4e86\u57fa\u4e8e\u89c6\u89c9\u76843D\u91cd\u5efa\u8fb9\u754c\uff0c\u4e3a\u4ece\u8d85\u9ad8\u901f\u8fd0\u52a8\u6a21\u7cca\u56fe\u50cf\u4e2d\u6062\u590d3D\u5f62\u72b6\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u8ba1\u7b97\u6548\u7387\u548c\u5f62\u72b6\u6062\u590d\u7cbe\u5ea6\u65b9\u9762\u5747\u6709\u663e\u8457\u63d0\u5347\u3002"}}
{"id": "2602.08804", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08804", "abs": "https://arxiv.org/abs/2602.08804", "authors": ["Liming Zhou", "Ailing Liu", "Hongwei Liu", "Min He", "Heng Zhang"], "title": "Root Cause Analysis Method Based on Large Language Models with Residual Connection Structures", "comment": null, "summary": "Root cause localization remain challenging in complex and large-scale microservice architectures. The complex fault propagation among microservices and the high dimensionality of telemetry data, including metrics, logs, and traces, limit the effectiveness of existing root cause analysis (RCA) methods. In this paper, a residual-connection-based RCA method using large language model (LLM), named RC-LLM, is proposed. A residual-like hierarchical fusion structure is designed to integrate multi-source telemetry data, while the contextual reasoning capability of large language models is leveraged to model temporal and cross-microservice causal dependencies. Experimental results on CCF-AIOps microservice datasets demonstrate that RC-LLM achieves strong accuracy and efficiency in root cause analysis.", "AI": {"tldr": "RC-LLM\uff1a\u4e00\u79cd\u57fa\u4e8e\u6b8b\u5dee\u8fde\u63a5\u548c\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5fae\u670d\u52a1\u6839\u56e0\u5206\u6790\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u6e90\u9065\u6d4b\u6570\u636e\u878d\u5408\u548c\u4e0a\u4e0b\u6587\u63a8\u7406\u80fd\u529b\u63d0\u5347\u590d\u6742\u5fae\u670d\u52a1\u67b6\u6784\u4e2d\u7684\u6545\u969c\u5b9a\u4f4d\u6548\u679c", "motivation": "\u5728\u590d\u6742\u5927\u89c4\u6a21\u5fae\u670d\u52a1\u67b6\u6784\u4e2d\uff0c\u6839\u56e0\u5b9a\u4f4d\u9762\u4e34\u6311\u6218\u3002\u5fae\u670d\u52a1\u95f4\u590d\u6742\u7684\u6545\u969c\u4f20\u64ad\u4ee5\u53ca\u9065\u6d4b\u6570\u636e\uff08\u6307\u6807\u3001\u65e5\u5fd7\u3001\u8ffd\u8e2a\uff09\u7684\u9ad8\u7ef4\u7279\u6027\u9650\u5236\u4e86\u73b0\u6709\u6839\u56e0\u5206\u6790\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "method": "\u63d0\u51faRC-LLM\u65b9\u6cd5\uff1a1\uff09\u8bbe\u8ba1\u6b8b\u5dee\u5f0f\u5206\u5c42\u878d\u5408\u7ed3\u6784\u6765\u6574\u5408\u591a\u6e90\u9065\u6d4b\u6570\u636e\uff1b2\uff09\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u7684\u4e0a\u4e0b\u6587\u63a8\u7406\u80fd\u529b\u6765\u5efa\u6a21\u65f6\u95f4\u548c\u8de8\u5fae\u670d\u52a1\u7684\u56e0\u679c\u4f9d\u8d56\u5173\u7cfb\u3002", "result": "\u5728CCF-AIOps\u5fae\u670d\u52a1\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cRC-LLM\u5728\u6839\u56e0\u5206\u6790\u65b9\u9762\u5b9e\u73b0\u4e86\u8f83\u9ad8\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "conclusion": "RC-LLM\u901a\u8fc7\u7ed3\u5408\u6b8b\u5dee\u8fde\u63a5\u7ed3\u6784\u548c\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u590d\u6742\u5fae\u670d\u52a1\u67b6\u6784\u4e2d\u7684\u6839\u56e0\u5b9a\u4f4d\u95ee\u9898\uff0c\u4e3a\u5fae\u670d\u52a1\u6545\u969c\u8bca\u65ad\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.07864", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.07864", "abs": "https://arxiv.org/abs/2602.07864", "authors": ["Chen Yang", "Guanxin Lin", "Youquan He", "Peiyao Chen", "Guanghe Liu", "Yufan Mo", "Zhouyuan Xu", "Linhao Wang", "Guohui Zhang", "Zihang Zhang", "Shenxiang Zeng", "Chen Wang", "Jiansheng Fan"], "title": "Thinking in Structures: Evaluating Spatial Intelligence through Reasoning on Constrained Manifolds", "comment": null, "summary": "Spatial intelligence is crucial for vision--language models (VLMs) in the physical world, yet many benchmarks evaluate largely unconstrained scenes where models can exploit 2D shortcuts. We introduce SSI-Bench, a VQA benchmark for spatial reasoning on constrained manifolds, built from complex real-world 3D structures whose feasible configurations are tightly governed by geometric, topological, and physical constraints. SSI-Bench contains 1,000 ranking questions spanning geometric and topological reasoning and requiring a diverse repertoire of compositional spatial operations, such as mental rotation, cross-sectional inference, occlusion reasoning, and force-path reasoning. It is created via a fully human-centered pipeline: ten researchers spent over 400 hours curating images, annotating structural components, and designing questions to minimize pixel-level cues. Evaluating 31 widely used VLMs reveals a large gap to humans: the best open-source model achieves 22.2% accuracy and the strongest closed-source model reaches 33.6%, while humans score 91.6%. Encouraging models to think yields only marginal gains, and error analysis points to failures in structural grounding and constraint-consistent 3D reasoning. Project page: https://ssi-bench.github.io.", "AI": {"tldr": "SSI-Bench\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u7684VQA\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4e13\u6ce8\u4e8e\u53d7\u7ea6\u675f\u6d41\u5f62\u4e0a\u7684\u7a7a\u95f4\u63a8\u7406\uff0c\u5305\u542b1000\u4e2a\u6392\u5e8f\u95ee\u9898\uff0c\u6d4b\u8bd5\u51e0\u4f55\u548c\u62d3\u6251\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u5927\u591a\u8bc4\u4f30\u65e0\u7ea6\u675f\u573a\u666f\uff0c\u6a21\u578b\u53ef\u4ee5\u5229\u75282D\u6377\u5f84\uff0c\u7f3a\u4e4f\u5bf9\u771f\u5b9e\u7269\u7406\u4e16\u754c\u4e2d\u53d7\u51e0\u4f55\u3001\u62d3\u6251\u548c\u7269\u7406\u7ea6\u675f\u7684\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u7684\u8bc4\u4f30\u3002", "method": "\u901a\u8fc7\u5b8c\u5168\u4eba\u5de5\u4e2d\u5fc3\u7684\u6d41\u7a0b\u521b\u5efa\uff1a10\u540d\u7814\u7a76\u4eba\u5458\u82b1\u8d39400\u591a\u5c0f\u65f6\u7b56\u5212\u56fe\u50cf\u3001\u6807\u6ce8\u7ed3\u6784\u7ec4\u4ef6\u3001\u8bbe\u8ba1\u95ee\u9898\u4ee5\u6700\u5c0f\u5316\u50cf\u7d20\u7ea7\u7ebf\u7d22\u3002\u5305\u542b\u51e0\u4f55\u548c\u62d3\u6251\u63a8\u7406\u95ee\u9898\uff0c\u9700\u8981\u5fc3\u7406\u65cb\u8f6c\u3001\u6a2a\u622a\u9762\u63a8\u7406\u3001\u906e\u6321\u63a8\u7406\u548c\u529b\u8def\u5f84\u63a8\u7406\u7b49\u590d\u5408\u7a7a\u95f4\u64cd\u4f5c\u3002", "result": "\u8bc4\u4f3031\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684VLM\u663e\u793a\u4e0e\u4eba\u7c7b\u5b58\u5728\u5de8\u5927\u5dee\u8ddd\uff1a\u6700\u4f73\u5f00\u6e90\u6a21\u578b\u51c6\u786e\u738722.2%\uff0c\u6700\u5f3a\u95ed\u6e90\u6a21\u578b33.6%\uff0c\u800c\u4eba\u7c7b\u5f97\u520691.6%\u3002\u9f13\u52b1\u6a21\u578b\u601d\u8003\u4ec5\u5e26\u6765\u8fb9\u9645\u6536\u76ca\uff0c\u9519\u8bef\u5206\u6790\u663e\u793a\u6a21\u578b\u5728\u7ed3\u6784\u57fa\u7840\u548c\u7ea6\u675f\u4e00\u81f4\u76843D\u63a8\u7406\u65b9\u9762\u5b58\u5728\u5931\u8d25\u3002", "conclusion": "SSI-Bench\u63ed\u793a\u4e86\u5f53\u524dVLM\u5728\u7a7a\u95f4\u63a8\u7406\u65b9\u9762\u7684\u4e25\u91cd\u4e0d\u8db3\uff0c\u7279\u522b\u662f\u5728\u53d7\u7ea6\u675f\u76843D\u7ed3\u6784\u63a8\u7406\u4e0a\uff0c\u4e3a\u672a\u6765\u6a21\u578b\u6539\u8fdb\u63d0\u4f9b\u4e86\u91cd\u8981\u7684\u8bc4\u4f30\u57fa\u51c6\u3002"}}
{"id": "2602.08815", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08815", "abs": "https://arxiv.org/abs/2602.08815", "authors": ["Yanglei Gan", "Peng He", "Yuxiang Cai", "Run Lin", "Guanyu Zhou", "Qiao Liu"], "title": "Negative-Aware Diffusion Process for Temporal Knowledge Graph Extrapolation", "comment": null, "summary": "Temporal Knowledge Graph (TKG) reasoning seeks to predict future missing facts from historical evidence. While diffusion models (DM) have recently gained attention for their ability to capture complex predictive distributions, two gaps remain: (i) the generative path is conditioned only on positive evidence, overlooking informative negative context, and (ii) training objectives are dominated by cross-entropy ranking, which improves candidate ordering but provides little supervision over the calibration of the denoised embedding. To bridge this gap, we introduce Negative-Aware Diffusion model for TKG Extrapolation (NADEx). Specifically, NADEx encodes subject-centric histories of entities, relations and temporal intervals into sequential embeddings. NADEx perturbs the query object in the forward process and reconstructs it in reverse with a Transformer denoiser conditioned on the temporal-relational context. We further derive a cosine-alignment regularizer derived from batch-wise negative prototypes, which tightens the decision boundary against implausible candidates. Comprehensive experiments on four public TKG benchmarks demonstrate that NADEx delivers state-of-the-art performance.", "AI": {"tldr": "NADEx\u662f\u4e00\u4e2a\u7528\u4e8e\u65f6\u5e8f\u77e5\u8bc6\u56fe\u8c31\u63a8\u7406\u7684\u8d1f\u611f\u77e5\u6269\u6563\u6a21\u578b\uff0c\u901a\u8fc7\u7ed3\u5408\u8d1f\u6837\u672c\u539f\u578b\u548c\u4f59\u5f26\u5bf9\u9f50\u6b63\u5219\u5316\u6765\u6539\u8fdb\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u6269\u6563\u6a21\u578b\u5728\u65f6\u5e8f\u77e5\u8bc6\u56fe\u8c31\u63a8\u7406\u4e2d\u5b58\u5728\u4e24\u4e2a\u95ee\u9898\uff1a1) \u751f\u6210\u8def\u5f84\u4ec5\u57fa\u4e8e\u6b63\u8bc1\u636e\uff0c\u5ffd\u7565\u4e86\u4fe1\u606f\u4e30\u5bcc\u7684\u8d1f\u4e0a\u4e0b\u6587\uff1b2) \u8bad\u7ec3\u76ee\u6807\u4e3b\u8981\u4f9d\u8d56\u4ea4\u53c9\u71b5\u6392\u5e8f\uff0c\u867d\u7136\u6539\u5584\u4e86\u5019\u9009\u6392\u5e8f\u4f46\u7f3a\u4e4f\u5bf9\u53bb\u566a\u5d4c\u5165\u6821\u51c6\u7684\u76d1\u7763\u3002", "method": "NADEx\u5c06\u5b9e\u4f53\u3001\u5173\u7cfb\u548c\u65f6\u5e8f\u95f4\u9694\u7684\u4e3b\u4f53\u4e2d\u5fc3\u5386\u53f2\u7f16\u7801\u4e3a\u5e8f\u5217\u5d4c\u5165\uff0c\u5728\u524d\u5411\u8fc7\u7a0b\u4e2d\u6270\u52a8\u67e5\u8be2\u5bf9\u8c61\uff0c\u5728\u53cd\u5411\u8fc7\u7a0b\u4e2d\u4f7f\u7528Transformer\u53bb\u566a\u5668\u57fa\u4e8e\u65f6\u5e8f\u5173\u7cfb\u4e0a\u4e0b\u6587\u8fdb\u884c\u91cd\u5efa\u3002\u5f15\u5165\u57fa\u4e8e\u6279\u8d1f\u6837\u672c\u539f\u578b\u7684\u4f59\u5f26\u5bf9\u9f50\u6b63\u5219\u5316\u5668\u6765\u6536\u7d27\u51b3\u7b56\u8fb9\u754c\u3002", "result": "\u5728\u56db\u4e2a\u516c\u5171\u65f6\u5e8f\u77e5\u8bc6\u56fe\u8c31\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u7efc\u5408\u5b9e\u9a8c\u8868\u660e\uff0cNADEx\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "NADEx\u901a\u8fc7\u7ed3\u5408\u8d1f\u6837\u672c\u611f\u77e5\u548c\u6821\u51c6\u76d1\u7763\uff0c\u6709\u6548\u6539\u8fdb\u4e86\u65f6\u5e8f\u77e5\u8bc6\u56fe\u8c31\u63a8\u7406\u4e2d\u7684\u6269\u6563\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2602.07872", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.07872", "abs": "https://arxiv.org/abs/2602.07872", "authors": ["Mert Sonmezer", "Serge Vasylechko", "Duygu Atasoy", "Seyda Ertekin", "Sila Kurugol"], "title": "WristMIR: Coarse-to-Fine Region-Aware Retrieval of Pediatric Wrist Radiographs with Radiology Report-Driven Learning", "comment": null, "summary": "Retrieving wrist radiographs with analogous fracture patterns is challenging because clinically important cues are subtle, highly localized and often obscured by overlapping anatomy or variable imaging views. Progress is further limited by the scarcity of large, well-annotated datasets for case-based medical image retrieval. We introduce WristMIR, a region-aware pediatric wrist radiograph retrieval framework that leverages dense radiology reports and bone-specific localization to learn fine-grained, clinically meaningful image representations without any manual image-level annotations. Using MedGemma-based structured report mining to generate both global and region-level captions, together with pre-processed wrist images and bone-specific crops of the distal radius, distal ulna, and ulnar styloid, WristMIR jointly trains global and local contrastive encoders and performs a two-stage retrieval process: (1) coarse global matching to identify candidate exams, followed by (2) region-conditioned reranking aligned to a predefined anatomical bone region. WristMIR improves retrieval performance over strong vision-language baselines, raising image-to-text Recall@5 from 0.82% to 9.35%. Its embeddings also yield stronger fracture classification (AUROC 0.949, AUPRC 0.953). In region-aware evaluation, the two-stage design markedly improves retrieval-based fracture diagnosis, increasing mean $F_1$ from 0.568 to 0.753, and radiologists rate its retrieved cases as more clinically relevant, with mean scores rising from 3.36 to 4.35. These findings highlight the potential of anatomically guided retrieval to enhance diagnostic reasoning and support clinical decision-making in pediatric musculoskeletal imaging. The source code is publicly available at https://github.com/quin-med-harvard-edu/WristMIR.", "AI": {"tldr": "WristMIR\uff1a\u4e00\u79cd\u57fa\u4e8e\u533a\u57df\u611f\u77e5\u7684\u513f\u79d1\u8155\u90e8X\u5149\u7247\u68c0\u7d22\u6846\u67b6\uff0c\u901a\u8fc7\u5168\u5c40\u548c\u5c40\u90e8\u5bf9\u6bd4\u5b66\u4e60\u63d0\u5347\u9aa8\u6298\u6a21\u5f0f\u68c0\u7d22\u6027\u80fd", "motivation": "\u8155\u90e8X\u5149\u7247\u4e2d\u9aa8\u6298\u6a21\u5f0f\u7684\u68c0\u7d22\u5177\u6709\u6311\u6218\u6027\uff0c\u56e0\u4e3a\u4e34\u5e8a\u91cd\u8981\u7ebf\u7d22\u7ec6\u5fae\u3001\u9ad8\u5ea6\u5c40\u90e8\u5316\uff0c\u5e38\u88ab\u91cd\u53e0\u89e3\u5256\u7ed3\u6784\u6216\u4e0d\u540c\u6210\u50cf\u89c6\u89d2\u6240\u63a9\u76d6\u3002\u73b0\u6709\u65b9\u6cd5\u53d7\u9650\u4e8e\u7f3a\u4e4f\u5927\u89c4\u6a21\u3001\u9ad8\u8d28\u91cf\u6807\u6ce8\u7684\u533b\u5b66\u56fe\u50cf\u68c0\u7d22\u6570\u636e\u96c6\u3002", "method": "\u63d0\u51faWristMIR\u6846\u67b6\uff1a1\uff09\u4f7f\u7528MedGemma\u7ed3\u6784\u5316\u62a5\u544a\u6316\u6398\u751f\u6210\u5168\u5c40\u548c\u533a\u57df\u7ea7\u63cf\u8ff0\uff1b2\uff09\u9884\u5904\u7406\u8155\u90e8\u56fe\u50cf\u548c\u7279\u5b9a\u9aa8\u9abc\u533a\u57df\uff08\u8fdc\u7aef\u6861\u9aa8\u3001\u8fdc\u7aef\u5c3a\u9aa8\u3001\u5c3a\u9aa8\u830e\u7a81\uff09\u88c1\u526a\uff1b3\uff09\u8054\u5408\u8bad\u7ec3\u5168\u5c40\u548c\u5c40\u90e8\u5bf9\u6bd4\u7f16\u7801\u5668\uff1b4\uff09\u91c7\u7528\u4e24\u9636\u6bb5\u68c0\u7d22\uff1a\u7c97\u7c92\u5ea6\u5168\u5c40\u5339\u914d\u7b5b\u9009\u5019\u9009\u56fe\u50cf\uff0c\u7136\u540e\u57fa\u4e8e\u89e3\u5256\u533a\u57df\u7684\u533a\u57df\u6761\u4ef6\u91cd\u6392\u5e8f\u3002", "result": "\u663e\u8457\u63d0\u5347\u68c0\u7d22\u6027\u80fd\uff1a\u56fe\u50cf\u5230\u6587\u672cRecall@5\u4ece0.82%\u63d0\u5347\u81f39.35%\uff1b\u9aa8\u6298\u5206\u7c7b\u6027\u80fd\u589e\u5f3a\uff08AUROC 0.949\uff0cAUPRC 0.953\uff09\uff1b\u533a\u57df\u611f\u77e5\u8bc4\u4f30\u4e2d\uff0c\u4e24\u9636\u6bb5\u8bbe\u8ba1\u5c06\u9aa8\u6298\u8bca\u65ad\u7684\u5e73\u5747F1\u5206\u6570\u4ece0.568\u63d0\u5347\u81f30.753\uff1b\u653e\u5c04\u79d1\u533b\u751f\u8bc4\u4ef7\u5176\u68c0\u7d22\u75c5\u4f8b\u4e34\u5e8a\u76f8\u5173\u6027\u66f4\u9ad8\uff08\u5e73\u5747\u8bc4\u5206\u4ece3.36\u5347\u81f34.35\uff09\u3002", "conclusion": "\u89e3\u5256\u5f15\u5bfc\u7684\u68c0\u7d22\u65b9\u6cd5\u5728\u513f\u79d1\u808c\u8089\u9aa8\u9abc\u6210\u50cf\u4e2d\u5177\u6709\u589e\u5f3a\u8bca\u65ad\u63a8\u7406\u548c\u652f\u6301\u4e34\u5e8a\u51b3\u7b56\u7684\u6f5c\u529b\uff0cWristMIR\u6846\u67b6\u901a\u8fc7\u533a\u57df\u611f\u77e5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u8155\u90e8\u9aa8\u6298\u6a21\u5f0f\u7684\u68c0\u7d22\u6027\u80fd\u3002"}}
{"id": "2602.08835", "categories": ["cs.AI", "cs.CY", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.08835", "abs": "https://arxiv.org/abs/2602.08835", "authors": ["Andr\u00e9s Holgado-S\u00e1nchez", "Peter Vamplew", "Richard Dazeley", "Sascha Ossowski", "Holger Billhardt"], "title": "Learning the Value Systems of Societies with Preference-based Multi-objective Reinforcement Learning", "comment": "18 pages, 3 figures. To be published in proceedings of the 25th International Conference on Autonomous Agents and Multi-Agent Systems (AAMAS 2026). This is a full version that includes the supplementary material", "summary": "Value-aware AI should recognise human values and adapt to the value systems (value-based preferences) of different users. This requires operationalization of values, which can be prone to misspecification. The social nature of values demands their representation to adhere to multiple users while value systems are diverse, yet exhibit patterns among groups. In sequential decision making, efforts have been made towards personalization for different goals or values from demonstrations of diverse agents. However, these approaches demand manually designed features or lack value-based interpretability and/or adaptability to diverse user preferences.\n  We propose algorithms for learning models of value alignment and value systems for a society of agents in Markov Decision Processes (MDPs), based on clustering and preference-based multi-objective reinforcement learning (PbMORL). We jointly learn socially-derived value alignment models (groundings) and a set of value systems that concisely represent different groups of users (clusters) in a society. Each cluster consists of a value system representing the value-based preferences of its members and an approximately Pareto-optimal policy that reflects behaviours aligned with this value system. We evaluate our method against a state-of-the-art PbMORL algorithm and baselines on two MDPs with human values.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u805a\u7c7b\u548c\u504f\u597d\u591a\u76ee\u6807\u5f3a\u5316\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u5b66\u4e60\u793e\u4f1a\u667a\u80fd\u4f53\u4e2d\u7684\u4ef7\u503c\u5bf9\u9f50\u6a21\u578b\u548c\u4ef7\u503c\u7cfb\u7edf\uff0c\u4ee5\u89e3\u51b3\u4ef7\u503c\u64cd\u4f5c\u5316\u4e2d\u7684\u8bef\u89c4\u8303\u95ee\u9898\u3002", "motivation": "\u4ef7\u503c\u611f\u77e5AI\u9700\u8981\u8bc6\u522b\u4eba\u7c7b\u4ef7\u503c\u89c2\u5e76\u9002\u5e94\u4e0d\u540c\u7528\u6237\u7684\u4ef7\u503c\u7cfb\u7edf\uff0c\u4f46\u4ef7\u503c\u64cd\u4f5c\u5316\u5bb9\u6613\u4ea7\u751f\u8bef\u89c4\u8303\u3002\u4ef7\u503c\u89c2\u7684\u793e\u4f1a\u6027\u8981\u6c42\u5176\u8868\u793a\u80fd\u540c\u65f6\u9002\u5e94\u591a\u4e2a\u7528\u6237\uff0c\u800c\u4ef7\u503c\u7cfb\u7edf\u867d\u7136\u591a\u6837\u4f46\u5b58\u5728\u7fa4\u4f53\u6a21\u5f0f\u3002\u73b0\u6709\u5e8f\u5217\u51b3\u7b56\u65b9\u6cd5\u867d\u7136\u5c1d\u8bd5\u4e3a\u4e0d\u540c\u76ee\u6807\u6216\u4ef7\u503c\u8fdb\u884c\u4e2a\u6027\u5316\uff0c\u4f46\u9700\u8981\u624b\u52a8\u8bbe\u8ba1\u7279\u5f81\u6216\u7f3a\u4e4f\u57fa\u4e8e\u4ef7\u503c\u7684\u53ef\u89e3\u91ca\u6027\u548c\u9002\u5e94\u6027\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u805a\u7c7b\u548c\u504f\u597d\u591a\u76ee\u6807\u5f3a\u5316\u5b66\u4e60(PbMORL)\u7684\u7b97\u6cd5\uff0c\u5728\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\u4e2d\u5b66\u4e60\u4ef7\u503c\u5bf9\u9f50\u6a21\u578b\u548c\u4ef7\u503c\u7cfb\u7edf\u3002\u8054\u5408\u5b66\u4e60\u793e\u4f1a\u884d\u751f\u7684\u4ef7\u503c\u5bf9\u9f50\u6a21\u578b(groundings)\u548c\u4e00\u7ec4\u7b80\u6d01\u8868\u793a\u793e\u4f1a\u4e2d\u4e0d\u540c\u7528\u6237\u7fa4\u4f53\u7684\u4ef7\u503c\u7cfb\u7edf\u3002\u6bcf\u4e2a\u805a\u7c7b\u5305\u542b\u4ee3\u8868\u5176\u6210\u5458\u4ef7\u503c\u504f\u597d\u7684\u4ef7\u503c\u7cfb\u7edf\uff0c\u4ee5\u53ca\u53cd\u6620\u4e0e\u8be5\u4ef7\u503c\u7cfb\u7edf\u5bf9\u9f50\u884c\u4e3a\u7684\u8fd1\u4f3c\u5e15\u7d2f\u6258\u6700\u4f18\u7b56\u7565\u3002", "result": "\u5728\u4e24\u4e2a\u5305\u542b\u4eba\u7c7b\u4ef7\u503c\u7684MDP\u4e0a\u8bc4\u4f30\u4e86\u8be5\u65b9\u6cd5\uff0c\u4e0e\u6700\u5148\u8fdb\u7684PbMORL\u7b97\u6cd5\u548c\u57fa\u7ebf\u8fdb\u884c\u6bd4\u8f83\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u5b66\u4e60\u793e\u4f1a\u667a\u80fd\u4f53\u7684\u4ef7\u503c\u5bf9\u9f50\u6a21\u578b\u548c\u4ef7\u503c\u7cfb\u7edf\uff0c\u89e3\u51b3\u4e86\u4ef7\u503c\u64cd\u4f5c\u5316\u4e2d\u7684\u8bef\u89c4\u8303\u95ee\u9898\uff0c\u540c\u65f6\u9002\u5e94\u591a\u6837\u5316\u7684\u7528\u6237\u504f\u597d\uff0c\u5e76\u63d0\u4f9b\u4e86\u57fa\u4e8e\u4ef7\u503c\u7684\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2602.07891", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07891", "abs": "https://arxiv.org/abs/2602.07891", "authors": ["Zihui Gao", "Ke Liu", "Donny Y. Chen", "Duochao Shi", "Guosheng Lin", "Hao Chen", "Chunhua Shen"], "title": "Scalable Adaptation of 3D Geometric Foundation Models via Weak Supervision from Internet Video", "comment": null, "summary": "Geometric foundation models show promise in 3D reconstruction, yet their progress is severely constrained by the scarcity of diverse, large-scale 3D annotations. While Internet videos offer virtually unlimited raw data, utilizing them as a scaling source for geometric learning is challenging due to the absence of ground-truth geometry and the presence of observational noise. To address this, we propose SAGE, a framework for Scalable Adaptation of GEometric foundation models from raw video streams. SAGE leverages a hierarchical mining pipeline to transform videos into training trajectories and hybrid supervision: (1) Informative training trajectory selection; (2) Sparse Geometric Anchoring via SfM point clouds for global structural guidance; and (3) Dense Differentiable Consistency via 3D Gaussian rendering for multi-view constraints. To prevent catastrophic forgetting, we introduce a regularization strategy using anchor data. Extensive experiments show that SAGE significantly enhances zero-shot generalization, reducing Chamfer Distance by 20-42% on unseen benchmarks (7Scenes, TUM-RGBD, Matterport3D) compared to state-of-the-art baselines. To our knowledge, SAGE pioneers the adaptation of geometric foundation models via Internet video, establishing a scalable paradigm for general-purpose 3D learning.", "AI": {"tldr": "SAGE\u6846\u67b6\u901a\u8fc7\u4e92\u8054\u7f51\u89c6\u9891\u6d41\u81ea\u9002\u5e94\u51e0\u4f55\u57fa\u7840\u6a21\u578b\uff0c\u89e3\u51b33D\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b", "motivation": "\u51e0\u4f55\u57fa\u7840\u6a21\u578b\u57283D\u91cd\u5efa\u65b9\u9762\u6709\u6f5c\u529b\uff0c\u4f46\u53d7\u5230\u591a\u6837\u5316\u5927\u89c4\u6a213D\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u7684\u9650\u5236\u3002\u4e92\u8054\u7f51\u89c6\u9891\u63d0\u4f9b\u51e0\u4e4e\u65e0\u9650\u7684\u539f\u59cb\u6570\u636e\uff0c\u4f46\u7531\u4e8e\u7f3a\u4e4f\u771f\u5b9e\u51e0\u4f55\u4fe1\u606f\u548c\u5b58\u5728\u89c2\u6d4b\u566a\u58f0\uff0c\u5c06\u5176\u4f5c\u4e3a\u51e0\u4f55\u5b66\u4e60\u7684\u6269\u5c55\u6e90\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u63d0\u51faSAGE\u6846\u67b6\uff0c\u91c7\u7528\u5206\u5c42\u6316\u6398\u7ba1\u9053\u5c06\u89c6\u9891\u8f6c\u6362\u4e3a\u8bad\u7ec3\u8f68\u8ff9\u548c\u6df7\u5408\u76d1\u7763\uff1a(1)\u4fe1\u606f\u4e30\u5bcc\u7684\u8bad\u7ec3\u8f68\u8ff9\u9009\u62e9\uff1b(2)\u901a\u8fc7SfM\u70b9\u4e91\u8fdb\u884c\u7a00\u758f\u51e0\u4f55\u951a\u5b9a\u63d0\u4f9b\u5168\u5c40\u7ed3\u6784\u6307\u5bfc\uff1b(3)\u901a\u8fc73D\u9ad8\u65af\u6e32\u67d3\u8fdb\u884c\u5bc6\u96c6\u53ef\u5fae\u4e00\u81f4\u6027\u63d0\u4f9b\u591a\u89c6\u56fe\u7ea6\u675f\u3002\u4e3a\u9632\u6b62\u707e\u96be\u6027\u9057\u5fd8\uff0c\u5f15\u5165\u57fa\u4e8e\u951a\u5b9a\u6570\u636e\u7684\u6b63\u5219\u5316\u7b56\u7565\u3002", "result": "\u5728\u672a\u89c1\u8fc7\u7684\u57fa\u51c6\u6d4b\u8bd5\uff087Scenes\u3001TUM-RGBD\u3001Matterport3D\uff09\u4e0a\uff0cSAGE\u663e\u8457\u589e\u5f3a\u4e86\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\uff0c\u4e0e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u76f8\u6bd4\uff0c\u5c06Chamfer\u8ddd\u79bb\u51cf\u5c11\u4e8620-42%\u3002", "conclusion": "SAGE\u5f00\u521b\u4e86\u901a\u8fc7\u4e92\u8054\u7f51\u89c6\u9891\u81ea\u9002\u5e94\u51e0\u4f55\u57fa\u7840\u6a21\u578b\u7684\u5148\u6cb3\uff0c\u4e3a\u901a\u75283D\u5b66\u4e60\u5efa\u7acb\u4e86\u53ef\u6269\u5c55\u7684\u8303\u5f0f\u3002"}}
{"id": "2602.08848", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08848", "abs": "https://arxiv.org/abs/2602.08848", "authors": ["Quentin Cohen-Solal", "Alexandre Niveau", "Maroua Bouzid"], "title": "Deciding the Satisfiability of Combined Qualitative Constraint Networks", "comment": null, "summary": "Among the various forms of reasoning studied in the context of artificial intelligence, qualitative reasoning makes it possible to infer new knowledge in the context of imprecise, incomplete information without numerical values. In this paper, we propose a formal framework unifying several forms of extensions and combinations of qualitative formalisms, including multi-scale reasoning, temporal sequences, and loose integrations. This framework makes it possible to reason in the context of each of these combinations and extensions, but also to study in a unified way the satisfiability decision and its complexity. In particular, we establish two complementary theorems guaranteeing that the satisfiability decision is polynomial, and we use them to recover the known results of the size-topology combination. We also generalize the main definition of qualitative formalism to include qualitative formalisms excluded from the definitions of the literature, important in the context of combinations.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u7edf\u4e00\u6846\u67b6\uff0c\u6574\u5408\u591a\u79cd\u5b9a\u6027\u5f62\u5f0f\u5316\u7684\u6269\u5c55\u4e0e\u7ec4\u5408\uff0c\u5305\u62ec\u591a\u5c3a\u5ea6\u63a8\u7406\u3001\u65f6\u95f4\u5e8f\u5217\u548c\u677e\u6563\u96c6\u6210\uff0c\u5e76\u7814\u7a76\u5176\u53ef\u6ee1\u8db3\u6027\u5224\u5b9a\u53ca\u590d\u6742\u5ea6\u3002", "motivation": "\u5728\u4eba\u5de5\u667a\u80fd\u63a8\u7406\u4e2d\uff0c\u5b9a\u6027\u63a8\u7406\u80fd\u591f\u5728\u4fe1\u606f\u4e0d\u7cbe\u786e\u3001\u4e0d\u5b8c\u6574\u4e14\u65e0\u6570\u503c\u7684\u60c5\u51b5\u4e0b\u63a8\u65ad\u65b0\u77e5\u8bc6\u3002\u73b0\u6709\u5b9a\u6027\u5f62\u5f0f\u5316\u7684\u6269\u5c55\u548c\u7ec4\u5408\u5f62\u5f0f\u591a\u6837\uff0c\u4f46\u7f3a\u4e4f\u7edf\u4e00\u6846\u67b6\u6765\u7cfb\u7edf\u7814\u7a76\u8fd9\u4e9b\u7ec4\u5408\u7684\u53ef\u6ee1\u8db3\u6027\u5224\u5b9a\u53ca\u5176\u590d\u6742\u5ea6\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u5f62\u5f0f\u5316\u6846\u67b6\uff0c\u7edf\u4e00\u591a\u79cd\u5b9a\u6027\u5f62\u5f0f\u5316\u7684\u6269\u5c55\u548c\u7ec4\u5408\uff0c\u5305\u62ec\u591a\u5c3a\u5ea6\u63a8\u7406\u3001\u65f6\u95f4\u5e8f\u5217\u548c\u677e\u6563\u96c6\u6210\u3002\u8be5\u6846\u67b6\u652f\u6301\u5728\u8fd9\u4e9b\u7ec4\u5408\u548c\u6269\u5c55\u4e2d\u8fdb\u884c\u63a8\u7406\uff0c\u5e76\u4ee5\u7edf\u4e00\u65b9\u5f0f\u7814\u7a76\u53ef\u6ee1\u8db3\u6027\u5224\u5b9a\u53ca\u5176\u590d\u6742\u5ea6\u3002\u7279\u522b\u5730\uff0c\u5efa\u7acb\u4e86\u4e24\u4e2a\u4e92\u8865\u5b9a\u7406\u6765\u4fdd\u8bc1\u53ef\u6ee1\u8db3\u6027\u5224\u5b9a\u662f\u591a\u9879\u5f0f\u65f6\u95f4\u7684\u3002", "result": "\u5efa\u7acb\u4e86\u4e24\u4e2a\u4e92\u8865\u5b9a\u7406\uff0c\u4fdd\u8bc1\u53ef\u6ee1\u8db3\u6027\u5224\u5b9a\u662f\u591a\u9879\u5f0f\u65f6\u95f4\u7684\u3002\u5229\u7528\u8fd9\u4e9b\u5b9a\u7406\u6062\u590d\u4e86\u5c3a\u5bf8-\u62d3\u6251\u7ec4\u5408\u7684\u5df2\u77e5\u7ed3\u679c\u3002\u8fd8\u5c06\u5b9a\u6027\u5f62\u5f0f\u5316\u7684\u4e3b\u8981\u5b9a\u4e49\u63a8\u5e7f\u5230\u5305\u542b\u6587\u732e\u5b9a\u4e49\u4e2d\u6392\u9664\u4f46\u5728\u7ec4\u5408\u80cc\u666f\u4e0b\u91cd\u8981\u7684\u5b9a\u6027\u5f62\u5f0f\u5316\u3002", "conclusion": "\u63d0\u51fa\u7684\u7edf\u4e00\u6846\u67b6\u80fd\u591f\u7cfb\u7edf\u5904\u7406\u591a\u79cd\u5b9a\u6027\u5f62\u5f0f\u5316\u7684\u6269\u5c55\u548c\u7ec4\u5408\uff0c\u4e3a\u7814\u7a76\u8fd9\u4e9b\u7ec4\u5408\u7684\u53ef\u6ee1\u8db3\u6027\u5224\u5b9a\u548c\u590d\u6742\u5ea6\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\uff0c\u5e76\u6269\u5c55\u4e86\u5b9a\u6027\u5f62\u5f0f\u5316\u7684\u5b9a\u4e49\u8303\u56f4\u3002"}}
{"id": "2602.07899", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.07899", "abs": "https://arxiv.org/abs/2602.07899", "authors": ["Zhenhao Shang", "Haizhao Jing", "Guoting Wei", "Haokui Zhang", "Rong Xiao", "Jianqing Gao", "Peng Wang"], "title": "Rethinking Practical and Efficient Quantization Calibration for Vision-Language Models", "comment": null, "summary": "Post-training quantization (PTQ) is a primary approach for deploying large language models without fine-tuning, and the quantized performance is often strongly affected by the calibration in PTQ. By contrast, in vision-language models (VLMs), substantial differences between visual and text tokens in their activation distributions and sensitivities to quantization error pose significant challenges for effective calibration during PTQ. In this work, we rethink what PTQ calibration should align with in VLMs and propose the Token-level Importance-aware Layer-wise Quantization framework (TLQ). Guided by gradient information, we design a token-level importance integration mechanism for quantization error, and use it to construct a token-level calibration set, enabling a more fine-grained calibration strategy. Furthermore, TLQ introduces a multi-GPU, quantization-exposed layer-wise calibration scheme. This scheme keeps the layer-wise calibration procedure consistent with the true quantized inference path and distributes the complex layer-wise calibration workload across multiple RTX3090 GPUs, thereby reducing reliance on the large memory of A100 GPUs. TLQ is evaluated across two models, three model scales, and two quantization settings, consistently achieving performance improvements across all settings, indicating its strong quantization stability. The code will be released publicly.", "AI": {"tldr": "TLQ\u662f\u4e00\u4e2a\u9488\u5bf9\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684token\u7ea7\u91cd\u8981\u6027\u611f\u77e5\u5c42\u91cf\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u68af\u5ea6\u5f15\u5bfc\u7684token\u91cd\u8981\u6027\u96c6\u6210\u673a\u5236\u548c\u591aGPU\u5c42\u6821\u51c6\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u91cf\u5316\u6027\u80fd", "motivation": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u89c6\u89c9token\u548c\u6587\u672ctoken\u5728\u6fc0\u6d3b\u5206\u5e03\u548c\u91cf\u5316\u8bef\u5dee\u654f\u611f\u6027\u65b9\u9762\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u7ed9PTQ\u6821\u51c6\u5e26\u6765\u91cd\u5927\u6311\u6218\uff0c\u9700\u8981\u91cd\u65b0\u601d\u8003VLM\u4e2d\u7684\u6821\u51c6\u7b56\u7565", "method": "\u63d0\u51faTLQ\u6846\u67b6\uff1a1) \u57fa\u4e8e\u68af\u5ea6\u4fe1\u606f\u7684token\u7ea7\u91cd\u8981\u6027\u96c6\u6210\u673a\u5236\uff0c\u6784\u5efatoken\u7ea7\u6821\u51c6\u96c6\uff1b2) \u591aGPU\u3001\u91cf\u5316\u66b4\u9732\u7684\u5c42\u6821\u51c6\u65b9\u6848\uff0c\u4fdd\u6301\u6821\u51c6\u4e0e\u771f\u5b9e\u91cf\u5316\u63a8\u7406\u8def\u5f84\u4e00\u81f4\uff0c\u5e76\u5206\u5e03\u5f0f\u5904\u7406\u6821\u51c6\u5de5\u4f5c\u8d1f\u8f7d", "result": "\u5728\u4e24\u4e2a\u6a21\u578b\u3001\u4e09\u4e2a\u6a21\u578b\u89c4\u6a21\u548c\u4e24\u79cd\u91cf\u5316\u8bbe\u7f6e\u4e0b\u8fdb\u884c\u8bc4\u4f30\uff0c\u5728\u6240\u6709\u8bbe\u7f6e\u4e2d\u5747\u5b9e\u73b0\u6027\u80fd\u63d0\u5347\uff0c\u663e\u793a\u51fa\u5f3a\u5927\u7684\u91cf\u5316\u7a33\u5b9a\u6027", "conclusion": "TLQ\u901a\u8fc7\u7ec6\u7c92\u5ea6\u7684token\u7ea7\u6821\u51c6\u7b56\u7565\u548c\u9ad8\u6548\u7684\u591aGPU\u5c42\u6821\u51c6\u65b9\u6848\uff0c\u6709\u6548\u89e3\u51b3\u4e86VLM\u91cf\u5316\u4e2d\u7684\u6821\u51c6\u6311\u6218\uff0c\u4e3a\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u90e8\u7f72\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u540e\u8bad\u7ec3\u91cf\u5316\u89e3\u51b3\u65b9\u6848"}}
{"id": "2602.07931", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.07931", "abs": "https://arxiv.org/abs/2602.07931", "authors": ["Olena Hrynenko", "Darya Baranouskaya", "Alina Elena Baia", "Andrea Cavallaro"], "title": "Which private attributes do VLMs agree on and predict well?", "comment": "This work has been accepted to the ICASSP 2026", "summary": "Visual Language Models (VLMs) are often used for zero-shot detection of visual attributes in the image. We present a zero-shot evaluation of open-source VLMs for privacy-related attribute recognition. We identify the attributes for which VLMs exhibit strong inter-annotator agreement, and discuss the disagreement cases of human and VLM annotations. Our results show that when evaluated against human annotations, VLMs tend to predict the presence of privacy attributes more often than human annotators. In addition to this, we find that in cases of high inter-annotator agreement between VLMs, they can complement human annotation by identifying attributes overlooked by human annotators. This highlights the potential of VLMs to support privacy annotations in large-scale image datasets.", "AI": {"tldr": "\u8be5\u7814\u7a76\u8bc4\u4f30\u4e86\u5f00\u6e90\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u9690\u79c1\u76f8\u5173\u5c5e\u6027\u8bc6\u522b\u4e2d\u7684\u96f6\u6837\u672c\u8868\u73b0\uff0c\u53d1\u73b0VLM\u503e\u5411\u4e8e\u6bd4\u4eba\u7c7b\u6807\u6ce8\u8005\u66f4\u9891\u7e41\u5730\u9884\u6d4b\u9690\u79c1\u5c5e\u6027\u5b58\u5728\uff0c\u4f46\u5728\u9ad8\u4e00\u81f4\u6027\u60c5\u51b5\u4e0b\u53ef\u4ee5\u8865\u5145\u4eba\u7c7b\u6807\u6ce8\u7684\u4e0d\u8db3\u3002", "motivation": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5e38\u7528\u4e8e\u56fe\u50cf\u89c6\u89c9\u5c5e\u6027\u7684\u96f6\u6837\u672c\u68c0\u6d4b\uff0c\u4f46\u5bf9\u5176\u5728\u9690\u79c1\u76f8\u5173\u5c5e\u6027\u8bc6\u522b\u65b9\u9762\u7684\u8868\u73b0\u7f3a\u4e4f\u7cfb\u7edf\u8bc4\u4f30\u3002\u672c\u7814\u7a76\u65e8\u5728\u8bc4\u4f30\u5f00\u6e90VLM\u5728\u9690\u79c1\u5c5e\u6027\u8bc6\u522b\u4e2d\u7684\u96f6\u6837\u672c\u80fd\u529b\uff0c\u63a2\u7d22VLM\u4e0e\u4eba\u7c7b\u6807\u6ce8\u7684\u4e00\u81f4\u6027\u5dee\u5f02\u3002", "method": "\u91c7\u7528\u96f6\u6837\u672c\u8bc4\u4f30\u65b9\u6cd5\uff0c\u4f7f\u7528\u5f00\u6e90\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u9690\u79c1\u76f8\u5173\u5c5e\u6027\u8bc6\u522b\u3002\u901a\u8fc7\u5206\u6790VLM\u4e0e\u4eba\u7c7b\u6807\u6ce8\u8005\u4e4b\u95f4\u7684\u6807\u6ce8\u4e00\u81f4\u6027\uff0c\u8bc6\u522bVLM\u8868\u73b0\u51fa\u9ad8\u5185\u90e8\u4e00\u81f4\u6027\u7684\u5c5e\u6027\uff0c\u5e76\u8ba8\u8bbaVLM\u4e0e\u4eba\u7c7b\u6807\u6ce8\u4e4b\u95f4\u7684\u5206\u6b67\u60c5\u51b5\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff1a1) \u4e0e\u4eba\u7c7b\u6807\u6ce8\u76f8\u6bd4\uff0cVLM\u503e\u5411\u4e8e\u66f4\u9891\u7e41\u5730\u9884\u6d4b\u9690\u79c1\u5c5e\u6027\u7684\u5b58\u5728\uff1b2) \u5728VLM\u5185\u90e8\u4e00\u81f4\u6027\u9ad8\u7684\u60c5\u51b5\u4e0b\uff0cVLM\u80fd\u591f\u8bc6\u522b\u4eba\u7c7b\u6807\u6ce8\u8005\u5ffd\u7565\u7684\u5c5e\u6027\uff1b3) VLM\u4e0e\u4eba\u7c7b\u6807\u6ce8\u5b58\u5728\u7cfb\u7edf\u6027\u7684\u5206\u6b67\u6a21\u5f0f\u3002", "conclusion": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u9690\u79c1\u5c5e\u6027\u8bc6\u522b\u65b9\u9762\u5177\u6709\u6f5c\u529b\uff0c\u7279\u522b\u662f\u5728\u5927\u89c4\u6a21\u56fe\u50cf\u6570\u636e\u96c6\u7684\u9690\u79c1\u6807\u6ce8\u652f\u6301\u65b9\u9762\u3002VLM\u53ef\u4ee5\u8865\u5145\u4eba\u7c7b\u6807\u6ce8\u7684\u4e0d\u8db3\uff0c\u4f46\u9700\u8981\u6ce8\u610f\u5176\u4e0e\u4eba\u7c7b\u6807\u6ce8\u7684\u7cfb\u7edf\u6027\u5dee\u5f02\u3002"}}
{"id": "2602.08905", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08905", "abs": "https://arxiv.org/abs/2602.08905", "authors": ["Jiawei Liu", "Xiting Wang", "Yuanyuan Zhong", "Defu Lian", "Yu Yang"], "title": "Efficient and Stable Reinforcement Learning for Diffusion Language Models", "comment": "13 pages, 3 figures", "summary": "Reinforcement Learning (RL) is crucial for unlocking the complex reasoning capabilities of Diffusion-based Large Language Models (dLLMs). However, applying RL to dLLMs faces unique challenges in efficiency and stability. To address these challenges, we propose Spatio-Temporal Pruning (STP), a framework designed to simultaneously improve the efficiency and stability of RL for dLLMs. STP compresses the redundancy in the generative process through: (1) \\textit{spatial pruning}, which constrains the exploration space using static priors; and (2) \\textit{temporal pruning}, which bypasses redundant late-stage refinement steps. Our theoretical analysis demonstrates that STP strictly reduces the variance of the log-likelihood estimation, thereby ensuring more stable policy updates. Extensive experiments demonstrate that STP surpasses state-of-the-art baselines in both efficiency and accuracy. Our code is available at https://github.com/Lolo1222/STP.", "AI": {"tldr": "\u63d0\u51faSTP\u6846\u67b6\uff0c\u901a\u8fc7\u7a7a\u95f4\u526a\u679d\u548c\u65f6\u95f4\u526a\u679d\u63d0\u9ad8\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b\u5f3a\u5316\u5b66\u4e60\u7684\u6548\u7387\u548c\u7a33\u5b9a\u6027", "motivation": "\u5f3a\u5316\u5b66\u4e60\u5bf9\u4e8e\u91ca\u653e\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b\u7684\u590d\u6742\u63a8\u7406\u80fd\u529b\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5e94\u7528\u4e8edLLMs\u65f6\u9762\u4e34\u6548\u7387\u548c\u7a33\u5b9a\u6027\u65b9\u9762\u7684\u72ec\u7279\u6311\u6218", "method": "\u63d0\u51fa\u65f6\u7a7a\u526a\u679d\u6846\u67b6\uff0c\u5305\u542b\uff1a1\uff09\u7a7a\u95f4\u526a\u679d\uff1a\u4f7f\u7528\u9759\u6001\u5148\u9a8c\u7ea6\u675f\u63a2\u7d22\u7a7a\u95f4\uff1b2\uff09\u65f6\u95f4\u526a\u679d\uff1a\u7ed5\u8fc7\u5197\u4f59\u7684\u540e\u671f\u7ec6\u5316\u6b65\u9aa4", "result": "\u7406\u8bba\u5206\u6790\u8868\u660eSTP\u4e25\u683c\u964d\u4f4e\u4e86\u5bf9\u6570\u4f3c\u7136\u4f30\u8ba1\u7684\u65b9\u5dee\uff0c\u786e\u4fdd\u66f4\u7a33\u5b9a\u7684\u7b56\u7565\u66f4\u65b0\uff1b\u5b9e\u9a8c\u8bc1\u660eSTP\u5728\u6548\u7387\u548c\u51c6\u786e\u6027\u4e0a\u90fd\u8d85\u8d8a\u4e86\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5", "conclusion": "STP\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u6548\u7387\u548c\u7a33\u5b9a\u6027\u95ee\u9898\uff0c\u4e3adLLMs\u7684RL\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u4f18\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2602.08939", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08939", "abs": "https://arxiv.org/abs/2602.08939", "authors": ["Longling Geng", "Andy Ouyang", "Theodore Wu", "Daphne Barretto", "Matthew John Hayes", "Rachael Cooper", "Yuqiao Zeng", "Sameer Vijay", "Gia Ancone", "Ankit Rai", "Matthew Wolfman", "Patrick Flanagan", "Edward Y. Chang"], "title": "CausalT5K: Diagnosing and Informing Refusal for Trustworthy Causal Reasoning of Skepticism, Sycophancy, Detection-Correction, and Rung Collapse", "comment": "17 pages, 20 tables, figures", "summary": "LLM failures in causal reasoning, including sycophancy, rung collapse, and miscalibrated refusal, are well-documented, yet progress on remediation is slow because no benchmark enables systematic diagnosis. We introduce CausalT5K, a diagnostic benchmark of over 5,000 cases across 10 domains that tests three critical capabilities: (1) detecting rung collapse, where models answer interventional queries with associational evidence; (2) resisting sycophantic drift under adversarial pressure; and (3) generating Wise Refusals that specify missing information when evidence is underdetermined. Unlike synthetic benchmarks, CausalT5K embeds causal traps in realistic narratives and decomposes performance into Utility (sensitivity) and Safety (specificity), revealing failure modes invisible to aggregate accuracy. Developed through a rigorous human-machine collaborative pipeline involving 40 domain experts, iterative cross-validation cycles, and composite verification via rule-based, LLM, and human scoring, CausalT5K implements Pearl's Ladder of Causation as research infrastructure. Preliminary experiments reveal a Four-Quadrant Control Landscape where static audit policies universally fail, a finding that demonstrates CausalT5K's value for advancing trustworthy reasoning systems. Repository: https://github.com/genglongling/CausalT5kBench", "AI": {"tldr": "CausalT5K\u662f\u4e00\u4e2a\u5305\u542b5000\u591a\u4e2a\u6848\u4f8b\u7684\u8bca\u65ad\u57fa\u51c6\uff0c\u7528\u4e8e\u7cfb\u7edf\u68c0\u6d4bLLM\u5728\u56e0\u679c\u63a8\u7406\u4e2d\u7684\u4e09\u79cd\u5173\u952e\u80fd\u529b\u7f3a\u9677\uff1a\u9636\u68af\u574d\u584c\u3001\u8c04\u5a9a\u6027\u6f02\u79fb\u548c\u9519\u8bef\u62d2\u7edd\uff0c\u901a\u8fc7\u5b9e\u7528\u6027\u548c\u5b89\u5168\u6027\u6307\u6807\u63ed\u793a\u805a\u5408\u51c6\u786e\u7387\u65e0\u6cd5\u53d1\u73b0\u7684\u6545\u969c\u6a21\u5f0f\u3002", "motivation": "LLM\u5728\u56e0\u679c\u63a8\u7406\u4e2d\u5b58\u5728\u9636\u68af\u574d\u584c\u3001\u8c04\u5a9a\u6027\u548c\u9519\u8bef\u62d2\u7edd\u7b49\u5df2\u77e5\u7f3a\u9677\uff0c\u4f46\u7531\u4e8e\u7f3a\u4e4f\u7cfb\u7edf\u8bca\u65ad\u57fa\u51c6\uff0c\u4fee\u590d\u8fdb\u5c55\u7f13\u6162\u3002\u9700\u8981\u6784\u5efa\u4e00\u4e2a\u80fd\u591f\u7cfb\u7edf\u68c0\u6d4b\u8fd9\u4e9b\u6545\u969c\u6a21\u5f0f\u7684\u8bca\u65ad\u5de5\u5177\u3002", "method": "\u5f00\u53d1\u4e86CausalT5K\u57fa\u51c6\uff0c\u5305\u542b5000\u591a\u4e2a\u6848\u4f8b\uff0c\u8986\u76d610\u4e2a\u9886\u57df\uff0c\u5c06\u56e0\u679c\u9677\u9631\u5d4c\u5165\u73b0\u5b9e\u53d9\u4e8b\u4e2d\u3002\u91c7\u7528\u4eba\u673a\u534f\u4f5c\u6d41\u7a0b\uff0c\u6d89\u53ca40\u540d\u9886\u57df\u4e13\u5bb6\u3001\u8fed\u4ee3\u4ea4\u53c9\u9a8c\u8bc1\u5468\u671f\uff0c\u4ee5\u53ca\u57fa\u4e8e\u89c4\u5219\u3001LLM\u548c\u4eba\u5de5\u8bc4\u5206\u7684\u590d\u5408\u9a8c\u8bc1\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86Pearl\u7684\u56e0\u679c\u9636\u68af\u4f5c\u4e3a\u7814\u7a76\u57fa\u7840\u8bbe\u65bd\u3002", "result": "\u521d\u6b65\u5b9e\u9a8c\u63ed\u793a\u4e86\u56db\u8c61\u9650\u63a7\u5236\u666f\u89c2\uff0c\u5176\u4e2d\u9759\u6001\u5ba1\u8ba1\u7b56\u7565\u666e\u904d\u5931\u8d25\u3002\u57fa\u51c6\u80fd\u591f\u5206\u89e3\u6027\u80fd\u4e3a\u5b9e\u7528\u6027\uff08\u654f\u611f\u6027\uff09\u548c\u5b89\u5168\u6027\uff08\u7279\u5f02\u6027\uff09\uff0c\u63ed\u793a\u805a\u5408\u51c6\u786e\u7387\u65e0\u6cd5\u53d1\u73b0\u7684\u6545\u969c\u6a21\u5f0f\u3002", "conclusion": "CausalT5K\u4f5c\u4e3a\u7814\u7a76\u57fa\u7840\u8bbe\u65bd\uff0c\u4e3a\u63a8\u8fdb\u53ef\u4fe1\u63a8\u7406\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u8bca\u65ad\u5de5\u5177\uff0c\u80fd\u591f\u7cfb\u7edf\u68c0\u6d4bLLM\u5728\u56e0\u679c\u63a8\u7406\u4e2d\u7684\u5173\u952e\u7f3a\u9677\uff0c\u586b\u8865\u4e86\u73b0\u6709\u57fa\u51c6\u7684\u7a7a\u767d\u3002"}}
{"id": "2602.07955", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.07955", "abs": "https://arxiv.org/abs/2602.07955", "authors": ["Jiwei Chen", "Qi Wang", "Junyu Gao", "Jing Zhang", "Dingyi Li", "Jing-Jia Luo"], "title": "One-Shot Crowd Counting With Density Guidance For Scene Adaptaion", "comment": null, "summary": "Crowd scenes captured by cameras at different locations vary greatly, and existing crowd models have limited generalization for unseen surveillance scenes. To improve the generalization of the model, we regard different surveillance scenes as different category scenes, and introduce few-shot learning to make the model adapt to the unseen surveillance scene that belongs to the given exemplar category scene. To this end, we propose to leverage local and global density characteristics to guide the model of crowd counting for unseen surveillance scenes. Specifically, to enable the model to adapt to the varying density variations in the target scene, we propose the multiple local density learner to learn multi prototypes which represent different density distributions in the support scene. Subsequently, these multiple local density similarity matrixes are encoded. And they are utilized to guide the model in a local way. To further adapt to the global density in the target scene, the global density features are extracted from the support image, then it is used to guide the model in a global way. Experiments on three surveillance datasets shows that proposed method can adapt to the unseen surveillance scene and outperform recent state-of-the-art methods in the few-shot crowd counting.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5c11\u6837\u672c\u5b66\u4e60\u7684\u8de8\u573a\u666f\u4eba\u7fa4\u8ba1\u6570\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c40\u90e8\u548c\u5168\u5c40\u5bc6\u5ea6\u7279\u5f81\u5f15\u5bfc\u6a21\u578b\u9002\u5e94\u672a\u89c1\u8fc7\u7684\u76d1\u63a7\u573a\u666f", "motivation": "\u73b0\u6709\u7684\u4eba\u7fa4\u8ba1\u6570\u6a21\u578b\u5728\u4e0d\u540c\u76d1\u63a7\u573a\u666f\u95f4\u7684\u6cdb\u5316\u80fd\u529b\u6709\u9650\uff0c\u56e0\u4e3a\u4e0d\u540c\u6444\u50cf\u5934\u6355\u83b7\u7684\u4eba\u7fa4\u573a\u666f\u5dee\u5f02\u5f88\u5927\u3002\u4e3a\u4e86\u63d0\u9ad8\u6a21\u578b\u5bf9\u672a\u89c1\u8fc7\u7684\u76d1\u63a7\u573a\u666f\u7684\u9002\u5e94\u6027\uff0c\u9700\u8981\u8ba9\u6a21\u578b\u80fd\u591f\u57fa\u4e8e\u5c11\u91cf\u6837\u672c\u5feb\u901f\u9002\u5e94\u65b0\u573a\u666f\u3002", "method": "1) \u63d0\u51fa\u591a\u5c40\u90e8\u5bc6\u5ea6\u5b66\u4e60\u5668\uff0c\u5b66\u4e60\u652f\u6301\u573a\u666f\u4e2d\u4e0d\u540c\u5bc6\u5ea6\u5206\u5e03\u7684\u539f\u578b\uff0c\u751f\u6210\u5c40\u90e8\u5bc6\u5ea6\u76f8\u4f3c\u5ea6\u77e9\u9635\u8fdb\u884c\u5c40\u90e8\u6307\u5bfc\uff1b2) \u4ece\u652f\u6301\u56fe\u50cf\u4e2d\u63d0\u53d6\u5168\u5c40\u5bc6\u5ea6\u7279\u5f81\uff0c\u8fdb\u884c\u5168\u5c40\u6307\u5bfc\uff1b3) \u5c06\u5c40\u90e8\u548c\u5168\u5c40\u5bc6\u5ea6\u7279\u5f81\u7ed3\u5408\uff0c\u5f15\u5bfc\u6a21\u578b\u9002\u5e94\u76ee\u6807\u573a\u666f\u3002", "result": "\u5728\u4e09\u4e2a\u76d1\u63a7\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u9002\u5e94\u672a\u89c1\u8fc7\u7684\u76d1\u63a7\u573a\u666f\uff0c\u5728\u5c11\u6837\u672c\u4eba\u7fa4\u8ba1\u6570\u4efb\u52a1\u4e2d\u4f18\u4e8e\u5f53\u524d\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u5408\u5c40\u90e8\u548c\u5168\u5c40\u5bc6\u5ea6\u7279\u5f81\u7684\u5c11\u6837\u672c\u5b66\u4e60\u65b9\u6cd5\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347\u4eba\u7fa4\u8ba1\u6570\u6a21\u578b\u5bf9\u672a\u89c1\u76d1\u63a7\u573a\u666f\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4e3a\u89e3\u51b3\u8de8\u573a\u666f\u4eba\u7fa4\u8ba1\u6570\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\u3002"}}
{"id": "2602.08948", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.08948", "abs": "https://arxiv.org/abs/2602.08948", "authors": ["Chen Jin", "Ryutaro Tanno", "Tom Diethe", "Philip Teare"], "title": "CoRefine: Confidence-Guided Self-Refinement for Adaptive Test-Time Compute", "comment": null, "summary": "Large Language Models (LLMs) often rely on test-time scaling via parallel decoding (for example, 512 samples) to boost reasoning accuracy, but this incurs substantial compute. We introduce CoRefine, a confidence-guided self-refinement method that achieves competitive accuracy using a fraction of the tokens via a lightweight 211k-parameter Conv1D controller atop a frozen LLM. The controller consumes full-trace confidence to decide whether to halt, re-examine, or try a different approach, enabling targeted self-correction with an average of 2.7 refinement steps per problem and roughly 190-fold token reduction relative to 512-sample baselines. Across diverse reasoning benchmarks and three open-source models, the controller achieves 92.6 percent precision when it confidently halts, indicating that confidence dynamics reliably signal correctness without ground-truth verification. We extend this to CoRefine-Tree, a hybrid sequential-parallel variant that adaptively balances exploration and exploitation, with easy serving integration and verifier compatibility. By treating confidence as a control signal rather than a correctness guarantee, CoRefine provides a modular primitive for scalable reasoning and agentic settings with imperfect verifiers.", "AI": {"tldr": "CoRefine\u662f\u4e00\u79cd\u57fa\u4e8e\u7f6e\u4fe1\u5ea6\u5f15\u5bfc\u7684\u81ea\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u63a7\u5236\u5668\u52a8\u6001\u51b3\u5b9a\u4f55\u65f6\u505c\u6b62\u3001\u91cd\u65b0\u68c0\u67e5\u6216\u5c1d\u8bd5\u4e0d\u540c\u65b9\u6cd5\uff0c\u663e\u8457\u51cf\u5c11\u63a8\u7406\u6240\u9700\u7684\u8ba1\u7b97\u91cf\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u901a\u5e38\u4f9d\u8d56\u6d4b\u8bd5\u65f6\u5e76\u884c\u89e3\u7801\uff08\u5982512\u4e2a\u6837\u672c\uff09\u6765\u63d0\u9ad8\u63a8\u7406\u51c6\u786e\u6027\uff0c\u4f46\u8fd9\u4f1a\u5e26\u6765\u5de8\u5927\u7684\u8ba1\u7b97\u5f00\u9500\u3002\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u6765\u51cf\u5c11\u63a8\u7406\u6240\u9700\u7684\u8ba1\u7b97\u8d44\u6e90\u3002", "method": "CoRefine\u5728\u51bb\u7ed3\u7684LLM\u4e4b\u4e0a\u6dfb\u52a0\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684211k\u53c2\u6570Conv1D\u63a7\u5236\u5668\uff0c\u8be5\u63a7\u5236\u5668\u5229\u7528\u5b8c\u6574\u8ddf\u8e2a\u7f6e\u4fe1\u5ea6\u6765\u51b3\u5b9a\uff1a1) \u505c\u6b62\u63a8\u7406\uff0c2) \u91cd\u65b0\u68c0\u67e5\uff0c\u62163) \u5c1d\u8bd5\u4e0d\u540c\u65b9\u6cd5\u3002\u8fd8\u6269\u5c55\u4e86CoRefine-Tree\uff0c\u8fd9\u662f\u4e00\u79cd\u6df7\u5408\u987a\u5e8f-\u5e76\u884c\u53d8\u4f53\uff0c\u81ea\u9002\u5e94\u5730\u5e73\u8861\u63a2\u7d22\u548c\u5229\u7528\u3002", "result": "CoRefine\u5e73\u5747\u6bcf\u4e2a\u95ee\u9898\u53ea\u97002.7\u4e2a\u4f18\u5316\u6b65\u9aa4\uff0c\u76f8\u5bf9\u4e8e512\u6837\u672c\u57fa\u7ebf\u51cf\u5c11\u4e86\u7ea6190\u500d\u7684token\u4f7f\u7528\u91cf\u3002\u63a7\u5236\u5668\u5728\u81ea\u4fe1\u505c\u6b62\u65f6\u8fbe\u523092.6%\u7684\u7cbe\u786e\u5ea6\uff0c\u8868\u660e\u7f6e\u4fe1\u5ea6\u52a8\u6001\u53ef\u9760\u5730\u6307\u793a\u6b63\u786e\u6027\u800c\u65e0\u9700\u771f\u5b9e\u6807\u7b7e\u9a8c\u8bc1\u3002\u5728\u591a\u6837\u5316\u7684\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u548c\u4e09\u4e2a\u5f00\u6e90\u6a21\u578b\u4e0a\u90fd\u8868\u73b0\u826f\u597d\u3002", "conclusion": "\u901a\u8fc7\u5c06\u7f6e\u4fe1\u5ea6\u89c6\u4e3a\u63a7\u5236\u4fe1\u53f7\u800c\u975e\u6b63\u786e\u6027\u4fdd\u8bc1\uff0cCoRefine\u4e3a\u53ef\u6269\u5c55\u63a8\u7406\u548c\u4e0d\u5b8c\u7f8e\u9a8c\u8bc1\u5668\u7684\u667a\u80fd\u4f53\u8bbe\u7f6e\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6a21\u5757\u5316\u539f\u8bed\uff0c\u5b9e\u73b0\u4e86\u8ba1\u7b97\u6548\u7387\u4e0e\u63a8\u7406\u51c6\u786e\u6027\u7684\u826f\u597d\u5e73\u8861\u3002"}}
{"id": "2602.07960", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.07960", "abs": "https://arxiv.org/abs/2602.07960", "authors": ["Changli Tang", "Tianyi Wang", "Fengyun Rao", "Jing Lyu", "Chao Zhang"], "title": "D-ORCA: Dialogue-Centric Optimization for Robust Audio-Visual Captioning", "comment": null, "summary": "Spoken dialogue is a primary source of information in videos; therefore, accurately identifying who spoke what and when is essential for deep video understanding. We introduce D-ORCA, a \\textbf{d}ialogue-centric \\textbf{o}mni-modal large language model optimized for \\textbf{r}obust audio-visual \\textbf{ca}ptioning. We further curate DVD, a large-scale, high-quality bilingual dataset comprising nearly 40,000 multi-party dialogue videos for training and 2000 videos for evaluation in English and Mandarin, addressing a critical gap in the open-source ecosystem. To ensure fine-grained captioning accuracy, we adopt group relative policy optimization with three novel reward functions that assess speaker attribution accuracy, global speech content accuracy, and sentence-level temporal boundary alignment. These rewards are derived from evaluation metrics widely used in speech processing and, to our knowledge, are applied for the first time as reinforcement learning objectives for audio-visual captioning. Extensive experiments demonstrate that D-ORCA substantially outperforms existing open-source models in speaker identification, speech recognition, and temporal grounding. Notably, despite having only 8 billion parameters, D-ORCA achieves performance competitive with Qwen3-Omni across several general-purpose audio-visual understanding benchmarks. Demos are available at \\href{https://d-orca-llm.github.io/}{https://d-orca-llm.github.io/}. Our code, data, and checkpoints will be available at \\href{https://github.com/WeChatCV/D-ORCA/}{https://github.com/WeChatCV/D-ORCA/}.", "AI": {"tldr": "D-ORCA\u662f\u4e00\u4e2a\u9762\u5411\u5bf9\u8bdd\u7684\u8de8\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u4e13\u6ce8\u4e8e\u89c6\u9891\u4e2d\u7684\u8bf4\u8bdd\u4eba\u8bc6\u522b\u3001\u8bed\u97f3\u8bc6\u522b\u548c\u65f6\u95f4\u5b9a\u4f4d\u4efb\u52a1\uff0c\u5728\u53cc\u8bed\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u5e76\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u5f00\u6e90\u6a21\u578b\u3002", "motivation": "\u89c6\u9891\u4e2d\u7684\u5bf9\u8bdd\u662f\u91cd\u8981\u7684\u4fe1\u606f\u6765\u6e90\uff0c\u51c6\u786e\u8bc6\u522b\u8c01\u5728\u4f55\u65f6\u8bf4\u4e86\u4ec0\u4e48\u5bf9\u4e8e\u6df1\u5ea6\u89c6\u9891\u7406\u89e3\u81f3\u5173\u91cd\u8981\u3002\u76ee\u524d\u5f00\u6e90\u751f\u6001\u4e2d\u7f3a\u4e4f\u5927\u89c4\u6a21\u9ad8\u8d28\u91cf\u7684\u591a\u65b9\u5bf9\u8bdd\u89c6\u9891\u6570\u636e\u96c6\u548c\u4e13\u95e8\u9488\u5bf9\u5bf9\u8bdd\u7406\u89e3\u7684\u8de8\u6a21\u6001\u6a21\u578b\u3002", "method": "1. \u6784\u5efaDVD\u53cc\u8bed\u6570\u636e\u96c6\uff08\u8fd14\u4e07\u8bad\u7ec3\u89c6\u9891+2000\u8bc4\u4f30\u89c6\u9891\uff09\uff1b2. \u5f00\u53d1D-ORCA\u5bf9\u8bdd\u4e2d\u5fc3\u8de8\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff1b3. \u91c7\u7528\u7fa4\u4f53\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\uff0c\u5f15\u5165\u4e09\u4e2a\u65b0\u9896\u7684\u5956\u52b1\u51fd\u6570\uff1a\u8bf4\u8bdd\u4eba\u5f52\u5c5e\u51c6\u786e\u6027\u3001\u5168\u5c40\u8bed\u97f3\u5185\u5bb9\u51c6\u786e\u6027\u3001\u53e5\u5b50\u7ea7\u65f6\u95f4\u8fb9\u754c\u5bf9\u9f50\u3002", "result": "D-ORCA\u5728\u8bf4\u8bdd\u4eba\u8bc6\u522b\u3001\u8bed\u97f3\u8bc6\u522b\u548c\u65f6\u95f4\u5b9a\u4f4d\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u5f00\u6e90\u6a21\u578b\u3002\u5c3d\u7ba1\u53ea\u670980\u4ebf\u53c2\u6570\uff0c\u4f46\u5728\u591a\u4e2a\u901a\u7528\u97f3\u9891-\u89c6\u89c9\u7406\u89e3\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4e0eQwen3-Omni\u8868\u73b0\u76f8\u5f53\u3002", "conclusion": "D-ORCA\u901a\u8fc7\u4e13\u95e8\u7684\u6570\u636e\u96c6\u548c\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u7b56\u7565\uff0c\u4e3a\u89c6\u9891\u5bf9\u8bdd\u7406\u89e3\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u586b\u8865\u4e86\u5f00\u6e90\u751f\u6001\u4e2d\u7684\u5173\u952e\u7a7a\u767d\uff0c\u5e76\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u5c55\u73b0\u4e86\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2602.07967", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.07967", "abs": "https://arxiv.org/abs/2602.07967", "authors": ["Xiaofeng Tan", "Wanjiang Weng", "Haodong Lei", "Hongsong Wang"], "title": "EasyTune: Efficient Step-Aware Fine-Tuning for Diffusion-Based Motion Generation", "comment": null, "summary": "In recent years, motion generative models have undergone significant advancement, yet pose challenges in aligning with downstream objectives. Recent studies have shown that using differentiable rewards to directly align the preference of diffusion models yields promising results. However, these methods suffer from (1) inefficient and coarse-grained optimization with (2) high memory consumption. In this work, we first theoretically and empirically identify the key reason of these limitations: the recursive dependence between different steps in the denoising trajectory. Inspired by this insight, we propose EasyTune, which fine-tunes diffusion at each denoising step rather than over the entire trajectory. This decouples the recursive dependence, allowing us to perform (1) a dense and fine-grained, and (2) memory-efficient optimization. Furthermore, the scarcity of preference motion pairs restricts the availability of motion reward model training. To this end, we further introduce a Self-refinement Preference Learning (SPL) mechanism that dynamically identifies preference pairs and conducts preference learning. Extensive experiments demonstrate that EasyTune outperforms DRaFT-50 by 8.2% in alignment (MM-Dist) improvement while requiring only 31.16% of its additional memory overhead and achieving a 7.3x training speedup. The project page is available at this link {https://xiaofeng-tan.github.io/projects/EasyTune/index.html}.", "AI": {"tldr": "EasyTune\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u6269\u6563\u6a21\u578b\u7684\u9ad8\u6548\u504f\u597d\u5bf9\u9f50\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u6b65\u5fae\u8c03\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u4e2d\u7684\u9012\u5f52\u4f9d\u8d56\u95ee\u9898\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u5185\u5b58\u6d88\u8017\u5e76\u63d0\u5347\u4e86\u8bad\u7ec3\u901f\u5ea6\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u53ef\u5fae\u5206\u5956\u52b1\u7684\u6269\u6563\u6a21\u578b\u504f\u597d\u5bf9\u9f50\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a1\uff09\u4f18\u5316\u6548\u7387\u4f4e\u4e14\u7c92\u5ea6\u7c97\u7cd9\uff1b2\uff09\u5185\u5b58\u6d88\u8017\u9ad8\u3002\u7814\u7a76\u53d1\u73b0\u8fd9\u4e9b\u9650\u5236\u6e90\u4e8e\u53bb\u566a\u8f68\u8ff9\u4e2d\u4e0d\u540c\u6b65\u9aa4\u4e4b\u95f4\u7684\u9012\u5f52\u4f9d\u8d56\u5173\u7cfb\u3002", "method": "\u63d0\u51faEasyTune\u65b9\u6cd5\uff0c\u5728\u53bb\u566a\u8fc7\u7a0b\u7684\u6bcf\u4e2a\u6b65\u9aa4\u5206\u522b\u5fae\u8c03\u6269\u6563\u6a21\u578b\uff0c\u800c\u4e0d\u662f\u5728\u6574\u4e2a\u8f68\u8ff9\u4e0a\u8fdb\u884c\u4f18\u5316\uff0c\u4ece\u800c\u89e3\u8026\u9012\u5f52\u4f9d\u8d56\u3002\u6b64\u5916\uff0c\u9488\u5bf9\u504f\u597d\u8fd0\u52a8\u5bf9\u7a00\u7f3a\u7684\u95ee\u9898\uff0c\u5f15\u5165\u81ea\u7cbe\u5316\u504f\u597d\u5b66\u4e60\u673a\u5236\uff0c\u52a8\u6001\u8bc6\u522b\u504f\u597d\u5bf9\u5e76\u8fdb\u884c\u504f\u597d\u5b66\u4e60\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cEasyTune\u5728MM-Dist\u5bf9\u9f50\u6307\u6807\u4e0a\u6bd4DRaFT-50\u63d0\u5347\u4e868.2%\uff0c\u540c\u65f6\u4ec5\u9700\u517631.16%\u7684\u989d\u5916\u5185\u5b58\u5f00\u9500\uff0c\u5e76\u5b9e\u73b0\u4e867.3\u500d\u7684\u8bad\u7ec3\u52a0\u901f\u3002", "conclusion": "\u901a\u8fc7\u5206\u6b65\u5fae\u8c03\u7b56\u7565\u89e3\u8026\u9012\u5f52\u4f9d\u8d56\uff0cEasyTune\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u5185\u5b58\u53cb\u597d\u7684\u6269\u6563\u6a21\u578b\u504f\u597d\u5bf9\u9f50\uff0c\u4e3a\u89e3\u51b3\u8fd0\u52a8\u751f\u6210\u6a21\u578b\u4e0e\u4e0b\u6e38\u76ee\u6807\u5bf9\u9f50\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\u3002"}}
{"id": "2602.08968", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08968", "abs": "https://arxiv.org/abs/2602.08968", "authors": ["Lucas Maes", "Quentin Le Lidec", "Dan Haramati", "Nassim Massaudi", "Damien Scieur", "Yann LeCun", "Randall Balestriero"], "title": "stable-worldmodel-v1: Reproducible World Modeling Research and Evaluation", "comment": null, "summary": "World Models have emerged as a powerful paradigm for learning compact, predictive representations of environment dynamics, enabling agents to reason, plan, and generalize beyond direct experience. Despite recent interest in World Models, most available implementations remain publication-specific, severely limiting their reusability, increasing the risk of bugs, and reducing evaluation standardization. To mitigate these issues, we introduce stable-worldmodel (SWM), a modular, tested, and documented world-model research ecosystem that provides efficient data-collection tools, standardized environments, planning algorithms, and baseline implementations. In addition, each environment in SWM enables controllable factors of variation, including visual and physical properties, to support robustness and continual learning research. Finally, we demonstrate the utility of SWM by using it to study zero-shot robustness in DINO-WM.", "AI": {"tldr": "SWM\u662f\u4e00\u4e2a\u6a21\u5757\u5316\u3001\u7ecf\u8fc7\u6d4b\u8bd5\u548c\u6587\u6863\u5316\u7684\u4e16\u754c\u6a21\u578b\u7814\u7a76\u751f\u6001\u7cfb\u7edf\uff0c\u63d0\u4f9b\u9ad8\u6548\u6570\u636e\u6536\u96c6\u5de5\u5177\u3001\u6807\u51c6\u5316\u73af\u5883\u3001\u89c4\u5212\u7b97\u6cd5\u548c\u57fa\u51c6\u5b9e\u73b0\uff0c\u65e8\u5728\u89e3\u51b3\u73b0\u6709\u4e16\u754c\u6a21\u578b\u5b9e\u73b0\u7f3a\u4e4f\u53ef\u91cd\u7528\u6027\u3001\u6807\u51c6\u5316\u7684\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u5927\u591a\u6570\u4e16\u754c\u6a21\u578b\u5b9e\u73b0\u90fd\u662f\u9488\u5bf9\u7279\u5b9a\u8bba\u6587\u7684\uff0c\u7f3a\u4e4f\u53ef\u91cd\u7528\u6027\uff0c\u589e\u52a0\u4e86bug\u98ce\u9669\uff0c\u5e76\u964d\u4f4e\u4e86\u8bc4\u4f30\u6807\u51c6\u5316\u7a0b\u5ea6\u3002\u8fd9\u9650\u5236\u4e86\u4e16\u754c\u6a21\u578b\u7814\u7a76\u7684\u8fdb\u5c55\u548c\u6bd4\u8f83\u3002", "method": "\u5f00\u53d1\u4e86stable-worldmodel\uff08SWM\uff09\u751f\u6001\u7cfb\u7edf\uff0c\u5305\u542b\u6a21\u5757\u5316\u8bbe\u8ba1\u3001\u6d4b\u8bd5\u6846\u67b6\u3001\u8be6\u7ec6\u6587\u6863\uff0c\u63d0\u4f9b\u6570\u636e\u6536\u96c6\u5de5\u5177\u3001\u6807\u51c6\u5316\u73af\u5883\u3001\u89c4\u5212\u7b97\u6cd5\u548c\u57fa\u51c6\u5b9e\u73b0\u3002\u6bcf\u4e2a\u73af\u5883\u90fd\u652f\u6301\u53ef\u63a7\u7684\u53d8\u5316\u56e0\u7d20\uff08\u89c6\u89c9\u548c\u7269\u7406\u5c5e\u6027\uff09\uff0c\u4ee5\u652f\u6301\u9c81\u68d2\u6027\u548c\u6301\u7eed\u5b66\u4e60\u7814\u7a76\u3002", "result": "\u901a\u8fc7SWM\u7814\u7a76\u4e86DINO-WM\u7684\u96f6\u6837\u672c\u9c81\u68d2\u6027\uff0c\u5c55\u793a\u4e86\u8be5\u751f\u6001\u7cfb\u7edf\u7684\u5b9e\u7528\u6027\u3002SWM\u4e3a\u4e16\u754c\u6a21\u578b\u7814\u7a76\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u3001\u53ef\u91cd\u7528\u7684\u6846\u67b6\u3002", "conclusion": "SWM\u89e3\u51b3\u4e86\u4e16\u754c\u6a21\u578b\u7814\u7a76\u4e2d\u5b9e\u73b0\u788e\u7247\u5316\u7684\u95ee\u9898\uff0c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6a21\u5757\u5316\u3001\u7ecf\u8fc7\u6d4b\u8bd5\u548c\u6587\u6863\u5316\u7684\u751f\u6001\u7cfb\u7edf\uff0c\u80fd\u591f\u652f\u6301\u9c81\u68d2\u6027\u548c\u6301\u7eed\u5b66\u4e60\u7814\u7a76\uff0c\u4fc3\u8fdb\u4e16\u754c\u6a21\u578b\u9886\u57df\u7684\u6807\u51c6\u5316\u548c\u53ef\u91cd\u590d\u6027\u3002"}}
{"id": "2602.07979", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.07979", "abs": "https://arxiv.org/abs/2602.07979", "authors": ["Peng Peng", "Xinrui Zhang", "Junlin Wang", "Lei Li", "Shaoyu Wang", "Qiegen Liu"], "title": "FSP-Diff: Full-Spectrum Prior-Enhanced DualDomain Latent Diffusion for Ultra-Low-Dose Spectral CT Reconstruction", "comment": null, "summary": "Spectral computed tomography (CT) with photon-counting detectors holds immense potential for material discrimination and tissue characterization. However, under ultra-low-dose conditions, the sharply degraded signal-to-noise ratio (SNR) in energy-specific projections poses a significant challenge, leading to severe artifacts and loss of structural details in reconstructed images. To address this, we propose FSP-Diff, a full-spectrum prior-enhanced dual-domain latent diffusion framework for ultra-low-dose spectral CT reconstruction. Our framework integrates three core strategies: 1) Complementary Feature Construction: We integrate direct image reconstructions with projection-domain denoised results. While the former preserves latent textural nuances amidst heavy noise, the latter provides a stable structural scaffold to balance detail fidelity and noise suppression. 2) Full-Spectrum Prior Integration: By fusing multi-energy projections into a high-SNR full-spectrum image, we establish a unified structural reference that guides the reconstruction across all energy bins. 3) Efficient Latent Diffusion Synthesis: To alleviate the high computational burden of high-dimensional spectral data, multi-path features are embedded into a compact latent space. This allows the diffusion process to facilitate interactive feature fusion in a lower-dimensional manifold, achieving accelerated reconstruction while maintaining fine-grained detail restoration. Extensive experiments on simulated and real-world datasets demonstrate that FSP-Diff significantly outperforms state-of-the-art methods in both image quality and computational efficiency, underscoring its potential for clinically viable ultra-low-dose spectral CT imaging.", "AI": {"tldr": "FSP-Diff\uff1a\u4e00\u79cd\u7528\u4e8e\u8d85\u4f4e\u5242\u91cf\u80fd\u8c31CT\u91cd\u5efa\u7684\u5168\u80fd\u8c31\u5148\u9a8c\u589e\u5f3a\u53cc\u57df\u6f5c\u5728\u6269\u6563\u6846\u67b6\uff0c\u901a\u8fc7\u4e92\u8865\u7279\u5f81\u6784\u5efa\u3001\u5168\u80fd\u8c31\u5148\u9a8c\u96c6\u6210\u548c\u9ad8\u6548\u6f5c\u5728\u6269\u6563\u5408\u6210\uff0c\u663e\u8457\u63d0\u5347\u56fe\u50cf\u8d28\u91cf\u548c\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u8d85\u4f4e\u5242\u91cf\u6761\u4ef6\u4e0b\uff0c\u80fd\u8c31CT\u4e2d\u80fd\u91cf\u7279\u5f02\u6027\u6295\u5f71\u7684\u4fe1\u566a\u6bd4\u6025\u5267\u4e0b\u964d\uff0c\u5bfc\u81f4\u91cd\u5efa\u56fe\u50cf\u51fa\u73b0\u4e25\u91cd\u4f2a\u5f71\u548c\u7ed3\u6784\u7ec6\u8282\u4e22\u5931\uff0c\u9700\u8981\u5f00\u53d1\u6709\u6548\u7684\u91cd\u5efa\u65b9\u6cd5\u6765\u5e94\u5bf9\u8fd9\u4e00\u6311\u6218\u3002", "method": "\u63d0\u51faFSP-Diff\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u7b56\u7565\uff1a1\uff09\u4e92\u8865\u7279\u5f81\u6784\u5efa\uff1a\u6574\u5408\u76f4\u63a5\u56fe\u50cf\u91cd\u5efa\u548c\u6295\u5f71\u57df\u53bb\u566a\u7ed3\u679c\uff1b2\uff09\u5168\u80fd\u8c31\u5148\u9a8c\u96c6\u6210\uff1a\u878d\u5408\u591a\u80fd\u91cf\u6295\u5f71\u4e3a\u9ad8\u4fe1\u566a\u6bd4\u5168\u80fd\u8c31\u56fe\u50cf\u4f5c\u4e3a\u7ed3\u6784\u53c2\u8003\uff1b3\uff09\u9ad8\u6548\u6f5c\u5728\u6269\u6563\u5408\u6210\uff1a\u5c06\u591a\u8def\u5f84\u7279\u5f81\u5d4c\u5165\u7d27\u51d1\u6f5c\u5728\u7a7a\u95f4\uff0c\u5728\u4f4e\u7ef4\u6d41\u5f62\u4e2d\u8fdb\u884c\u4ea4\u4e92\u7279\u5f81\u878d\u5408\u3002", "result": "\u5728\u6a21\u62df\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cFSP-Diff\u5728\u56fe\u50cf\u8d28\u91cf\u548c\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u5747\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "FSP-Diff\u6846\u67b6\u4e3a\u4e34\u5e8a\u53ef\u884c\u7684\u8d85\u4f4e\u5242\u91cf\u80fd\u8c31CT\u6210\u50cf\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u53cc\u57df\u7279\u5f81\u6574\u5408\u548c\u6f5c\u5728\u6269\u6563\u673a\u5236\u5b9e\u73b0\u4e86\u7ec6\u8282\u4fdd\u771f\u4e0e\u566a\u58f0\u6291\u5236\u7684\u826f\u597d\u5e73\u8861\u3002"}}
{"id": "2602.08990", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08990", "abs": "https://arxiv.org/abs/2602.08990", "authors": ["Shiyang Feng", "Runmin Ma", "Xiangchao Yan", "Yue Fan", "Yusong Hu", "Songtao Huang", "Shuaiyu Zhang", "Zongsheng Cao", "Tianshuo Peng", "Jiakang Yuan", "Zijie Guo", "Zhijie Zhong", "Shangheng Du", "Weida Wang", "Jinxin Shi", "Yuhao Zhou", "Xiaohan He", "Zhiyin Yu", "Fangchen Yu", "Qihao Zheng", "Jiamin Wu", "Mianxin Liu", "Chi Zhang", "Shaowei Hou", "Shuya Li", "Yankai Jiang", "Wenjie Lou", "Lilong Wang", "Zifu Wang", "Jiong Wang", "Wanghan Xu", "Yue Deng", "Dongrui Liu", "Yiheng Wang", "Wenlong Zhang", "Fenghua Ling", "Shufei Zhang", "Xiaosong Wang", "Shuangjia Zheng", "Xun Huang", "Siqi Sun", "Shuyue Hu", "Peng Ye", "Chunfeng Song", "Bin Wang", "Conghui He", "Yihao Liu", "Xin Li", "Qibin Hou", "Tao Chen", "Xiangyu Yue", "Bin Wang", "Liang He", "Dahua Lin", "Bowen Zhou", "Bo Zhang", "Lei Bai"], "title": "InternAgent-1.5: A Unified Agentic Framework for Long-Horizon Autonomous Scientific Discovery", "comment": "Code and project page: https://github.com/InternScience/InternAgent", "summary": "We introduce InternAgent-1.5, a unified system designed for end-to-end scientific discovery across computational and empirical domains. The system is built on a structured architecture composed of three coordinated subsystems for generation, verification, and evolution. These subsystems are supported by foundational capabilities for deep research, solution optimization, and long horizon memory. The architecture allows InternAgent-1.5 to operate continuously across extended discovery cycles while maintaining coherent and improving behavior. It also enables the system to coordinate computational modeling and laboratory experimentation within a single unified system. We evaluate InternAgent-1.5 on scientific reasoning benchmarks such as GAIA, HLE, GPQA, and FrontierScience, and the system achieves leading performance that demonstrates strong foundational capabilities. Beyond these benchmarks, we further assess two categories of discovery tasks. In algorithm discovery tasks, InternAgent-1.5 autonomously designs competitive methods for core machine learning problems. In empirical discovery tasks, it executes complete computational or wet lab experiments and produces scientific findings in earth, life, biological, and physical domains. Overall, these results show that InternAgent-1.5 provides a general and scalable framework for autonomous scientific discovery.", "AI": {"tldr": "InternAgent-1.5\u662f\u4e00\u4e2a\u7aef\u5230\u7aef\u79d1\u5b66\u53d1\u73b0\u7cfb\u7edf\uff0c\u901a\u8fc7\u751f\u6210\u3001\u9a8c\u8bc1\u3001\u6f14\u5316\u4e09\u4e2a\u534f\u8c03\u5b50\u7cfb\u7edf\uff0c\u5728\u8ba1\u7b97\u548c\u5b9e\u9a8c\u9886\u57df\u5b9e\u73b0\u81ea\u4e3b\u79d1\u5b66\u53d1\u73b0\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u548c\u5b9e\u9645\u53d1\u73b0\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u5f00\u53d1\u4e00\u4e2a\u7edf\u4e00\u7684\u7aef\u5230\u7aef\u7cfb\u7edf\uff0c\u80fd\u591f\u5728\u8ba1\u7b97\u5efa\u6a21\u548c\u5b9e\u9a8c\u5ba4\u5b9e\u9a8c\u4e24\u4e2a\u9886\u57df\u8fdb\u884c\u81ea\u4e3b\u79d1\u5b66\u53d1\u73b0\uff0c\u89e3\u51b3\u8de8\u9886\u57df\u79d1\u5b66\u53d1\u73b0\u7684\u534f\u8c03\u95ee\u9898\u3002", "method": "\u91c7\u7528\u7ed3\u6784\u5316\u67b6\u6784\uff0c\u5305\u542b\u751f\u6210\u3001\u9a8c\u8bc1\u3001\u6f14\u5316\u4e09\u4e2a\u534f\u8c03\u5b50\u7cfb\u7edf\uff0c\u652f\u6301\u6df1\u5ea6\u7814\u7a76\u3001\u89e3\u51b3\u65b9\u6848\u4f18\u5316\u548c\u957f\u65f6\u7a0b\u8bb0\u5fc6\u7b49\u57fa\u7840\u80fd\u529b\uff0c\u80fd\u591f\u5728\u6269\u5c55\u7684\u53d1\u73b0\u5468\u671f\u4e2d\u6301\u7eed\u8fd0\u884c\u3002", "result": "\u5728GAIA\u3001HLE\u3001GPQA\u3001FrontierScience\u7b49\u79d1\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u9886\u5148\u6027\u80fd\uff1b\u5728\u7b97\u6cd5\u53d1\u73b0\u4efb\u52a1\u4e2d\u81ea\u4e3b\u8bbe\u8ba1\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff1b\u5728\u5b9e\u9a8c\u53d1\u73b0\u4efb\u52a1\u4e2d\u6267\u884c\u5b8c\u6574\u8ba1\u7b97\u6216\u6e7f\u5b9e\u9a8c\u5ba4\u5b9e\u9a8c\uff0c\u5728\u5730\u7403\u3001\u751f\u547d\u3001\u751f\u7269\u3001\u7269\u7406\u7b49\u9886\u57df\u4ea7\u751f\u79d1\u5b66\u53d1\u73b0\u3002", "conclusion": "InternAgent-1.5\u4e3a\u81ea\u4e3b\u79d1\u5b66\u53d1\u73b0\u63d0\u4f9b\u4e86\u4e00\u4e2a\u901a\u7528\u4e14\u53ef\u6269\u5c55\u7684\u6846\u67b6\uff0c\u80fd\u591f\u534f\u8c03\u8ba1\u7b97\u5efa\u6a21\u548c\u5b9e\u9a8c\u5ba4\u5b9e\u9a8c\uff0c\u5728\u591a\u4e2a\u79d1\u5b66\u9886\u57df\u5b9e\u73b0\u6709\u6548\u7684\u79d1\u5b66\u53d1\u73b0\u3002"}}
{"id": "2602.07980", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.07980", "abs": "https://arxiv.org/abs/2602.07980", "authors": ["Junlin Wang", "Jiancheng Fang", "Peng Peng", "Shaoyu Wang", "Qiegen Liu"], "title": "Continuity-driven Synergistic Diffusion with Neural Priors for Ultra-Sparse-View CBCT Reconstruction", "comment": null, "summary": "The clinical application of cone-beam computed tomography (CBCT) is constrained by the inherent trade-off between radiation exposure and image quality. Ultra-sparse angular sampling, employed to reduce dose, introduces severe undersampling artifacts and inter-slice inconsistencies, compromising diagnostic reliability. Existing reconstruction methods often struggle to balance angular continuity with spatial detail fidelity. To address these challenges, we propose a Continuity-driven Synergistic Diffusion with Neural priors (CSDN) for ultra-sparse-view CBCT reconstruction. Neural priors are introduced as a structural foundation to encode a continuous threedimensional attenuation representation, enabling the synthesis of physically consistent dense projections from ultra-sparse measurements. Building upon this neural-prior-based initialization, a synergistic diffusion strategy is developed, consisting of two collaborative refinement paths: a Sinogram Refinement Diffusion (Sino-RD) process that restores angular continuity and a Digital Radiography Refinement Diffusion (DR-RD) process that enforces inter-slice consistency from the projection image perspective. The outputs of the two diffusion paths are adaptively fused by the Dual-Projection Reconstruction Fusion (DPRF) module to achieve coherent volumetric reconstruction. Extensive experiments demonstrate that the proposed CSDN effectively suppresses artifacts and recovers fine textures under ultra-sparse-view conditions, outperforming existing state-of-the-art techniques.", "AI": {"tldr": "\u63d0\u51faCSDN\u65b9\u6cd5\u7528\u4e8e\u8d85\u7a00\u758f\u89d2\u91c7\u6837CBCT\u91cd\u5efa\uff0c\u901a\u8fc7\u795e\u7ecf\u5148\u9a8c\u7f16\u7801\u8fde\u7eed3D\u8870\u51cf\u8868\u793a\uff0c\u7ed3\u5408\u6b63\u5f26\u56fe\u7ec6\u5316\u6269\u6563\u548c\u6570\u5b57\u653e\u5c04\u6444\u5f71\u7ec6\u5316\u6269\u6563\u7684\u53cc\u8def\u5f84\u534f\u540c\u6269\u6563\u7b56\u7565\uff0c\u6709\u6548\u6291\u5236\u4f2a\u5f71\u5e76\u6062\u590d\u7ec6\u8282\u7eb9\u7406\u3002", "motivation": "CBCT\u4e34\u5e8a\u5e94\u7528\u53d7\u9650\u4e8e\u8f90\u5c04\u66b4\u9732\u4e0e\u56fe\u50cf\u8d28\u91cf\u4e4b\u95f4\u7684\u56fa\u6709\u6743\u8861\u3002\u8d85\u7a00\u758f\u89d2\u91c7\u6837\u867d\u80fd\u964d\u4f4e\u5242\u91cf\uff0c\u4f46\u4f1a\u5f15\u5165\u4e25\u91cd\u7684\u6b20\u91c7\u6837\u4f2a\u5f71\u548c\u5207\u7247\u95f4\u4e0d\u4e00\u81f4\u6027\uff0c\u5f71\u54cd\u8bca\u65ad\u53ef\u9760\u6027\u3002\u73b0\u6709\u91cd\u5efa\u65b9\u6cd5\u96be\u4ee5\u5e73\u8861\u89d2\u5ea6\u8fde\u7eed\u6027\u548c\u7a7a\u95f4\u7ec6\u8282\u4fdd\u771f\u5ea6\u3002", "method": "\u63d0\u51fa\u8fde\u7eed\u6027\u9a71\u52a8\u7684\u534f\u540c\u6269\u6563\u4e0e\u795e\u7ecf\u5148\u9a8c\uff08CSDN\uff09\u65b9\u6cd5\uff1a1\uff09\u5f15\u5165\u795e\u7ecf\u5148\u9a8c\u4f5c\u4e3a\u7ed3\u6784\u57fa\u7840\uff0c\u7f16\u7801\u8fde\u7eed3D\u8870\u51cf\u8868\u793a\uff0c\u4ece\u8d85\u7a00\u758f\u6d4b\u91cf\u5408\u6210\u7269\u7406\u4e00\u81f4\u7684\u5bc6\u96c6\u6295\u5f71\uff1b2\uff09\u57fa\u4e8e\u795e\u7ecf\u5148\u9a8c\u521d\u59cb\u5316\uff0c\u5f00\u53d1\u534f\u540c\u6269\u6563\u7b56\u7565\uff0c\u5305\u542b\u6b63\u5f26\u56fe\u7ec6\u5316\u6269\u6563\uff08\u6062\u590d\u89d2\u5ea6\u8fde\u7eed\u6027\uff09\u548c\u6570\u5b57\u653e\u5c04\u6444\u5f71\u7ec6\u5316\u6269\u6563\uff08\u4ece\u6295\u5f71\u56fe\u50cf\u89c6\u89d2\u589e\u5f3a\u5207\u7247\u95f4\u4e00\u81f4\u6027\uff09\uff1b3\uff09\u901a\u8fc7\u53cc\u6295\u5f71\u91cd\u5efa\u878d\u5408\u6a21\u5757\u81ea\u9002\u5e94\u878d\u5408\u4e24\u4e2a\u6269\u6563\u8def\u5f84\u7684\u8f93\u51fa\uff0c\u5b9e\u73b0\u8fde\u8d2f\u7684\u4f53\u79ef\u91cd\u5efa\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cCSDN\u5728\u8d85\u7a00\u758f\u89c6\u89d2\u6761\u4ef6\u4e0b\u80fd\u6709\u6548\u6291\u5236\u4f2a\u5f71\u5e76\u6062\u590d\u7cbe\u7ec6\u7eb9\u7406\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u6280\u672f\u3002", "conclusion": "CSDN\u65b9\u6cd5\u901a\u8fc7\u795e\u7ecf\u5148\u9a8c\u548c\u534f\u540c\u6269\u6563\u7b56\u7565\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u8d85\u7a00\u758f\u89d2\u91c7\u6837CBCT\u91cd\u5efa\u4e2d\u7684\u89d2\u5ea6\u8fde\u7eed\u6027\u548c\u5207\u7247\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u4e3a\u4f4e\u5242\u91cf\u9ad8\u8d28\u91cfCBCT\u6210\u50cf\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.09000", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.09000", "abs": "https://arxiv.org/abs/2602.09000", "authors": ["Ali Hatamizadeh", "Shrimai Prabhumoye", "Igor Gitman", "Ximing Lu", "Seungju Han", "Wei Ping", "Yejin Choi", "Jan Kautz"], "title": "iGRPO: Self-Feedback-Driven LLM Reasoning", "comment": "Tech report", "summary": "Large Language Models (LLMs) have shown promise in solving complex mathematical problems, yet they still fall short of producing accurate and consistent solutions. Reinforcement Learning (RL) is a framework for aligning these models with task-specific rewards, improving overall quality and reliability. Group Relative Policy Optimization (GRPO) is an efficient, value-function-free alternative to Proximal Policy Optimization (PPO) that leverages group-relative reward normalization. We introduce Iterative Group Relative Policy Optimization (iGRPO), a two-stage extension of GRPO that adds dynamic self-conditioning through model-generated drafts. In Stage 1, iGRPO samples multiple exploratory drafts and selects the highest-reward draft using the same scalar reward signal used for optimization. In Stage 2, it appends this best draft to the original prompt and applies a GRPO-style update on draft-conditioned refinements, training the policy to improve beyond its strongest prior attempt. Under matched rollout budgets, iGRPO consistently outperforms GRPO across base models (e.g., Nemotron-H-8B-Base-8K and DeepSeek-R1 Distilled), validating its effectiveness on diverse reasoning benchmarks. Moreover, applying iGRPO to OpenReasoning-Nemotron-7B trained on AceReason-Math achieves new state-of-the-art results of 85.62\\% and 79.64\\% on AIME24 and AIME25, respectively. Ablations further show that the refinement wrapper generalizes beyond GRPO variants, benefits from a generative judge, and alters learning dynamics by delaying entropy collapse. These results underscore the potential of iterative, self-feedback-based RL for advancing verifiable mathematical reasoning.", "AI": {"tldr": "iGRPO\u662f\u4e00\u79cd\u4e24\u9636\u6bb5\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u6a21\u578b\u81ea\u751f\u6210\u8349\u7a3f\u548c\u52a8\u6001\u81ea\u6761\u4ef6\u4f18\u5316\uff0c\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\uff0c\u8fbe\u5230\u65b0\u7684SOTA\u7ed3\u679c\u3002", "motivation": "\u5c3d\u7ba1\u5927\u8bed\u8a00\u6a21\u578b\u5728\u89e3\u51b3\u590d\u6742\u6570\u5b66\u95ee\u9898\u65b9\u9762\u663e\u793a\u51fa\u6f5c\u529b\uff0c\u4f46\u5b83\u4eec\u4ecd\u7136\u96be\u4ee5\u4ea7\u751f\u51c6\u786e\u4e14\u4e00\u81f4\u7684\u89e3\u51b3\u65b9\u6848\u3002\u5f3a\u5316\u5b66\u4e60\u53ef\u4ee5\u5bf9\u9f50\u6a21\u578b\u4e0e\u4efb\u52a1\u7279\u5b9a\u7684\u5956\u52b1\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5982PPO\u548cGRPO\u4ecd\u6709\u6539\u8fdb\u7a7a\u95f4\u3002", "method": "iGRPO\u662fGRPO\u7684\u4e24\u9636\u6bb5\u6269\u5c55\uff1a\u7b2c\u4e00\u9636\u6bb5\u91c7\u6837\u591a\u4e2a\u63a2\u7d22\u6027\u8349\u7a3f\u5e76\u9009\u62e9\u6700\u9ad8\u5956\u52b1\u7684\u8349\u7a3f\uff1b\u7b2c\u4e8c\u9636\u6bb5\u5c06\u6700\u4f73\u8349\u7a3f\u9644\u52a0\u5230\u539f\u59cb\u63d0\u793a\u540e\uff0c\u5728\u8349\u7a3f\u6761\u4ef6\u5316\u7684\u6539\u8fdb\u4e0a\u5e94\u7528GRPO\u98ce\u683c\u66f4\u65b0\uff0c\u8bad\u7ec3\u7b56\u7565\u8d85\u8d8a\u5176\u5148\u524d\u7684\u6700\u4f73\u5c1d\u8bd5\u3002", "result": "\u5728\u5339\u914d\u7684rollout\u9884\u7b97\u4e0b\uff0ciGRPO\u5728\u591a\u4e2a\u57fa\u7840\u6a21\u578b\u4e0a\u6301\u7eed\u8d85\u8d8aGRPO\u3002\u5e94\u7528\u4e8eOpenReasoning-Nemotron-7B\u6a21\u578b\u65f6\uff0c\u5728AIME24\u548cAIME25\u4e0a\u5206\u522b\u8fbe\u523085.62%\u548c79.64%\u7684\u65b0SOTA\u7ed3\u679c\u3002", "conclusion": "iGRPO\u5c55\u793a\u4e86\u8fed\u4ee3\u3001\u81ea\u53cd\u9988\u5f3a\u5316\u5b66\u4e60\u5728\u63a8\u8fdb\u53ef\u9a8c\u8bc1\u6570\u5b66\u63a8\u7406\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u5176\u7ec6\u5316\u5305\u88c5\u5668\u4e0d\u4ec5\u9650\u4e8eGRPO\u53d8\u4f53\uff0c\u8fd8\u80fd\u4ece\u751f\u6210\u5f0f\u8bc4\u5224\u4e2d\u53d7\u76ca\uff0c\u5e76\u901a\u8fc7\u5ef6\u8fdf\u71b5\u5d29\u6e83\u6539\u53d8\u5b66\u4e60\u52a8\u6001\u3002"}}
{"id": "2602.07986", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.07986", "abs": "https://arxiv.org/abs/2602.07986", "authors": ["Md. Tarek Hasan", "Sanjay Saha", "Shaojing Fan", "Swakkhar Shatabda", "Terence Sim"], "title": "Deepfake Synthesis vs. Detection: An Uneven Contest", "comment": null, "summary": "The rapid advancement of deepfake technology has significantly elevated the realism and accessibility of synthetic media. Emerging techniques, such as diffusion-based models and Neural Radiance Fields (NeRF), alongside enhancements in traditional Generative Adversarial Networks (GANs), have contributed to the sophisticated generation of deepfake videos. Concurrently, deepfake detection methods have seen notable progress, driven by innovations in Transformer architectures, contrastive learning, and other machine learning approaches. In this study, we conduct a comprehensive empirical analysis of state-of-the-art deepfake detection techniques, including human evaluation experiments against cutting-edge synthesis methods. Our findings highlight a concerning trend: many state-of-the-art detection models exhibit markedly poor performance when challenged with deepfakes produced by modern synthesis techniques, including poor performance by human participants against the best quality deepfakes. Through extensive experimentation, we provide evidence that underscores the urgent need for continued refinement of detection models to keep pace with the evolving capabilities of deepfake generation technologies. This research emphasizes the critical gap between current detection methodologies and the sophistication of new generation techniques, calling for intensified efforts in this crucial area of study.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5bf9\u6700\u65b0\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u6280\u672f\u8fdb\u884c\u5b9e\u8bc1\u5206\u6790\uff0c\u53d1\u73b0\u73b0\u6709\u68c0\u6d4b\u6a21\u578b\u5728\u9762\u5bf9\u73b0\u4ee3\u5408\u6210\u6280\u672f\u751f\u6210\u7684\u6df1\u5ea6\u4f2a\u9020\u5185\u5bb9\u65f6\u8868\u73b0\u663e\u8457\u4e0b\u964d\uff0c\u751a\u81f3\u4eba\u7c7b\u53c2\u4e0e\u8005\u4e5f\u96be\u4ee5\u8bc6\u522b\u6700\u9ad8\u8d28\u91cf\u7684\u6df1\u5ea6\u4f2a\u9020\u5185\u5bb9\uff0c\u63ed\u793a\u4e86\u68c0\u6d4b\u65b9\u6cd5\u4e0e\u751f\u6210\u6280\u672f\u4e4b\u95f4\u7684\u4e25\u91cd\u5dee\u8ddd\u3002", "motivation": "\u968f\u7740\u6269\u6563\u6a21\u578b\u3001\u795e\u7ecf\u8f90\u5c04\u573a\uff08NeRF\uff09\u548c\u589e\u5f3a\u578bGAN\u7b49\u6df1\u5ea6\u4f2a\u9020\u751f\u6210\u6280\u672f\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u5408\u6210\u5a92\u4f53\u7684\u771f\u5b9e\u6027\u548c\u53ef\u8bbf\u95ee\u6027\u663e\u8457\u63d0\u5347\u3002\u540c\u65f6\uff0c\u57fa\u4e8eTransformer\u67b6\u6784\u3001\u5bf9\u6bd4\u5b66\u4e60\u7b49\u65b9\u6cd5\u7684\u68c0\u6d4b\u6280\u672f\u4e5f\u53d6\u5f97\u8fdb\u5c55\u3002\u7814\u7a76\u65e8\u5728\u8bc4\u4f30\u5f53\u524d\u6700\u5148\u8fdb\u7684\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u6280\u672f\u5728\u5b9e\u9645\u5bf9\u6297\u73b0\u4ee3\u5408\u6210\u6280\u672f\u65f6\u7684\u6709\u6548\u6027\u3002", "method": "\u91c7\u7528\u5168\u9762\u7684\u5b9e\u8bc1\u5206\u6790\u65b9\u6cd5\uff0c\u5305\u62ec\u5bf9\u6700\u5148\u8fdb\u7684\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u6280\u672f\u8fdb\u884c\u8bc4\u4f30\uff0c\u5e76\u8bbe\u8ba1\u4eba\u7c7b\u8bc4\u4f30\u5b9e\u9a8c\u6765\u6d4b\u8bd5\u53c2\u4e0e\u8005\u5bf9\u6297\u5c16\u7aef\u5408\u6210\u65b9\u6cd5\u751f\u6210\u7684\u6df1\u5ea6\u4f2a\u9020\u5185\u5bb9\u7684\u80fd\u529b\u3002\u901a\u8fc7\u5927\u91cf\u5b9e\u9a8c\u5bf9\u6bd4\u4e0d\u540c\u68c0\u6d4b\u6a21\u578b\u5728\u9762\u5bf9\u73b0\u4ee3\u5408\u6210\u6280\u672f\u65f6\u7684\u8868\u73b0\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u4ee4\u4eba\u62c5\u5fe7\u7684\u8d8b\u52bf\uff1a\u8bb8\u591a\u6700\u5148\u8fdb\u7684\u68c0\u6d4b\u6a21\u578b\u5728\u9762\u5bf9\u73b0\u4ee3\u5408\u6210\u6280\u672f\uff08\u5305\u62ec\u6269\u6563\u6a21\u578b\u3001NeRF\u7b49\uff09\u751f\u6210\u7684\u6df1\u5ea6\u4f2a\u9020\u5185\u5bb9\u65f6\u8868\u73b0\u663e\u8457\u4e0b\u964d\u3002\u4eba\u7c7b\u53c2\u4e0e\u8005\u5728\u9762\u5bf9\u6700\u9ad8\u8d28\u91cf\u7684\u6df1\u5ea6\u4f2a\u9020\u5185\u5bb9\u65f6\u4e5f\u8868\u73b0\u4e0d\u4f73\u3002\u5b9e\u9a8c\u8bc1\u636e\u8868\u660e\u5f53\u524d\u68c0\u6d4b\u65b9\u6cd5\u5df2\u843d\u540e\u4e8e\u751f\u6210\u6280\u672f\u7684\u53d1\u5c55\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u6a21\u578b\u4e0e\u4e0d\u65ad\u53d1\u5c55\u7684\u751f\u6210\u6280\u672f\u4e4b\u95f4\u5b58\u5728\u4e25\u91cd\u5dee\u8ddd\uff0c\u8feb\u5207\u9700\u8981\u6301\u7eed\u6539\u8fdb\u68c0\u6d4b\u6a21\u578b\u4ee5\u8ddf\u4e0a\u6df1\u5ea6\u4f2a\u9020\u751f\u6210\u6280\u672f\u7684\u6f14\u8fdb\u3002\u8fd9\u547c\u5401\u5728\u8be5\u5173\u952e\u7814\u7a76\u9886\u57df\u52a0\u5f3a\u52aa\u529b\uff0c\u4ee5\u5e94\u5bf9\u65e5\u76ca\u590d\u6742\u7684\u6df1\u5ea6\u4f2a\u9020\u5a01\u80c1\u3002"}}
{"id": "2602.09003", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.09003", "abs": "https://arxiv.org/abs/2602.09003", "authors": ["Yudong Wang", "Zixuan Fu", "Hengyu Zhao", "Chen Zhao", "Chuyue Zhou", "Xinle Lin", "Hongya Lyu", "Shuaikang Xue", "Yi Yi", "Yingjiao Wang", "Zhi Zheng", "Yuzhou Zhang", "Jie Zhou", "Chaojun Xiao", "Xu Han", "Zhiyuan Liu", "Maosong Sun"], "title": "Data Science and Technology Towards AGI Part I: Tiered Data Management", "comment": "16 pages, 3 figures, 7 tables", "summary": "The development of artificial intelligence can be viewed as an evolution of data-driven learning paradigms, with successive shifts in data organization and utilization continuously driving advances in model capability. Current LLM research is dominated by a paradigm that relies heavily on unidirectional scaling of data size, increasingly encountering bottlenecks in data availability, acquisition cost, and training efficiency. In this work, we argue that the development of AGI is entering a new phase of data-model co-evolution, in which models actively guide data management while high-quality data, in turn, amplifies model capabilities. To implement this vision, we propose a tiered data management framework, designed to support the full LLM training lifecycle across heterogeneous learning objectives and cost constraints. Specifically, we introduce an L0-L4 tiered data management framework, ranging from raw uncurated resources to organized and verifiable knowledge. Importantly, LLMs are fully used in data management processes, such as quality scoring and content editing, to refine data across tiers. Each tier is characterized by distinct data properties, management strategies, and training roles, enabling data to be strategically allocated across LLM training stages, including pre-training, mid-training, and alignment. The framework balances data quality, acquisition cost, and marginal training benefit, providing a systematic approach to scalable and sustainable data management. We validate the effectiveness of the proposed framework through empirical studies, in which tiered datasets are constructed from raw corpora and used across multiple training phases. Experimental results demonstrate that tier-aware data utilization significantly improves training efficiency and model performance. To facilitate further research, we release our tiered datasets and processing tools to the community.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faL0-L4\u5206\u5c42\u6570\u636e\u7ba1\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u6570\u636e\u4e0e\u6a21\u578b\u534f\u540c\u8fdb\u5316\u63d0\u5347LLM\u8bad\u7ec3\u6548\u7387\uff0c\u89e3\u51b3\u5f53\u524d\u5355\u7eaf\u6269\u5927\u6570\u636e\u89c4\u6a21\u9762\u4e34\u7684\u74f6\u9888\u95ee\u9898\u3002", "motivation": "\u5f53\u524dLLM\u7814\u7a76\u8fc7\u5ea6\u4f9d\u8d56\u6570\u636e\u89c4\u6a21\u5355\u5411\u6269\u5c55\uff0c\u9762\u4e34\u6570\u636e\u53ef\u7528\u6027\u3001\u83b7\u53d6\u6210\u672c\u548c\u8bad\u7ec3\u6548\u7387\u74f6\u9888\u3002\u4f5c\u8005\u8ba4\u4e3aAGI\u53d1\u5c55\u6b63\u8fdb\u5165\u6570\u636e-\u6a21\u578b\u534f\u540c\u8fdb\u5316\u65b0\u9636\u6bb5\uff0c\u9700\u8981\u66f4\u667a\u80fd\u7684\u6570\u636e\u7ba1\u7406\u7b56\u7565\u3002", "method": "\u63d0\u51faL0-L4\u5206\u5c42\u6570\u636e\u7ba1\u7406\u6846\u67b6\uff1a\u4ece\u539f\u59cb\u672a\u7b5b\u9009\u8d44\u6e90\u5230\u7ec4\u7ec7\u5316\u53ef\u9a8c\u8bc1\u77e5\u8bc6\u3002\u5229\u7528LLM\u8fdb\u884c\u6570\u636e\u8d28\u91cf\u8bc4\u5206\u548c\u5185\u5bb9\u7f16\u8f91\u7b49\u7ba1\u7406\u8fc7\u7a0b\uff0c\u6839\u636e\u6570\u636e\u7279\u6027\u3001\u7ba1\u7406\u7b56\u7565\u548c\u8bad\u7ec3\u89d2\u8272\u5c06\u6570\u636e\u6218\u7565\u6027\u5730\u5206\u914d\u5230\u9884\u8bad\u7ec3\u3001\u4e2d\u671f\u8bad\u7ec3\u548c\u5bf9\u9f50\u7b49\u4e0d\u540c\u9636\u6bb5\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u8868\u660e\uff0c\u5206\u5c42\u611f\u77e5\u7684\u6570\u636e\u5229\u7528\u663e\u8457\u63d0\u9ad8\u4e86\u8bad\u7ec3\u6548\u7387\u548c\u6a21\u578b\u6027\u80fd\u3002\u4f5c\u8005\u5411\u793e\u533a\u53d1\u5e03\u4e86\u5206\u5c42\u6570\u636e\u96c6\u548c\u5904\u7406\u5de5\u5177\u3002", "conclusion": "\u5206\u5c42\u6570\u636e\u7ba1\u7406\u6846\u67b6\u5e73\u8861\u4e86\u6570\u636e\u8d28\u91cf\u3001\u83b7\u53d6\u6210\u672c\u548c\u8fb9\u9645\u8bad\u7ec3\u6548\u76ca\uff0c\u4e3a\u53ef\u6269\u5c55\u548c\u53ef\u6301\u7eed\u7684\u6570\u636e\u7ba1\u7406\u63d0\u4f9b\u4e86\u7cfb\u7edf\u5316\u65b9\u6cd5\uff0c\u652f\u6301\u6570\u636e\u4e0e\u6a21\u578b\u7684\u534f\u540c\u8fdb\u5316\u3002"}}
{"id": "2602.09007", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.09007", "abs": "https://arxiv.org/abs/2602.09007", "authors": ["Haodong Li", "Jingwei Wu", "Quan Sun", "Guopeng Li", "Juanxi Tian", "Huanyu Zhang", "Yanlin Lai", "Ruichuan An", "Hongbo Peng", "Yuhong Dai", "Chenxi Li", "Chunmei Qing", "Jia Wang", "Ziyang Meng", "Zheng Ge", "Xiangyu Zhang", "Daxin Jiang"], "title": "GEBench: Benchmarking Image Generation Models as GUI Environments", "comment": "23 pages, 5 figures, 4 tables", "summary": "Recent advancements in image generation models have enabled the prediction of future Graphical User Interface (GUI) states based on user instructions. However, existing benchmarks primarily focus on general domain visual fidelity, leaving the evaluation of state transitions and temporal coherence in GUI-specific contexts underexplored. To address this gap, we introduce GEBench, a comprehensive benchmark for evaluating dynamic interaction and temporal coherence in GUI generation. GEBench comprises 700 carefully curated samples spanning five task categories, covering both single-step interactions and multi-step trajectories across real-world and fictional scenarios, as well as grounding point localization. To support systematic evaluation, we propose GE-Score, a novel five-dimensional metric that assesses Goal Achievement, Interaction Logic, Content Consistency, UI Plausibility, and Visual Quality. Extensive evaluations on current models indicate that while they perform well on single-step transitions, they struggle significantly with maintaining temporal coherence and spatial grounding over longer interaction sequences. Our findings identify icon interpretation, text rendering, and localization precision as critical bottlenecks. This work provides a foundation for systematic assessment and suggests promising directions for future research toward building high-fidelity generative GUI environments. The code is available at: https://github.com/stepfun-ai/GEBench.", "AI": {"tldr": "GEBench\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30GUI\u751f\u6210\u4e2d\u52a8\u6001\u4ea4\u4e92\u548c\u65f6\u95f4\u8fde\u8d2f\u6027\u7684\u65b0\u57fa\u51c6\uff0c\u5305\u542b700\u4e2a\u6837\u672c\u548c\u4e94\u7ef4\u8bc4\u4f30\u6307\u6807GE-Score\uff0c\u53d1\u73b0\u73b0\u6709\u6a21\u578b\u5728\u5355\u6b65\u8f6c\u6362\u8868\u73b0\u826f\u597d\u4f46\u5728\u957f\u5e8f\u5217\u4e2d\u4fdd\u6301\u65f6\u95f4\u8fde\u8d2f\u6027\u65b9\u9762\u5b58\u5728\u56f0\u96be\u3002", "motivation": "\u73b0\u6709\u56fe\u50cf\u751f\u6210\u6a21\u578b\u80fd\u591f\u57fa\u4e8e\u7528\u6237\u6307\u4ee4\u9884\u6d4b\u672a\u6765GUI\u72b6\u6001\uff0c\u4f46\u73b0\u6709\u57fa\u51c6\u4e3b\u8981\u5173\u6ce8\u901a\u7528\u9886\u57df\u7684\u89c6\u89c9\u4fdd\u771f\u5ea6\uff0c\u5bf9GUI\u7279\u5b9a\u4e0a\u4e0b\u6587\u4e2d\u7684\u72b6\u6001\u8f6c\u6362\u548c\u65f6\u95f4\u8fde\u8d2f\u6027\u8bc4\u4f30\u4e0d\u8db3\u3002", "method": "\u63d0\u51faGEBench\u57fa\u51c6\uff0c\u5305\u542b700\u4e2a\u7cbe\u5fc3\u7b56\u5212\u7684\u6837\u672c\uff0c\u6db5\u76d6\u4e94\u4e2a\u4efb\u52a1\u7c7b\u522b\uff0c\u5305\u62ec\u5355\u6b65\u4ea4\u4e92\u548c\u591a\u6b65\u8f68\u8ff9\uff0c\u4ee5\u53ca\u5b9a\u4f4d\u70b9\u6807\u6ce8\u3002\u63d0\u51faGE-Score\u4e94\u7ef4\u8bc4\u4f30\u6307\u6807\uff1a\u76ee\u6807\u8fbe\u6210\u3001\u4ea4\u4e92\u903b\u8f91\u3001\u5185\u5bb9\u4e00\u81f4\u6027\u3001UI\u5408\u7406\u6027\u548c\u89c6\u89c9\u8d28\u91cf\u3002", "result": "\u5bf9\u5f53\u524d\u6a21\u578b\u7684\u5e7f\u6cdb\u8bc4\u4f30\u8868\u660e\uff0c\u5b83\u4eec\u5728\u5355\u6b65\u8f6c\u6362\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u4fdd\u6301\u957f\u65f6\u95f4\u4ea4\u4e92\u5e8f\u5217\u7684\u65f6\u95f4\u8fde\u8d2f\u6027\u548c\u7a7a\u95f4\u5b9a\u4f4d\u65b9\u9762\u5b58\u5728\u663e\u8457\u56f0\u96be\u3002\u56fe\u6807\u89e3\u91ca\u3001\u6587\u672c\u6e32\u67d3\u548c\u5b9a\u4f4d\u7cbe\u5ea6\u662f\u5173\u952e\u74f6\u9888\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u7cfb\u7edf\u8bc4\u4f30\u63d0\u4f9b\u4e86\u57fa\u7840\uff0c\u5e76\u4e3a\u672a\u6765\u6784\u5efa\u9ad8\u4fdd\u771f\u751f\u6210GUI\u73af\u5883\u7684\u7814\u7a76\u6307\u660e\u4e86\u6709\u524d\u666f\u7684\u65b9\u5411\u3002\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2512.22730", "categories": ["cs.CV", "cs.AI", "eess.IV"], "pdf": "https://arxiv.org/pdf/2512.22730", "abs": "https://arxiv.org/abs/2512.22730", "authors": ["Youssef Megahed", "Robin Ducharme", "Inok Lee", "Inbal Willner", "Adrian D. C. Chan", "Mark Walker", "Steven Hawken"], "title": "Improved cystic hygroma detection from prenatal imaging using ultrasound-specific self-supervised representation learning", "comment": "13 pages, 6 figures, 2 tables", "summary": "Cystic hygroma is a high-risk prenatal ultrasound finding that portends high rates of chromosomal abnormalities, structural malformations, and adverse pregnancy outcomes. Automated detection can increase reproducibility and support scalable early screening programs, but supervised deep learning methods are limited by small labelled datasets. This study assesses whether ultrasound-specific self-supervised pretraining can facilitate accurate, robust deep learning detection of cystic hygroma in first-trimester ultrasound images. We fine-tuned the Ultrasound Self-Supervised Foundation Model with Masked Autoencoding (USF-MAE), pretrained on over 370,000 unlabelled ultrasound images, for binary classification of normal controls and cystic hygroma cases used in this study. Performance was evaluated on the same curated ultrasound dataset, preprocessing pipeline, and 4-fold cross-validation protocol as for the DenseNet-169 baseline, using accuracy, sensitivity, specificity, and the area under the receiver operating characteristic curve (ROC-AUC). Model interpretability was analyzed qualitatively using Score-CAM visualizations. USF-MAE outperformed the DenseNet-169 baseline on all evaluation metrics. The proposed model yielded a mean accuracy of 0.96, sensitivity of 0.94, specificity of 0.98, and ROC-AUC of 0.98 compared to 0.93, 0.92, 0.94, and 0.94 for the DenseNet-169 baseline, respectively. Qualitative Score-CAM visualizations of model predictions demonstrated clinical relevance by highlighting expected regions in the fetal neck for both positive and negative cases. Paired statistical analysis using a Wilcoxon signed-rank test confirmed that performance improvements achieved by USF-MAE were statistically significant (p = 0.0057).", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e86\u8d85\u58f0\u7279\u5f02\u6027\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u6a21\u578bUSF-MAE\u5728\u65e9\u5b55\u671f\u8d85\u58f0\u56fe\u50cf\u4e2d\u68c0\u6d4b\u56ca\u6027\u6c34\u7624\u7684\u6027\u80fd\uff0c\u76f8\u6bd4\u4f20\u7edfDenseNet-169\u57fa\u7ebf\u6a21\u578b\uff0c\u5728\u6240\u6709\u8bc4\u4f30\u6307\u6807\u4e0a\u5747\u6709\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u56ca\u6027\u6c34\u7624\u662f\u9ad8\u98ce\u9669\u4ea7\u524d\u8d85\u58f0\u53d1\u73b0\uff0c\u4e0e\u67d3\u8272\u4f53\u5f02\u5e38\u3001\u7ed3\u6784\u7578\u5f62\u548c\u4e0d\u826f\u598a\u5a20\u7ed3\u5c40\u5bc6\u5207\u76f8\u5173\u3002\u81ea\u52a8\u68c0\u6d4b\u53ef\u63d0\u9ad8\u53ef\u91cd\u590d\u6027\u5e76\u652f\u6301\u53ef\u6269\u5c55\u7684\u65e9\u671f\u7b5b\u67e5\uff0c\u4f46\u76d1\u7763\u5f0f\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u53d7\u9650\u4e8e\u5c0f\u89c4\u6a21\u6807\u6ce8\u6570\u636e\u96c6\u3002", "method": "\u4f7f\u7528\u57fa\u4e8e\u63a9\u7801\u81ea\u7f16\u7801\uff08MAE\uff09\u7684\u8d85\u58f0\u81ea\u76d1\u7763\u57fa\u7840\u6a21\u578b\uff08USF-MAE\uff09\uff0c\u8be5\u6a21\u578b\u5728\u8d85\u8fc737\u4e07\u5f20\u672a\u6807\u6ce8\u8d85\u58f0\u56fe\u50cf\u4e0a\u9884\u8bad\u7ec3\uff0c\u7136\u540e\u9488\u5bf9\u56ca\u6027\u6c34\u7624\u4e0e\u6b63\u5e38\u5bf9\u7167\u7684\u4e8c\u5206\u7c7b\u4efb\u52a1\u8fdb\u884c\u5fae\u8c03\u3002\u91c7\u7528\u4e0eDenseNet-169\u57fa\u7ebf\u76f8\u540c\u7684\u8d85\u58f0\u6570\u636e\u96c6\u3001\u9884\u5904\u7406\u6d41\u7a0b\u548c4\u6298\u4ea4\u53c9\u9a8c\u8bc1\u534f\u8bae\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "USF-MAE\u5728\u6240\u6709\u8bc4\u4f30\u6307\u6807\u4e0a\u5747\u4f18\u4e8eDenseNet-169\u57fa\u7ebf\uff1a\u5e73\u5747\u51c6\u786e\u73870.96 vs 0.93\uff0c\u7075\u654f\u5ea60.94 vs 0.92\uff0c\u7279\u5f02\u60270.98 vs 0.94\uff0cROC-AUC 0.98 vs 0.94\u3002Score-CAM\u53ef\u89c6\u5316\u663e\u793a\u6a21\u578b\u5173\u6ce8\u80ce\u513f\u9888\u90e8\u76f8\u5173\u533a\u57df\uff0cWilcoxon\u7b26\u53f7\u79e9\u68c0\u9a8c\u8bc1\u5b9e\u6027\u80fd\u63d0\u5347\u5177\u6709\u7edf\u8ba1\u5b66\u663e\u8457\u6027\uff08p=0.0057\uff09\u3002", "conclusion": "\u8d85\u58f0\u7279\u5f02\u6027\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u80fd\u591f\u4fc3\u8fdb\u51c6\u786e\u3001\u7a33\u5065\u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u68c0\u6d4b\u65e9\u5b55\u671f\u8d85\u58f0\u56fe\u50cf\u4e2d\u7684\u56ca\u6027\u6c34\u7624\uff0c\u4e3a\u53ef\u6269\u5c55\u7684\u65e9\u671f\u7b5b\u67e5\u9879\u76ee\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u6280\u672f\u652f\u6301\u3002"}}
{"id": "2602.08024", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.08024", "abs": "https://arxiv.org/abs/2602.08024", "authors": ["Ziyang Fan", "Keyu Chen", "Ruilong Xing", "Yulin Li", "Li Jiang", "Zhuotao Tian"], "title": "FlashVID: Efficient Video Large Language Models via Training-free Tree-based Spatiotemporal Token Merging", "comment": "Accepted by ICLR 2026 (Oral)", "summary": "Although Video Large Language Models (VLLMs) have shown remarkable capabilities in video understanding, they are required to process high volumes of visual tokens, causing significant computational inefficiency. Existing VLLMs acceleration frameworks usually compress spatial and temporal redundancy independently, which overlooks the spatiotemporal relationships, thereby leading to suboptimal spatiotemporal compression. The highly correlated visual features are likely to change in spatial position, scale, orientation, and other attributes over time due to the dynamic nature of video. Building on this insight, we introduce FlashVID, a training-free inference acceleration framework for VLLMs. Specifically, FlashVID utilizes Attention and Diversity-based Token Selection (ADTS) to select the most representative tokens for basic video representation, then applies Tree-based Spatiotemporal Token Merging (TSTM) for fine-grained spatiotemporal redundancy elimination. Extensive experiments conducted on three representative VLLMs across five video understanding benchmarks demonstrate the effectiveness and generalization of our method. Notably, by retaining only 10% of visual tokens, FlashVID preserves 99.1% of the performance of LLaVA-OneVision. Consequently, FlashVID can serve as a training-free and plug-and-play module for extending long video frames, which enables a 10x increase in video frame input to Qwen2.5-VL, resulting in a relative improvement of 8.6% within the same computational budget. Code is available at https://github.com/Fanziyang-v/FlashVID.", "AI": {"tldr": "FlashVID\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684\u89c6\u9891\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u52a0\u901f\u6846\u67b6\uff0c\u901a\u8fc7\u6ce8\u610f\u529b\u4e0e\u591a\u6837\u6027\u4ee4\u724c\u9009\u62e9\u548c\u6811\u72b6\u65f6\u7a7a\u4ee4\u724c\u5408\u5e76\uff0c\u5728\u4fdd\u755910%\u89c6\u89c9\u4ee4\u724c\u7684\u60c5\u51b5\u4e0b\u4fdd\u630199.1%\u6027\u80fd\uff0c\u5b9e\u73b010\u500d\u89c6\u9891\u5e27\u8f93\u5165\u6269\u5c55\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u5927\u8bed\u8a00\u6a21\u578b\u9700\u8981\u5904\u7406\u5927\u91cf\u89c6\u89c9\u4ee4\u724c\uff0c\u8ba1\u7b97\u6548\u7387\u4f4e\u4e0b\u3002\u73b0\u6709\u52a0\u901f\u6846\u67b6\u72ec\u7acb\u538b\u7f29\u7a7a\u95f4\u548c\u65f6\u95f4\u5197\u4f59\uff0c\u5ffd\u7565\u4e86\u65f6\u7a7a\u5173\u7cfb\uff0c\u5bfc\u81f4\u6b21\u4f18\u538b\u7f29\u6548\u679c\u3002\u89c6\u9891\u7684\u52a8\u6001\u7279\u6027\u4f7f\u5f97\u89c6\u89c9\u7279\u5f81\u5728\u65f6\u7a7a\u4f4d\u7f6e\u3001\u5c3a\u5ea6\u3001\u65b9\u5411\u7b49\u5c5e\u6027\u4e0a\u968f\u65f6\u95f4\u53d8\u5316\u3002", "method": "FlashVID\u91c7\u7528\u8bad\u7ec3\u514d\u8d39\u7684\u63a8\u7406\u52a0\u901f\u6846\u67b6\uff1a1) \u6ce8\u610f\u529b\u4e0e\u591a\u6837\u6027\u4ee4\u724c\u9009\u62e9(ADTS)\u9009\u62e9\u6700\u5177\u4ee3\u8868\u6027\u7684\u4ee4\u724c\u8fdb\u884c\u57fa\u7840\u89c6\u9891\u8868\u793a\uff1b2) \u6811\u72b6\u65f6\u7a7a\u4ee4\u724c\u5408\u5e76(TSTM)\u8fdb\u884c\u7ec6\u7c92\u5ea6\u65f6\u7a7a\u5197\u4f59\u6d88\u9664\u3002", "result": "\u5728\u4e09\u4e2a\u4ee3\u8868\u6027VLLM\u548c\u4e94\u4e2a\u89c6\u9891\u7406\u89e3\u57fa\u51c6\u4e0a\u7684\u5b9e\u9a8c\u8bc1\u660e\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u6cdb\u5316\u6027\u3002\u4ec5\u4fdd\u755910%\u89c6\u89c9\u4ee4\u724c\u65f6\uff0cFlashVID\u4fdd\u6301\u4e86LLaVA-OneVision 99.1%\u7684\u6027\u80fd\u3002\u4f7fQwen2.5-VL\u7684\u89c6\u9891\u5e27\u8f93\u5165\u589e\u52a010\u500d\uff0c\u5728\u76f8\u540c\u8ba1\u7b97\u9884\u7b97\u4e0b\u76f8\u5bf9\u63d0\u53478.6%\u3002", "conclusion": "FlashVID\u53ef\u4f5c\u4e3a\u65e0\u9700\u8bad\u7ec3\u3001\u5373\u63d2\u5373\u7528\u7684\u6a21\u5757\u6269\u5c55\u957f\u89c6\u9891\u5e27\u5904\u7406\u80fd\u529b\uff0c\u663e\u8457\u63d0\u5347\u89c6\u9891\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u6548\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u6027\u80fd\u3002"}}
{"id": "2602.08025", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08025", "abs": "https://arxiv.org/abs/2602.08025", "authors": ["Yixuan Ye", "Xuanyu Lu", "Yuxin Jiang", "Yuchao Gu", "Rui Zhao", "Qiwei Liang", "Jiachun Pan", "Fengda Zhang", "Weijia Wu", "Alex Jinpeng Wang"], "title": "MIND: Benchmarking Memory Consistency and Action Control in World Models", "comment": null, "summary": "World models aim to understand, remember, and predict dynamic visual environments, yet a unified benchmark for evaluating their fundamental abilities remains lacking. To address this gap, we introduce MIND, the first open-domain closed-loop revisited benchmark for evaluating Memory consIstency and action coNtrol in worlD models. MIND contains 250 high-quality videos at 1080p and 24 FPS, including 100 (first-person) + 100 (third-person) video clips under a shared action space and 25 + 25 clips across varied action spaces covering eight diverse scenes. We design an efficient evaluation framework to measure two core abilities: memory consistency and action control, capturing temporal stability and contextual coherence across viewpoints. Furthermore, we design various action spaces, including different character movement speeds and camera rotation angles, to evaluate the action generalization capability across different action spaces under shared scenes. To facilitate future performance benchmarking on MIND, we introduce MIND-World, a novel interactive Video-to-World baseline. Extensive experiments demonstrate the completeness of MIND and reveal key challenges in current world models, including the difficulty of maintaining long-term memory consistency and generalizing across action spaces. Project page: https://csu-jpg.github.io/MIND.github.io/", "AI": {"tldr": "MIND\u662f\u9996\u4e2a\u5f00\u653e\u57df\u95ed\u73af\u91cd\u8bbf\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u4e16\u754c\u6a21\u578b\u7684\u8bb0\u5fc6\u4e00\u81f4\u6027\u548c\u52a8\u4f5c\u63a7\u5236\u80fd\u529b\uff0c\u5305\u542b250\u4e2a\u9ad8\u8d28\u91cf\u89c6\u9891\uff0c\u8bbe\u8ba1\u4e86\u8bc4\u4f30\u6846\u67b6\u548c\u57fa\u7ebf\u6a21\u578b\u3002", "motivation": "\u5f53\u524d\u7f3a\u4e4f\u8bc4\u4f30\u4e16\u754c\u6a21\u578b\u6838\u5fc3\u80fd\u529b\u7684\u7edf\u4e00\u57fa\u51c6\uff0c\u4e16\u754c\u6a21\u578b\u9700\u8981\u7406\u89e3\u3001\u8bb0\u5fc6\u548c\u9884\u6d4b\u52a8\u6001\u89c6\u89c9\u73af\u5883\uff0c\u4f46\u73b0\u6709\u57fa\u51c6\u65e0\u6cd5\u5168\u9762\u8bc4\u4f30\u5176\u8bb0\u5fc6\u4e00\u81f4\u6027\u548c\u52a8\u4f5c\u63a7\u5236\u80fd\u529b\u3002", "method": "\u521b\u5efaMIND\u57fa\u51c6\uff1a\u5305\u542b250\u4e2a1080p\u300124FPS\u9ad8\u8d28\u91cf\u89c6\u9891\uff08\u7b2c\u4e00\u4eba\u79f0\u548c\u7b2c\u4e09\u4eba\u79f0\uff09\uff0c\u8bbe\u8ba1\u9ad8\u6548\u8bc4\u4f30\u6846\u67b6\u6d4b\u91cf\u8bb0\u5fc6\u4e00\u81f4\u6027\u548c\u52a8\u4f5c\u63a7\u5236\u80fd\u529b\uff0c\u5f15\u5165MIND-World\u4f5c\u4e3a\u57fa\u7ebf\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eMIND\u57fa\u51c6\u7684\u5b8c\u6574\u6027\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u4e16\u754c\u6a21\u578b\u7684\u5173\u952e\u6311\u6218\uff1a\u96be\u4ee5\u7ef4\u6301\u957f\u671f\u8bb0\u5fc6\u4e00\u81f4\u6027\uff0c\u4ee5\u53ca\u5728\u5171\u4eab\u573a\u666f\u4e0b\u8de8\u52a8\u4f5c\u7a7a\u95f4\u7684\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u3002", "conclusion": "MIND\u4e3a\u4e16\u754c\u6a21\u578b\u8bc4\u4f30\u63d0\u4f9b\u4e86\u9996\u4e2a\u5f00\u653e\u57df\u95ed\u73af\u91cd\u8bbf\u57fa\u51c6\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u4e16\u754c\u6a21\u578b\u5728\u8bb0\u5fc6\u4e00\u81f4\u6027\u548c\u52a8\u4f5c\u63a7\u5236\u65b9\u9762\u7684\u7814\u7a76\uff0c\u5e76\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2602.08047", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.08047", "abs": "https://arxiv.org/abs/2602.08047", "authors": ["Jiahong Fu", "Qi Xie", "Deyu Meng", "Zongben Xu"], "title": "Vanilla Group Equivariant Vision Transformer: Simple and Effective", "comment": null, "summary": "Incorporating symmetry priors as inductive biases to design equivariant Vision Transformers (ViTs) has emerged as a promising avenue for enhancing their performance. However, existing equivariant ViTs often struggle to balance performance with equivariance, primarily due to the challenge of achieving holistic equivariant modifications across the diverse modules in ViTs-particularly in harmonizing the Self-Attention mechanism with Patch Embedding. To address this, we propose a straightforward framework that systematically renders key ViT components, including patch embedding, self-attention, positional encodings, and Down/Up-Sampling, equivariant, thereby constructing ViTs with guaranteed equivariance. The resulting architecture serves as a plug-and-play replacement that is both theoretically grounded and practically versatile, scaling seamlessly even to Swin Transformers. Extensive experiments demonstrate that our equivariant ViTs consistently improve performance and data efficiency across a wide spectrum of vision tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7cfb\u7edf\u5316\u6784\u5efa\u7b49\u53d8\u89c6\u89c9Transformer\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u4f7fViT\u5173\u952e\u7ec4\u4ef6\uff08\u5305\u62ecpatch embedding\u3001\u81ea\u6ce8\u610f\u529b\u3001\u4f4d\u7f6e\u7f16\u7801\u548c\u4e0a\u4e0b\u91c7\u6837\uff09\u7b49\u53d8\u5316\uff0c\u5b9e\u73b0\u7406\u8bba\u4fdd\u8bc1\u7684\u7b49\u53d8\u6027\uff0c\u63d0\u5347\u6027\u80fd\u548c\u6570\u636e\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u7b49\u53d8ViT\u5728\u6027\u80fd\u548c\u7b49\u53d8\u6027\u4e4b\u95f4\u96be\u4ee5\u5e73\u8861\uff0c\u4e3b\u8981\u56e0\u4e3a\u96be\u4ee5\u5728ViT\u7684\u591a\u6837\u5316\u6a21\u5757\u4e2d\u5b9e\u73b0\u6574\u4f53\u7b49\u53d8\u4fee\u6539\uff0c\u7279\u522b\u662f\u534f\u8c03\u81ea\u6ce8\u610f\u529b\u673a\u5236\u4e0epatch embedding\u7684\u7b49\u53d8\u6027\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u7b80\u5355\u6846\u67b6\uff0c\u7cfb\u7edf\u5316\u5730\u4f7fViT\u5173\u952e\u7ec4\u4ef6\u7b49\u53d8\u5316\uff0c\u5305\u62ecpatch embedding\u3001\u81ea\u6ce8\u610f\u529b\u3001\u4f4d\u7f6e\u7f16\u7801\u548cDown/Up-Sampling\uff0c\u6784\u5efa\u5177\u6709\u7406\u8bba\u4fdd\u8bc1\u7b49\u53d8\u6027\u7684ViT\u67b6\u6784\uff0c\u53ef\u4f5c\u4e3a\u5373\u63d2\u5373\u7528\u66ff\u4ee3\u65b9\u6848\uff0c\u5e76\u53ef\u6269\u5c55\u5230Swin Transformers\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u7b49\u53d8ViT\u5728\u5404\u79cd\u89c6\u89c9\u4efb\u52a1\u4e2d\u6301\u7eed\u63d0\u5347\u6027\u80fd\u548c\u6570\u636e\u6548\u7387\u3002", "conclusion": "\u8be5\u6846\u67b6\u63d0\u4f9b\u4e86\u4e00\u79cd\u7cfb\u7edf\u5316\u6784\u5efa\u7b49\u53d8ViT\u7684\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u7406\u8bba\u4fdd\u8bc1\u7684\u7b49\u53d8\u6027\uff0c\u540c\u65f6\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u548c\u6570\u636e\u6548\u7387\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5b9e\u7528\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2602.08058", "categories": ["cs.CV", "cs.AI", "cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.08058", "abs": "https://arxiv.org/abs/2602.08058", "authors": ["Xihang Yu", "Rajat Talak", "Lorenzo Shaikewitz", "Luca Carlone"], "title": "Picasso: Holistic Scene Reconstruction with Physics-Constrained Sampling", "comment": "15 pages", "summary": "In the presence of occlusions and measurement noise, geometrically accurate scene reconstructions -- which fit the sensor data -- can still be physically incorrect. For instance, when estimating the poses and shapes of objects in the scene and importing the resulting estimates into a simulator, small errors might translate to implausible configurations including object interpenetration or unstable equilibrium. This makes it difficult to predict the dynamic behavior of the scene using a digital twin, an important step in simulation-based planning and control of contact-rich behaviors. In this paper, we posit that object pose and shape estimation requires reasoning holistically over the scene (instead of reasoning about each object in isolation), accounting for object interactions and physical plausibility. Towards this goal, our first contribution is Picasso, a physics-constrained reconstruction pipeline that builds multi-object scene reconstructions by considering geometry, non-penetration, and physics. Picasso relies on a fast rejection sampling method that reasons over multi-object interactions, leveraging an inferred object contact graph to guide samples. Second, we propose the Picasso dataset, a collection of 10 contact-rich real-world scenes with ground truth annotations, as well as a metric to quantify physical plausibility, which we open-source as part of our benchmark. Finally, we provide an extensive evaluation of Picasso on our newly introduced dataset and on the YCB-V dataset, and show it largely outperforms the state of the art while providing reconstructions that are both physically plausible and more aligned with human intuition.", "AI": {"tldr": "Picasso\u662f\u4e00\u4e2a\u7269\u7406\u7ea6\u675f\u7684\u91cd\u5efa\u7ba1\u9053\uff0c\u901a\u8fc7\u8003\u8651\u51e0\u4f55\u3001\u975e\u7a7f\u900f\u6027\u548c\u7269\u7406\u539f\u7406\u6765\u6784\u5efa\u591a\u7269\u4f53\u573a\u666f\u91cd\u5efa\uff0c\u5728\u63a5\u89e6\u4e30\u5bcc\u7684\u573a\u666f\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5728\u906e\u6321\u548c\u6d4b\u91cf\u566a\u58f0\u5b58\u5728\u7684\u60c5\u51b5\u4e0b\uff0c\u51e0\u4f55\u4e0a\u51c6\u786e\u7684\u573a\u666f\u91cd\u5efa\u53ef\u80fd\u4ecd\u7136\u5728\u7269\u7406\u4e0a\u4e0d\u6b63\u786e\u3002\u5c0f\u7684\u8bef\u5dee\u53ef\u80fd\u5bfc\u81f4\u7269\u4f53\u76f8\u4e92\u7a7f\u900f\u6216\u4e0d\u7a33\u5b9a\u5e73\u8861\u7b49\u4e0d\u53ef\u4fe1\u7684\u914d\u7f6e\uff0c\u8fd9\u4f7f\u5f97\u4f7f\u7528\u6570\u5b57\u5b6a\u751f\u9884\u6d4b\u573a\u666f\u52a8\u6001\u884c\u4e3a\u53d8\u5f97\u56f0\u96be\u3002", "method": "\u63d0\u51faPicasso\u7269\u7406\u7ea6\u675f\u91cd\u5efa\u7ba1\u9053\uff0c\u4f7f\u7528\u5feb\u901f\u62d2\u7edd\u91c7\u6837\u65b9\u6cd5\u8003\u8651\u591a\u7269\u4f53\u4ea4\u4e92\uff0c\u5229\u7528\u63a8\u65ad\u7684\u7269\u4f53\u63a5\u89e6\u56fe\u6765\u6307\u5bfc\u91c7\u6837\u3002\u540c\u65f6\u521b\u5efa\u4e86Picasso\u6570\u636e\u96c6\u548c\u7269\u7406\u53ef\u4fe1\u6027\u5ea6\u91cf\u57fa\u51c6\u3002", "result": "\u5728\u65b0\u5efa\u7684Picasso\u6570\u636e\u96c6\u548cYCB-V\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u8bc4\u4f30\u8868\u660e\uff0cPicasso\u5927\u5e45\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u63d0\u4f9b\u65e2\u7269\u7406\u53ef\u4fe1\u53c8\u66f4\u7b26\u5408\u4eba\u7c7b\u76f4\u89c9\u7684\u91cd\u5efa\u7ed3\u679c\u3002", "conclusion": "\u7269\u4f53\u59ff\u6001\u548c\u5f62\u72b6\u4f30\u8ba1\u9700\u8981\u5bf9\u573a\u666f\u8fdb\u884c\u6574\u4f53\u63a8\u7406\uff0c\u8003\u8651\u7269\u4f53\u4ea4\u4e92\u548c\u7269\u7406\u53ef\u4fe1\u6027\u3002Picasso\u65b9\u6cd5\u901a\u8fc7\u7269\u7406\u7ea6\u675f\u91cd\u5efa\u7ba1\u9053\u5b9e\u73b0\u4e86\u8fd9\u4e00\u76ee\u6807\uff0c\u4e3a\u57fa\u4e8e\u4eff\u771f\u7684\u89c4\u5212\u548c\u63a5\u89e6\u4e30\u5bcc\u884c\u4e3a\u63a7\u5236\u63d0\u4f9b\u4e86\u91cd\u8981\u5de5\u5177\u3002"}}
{"id": "2602.08236", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.08236", "abs": "https://arxiv.org/abs/2602.08236", "authors": ["Shoubin Yu", "Yue Zhang", "Zun Wang", "Jaehong Yoon", "Huaxiu Yao", "Mingyu Ding", "Mohit Bansal"], "title": "When and How Much to Imagine: Adaptive Test-Time Scaling with World Models for Visual Spatial Reasoning", "comment": "the first two authors are equally contributed. Project page: https://adaptive-visual-tts.github.io/", "summary": "Despite rapid progress in Multimodal Large Language Models (MLLMs), visual spatial reasoning remains unreliable when correct answers depend on how a scene would appear under unseen or alternative viewpoints. Recent work addresses this by augmenting reasoning with world models for visual imagination, but questions such as when imagination is actually necessary, how much of it is beneficial, and when it becomes harmful, remain poorly understood. In practice, indiscriminate imagination can increase computation and even degrade performance by introducing misleading evidence. In this work, we present an in-depth analysis of test-time visual imagination as a controllable resource for spatial reasoning. We study when static visual evidence is sufficient, when imagination improves reasoning, and how excessive or unnecessary imagination affects accuracy and efficiency. To support this analysis, we introduce AVIC, an adaptive test-time framework with world models that explicitly reasons about the sufficiency of current visual evidence before selectively invoking and scaling visual imagination. Across spatial reasoning benchmarks (SAT, MMSI) and an embodied navigation benchmark (R2R), our results reveal clear scenarios where imagination is critical, marginal, or detrimental, and show that selective control can match or outperform fixed imagination strategies with substantially fewer world-model calls and language tokens. Overall, our findings highlight the importance of analyzing and controlling test-time imagination for efficient and reliable spatial reasoning.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faAVIC\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u63a7\u5236\u89c6\u89c9\u60f3\u8c61\u6765\u63d0\u5347\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\uff0c\u5206\u6790\u4f55\u65f6\u9700\u8981\u60f3\u8c61\u3001\u9700\u8981\u591a\u5c11\u60f3\u8c61\uff0c\u4ee5\u53ca\u4f55\u65f6\u60f3\u8c61\u53cd\u800c\u6709\u5bb3\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u89c6\u89c9\u7a7a\u95f4\u63a8\u7406\u4e2d\u5b58\u5728\u5c40\u9650\u6027\uff0c\u7279\u522b\u662f\u5728\u9700\u8981\u4ece\u672a\u89c1\u8fc7\u6216\u66ff\u4ee3\u89c6\u89d2\u7406\u89e3\u573a\u666f\u65f6\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u8fc7\u4e16\u754c\u6a21\u578b\u589e\u5f3a\u89c6\u89c9\u60f3\u8c61\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u4f55\u65f6\u9700\u8981\u60f3\u8c61\u3001\u9700\u8981\u591a\u5c11\u60f3\u8c61\u4ee5\u53ca\u4f55\u65f6\u60f3\u8c61\u53cd\u800c\u6709\u5bb3\u7684\u7cfb\u7edf\u5206\u6790\u3002\u4e0d\u52a0\u9009\u62e9\u7684\u60f3\u8c61\u4f1a\u589e\u52a0\u8ba1\u7b97\u6210\u672c\uff0c\u751a\u81f3\u56e0\u5f15\u5165\u8bef\u5bfc\u6027\u8bc1\u636e\u800c\u964d\u4f4e\u6027\u80fd\u3002", "method": "\u63d0\u51faAVIC\u81ea\u9002\u5e94\u6d4b\u8bd5\u65f6\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u4f7f\u7528\u4e16\u754c\u6a21\u578b\uff0c\u5728\u9009\u62e9\u6027\u8c03\u7528\u548c\u6269\u5c55\u89c6\u89c9\u60f3\u8c61\u4e4b\u524d\uff0c\u660e\u786e\u63a8\u7406\u5f53\u524d\u89c6\u89c9\u8bc1\u636e\u7684\u5145\u5206\u6027\u3002\u901a\u8fc7\u5206\u6790\u9759\u6001\u89c6\u89c9\u8bc1\u636e\u4f55\u65f6\u8db3\u591f\u3001\u60f3\u8c61\u4f55\u65f6\u80fd\u6539\u5584\u63a8\u7406\uff0c\u4ee5\u53ca\u8fc7\u5ea6\u6216\u4e0d\u5fc5\u8981\u7684\u60f3\u8c61\u5982\u4f55\u5f71\u54cd\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "result": "\u5728\u7a7a\u95f4\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\uff08SAT\u3001MMSI\uff09\u548c\u5177\u8eab\u5bfc\u822a\u57fa\u51c6\u6d4b\u8bd5\uff08R2R\uff09\u4e2d\uff0c\u7ed3\u679c\u663e\u793a\u5b58\u5728\u660e\u786e\u573a\u666f\uff1a\u60f3\u8c61\u662f\u5173\u952e\u7684\u3001\u8fb9\u9645\u7684\u6216\u6709\u5bb3\u7684\u3002\u9009\u62e9\u6027\u63a7\u5236\u7b56\u7565\u80fd\u591f\u5339\u914d\u6216\u4f18\u4e8e\u56fa\u5b9a\u60f3\u8c61\u7b56\u7565\uff0c\u540c\u65f6\u663e\u8457\u51cf\u5c11\u4e16\u754c\u6a21\u578b\u8c03\u7528\u548c\u8bed\u8a00\u6807\u8bb0\u6570\u91cf\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u5206\u6790\u548c\u63a7\u5236\u6d4b\u8bd5\u65f6\u60f3\u8c61\u5bf9\u4e8e\u9ad8\u6548\u53ef\u9760\u7684\u7a7a\u95f4\u63a8\u7406\u7684\u91cd\u8981\u6027\u3002\u81ea\u9002\u5e94\u63a7\u5236\u89c6\u89c9\u60f3\u8c61\u53ef\u4ee5\u5e73\u8861\u51c6\u786e\u6027\u548c\u6548\u7387\uff0c\u907f\u514d\u4e0d\u5fc5\u8981\u7684\u8ba1\u7b97\u5f00\u9500\u548c\u6027\u80fd\u4e0b\u964d\u3002"}}
{"id": "2602.08059", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08059", "abs": "https://arxiv.org/abs/2602.08059", "authors": ["Tong Zhang", "Ru Zhang", "Jianyi Liu"], "title": "DICE: Disentangling Artist Style from Content via Contrastive Subspace Decomposition in Diffusion Models", "comment": null, "summary": "The recent proliferation of diffusion models has made style mimicry effortless, enabling users to imitate unique artistic styles without authorization. In deployed platforms, this raises copyright and intellectual-property risks and calls for reliable protection. However, existing countermeasures either require costly weight editing as new styles emerge or rely on an explicitly specified editing style, limiting their practicality for deployment-side safety. To address this challenge, we propose DICE (Disentanglement of artist Style from Content via Contrastive Subspace Decomposition), a training-free framework for on-the-fly artist style erasure. Unlike style editing that require an explicitly specified replacement style, DICE performs style purification, removing the artist's characteristics while preserving the user-intended content. Our core insight is that a model cannot truly comprehend the artist style from a single text or image alone. Consequently, we abandon the traditional paradigm of identifying style from isolated samples. Instead, we construct contrastive triplets to compel the model to distinguish between style and non-style features in the latent space. By formalizing this disentanglement process as a solvable generalized eigenvalue problem, we achieve precise identification of the style subspace. Furthermore, we introduce an Adaptive Attention Decoupling Editing strategy dynamically assesses the style concentration of each token and performs differential suppression and content enhancement on the QKV vectors. Extensive experiments demonstrate that DICE achieves a superior balance between the thoroughness of style erasure and the preservation of content integrity. DICE introduces an additional overhead of only 3 seconds to disentangle style, providing a practical and efficient technique for curbing style mimicry.", "AI": {"tldr": "DICE\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684\u827a\u672f\u98ce\u683c\u64e6\u9664\u6846\u67b6\uff0c\u901a\u8fc7\u5bf9\u6bd4\u5b50\u7a7a\u95f4\u5206\u89e3\u5b9e\u73b0\u98ce\u683c\u4e0e\u5185\u5bb9\u7684\u89e3\u8026\uff0c\u6709\u6548\u9632\u6b62\u672a\u7ecf\u6388\u6743\u7684\u827a\u672f\u98ce\u683c\u6a21\u4eff\uff0c\u540c\u65f6\u4fdd\u6301\u5185\u5bb9\u5b8c\u6574\u6027\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u7684\u666e\u53ca\u4f7f\u5f97\u827a\u672f\u98ce\u683c\u6a21\u4eff\u53d8\u5f97\u5bb9\u6613\uff0c\u5f15\u53d1\u7248\u6743\u548c\u77e5\u8bc6\u4ea7\u6743\u98ce\u9669\u3002\u73b0\u6709\u5bf9\u7b56\u8981\u4e48\u9700\u8981\u6602\u8d35\u7684\u6743\u91cd\u7f16\u8f91\uff0c\u8981\u4e48\u4f9d\u8d56\u660e\u786e\u6307\u5b9a\u7684\u7f16\u8f91\u98ce\u683c\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u90e8\u7f72\u7684\u53ef\u884c\u6027\u3002", "method": "DICE\u91c7\u7528\u8bad\u7ec3\u81ea\u7531\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u6784\u5efa\u5bf9\u6bd4\u4e09\u5143\u7ec4\u8ba9\u6a21\u578b\u533a\u5206\u98ce\u683c\u4e0e\u975e\u98ce\u683c\u7279\u5f81\uff0c\u5c06\u89e3\u8026\u8fc7\u7a0b\u5f62\u5f0f\u5316\u4e3a\u53ef\u89e3\u7684\u5e7f\u4e49\u7279\u5f81\u503c\u95ee\u9898\uff0c\u7cbe\u786e\u5b9a\u4f4d\u98ce\u683c\u5b50\u7a7a\u95f4\u3002\u540c\u65f6\u5f15\u5165\u81ea\u9002\u5e94\u6ce8\u610f\u529b\u89e3\u8026\u7f16\u8f91\u7b56\u7565\uff0c\u52a8\u6001\u8bc4\u4f30\u6bcf\u4e2atoken\u7684\u98ce\u683c\u6d53\u5ea6\uff0c\u5bf9QKV\u5411\u91cf\u8fdb\u884c\u5dee\u5f02\u5316\u6291\u5236\u548c\u5185\u5bb9\u589e\u5f3a\u3002", "result": "\u5b9e\u9a8c\u8868\u660eDICE\u5728\u98ce\u683c\u64e6\u9664\u5f7b\u5e95\u6027\u548c\u5185\u5bb9\u5b8c\u6574\u6027\u4fdd\u62a4\u4e4b\u95f4\u5b9e\u73b0\u4e86\u4f18\u8d8a\u7684\u5e73\u8861\u3002\u98ce\u683c\u89e3\u8026\u4ec5\u589e\u52a03\u79d2\u989d\u5916\u5f00\u9500\uff0c\u4e3a\u904f\u5236\u98ce\u683c\u6a21\u4eff\u63d0\u4f9b\u4e86\u5b9e\u7528\u9ad8\u6548\u7684\u6280\u672f\u65b9\u6848\u3002", "conclusion": "DICE\u901a\u8fc7\u5bf9\u6bd4\u5b50\u7a7a\u95f4\u5206\u89e3\u5b9e\u73b0\u4e86\u827a\u672f\u98ce\u683c\u4e0e\u5185\u5bb9\u7684\u6709\u6548\u89e3\u8026\uff0c\u4e3a\u90e8\u7f72\u4fa7\u5b89\u5168\u63d0\u4f9b\u4e86\u5b9e\u7528\u3001\u9ad8\u6548\u7684\u98ce\u683c\u64e6\u9664\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u6709\u6548\u4fdd\u62a4\u827a\u672f\u5bb6\u7684\u72ec\u7279\u98ce\u683c\u514d\u906d\u672a\u7ecf\u6388\u6743\u7684\u6a21\u4eff\u3002"}}
{"id": "2602.08068", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.08068", "abs": "https://arxiv.org/abs/2602.08068", "authors": ["Chunyang Li", "Yuanbo Yang", "Jiahao Shao", "Hongyu Zhou", "Katja Schwarz", "Yiyi Liao"], "title": "ReRoPE: Repurposing RoPE for Relative Camera Control", "comment": null, "summary": "Video generation with controllable camera viewpoints is essential for applications such as interactive content creation, gaming, and simulation. Existing methods typically adapt pre-trained video models using camera poses relative to a fixed reference, e.g., the first frame. However, these encodings lack shift-invariance, often leading to poor generalization and accumulated drift. While relative camera pose embeddings defined between arbitrary view pairs offer a more robust alternative, integrating them into pre-trained video diffusion models without prohibitive training costs or architectural changes remains challenging. We introduce ReRoPE, a plug-and-play framework that incorporates relative camera information into pre-trained video diffusion models without compromising their generation capability. Our approach is based on the insight that Rotary Positional Embeddings (RoPE) in existing models underutilize their full spectral bandwidth, particularly in the low-frequency components. By seamlessly injecting relative camera pose information into these underutilized bands, ReRoPE achieves precise control while preserving strong pre-trained generative priors. We evaluate our method on both image-to-video (I2V) and video-to-video (V2V) tasks in terms of camera control accuracy and visual fidelity. Our results demonstrate that ReRoPE offers a training-efficient path toward controllable, high-fidelity video generation. See project page for more results: https://sisyphe-lee.github.io/ReRoPE/", "AI": {"tldr": "ReRoPE\u662f\u4e00\u4e2a\u5373\u63d2\u5373\u7528\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u76f8\u5bf9\u76f8\u673a\u59ff\u6001\u4fe1\u606f\u6ce8\u5165\u9884\u8bad\u7ec3\u89c6\u9891\u6269\u6563\u6a21\u578b\u4e2d\u672a\u5145\u5206\u5229\u7528\u7684RoPE\u4f4e\u9891\u5206\u91cf\uff0c\u5b9e\u73b0\u53ef\u63a7\u89c6\u89d2\u89c6\u9891\u751f\u6210\uff0c\u65e0\u9700\u5927\u91cf\u8bad\u7ec3\u6216\u67b6\u6784\u4fee\u6539\u3002", "motivation": "\u73b0\u6709\u53ef\u63a7\u89c6\u89d2\u89c6\u9891\u751f\u6210\u65b9\u6cd5\u901a\u5e38\u4f7f\u7528\u76f8\u5bf9\u4e8e\u56fa\u5b9a\u53c2\u8003\u5e27\uff08\u5982\u7b2c\u4e00\u5e27\uff09\u7684\u76f8\u673a\u59ff\u6001\u7f16\u7801\uff0c\u4f46\u8fd9\u4e9b\u7f16\u7801\u7f3a\u4e4f\u5e73\u79fb\u4e0d\u53d8\u6027\uff0c\u5bfc\u81f4\u6cdb\u5316\u80fd\u529b\u5dee\u548c\u7d2f\u79ef\u6f02\u79fb\u95ee\u9898\u3002\u76f8\u5bf9\u76f8\u673a\u59ff\u6001\u5d4c\u5165\u867d\u7136\u66f4\u9c81\u68d2\uff0c\u4f46\u5c06\u5176\u96c6\u6210\u5230\u9884\u8bad\u7ec3\u89c6\u9891\u6269\u6563\u6a21\u578b\u4e2d\u9700\u8981\u9ad8\u6602\u8bad\u7ec3\u6210\u672c\u6216\u67b6\u6784\u4fee\u6539\u3002", "method": "ReRoPE\u57fa\u4e8e\u4e00\u4e2a\u5173\u952e\u6d1e\u5bdf\uff1a\u73b0\u6709\u6a21\u578b\u4e2d\u7684\u65cb\u8f6c\u4f4d\u7f6e\u5d4c\u5165\uff08RoPE\uff09\u672a\u5145\u5206\u5229\u7528\u5176\u5168\u9891\u8c31\u5e26\u5bbd\uff0c\u7279\u522b\u662f\u4f4e\u9891\u5206\u91cf\u3002\u901a\u8fc7\u5c06\u76f8\u5bf9\u76f8\u673a\u59ff\u6001\u4fe1\u606f\u65e0\u7f1d\u6ce8\u5165\u8fd9\u4e9b\u672a\u5145\u5206\u5229\u7528\u7684\u9891\u5e26\uff0c\u5728\u4fdd\u6301\u9884\u8bad\u7ec3\u751f\u6210\u5148\u9a8c\u7684\u540c\u65f6\u5b9e\u73b0\u7cbe\u786e\u76f8\u673a\u63a7\u5236\u3002", "result": "\u5728\u56fe\u50cf\u5230\u89c6\u9891\uff08I2V\uff09\u548c\u89c6\u9891\u5230\u89c6\u9891\uff08V2V\uff09\u4efb\u52a1\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0cReRoPE\u5728\u76f8\u673a\u63a7\u5236\u7cbe\u5ea6\u548c\u89c6\u89c9\u4fdd\u771f\u5ea6\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u63d0\u4f9b\u4e86\u8bad\u7ec3\u9ad8\u6548\u7684\u53ef\u63a7\u9ad8\u4fdd\u771f\u89c6\u9891\u751f\u6210\u8def\u5f84\u3002", "conclusion": "ReRoPE\u662f\u4e00\u4e2a\u5373\u63d2\u5373\u7528\u6846\u67b6\uff0c\u901a\u8fc7\u5de7\u5999\u5229\u7528RoPE\u7684\u672a\u5145\u5206\u5229\u7528\u9891\u8c31\u8d44\u6e90\uff0c\u5b9e\u73b0\u4e86\u5bf9\u9884\u8bad\u7ec3\u89c6\u9891\u6269\u6563\u6a21\u578b\u7684\u9ad8\u6548\u76f8\u5bf9\u76f8\u673a\u59ff\u6001\u63a7\u5236\uff0c\u4e3a\u53ef\u63a7\u89c6\u9891\u751f\u6210\u63d0\u4f9b\u4e86\u8bad\u7ec3\u6210\u672c\u4f4e\u3001\u6027\u80fd\u4f18\u8d8a\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.08503", "categories": ["cs.CV", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.08503", "abs": "https://arxiv.org/abs/2602.08503", "authors": ["Yi Ding", "Ziliang Qiu", "Bolian Li", "Ruqi Zhang"], "title": "Learning Self-Correction in Vision-Language Models via Rollout Augmentation", "comment": "17 pages", "summary": "Self-correction is essential for solving complex reasoning problems in vision-language models (VLMs). However, existing reinforcement learning (RL) methods struggle to learn it, as effective self-correction behaviors emerge only rarely, making learning signals extremely sparse. To address this challenge, we propose correction-specific rollouts (Octopus), an RL rollout augmentation framework that synthesizes dense self-correction examples by recombining existing rollouts. This augmentation simultaneously improves sample efficiency due to rollout reuse and stabilizes RL optimization through balanced supervision. Furthermore, we introduce a response-masking strategy that decouples self-correction from direct reasoning, avoiding signal conflicts and enabling both behaviors to be learned effectively. Building on this, we introduce Octopus-8B, a reasoning VLM with controllable self-correction capability. Across 7 benchmarks, it achieves SoTA performance among open-source VLMs, outperforming the best RLVR baseline by 1.0 score while requiring only $0.72\\times$ training time per step.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faOctopus\u6846\u67b6\uff0c\u901a\u8fc7\u91cd\u7ec4\u73b0\u6709rollouts\u5408\u6210\u5bc6\u96c6\u7684\u81ea\u6821\u6b63\u793a\u4f8b\uff0c\u89e3\u51b3VLMs\u4e2d\u81ea\u6821\u6b63\u5b66\u4e60\u4fe1\u53f7\u7a00\u758f\u7684\u95ee\u9898\uff0c\u5e76\u5f00\u53d1\u4e86\u5177\u6709\u53ef\u63a7\u81ea\u6821\u6b63\u80fd\u529b\u7684Octopus-8B\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u5b66\u4e60\u81ea\u6821\u6b63\u884c\u4e3a\u56f0\u96be\uff0c\u56e0\u4e3a\u6709\u6548\u7684\u81ea\u6821\u6b63\u884c\u4e3a\u51fa\u73b0\u9891\u7387\u4f4e\uff0c\u5bfc\u81f4\u5b66\u4e60\u4fe1\u53f7\u6781\u5176\u7a00\u758f\u3002", "method": "\u63d0\u51fa\u6821\u6b63\u7279\u5b9arollouts\uff08Octopus\uff09\u6846\u67b6\uff1a1\uff09\u901a\u8fc7\u91cd\u7ec4\u73b0\u6709rollouts\u5408\u6210\u5bc6\u96c6\u7684\u81ea\u6821\u6b63\u793a\u4f8b\uff1b2\uff09\u5f15\u5165\u54cd\u5e94\u63a9\u7801\u7b56\u7565\uff0c\u5c06\u81ea\u6821\u6b63\u4e0e\u76f4\u63a5\u63a8\u7406\u89e3\u8026\uff1b3\uff09\u5f00\u53d1Octopus-8B\u6a21\u578b\u3002", "result": "\u57287\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cOctopus-8B\u5728\u5f00\u6e90VLMs\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u6bd4\u6700\u4f73RLVR\u57fa\u7ebf\u63d0\u9ad81.0\u5206\uff0c\u540c\u65f6\u6bcf\u4e2a\u8bad\u7ec3\u6b65\u9aa4\u4ec5\u97000.72\u500d\u65f6\u95f4\u3002", "conclusion": "Octopus\u6846\u67b6\u901a\u8fc7rollout\u91cd\u7ec4\u548c\u54cd\u5e94\u63a9\u7801\u7b56\u7565\u6709\u6548\u89e3\u51b3\u4e86VLMs\u4e2d\u81ea\u6821\u6b63\u5b66\u4e60\u7684\u7a00\u758f\u4fe1\u53f7\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u7a33\u5b9a\u7684\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u3002"}}
{"id": "2602.08099", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08099", "abs": "https://arxiv.org/abs/2602.08099", "authors": ["Issar Tzachor", "Dvir Samuel", "Rami Ben-Ari"], "title": "VidVec: Unlocking Video MLLM Embeddings for Video-Text Retrieval", "comment": "Project page: https://iyttor.github.io/VidVec/", "summary": "Recent studies have adapted generative Multimodal Large Language Models (MLLMs) into embedding extractors for vision tasks, typically through fine-tuning to produce universal representations. However, their performance on video remains inferior to Video Foundation Models (VFMs). In this paper, we focus on leveraging MLLMs for video-text embedding and retrieval. We first conduct a systematic layer-wise analysis, showing that intermediate (pre-trained) MLLM layers already encode substantial task-relevant information. Leveraging this insight, we demonstrate that combining intermediate-layer embeddings with a calibrated MLLM head yields strong zero-shot retrieval performance without any training. Building on these findings, we introduce a lightweight text-based alignment strategy which maps dense video captions to short summaries and enables task-related video-text embedding learning without visual supervision. Remarkably, without any fine-tuning beyond text, our method outperforms current methods, often by a substantial margin, achieving state-of-the-art results across common video retrieval benchmarks.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u89c6\u9891\u6587\u672c\u5d4c\u5165\u548c\u68c0\u7d22\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e2d\u95f4\u5c42\u5206\u6790\u548c\u8f7b\u91cf\u7ea7\u6587\u672c\u5bf9\u9f50\u7b56\u7565\uff0c\u5728\u96f6\u6837\u672c\u8bbe\u7f6e\u4e0b\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u89c6\u9891\u68c0\u7d22\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u5c06\u751f\u6210\u5f0f\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u9002\u914d\u4e3a\u89c6\u89c9\u4efb\u52a1\u7684\u5d4c\u5165\u63d0\u53d6\u5668\uff0c\u4f46\u5b83\u4eec\u5728\u89c6\u9891\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u4ecd\u4e0d\u5982\u4e13\u95e8\u7684\u89c6\u9891\u57fa\u7840\u6a21\u578b\u3002\u672c\u6587\u65e8\u5728\u63a2\u7d22\u5982\u4f55\u66f4\u597d\u5730\u5229\u7528MLLMs\u8fdb\u884c\u89c6\u9891\u6587\u672c\u5d4c\u5165\u548c\u68c0\u7d22\u3002", "method": "\u9996\u5148\u8fdb\u884c\u7cfb\u7edf\u6027\u7684\u5c42\u95f4\u5206\u6790\uff0c\u53d1\u73b0MLLMs\u7684\u4e2d\u95f4\u5c42\u5df2\u7f16\u7801\u5927\u91cf\u4efb\u52a1\u76f8\u5173\u4fe1\u606f\uff1b\u7136\u540e\u7ed3\u5408\u4e2d\u95f4\u5c42\u5d4c\u5165\u548c\u6821\u51c6\u7684MLLM\u5934\u90e8\u5b9e\u73b0\u96f6\u6837\u672c\u68c0\u7d22\uff1b\u6700\u540e\u5f15\u5165\u8f7b\u91cf\u7ea7\u6587\u672c\u5bf9\u9f50\u7b56\u7565\uff0c\u5c06\u5bc6\u96c6\u89c6\u9891\u63cf\u8ff0\u6620\u5c04\u4e3a\u7b80\u77ed\u6458\u8981\uff0c\u5b9e\u73b0\u65e0\u9700\u89c6\u89c9\u76d1\u7763\u7684\u4efb\u52a1\u76f8\u5173\u89c6\u9891\u6587\u672c\u5d4c\u5165\u5b66\u4e60\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u96f6\u6837\u672c\u8bbe\u7f6e\u4e0b\u8d85\u8d8a\u4e86\u5f53\u524d\u65b9\u6cd5\uff0c\u901a\u5e38\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0c\u5728\u5e38\u89c1\u89c6\u9891\u68c0\u7d22\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\uff0c\u4e14\u65e0\u9700\u4efb\u4f55\u89c6\u89c9\u76d1\u7763\u7684\u5fae\u8c03\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u901a\u8fc7\u5229\u7528MLLMs\u4e2d\u95f4\u5c42\u7684\u4e30\u5bcc\u8868\u793a\u80fd\u529b\u548c\u8f7b\u91cf\u7ea7\u6587\u672c\u5bf9\u9f50\u7b56\u7565\uff0c\u53ef\u4ee5\u5728\u65e0\u9700\u89c6\u89c9\u76d1\u7763\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u5f3a\u5927\u7684\u89c6\u9891\u6587\u672c\u68c0\u7d22\u6027\u80fd\uff0c\u4e3a\u89c6\u9891\u7406\u89e3\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u7684\u6709\u6548\u9014\u5f84\u3002"}}
{"id": "2602.08112", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.08112", "abs": "https://arxiv.org/abs/2602.08112", "authors": ["Sidike Paheding", "Abel Reyes-Angulo", "Leo Thomas Ramos", "Angel D. Sappa", "Rajaneesh A.", "Hiral P. B.", "Sajin Kumar K. S.", "Thomas Oommen"], "title": "MMLSv2: A Multimodal Dataset for Martian Landslide Detection in Remote Sensing Imagery", "comment": null, "summary": "We present MMLSv2, a dataset for landslide segmentation on Martian surfaces. MMLSv2 consists of multimodal imagery with seven bands: RGB, digital elevation model, slope, thermal inertia, and grayscale channels. MMLSv2 comprises 664 images distributed across training, validation, and test splits. In addition, an isolated test set of 276 images from a geographically disjoint region from the base dataset is released to evaluate spatial generalization. Experiments conducted with multiple segmentation models show that the dataset supports stable training and achieves competitive performance, while still posing challenges in fragmented, elongated, and small-scale landslide regions. Evaluation on the isolated test set leads to a noticeable performance drop, indicating increased difficulty and highlighting its value for assessing model robustness and generalization beyond standard in-distribution settings. Dataset will be available at: https://github.com/MAIN-Lab/MMLS_v2", "AI": {"tldr": "MMLSv2\u662f\u4e00\u4e2a\u7528\u4e8e\u706b\u661f\u8868\u9762\u6ed1\u5761\u5206\u5272\u7684\u591a\u6a21\u6001\u6570\u636e\u96c6\uff0c\u5305\u542b7\u4e2a\u6ce2\u6bb5\u56fe\u50cf\u548c664\u5f20\u56fe\u50cf\uff0c\u8fd8\u6709\u4e00\u4e2a276\u5f20\u56fe\u50cf\u7684\u72ec\u7acb\u6d4b\u8bd5\u96c6\u7528\u4e8e\u8bc4\u4f30\u7a7a\u95f4\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u9700\u8981\u521b\u5efa\u4e13\u95e8\u7684\u706b\u661f\u6ed1\u5761\u5206\u5272\u6570\u636e\u96c6\u6765\u652f\u6301\u76f8\u5173\u7814\u7a76\uff0c\u73b0\u6709\u6570\u636e\u96c6\u53ef\u80fd\u4e0d\u8db3\u4ee5\u8bc4\u4f30\u6a21\u578b\u5728\u706b\u661f\u73af\u5883\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u548c\u9c81\u68d2\u6027\u3002", "method": "\u6784\u5efa\u5305\u542bRGB\u3001\u6570\u5b57\u9ad8\u7a0b\u6a21\u578b\u3001\u5761\u5ea6\u3001\u70ed\u60ef\u6027\u548c\u7070\u5ea6\u901a\u9053\u7684\u4e03\u6ce2\u6bb5\u591a\u6a21\u6001\u56fe\u50cf\u6570\u636e\u96c6\uff0c\u5206\u4e3a\u8bad\u7ec3\u3001\u9a8c\u8bc1\u548c\u6d4b\u8bd5\u96c6\uff0c\u5e76\u521b\u5efa\u5730\u7406\u9694\u79bb\u7684\u72ec\u7acb\u6d4b\u8bd5\u96c6\u3002", "result": "\u6570\u636e\u96c6\u652f\u6301\u7a33\u5b9a\u8bad\u7ec3\u5e76\u8fbe\u5230\u7ade\u4e89\u6027\u6027\u80fd\uff0c\u4f46\u5728\u788e\u7247\u5316\u3001\u7ec6\u957f\u548c\u5c0f\u89c4\u6a21\u6ed1\u5761\u533a\u57df\u4ecd\u5b58\u5728\u6311\u6218\uff1b\u5728\u72ec\u7acb\u6d4b\u8bd5\u96c6\u4e0a\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff0c\u8868\u660e\u5176\u80fd\u6709\u6548\u8bc4\u4f30\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "MMLSv2\u6570\u636e\u96c6\u4e3a\u706b\u661f\u6ed1\u5761\u5206\u5272\u7814\u7a76\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u57fa\u51c6\uff0c\u7279\u522b\u9002\u5408\u8bc4\u4f30\u6a21\u578b\u5728\u5206\u5e03\u5916\u573a\u666f\u4e0b\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2602.08117", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.08117", "abs": "https://arxiv.org/abs/2602.08117", "authors": ["Smriti Siva", "Jan Cross-Zamirski"], "title": "Building Damage Detection using Satellite Images and Patch-Based Transformer Methods", "comment": "8 pages, 5 figures", "summary": "Rapid building damage assessment is critical for post-disaster response. Damage classification models built on satellite imagery provide a scalable means of obtaining situational awareness. However, label noise and severe class imbalance in satellite data create major challenges. The xBD dataset offers a standardized benchmark for building-level damage across diverse geographic regions. In this study, we evaluate Vision Transformer (ViT) model performance on the xBD dataset, specifically investigating how these models distinguish between types of structural damage when training on noisy, imbalanced data.\n  In this study, we specifically evaluate DINOv2-small and DeiT for multi-class damage classification. We propose a targeted patch-based pre-processing pipeline to isolate structural features and minimize background noise in training. We adopt a frozen-head fine-tuning strategy to keep computational requirements manageable. Model performance is evaluated through accuracy, precision, recall, and macro-averaged F1 scores. We show that small ViT architectures with our novel training method achieves competitive macro-averaged F1 relative to prior CNN baselines for disaster classification.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e86Vision Transformer\u6a21\u578b\u5728xBD\u6570\u636e\u96c6\u4e0a\u7684\u5efa\u7b51\u635f\u4f24\u5206\u7c7b\u6027\u80fd\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u6027\u7684\u8865\u4e01\u9884\u5904\u7406\u6d41\u7a0b\u548c\u51bb\u7ed3\u5934\u5fae\u8c03\u7b56\u7565\uff0c\u5728\u566a\u58f0\u548c\u4e0d\u5e73\u8861\u6570\u636e\u4e0a\u53d6\u5f97\u4e86\u4e0eCNN\u57fa\u7ebf\u76f8\u5f53\u7684F1\u5206\u6570\u3002", "motivation": "\u707e\u540e\u5feb\u901f\u5efa\u7b51\u635f\u4f24\u8bc4\u4f30\u5bf9\u5e94\u6025\u54cd\u5e94\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u536b\u661f\u56fe\u50cf\u6570\u636e\u5b58\u5728\u6807\u7b7e\u566a\u58f0\u548c\u4e25\u91cd\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u8fd9\u7ed9\u635f\u4f24\u5206\u7c7b\u6a21\u578b\u5e26\u6765\u4e86\u91cd\u5927\u6311\u6218\u3002xBD\u6570\u636e\u96c6\u63d0\u4f9b\u4e86\u8de8\u5730\u7406\u533a\u57df\u7684\u6807\u51c6\u5316\u57fa\u51c6\uff0c\u672c\u7814\u7a76\u65e8\u5728\u8bc4\u4f30ViT\u6a21\u578b\u5728\u8be5\u6570\u636e\u96c6\u4e0a\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u566a\u58f0\u548c\u4e0d\u5e73\u8861\u6570\u636e\u4e0b\u533a\u5206\u7ed3\u6784\u635f\u4f24\u7c7b\u578b\u7684\u80fd\u529b\u3002", "method": "\u7814\u7a76\u8bc4\u4f30\u4e86DINOv2-small\u548cDeiT\u6a21\u578b\u8fdb\u884c\u591a\u7c7b\u522b\u635f\u4f24\u5206\u7c7b\u3002\u63d0\u51fa\u4e86\u9488\u5bf9\u6027\u7684\u8865\u4e01\u9884\u5904\u7406\u6d41\u7a0b\u6765\u9694\u79bb\u7ed3\u6784\u7279\u5f81\u5e76\u6700\u5c0f\u5316\u8bad\u7ec3\u4e2d\u7684\u80cc\u666f\u566a\u58f0\u3002\u91c7\u7528\u51bb\u7ed3\u5934\u5fae\u8c03\u7b56\u7565\u4ee5\u4fdd\u6301\u8ba1\u7b97\u9700\u6c42\u53ef\u63a7\u3002\u901a\u8fc7\u51c6\u786e\u7387\u3001\u7cbe\u786e\u7387\u3001\u53ec\u56de\u7387\u548c\u5b8f\u89c2\u5e73\u5747F1\u5206\u6570\u8bc4\u4f30\u6a21\u578b\u6027\u80fd\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u91c7\u7528\u65b0\u9896\u8bad\u7ec3\u65b9\u6cd5\u7684\u5c0f\u578bViT\u67b6\u6784\u5728\u707e\u5bb3\u5206\u7c7b\u4efb\u52a1\u4e2d\uff0c\u76f8\u5bf9\u4e8e\u5148\u524d\u7684CNN\u57fa\u7ebf\u6a21\u578b\uff0c\u53d6\u5f97\u4e86\u5177\u6709\u7ade\u4e89\u529b\u7684\u5b8f\u89c2\u5e73\u5747F1\u5206\u6570\u3002", "conclusion": "\u5c0f\u578bVision Transformer\u67b6\u6784\u914d\u5408\u9488\u5bf9\u6027\u7684\u9884\u5904\u7406\u548c\u5fae\u8c03\u7b56\u7565\uff0c\u80fd\u591f\u5728\u566a\u58f0\u548c\u4e0d\u5e73\u8861\u7684\u536b\u661f\u56fe\u50cf\u6570\u636e\u4e0a\u5b9e\u73b0\u6709\u6548\u7684\u5efa\u7b51\u635f\u4f24\u5206\u7c7b\uff0c\u4e3a\u707e\u540e\u5feb\u901f\u8bc4\u4f30\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u6280\u672f\u65b9\u6848\u3002"}}
{"id": "2602.08131", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.08131", "abs": "https://arxiv.org/abs/2602.08131", "authors": ["Isaac Corley", "Hannah Kerner", "Caleb Robinson", "Jennifer Marcus"], "title": "Fields of The World: A Field Guide for Extracting Agricultural Field Boundaries", "comment": null, "summary": "Field boundary maps are a building block for agricultural data products and support crop monitoring, yield estimation, and disease estimation. This tutorial presents the Fields of The World (FTW) ecosystem: a benchmark of 1.6M field polygons across 24 countries, pre-trained segmentation models, and command-line inference tools. We provide two notebooks that cover (1) local-scale field boundary extraction with crop classification and forest loss attribution, and (2) country-scale inference using cloud-optimized data. We use MOSAIKS random convolutional features and FTW derived field boundaries to map crop type at the field level and report macro F1 scores of 0.65--0.75 for crop type classification with limited labels. Finally, we show how to explore pre-computed predictions over five countries (4.76M km\\textsuperscript{2}), with median predicted field areas from 0.06 ha (Rwanda) to 0.28 ha (Switzerland).", "AI": {"tldr": "Fields of The World (FTW)\u751f\u6001\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5305\u542b160\u4e07\u519c\u7530\u8fb9\u754c\u7684\u57fa\u51c6\u6570\u636e\u96c6\u3001\u9884\u8bad\u7ec3\u5206\u5272\u6a21\u578b\u548c\u547d\u4ee4\u884c\u5de5\u5177\uff0c\u652f\u6301\u519c\u7530\u8fb9\u754c\u63d0\u53d6\u3001\u4f5c\u7269\u5206\u7c7b\u548c\u68ee\u6797\u635f\u5931\u5f52\u56e0\u7b49\u519c\u4e1a\u5e94\u7528\u3002", "motivation": "\u519c\u7530\u8fb9\u754c\u5730\u56fe\u662f\u519c\u4e1a\u6570\u636e\u4ea7\u54c1\u7684\u57fa\u7840\uff0c\u5bf9\u4e8e\u4f5c\u7269\u76d1\u6d4b\u3001\u4ea7\u91cf\u4f30\u7b97\u548c\u75c5\u5bb3\u8bc4\u4f30\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u8986\u76d6\u8303\u56f4\u548c\u53ef\u6269\u5c55\u6027\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u9700\u8981\u66f4\u5168\u9762\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u6784\u5efa\u4e86FTW\u751f\u6001\u7cfb\u7edf\uff0c\u5305\u542b\uff1a1\uff09\u8986\u76d624\u4e2a\u56fd\u5bb6160\u4e07\u519c\u7530\u591a\u8fb9\u5f62\u7684\u57fa\u51c6\u6570\u636e\u96c6\uff1b2\uff09\u9884\u8bad\u7ec3\u5206\u5272\u6a21\u578b\uff1b3\uff09\u547d\u4ee4\u884c\u63a8\u7406\u5de5\u5177\uff1b4\uff09\u4e24\u4e2a\u7b14\u8bb0\u672c\u5206\u522b\u5904\u7406\u5c40\u90e8\u5c3a\u5ea6\u548c\u56fd\u5bb6\u5c3a\u5ea6\u7684\u63a8\u7406\uff1b5\uff09\u4f7f\u7528MOSAIKS\u968f\u673a\u5377\u79ef\u7279\u5f81\u548cFTW\u519c\u7530\u8fb9\u754c\u8fdb\u884c\u4f5c\u7269\u7c7b\u578b\u5206\u7c7b\u3002", "result": "\u4f5c\u7269\u7c7b\u578b\u5206\u7c7b\u7684\u5b8f\u89c2F1\u5206\u6570\u8fbe\u52300.65-0.75\uff08\u4f7f\u7528\u6709\u9650\u6807\u7b7e\uff09\uff1b\u5728\u4e94\u4e2a\u56fd\u5bb6\uff08476\u4e07\u5e73\u65b9\u516c\u91cc\uff09\u8fdb\u884c\u4e86\u9884\u6d4b\uff0c\u9884\u6d4b\u519c\u7530\u9762\u79ef\u4e2d\u4f4d\u6570\u4ece\u5362\u65fa\u8fbe\u76840.06\u516c\u9877\u5230\u745e\u58eb\u76840.28\u516c\u9877\u4e0d\u7b49\u3002", "conclusion": "FTW\u751f\u6001\u7cfb\u7edf\u4e3a\u519c\u7530\u8fb9\u754c\u63d0\u53d6\u548c\u4f5c\u7269\u5206\u7c7b\u63d0\u4f9b\u4e86\u5168\u9762\u7684\u5de5\u5177\u548c\u57fa\u51c6\uff0c\u652f\u6301\u4ece\u5c40\u90e8\u5230\u56fd\u5bb6\u5c3a\u5ea6\u7684\u519c\u4e1a\u76d1\u6d4b\u5e94\u7528\uff0c\u5728\u6709\u9650\u6807\u7b7e\u4e0b\u4ecd\u80fd\u83b7\u5f97\u826f\u597d\u7684\u5206\u7c7b\u6027\u80fd\u3002"}}
{"id": "2602.08136", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08136", "abs": "https://arxiv.org/abs/2602.08136", "authors": ["Md Rafi Ur Rashid", "MD Sadik Hossain Shanto", "Vishnu Asutosh Dasu", "Shagufta Mehnaz"], "title": "Robustness of Vision Language Models Against Split-Image Harmful Input Attacks", "comment": "22 Pages, long conference paper", "summary": "Vision-Language Models (VLMs) are now a core part of modern AI. Recent work proposed several visual jailbreak attacks using single/ holistic images. However, contemporary VLMs demonstrate strong robustness against such attacks due to extensive safety alignment through preference optimization (e.g., RLHF). In this work, we identify a new vulnerability: while VLM pretraining and instruction tuning generalize well to split-image inputs, safety alignment is typically performed only on holistic images and does not account for harmful semantics distributed across multiple image fragments. Consequently, VLMs often fail to detect and refuse harmful split-image inputs, where unsafe cues emerge only after combining images. We introduce novel split-image visual jailbreak attacks (SIVA) that exploit this misalignment. Unlike prior optimization-based attacks, which exhibit poor black-box transferability due to architectural and prior mismatches across models, our attacks evolve in progressive phases from naive splitting to an adaptive white-box attack, culminating in a black-box transfer attack. Our strongest strategy leverages a novel adversarial knowledge distillation (Adv-KD) algorithm to substantially improve cross-model transferability. Evaluations on three state-of-the-art modern VLMs and three jailbreak datasets demonstrate that our strongest attack achieves up to 60% higher transfer success than existing baselines. Lastly, we propose efficient ways to address this critical vulnerability in the current VLM safety alignment.", "AI": {"tldr": "\u8be5\u8bba\u6587\u53d1\u73b0\u89c6\u89c9\u8bed\u8a00\u6a21\u578b(VLMs)\u5728\u5b89\u5168\u5bf9\u9f50\u65b9\u9762\u5b58\u5728\u6f0f\u6d1e\uff1a\u867d\u7136\u6a21\u578b\u8bad\u7ec3\u80fd\u5904\u7406\u5206\u5272\u56fe\u50cf\uff0c\u4f46\u5b89\u5168\u5bf9\u9f50\u53ea\u5728\u5b8c\u6574\u56fe\u50cf\u4e0a\u8fdb\u884c\uff0c\u5bfc\u81f4\u65e0\u6cd5\u8bc6\u522b\u8de8\u591a\u4e2a\u56fe\u50cf\u7247\u6bb5\u7684\u6709\u5bb3\u8bed\u4e49\u3002\u7814\u7a76\u8005\u63d0\u51fa\u4e86SIVA\u5206\u5272\u56fe\u50cf\u89c6\u89c9\u8d8a\u72f1\u653b\u51fb\uff0c\u5229\u7528\u8fd9\u79cd\u4e0d\u5bf9\u9f50\u6027\uff0c\u901a\u8fc7\u5bf9\u6297\u77e5\u8bc6\u84b8\u998f\u5b9e\u73b0\u9ad8\u8fbe60%\u7684\u8de8\u6a21\u578b\u653b\u51fb\u6210\u529f\u7387\u63d0\u5347\u3002", "motivation": "\u73b0\u4ee3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b(VLMs)\u7ecf\u8fc7\u5e7f\u6cdb\u7684\u5b89\u5168\u5bf9\u9f50\uff08\u5982RLHF\uff09\u540e\uff0c\u5bf9\u4f20\u7edf\u7684\u5355\u56fe\u50cf/\u5b8c\u6574\u56fe\u50cf\u8d8a\u72f1\u653b\u51fb\u8868\u73b0\u51fa\u5f3a\u9c81\u68d2\u6027\u3002\u7136\u800c\uff0c\u7814\u7a76\u8005\u53d1\u73b0\u4e86\u4e00\u4e2a\u65b0\u7684\u5b89\u5168\u6f0f\u6d1e\uff1a\u867d\u7136VLM\u7684\u9884\u8bad\u7ec3\u548c\u6307\u4ee4\u8c03\u4f18\u80fd\u5f88\u597d\u5730\u6cdb\u5316\u5230\u5206\u5272\u56fe\u50cf\u8f93\u5165\uff0c\u4f46\u5b89\u5168\u5bf9\u9f50\u901a\u5e38\u53ea\u5728\u5b8c\u6574\u56fe\u50cf\u4e0a\u8fdb\u884c\uff0c\u6ca1\u6709\u8003\u8651\u6709\u5bb3\u8bed\u4e49\u5206\u5e03\u5728\u591a\u4e2a\u56fe\u50cf\u7247\u6bb5\u4e2d\u7684\u60c5\u51b5\u3002\u8fd9\u5bfc\u81f4VLMs\u5f80\u5f80\u65e0\u6cd5\u68c0\u6d4b\u548c\u62d2\u7edd\u6709\u5bb3\u7684\u5206\u5272\u56fe\u50cf\u8f93\u5165\uff0c\u56e0\u4e3a\u4e0d\u5b89\u5168\u7ebf\u7d22\u53ea\u6709\u5728\u7ec4\u5408\u56fe\u50cf\u540e\u624d\u663e\u73b0\u3002", "method": "\u63d0\u51fa\u4e86\u65b0\u9896\u7684\u5206\u5272\u56fe\u50cf\u89c6\u89c9\u8d8a\u72f1\u653b\u51fb(SIVA)\uff0c\u8be5\u65b9\u6cd5\u4ece\u7b80\u5355\u7684\u56fe\u50cf\u5206\u5272\u5f00\u59cb\uff0c\u9010\u6b65\u53d1\u5c55\u5230\u81ea\u9002\u5e94\u767d\u76d2\u653b\u51fb\uff0c\u6700\u7ec8\u5f62\u6210\u9ed1\u76d2\u8fc1\u79fb\u653b\u51fb\u3002\u6700\u5f3a\u7684\u653b\u51fb\u7b56\u7565\u91c7\u7528\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5bf9\u6297\u77e5\u8bc6\u84b8\u998f(Adv-KD)\u7b97\u6cd5\uff0c\u901a\u8fc7\u77e5\u8bc6\u84b8\u998f\u6280\u672f\u5c06\u653b\u51fb\u4ece\u4e00\u4e2a\u6a21\u578b\u8fc1\u79fb\u5230\u53e6\u4e00\u4e2a\u6a21\u578b\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8de8\u6a21\u578b\u7684\u53ef\u8fc1\u79fb\u6027\u3002", "result": "\u5728\u4e09\u4e2a\u6700\u5148\u8fdb\u7684\u73b0\u4ee3VLMs\u548c\u4e09\u4e2a\u8d8a\u72f1\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u8868\u660e\uff0c\u6700\u5f3a\u7684\u653b\u51fb\u7b56\u7565\u6bd4\u73b0\u6709\u57fa\u7ebf\u5b9e\u73b0\u4e86\u9ad8\u8fbe60%\u7684\u8fc1\u79fb\u6210\u529f\u7387\u63d0\u5347\u3002\u8fd9\u8bc1\u660e\u4e86\u5f53\u524dVLM\u5b89\u5168\u5bf9\u9f50\u4e2d\u5b58\u5728\u7684\u4e25\u91cd\u6f0f\u6d1e\u3002", "conclusion": "\u5f53\u524d\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u5b89\u5168\u5bf9\u9f50\u5b58\u5728\u5173\u952e\u6f0f\u6d1e\uff0c\u65e0\u6cd5\u6709\u6548\u5904\u7406\u6709\u5bb3\u8bed\u4e49\u5206\u5e03\u5728\u591a\u4e2a\u56fe\u50cf\u7247\u6bb5\u4e2d\u7684\u60c5\u51b5\u3002SIVA\u653b\u51fb\u5229\u7528\u8fd9\u79cd\u4e0d\u5bf9\u9f50\u6027\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u8d8a\u72f1\u653b\u51fb\u3002\u8bba\u6587\u6700\u540e\u63d0\u51fa\u4e86\u89e3\u51b3\u8fd9\u4e00\u5173\u952e\u6f0f\u6d1e\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u5f3a\u8c03\u4e86\u9700\u8981\u6539\u8fdbVLM\u5b89\u5168\u5bf9\u9f50\u4ee5\u8986\u76d6\u5206\u5272\u56fe\u50cf\u573a\u666f\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2602.08198", "categories": ["cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2602.08198", "abs": "https://arxiv.org/abs/2602.08198", "authors": ["Jingyu Hu", "Bin Hu", "Ka-Hei Hui", "Haipeng Li", "Zhengzhe Liu", "Daniel Cohen-Or", "Chi-Wing Fu"], "title": "PEGAsus: 3D Personalization of Geometry and Appearance", "comment": null, "summary": "We present PEGAsus, a new framework capable of generating Personalized 3D shapes by learning shape concepts at both Geometry and Appearance levels. First, we formulate 3D shape personalization as extracting reusable, category-agnostic geometric and appearance attributes from reference shapes, and composing these attributes with text to generate novel shapes. Second, we design a progressive optimization strategy to learn shape concepts at both the geometry and appearance levels, decoupling the shape concept learning process. Third, we extend our approach to region-wise concept learning, enabling flexible concept extraction, with context-aware and context-free losses. Extensive experimental results show that PEGAsus is able to effectively extract attributes from a wide range of reference shapes and then flexibly compose these concepts with text to synthesize new shapes. This enables fine-grained control over shape generation and supports the creation of diverse, personalized results, even in challenging cross-category scenarios. Both quantitative and qualitative experiments demonstrate that our approach outperforms existing state-of-the-art solutions.", "AI": {"tldr": "PEGAsus\u662f\u4e00\u4e2a\u80fd\u591f\u751f\u6210\u4e2a\u6027\u53163D\u5f62\u72b6\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u51e0\u4f55\u548c\u5916\u89c2\u5c42\u9762\u7684\u6982\u5ff5\u5b66\u4e60\uff0c\u4ece\u53c2\u8003\u5f62\u72b6\u63d0\u53d6\u53ef\u91cd\u7528\u7684\u5c5e\u6027\u5e76\u4e0e\u6587\u672c\u7ed3\u5408\u751f\u6210\u65b0\u5f62\u72b6\u3002", "motivation": "\u73b0\u67093D\u5f62\u72b6\u751f\u6210\u65b9\u6cd5\u5728\u4e2a\u6027\u5316\u63a7\u5236\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u96be\u4ee5\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u7684\u51e0\u4f55\u548c\u5916\u89c2\u5c5e\u6027\u63d0\u53d6\u4e0e\u7ec4\u5408\uff0c\u7279\u522b\u662f\u5728\u8de8\u7c7b\u522b\u573a\u666f\u4e2d\u7f3a\u4e4f\u7075\u6d3b\u6027\u3002", "method": "1) \u5c063D\u5f62\u72b6\u4e2a\u6027\u5316\u5b9a\u4e49\u4e3a\u63d0\u53d6\u7c7b\u522b\u65e0\u5173\u7684\u51e0\u4f55\u548c\u5916\u89c2\u5c5e\u6027\uff1b2) \u8bbe\u8ba1\u6e10\u8fdb\u4f18\u5316\u7b56\u7565\u5206\u79bb\u51e0\u4f55\u548c\u5916\u89c2\u6982\u5ff5\u5b66\u4e60\uff1b3) \u6269\u5c55\u5230\u533a\u57df\u7ea7\u6982\u5ff5\u5b66\u4e60\uff0c\u4f7f\u7528\u4e0a\u4e0b\u6587\u611f\u77e5\u548c\u65e0\u4e0a\u4e0b\u6587\u635f\u5931\u3002", "result": "PEGAsus\u80fd\u591f\u4ece\u5e7f\u6cdb\u7684\u53c2\u8003\u5f62\u72b6\u4e2d\u6709\u6548\u63d0\u53d6\u5c5e\u6027\uff0c\u5e76\u4e0e\u6587\u672c\u7075\u6d3b\u7ec4\u5408\u751f\u6210\u65b0\u5f62\u72b6\uff0c\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u63a7\u5236\uff0c\u5728\u8de8\u7c7b\u522b\u573a\u666f\u4e2d\u4e5f\u80fd\u4ea7\u751f\u591a\u6837\u5316\u7684\u4e2a\u6027\u5316\u7ed3\u679c\uff0c\u5728\u5b9a\u91cf\u548c\u5b9a\u6027\u5b9e\u9a8c\u4e2d\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u51e0\u4f55\u548c\u5916\u89c2\u5c42\u9762\u7684\u6982\u5ff5\u5b66\u4e60\uff0c\u5b9e\u73b0\u4e863D\u5f62\u72b6\u751f\u6210\u7684\u7ec6\u7c92\u5ea6\u4e2a\u6027\u5316\u63a7\u5236\uff0c\u4e3a\u8de8\u7c7b\u522b\u5f62\u72b6\u5408\u6210\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.07071", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07071", "abs": "https://arxiv.org/abs/2602.07071", "authors": ["S M Rakib UI Karim", "Wenyi Lu", "Sean Goggins"], "title": "Artificial Intelligence in Open Source Software Engineering: A Foundation for Sustainability", "comment": null, "summary": "Open-source software (OSS) is foundational to modern digital infrastructure, yet this context for group work continues to struggle to ensure sufficient contributions in many critical cases. This literature review explores how artificial intelligence (AI) is being leveraged to address critical challenges to OSS sustainability, including maintaining contributor engagement, securing funding, ensuring code quality and security, fostering healthy community dynamics, and preventing project abandonment. Synthesizing recent interdisciplinary research, the paper identifies key applications of AI in this domain, including automated bug triaging, system maintenance, contributor onboarding and mentorship, community health analytics, vulnerability detection, and task automation. The review also examines the limitations and ethical concerns that arise from applying AI in OSS contexts, including data availability, bias and fairness, transparency, risks of misuse, and the preservation of human-centered values in collaborative development. By framing AI not as a replacement but as a tool to augment human infrastructure, this study highlights both the promise and pitfalls of AI-driven interventions. It concludes by identifying critical research gaps and proposing future directions at the intersection of AI, sustainability, and OSS, aiming to support more resilient and equitable open-source ecosystems.", "AI": {"tldr": "\u8fd9\u7bc7\u6587\u732e\u7efc\u8ff0\u63a2\u8ba8\u4e86\u4eba\u5de5\u667a\u80fd\u5982\u4f55\u88ab\u7528\u6765\u89e3\u51b3\u5f00\u6e90\u8f6f\u4ef6\u53ef\u6301\u7eed\u6027\u9762\u4e34\u7684\u6311\u6218\uff0c\u5305\u62ec\u7ef4\u62a4\u8d21\u732e\u8005\u53c2\u4e0e\u5ea6\u3001\u786e\u4fdd\u8d44\u91d1\u3001\u4ee3\u7801\u8d28\u91cf\u4e0e\u5b89\u5168\u3001\u793e\u533a\u5065\u5eb7\u4ee5\u53ca\u9632\u6b62\u9879\u76ee\u5e9f\u5f03\u7b49\u95ee\u9898\u3002", "motivation": "\u5f00\u6e90\u8f6f\u4ef6\u662f\u73b0\u4ee3\u6570\u5b57\u57fa\u7840\u8bbe\u65bd\u7684\u57fa\u7840\uff0c\u4f46\u5728\u8bb8\u591a\u5173\u952e\u6848\u4f8b\u4e2d\u4ecd\u7136\u96be\u4ee5\u786e\u4fdd\u8db3\u591f\u7684\u8d21\u732e\u3002\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u5982\u4f55\u5229\u7528\u4eba\u5de5\u667a\u80fd\u89e3\u51b3\u5f00\u6e90\u8f6f\u4ef6\u53ef\u6301\u7eed\u6027\u9762\u4e34\u7684\u6311\u6218\u3002", "method": "\u901a\u8fc7\u6587\u732e\u7efc\u8ff0\u65b9\u6cd5\uff0c\u7efc\u5408\u8fd1\u671f\u8de8\u5b66\u79d1\u7814\u7a76\uff0c\u5206\u6790\u4eba\u5de5\u667a\u80fd\u5728\u5f00\u6e90\u8f6f\u4ef6\u53ef\u6301\u7eed\u6027\u9886\u57df\u7684\u5e94\u7528\u3001\u5c40\u9650\u6027\u548c\u4f26\u7406\u95ee\u9898\u3002", "result": "\u8bc6\u522b\u4e86\u4eba\u5de5\u667a\u80fd\u5728\u5f00\u6e90\u8f6f\u4ef6\u4e2d\u7684\u5173\u952e\u5e94\u7528\u9886\u57df\uff0c\u5305\u62ec\u81ea\u52a8\u5316\u9519\u8bef\u5206\u7c7b\u3001\u7cfb\u7edf\u7ef4\u62a4\u3001\u8d21\u732e\u8005\u5f15\u5bfc\u3001\u793e\u533a\u5065\u5eb7\u5206\u6790\u3001\u6f0f\u6d1e\u68c0\u6d4b\u548c\u4efb\u52a1\u81ea\u52a8\u5316\u3002\u540c\u65f6\u63ed\u793a\u4e86\u6570\u636e\u53ef\u7528\u6027\u3001\u504f\u89c1\u4e0e\u516c\u5e73\u6027\u3001\u900f\u660e\u5ea6\u3001\u6ee5\u7528\u98ce\u9669\u7b49\u5c40\u9650\u6027\u3002", "conclusion": "\u4eba\u5de5\u667a\u80fd\u4e0d\u5e94\u88ab\u89c6\u4e3a\u66ff\u4ee3\u54c1\uff0c\u800c\u662f\u589e\u5f3a\u4eba\u7c7b\u57fa\u7840\u8bbe\u65bd\u7684\u5de5\u5177\u3002\u7814\u7a76\u5f3a\u8c03\u4e86\u4eba\u5de5\u667a\u80fd\u9a71\u52a8\u5e72\u9884\u7684\u627f\u8bfa\u4e0e\u9677\u9631\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\uff0c\u65e8\u5728\u652f\u6301\u66f4\u5177\u97e7\u6027\u548c\u516c\u5e73\u6027\u7684\u5f00\u6e90\u751f\u6001\u7cfb\u7edf\u3002"}}
{"id": "2602.08202", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.08202", "abs": "https://arxiv.org/abs/2602.08202", "authors": ["Jinrong Lv", "Xun Gong", "Zhaohuan Li", "Weili Jiang"], "title": "Generative Regression for Left Ventricular Ejection Fraction Estimation from Echocardiography Video", "comment": "11 pages, 5 tables, 10 figures. Under peer review", "summary": "Estimating Left Ventricular Ejection Fraction (LVEF) from echocardiograms constitutes an ill-posed inverse problem. Inherent noise, artifacts, and limited viewing angles introduce ambiguity, where a single video sequence may map not to a unique ground truth, but rather to a distribution of plausible physiological values. Prevailing deep learning approaches typically formulate this task as a standard regression problem that minimizes the Mean Squared Error (MSE). However, this paradigm compels the model to learn the conditional expectation, which may yield misleading predictions when the underlying posterior distribution is multimodal or heavy-tailed -- a common phenomenon in pathological scenarios. In this paper, we investigate the paradigm shift from deterministic regression toward generative regression. We propose the Multimodal Conditional Score-based Diffusion model for Regression (MCSDR), a probabilistic framework designed to model the continuous posterior distribution of LVEF conditioned on echocardiogram videos and patient demographic attribute priors. Extensive experiments conducted on the EchoNet-Dynamic, EchoNet-Pediatric, and CAMUS datasets demonstrate that MCSDR achieves state-of-the-art performance. Notably, qualitative analysis reveals that the generation trajectories of our model exhibit distinct behaviors in cases characterized by high noise or significant physiological variability, thereby offering a novel layer of interpretability for AI-aided diagnosis.", "AI": {"tldr": "\u63d0\u51faMCSDR\u6a21\u578b\uff0c\u4f7f\u7528\u57fa\u4e8e\u5206\u6570\u7684\u6269\u6563\u6a21\u578b\u8fdb\u884c\u591a\u6a21\u6001\u6761\u4ef6\u56de\u5f52\uff0c\u7528\u4e8e\u4ece\u8d85\u58f0\u5fc3\u52a8\u56fe\u89c6\u9891\u4e2d\u4f30\u8ba1\u5de6\u5fc3\u5ba4\u5c04\u8840\u5206\u6570\uff0c\u89e3\u51b3\u4f20\u7edf\u56de\u5f52\u65b9\u6cd5\u5728\u5904\u7406\u591a\u6a21\u6001\u540e\u9a8c\u5206\u5e03\u65f6\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u4ece\u8d85\u58f0\u5fc3\u52a8\u56fe\u4f30\u8ba1\u5de6\u5fc3\u5ba4\u5c04\u8840\u5206\u6570\u662f\u4e00\u4e2a\u4e0d\u9002\u5b9a\u9006\u95ee\u9898\uff0c\u5b58\u5728\u566a\u58f0\u3001\u4f2a\u5f71\u548c\u6709\u9650\u89c6\u89d2\u5e26\u6765\u7684\u6a21\u7cca\u6027\u3002\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u4f7f\u7528\u5747\u65b9\u8bef\u5dee\u56de\u5f52\uff0c\u5f3a\u5236\u6a21\u578b\u5b66\u4e60\u6761\u4ef6\u671f\u671b\uff0c\u5f53\u540e\u9a8c\u5206\u5e03\u5448\u73b0\u591a\u6a21\u6001\u6216\u91cd\u5c3e\u5206\u5e03\u65f6\uff08\u5e38\u89c1\u4e8e\u75c5\u7406\u60c5\u51b5\uff09\uff0c\u4f1a\u4ea7\u751f\u8bef\u5bfc\u6027\u9884\u6d4b\u3002", "method": "\u63d0\u51fa\u591a\u6a21\u6001\u6761\u4ef6\u57fa\u4e8e\u5206\u6570\u7684\u6269\u6563\u56de\u5f52\u6a21\u578b\uff08MCSDR\uff09\uff0c\u8fd9\u662f\u4e00\u4e2a\u6982\u7387\u6846\u67b6\uff0c\u65e8\u5728\u5efa\u6a21\u4ee5\u8d85\u58f0\u5fc3\u52a8\u56fe\u89c6\u9891\u548c\u60a3\u8005\u4eba\u53e3\u7edf\u8ba1\u5b66\u5c5e\u6027\u5148\u9a8c\u4e3a\u6761\u4ef6\u7684LVEF\u8fde\u7eed\u540e\u9a8c\u5206\u5e03\u3002\u4f7f\u7528\u57fa\u4e8e\u5206\u6570\u7684\u6269\u6563\u6a21\u578b\u8fdb\u884c\u751f\u6210\u5f0f\u56de\u5f52\u3002", "result": "\u5728EchoNet-Dynamic\u3001EchoNet-Pediatric\u548cCAMUS\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cMCSDR\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002\u5b9a\u6027\u5206\u6790\u663e\u793a\uff0c\u5728\u9ad8\u566a\u58f0\u6216\u663e\u8457\u751f\u7406\u53d8\u5f02\u6027\u7684\u60c5\u51b5\u4e0b\uff0c\u6a21\u578b\u7684\u751f\u6210\u8f68\u8ff9\u8868\u73b0\u51fa\u72ec\u7279\u884c\u4e3a\uff0c\u4e3aAI\u8f85\u52a9\u8bca\u65ad\u63d0\u4f9b\u4e86\u65b0\u7684\u53ef\u89e3\u91ca\u6027\u5c42\u3002", "conclusion": "\u4ece\u786e\u5b9a\u6027\u56de\u5f52\u5411\u751f\u6210\u5f0f\u56de\u5f52\u7684\u8303\u5f0f\u8f6c\u53d8\u662f\u6709\u6548\u7684\u3002MCSDR\u80fd\u591f\u5efa\u6a21LVEF\u7684\u8fde\u7eed\u540e\u9a8c\u5206\u5e03\uff0c\u5728\u5904\u7406\u8d85\u58f0\u5fc3\u52a8\u56fe\u4f30\u8ba1\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u548c\u591a\u6a21\u6001\u6027\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u540c\u65f6\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2602.08206", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.08206", "abs": "https://arxiv.org/abs/2602.08206", "authors": ["Chufeng Zhou", "Jian Wang", "Xinyuan Liu", "Xiaokang Zhang"], "title": "Geospatial-Reasoning-Driven Vocabulary-Agnostic Remote Sensing Semantic Segmentation", "comment": "5 pages, 3 figures", "summary": "Open-vocabulary semantic segmentation has emerged as a promising research direction in remote sensing, enabling the recognition of diverse land-cover types beyond pre-defined category sets. However, existing methods predominantly rely on the passive mapping of visual features and textual embeddings. This ``appearance-based\" paradigm lacks geospatial contextual awareness, leading to severe semantic ambiguity and misclassification when encountering land-cover classes with similar spectral features but distinct semantic attributes. To address this, we propose a Geospatial Reasoning Chain-of-Thought (GR-CoT) framework designed to enhance the scene understanding capabilities of Multimodal Large Language Models (MLLMs), thereby guiding open-vocabulary segmentation models toward precise mapping. The framework comprises two collaborative components: an offline knowledge distillation stream and an online instance reasoning stream. The offline stream establishes fine-grained category interpretation standards to resolve semantic conflicts between similar land-cover types. During online inference, the framework executes a sequential reasoning process involving macro-scenario anchoring, visual feature decoupling, and knowledge-driven decision synthesis. This process generates an image-adaptive vocabulary that guides downstream models to achieve pixel-level alignment with correct geographical semantics. Extensive experiments on the LoveDA and GID5 benchmarks demonstrate the superiority of our approach.", "AI": {"tldr": "\u63d0\u51faGR-CoT\u6846\u67b6\uff0c\u901a\u8fc7\u5730\u7406\u7a7a\u95f4\u63a8\u7406\u589e\u5f3aMLLM\u7684\u573a\u666f\u7406\u89e3\u80fd\u529b\uff0c\u89e3\u51b3\u9065\u611f\u5f00\u653e\u8bcd\u6c47\u5206\u5272\u4e2d\u56e0\u76f8\u4f3c\u5149\u8c31\u7279\u5f81\u5bfc\u81f4\u7684\u8bed\u4e49\u6b67\u4e49\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u5f00\u653e\u8bcd\u6c47\u8bed\u4e49\u5206\u5272\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u89c6\u89c9\u7279\u5f81\u4e0e\u6587\u672c\u5d4c\u5165\u7684\u88ab\u52a8\u6620\u5c04\uff0c\u8fd9\u79cd\"\u57fa\u4e8e\u5916\u89c2\"\u7684\u8303\u5f0f\u7f3a\u4e4f\u5730\u7406\u7a7a\u95f4\u4e0a\u4e0b\u6587\u610f\u8bc6\uff0c\u5bfc\u81f4\u9047\u5230\u5149\u8c31\u7279\u5f81\u76f8\u4f3c\u4f46\u8bed\u4e49\u5c5e\u6027\u4e0d\u540c\u7684\u5730\u7269\u7c7b\u522b\u65f6\u4ea7\u751f\u4e25\u91cd\u8bed\u4e49\u6b67\u4e49\u548c\u8bef\u5206\u7c7b\u3002", "method": "\u63d0\u51fa\u5730\u7406\u7a7a\u95f4\u63a8\u7406\u601d\u7ef4\u94fe\uff08GR-CoT\uff09\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u534f\u4f5c\u7ec4\u4ef6\uff1a\u79bb\u7ebf\u77e5\u8bc6\u84b8\u998f\u6d41\u548c\u5728\u7ebf\u5b9e\u4f8b\u63a8\u7406\u6d41\u3002\u79bb\u7ebf\u6d41\u5efa\u7acb\u7ec6\u7c92\u5ea6\u7c7b\u522b\u89e3\u91ca\u6807\u51c6\u4ee5\u89e3\u51b3\u76f8\u4f3c\u5730\u7269\u7c7b\u578b\u95f4\u7684\u8bed\u4e49\u51b2\u7a81\uff1b\u5728\u7ebf\u63a8\u7406\u6267\u884c\u987a\u5e8f\u63a8\u7406\u8fc7\u7a0b\uff0c\u5305\u62ec\u5b8f\u89c2\u573a\u666f\u951a\u5b9a\u3001\u89c6\u89c9\u7279\u5f81\u89e3\u8026\u548c\u77e5\u8bc6\u9a71\u52a8\u51b3\u7b56\u5408\u6210\uff0c\u751f\u6210\u56fe\u50cf\u81ea\u9002\u5e94\u8bcd\u6c47\u8868\u6307\u5bfc\u4e0b\u6e38\u6a21\u578b\u5b9e\u73b0\u50cf\u7d20\u7ea7\u5730\u7406\u8bed\u4e49\u5bf9\u9f50\u3002", "result": "\u5728LoveDA\u548cGID5\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u8bc1\u660e\u4e86\u6240\u63d0\u65b9\u6cd5\u7684\u4f18\u8d8a\u6027\u3002", "conclusion": "GR-CoT\u6846\u67b6\u901a\u8fc7\u589e\u5f3aMLLM\u7684\u573a\u666f\u7406\u89e3\u80fd\u529b\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u9065\u611f\u5f00\u653e\u8bcd\u6c47\u5206\u5272\u4e2d\u7684\u8bed\u4e49\u6b67\u4e49\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u66f4\u7cbe\u786e\u7684\u5730\u7269\u5206\u7c7b\u6620\u5c04\u3002"}}
{"id": "2602.08211", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.08211", "abs": "https://arxiv.org/abs/2602.08211", "authors": ["Yik Lung Pang", "Changjae Oh"], "title": "Chain-of-Caption: Training-free improvement of multimodal large language model on referring expression comprehension", "comment": "4 pages, 5 figures, 2 tables", "summary": "Given a textual description, the task of referring expression comprehension (REC) involves the localisation of the referred object in an image. Multimodal large language models (MLLMs) have achieved high accuracy on REC benchmarks through scaling up the model size and training data. Moreover, the performance of MLLMs can be further improved using techniques such as Chain-of-Thought and tool use, which provides additional visual or textual context to the model. In this paper, we analyse the effect of various techniques for providing additional visual and textual context via tool use to the MLLM and its effect on the REC task. Furthermore, we propose a training-free framework named Chain-of-Caption to improve the REC performance of MLLMs. We perform experiments on RefCOCO/RefCOCOg/RefCOCO+ and Ref-L4 datasets and show that individual textual or visual context can improve the REC performance without any fine-tuning. By combining multiple contexts, our training-free framework shows between 5% to 30% performance gain over the baseline model on accuracy at various Intersection over Union (IoU) thresholds.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aChain-of-Caption\u7684\u8bad\u7ec3\u514d\u8d39\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u591a\u79cd\u89c6\u89c9\u548c\u6587\u672c\u4e0a\u4e0b\u6587\u6765\u63d0\u5347\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6307\u4ee3\u8868\u8fbe\u7406\u89e3\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e865%\u523030%\u7684\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u6307\u4ee3\u8868\u8fbe\u7406\u89e3\u4efb\u52a1\u9700\u8981\u6839\u636e\u6587\u672c\u63cf\u8ff0\u5728\u56fe\u50cf\u4e2d\u5b9a\u4f4d\u76ee\u6807\u5bf9\u8c61\u3002\u867d\u7136\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u901a\u8fc7\u6269\u5927\u6a21\u578b\u89c4\u6a21\u548c\u8bad\u7ec3\u6570\u636e\u5728REC\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u9ad8\u51c6\u786e\u7387\uff0c\u4f46\u73b0\u6709\u6280\u672f\u5982\u601d\u7ef4\u94fe\u548c\u5de5\u5177\u4f7f\u7528\u53ef\u4ee5\u901a\u8fc7\u63d0\u4f9b\u989d\u5916\u7684\u89c6\u89c9\u6216\u6587\u672c\u4e0a\u4e0b\u6587\u6765\u8fdb\u4e00\u6b65\u63d0\u5347\u6027\u80fd\u3002\u672c\u6587\u65e8\u5728\u5206\u6790\u901a\u8fc7\u5de5\u5177\u4f7f\u7528\u63d0\u4f9b\u989d\u5916\u4e0a\u4e0b\u6587\u7684\u5404\u79cd\u6280\u672f\u5bf9REC\u4efb\u52a1\u7684\u5f71\u54cd\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8bad\u7ec3\u514d\u8d39\u7684Chain-of-Caption\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u5de5\u5177\u4f7f\u7528\u4e3a\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u591a\u79cd\u89c6\u89c9\u548c\u6587\u672c\u4e0a\u4e0b\u6587\u3002\u7814\u7a76\u5206\u6790\u4e86\u5355\u72ec\u4f7f\u7528\u6587\u672c\u6216\u89c6\u89c9\u4e0a\u4e0b\u6587\u7684\u6548\u679c\uff0c\u5e76\u5c55\u793a\u4e86\u7ed3\u5408\u591a\u79cd\u4e0a\u4e0b\u6587\u7684\u65b9\u6cd5\u3002", "result": "\u5728RefCOCO\u3001RefCOCOg\u3001RefCOCO+\u548cRef-L4\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u5355\u72ec\u7684\u6587\u672c\u6216\u89c6\u89c9\u4e0a\u4e0b\u6587\u53ef\u4ee5\u5728\u4e0d\u8fdb\u884c\u5fae\u8c03\u7684\u60c5\u51b5\u4e0b\u63d0\u5347REC\u6027\u80fd\u3002\u901a\u8fc7\u7ed3\u5408\u591a\u79cd\u4e0a\u4e0b\u6587\uff0c\u8be5\u8bad\u7ec3\u514d\u8d39\u6846\u67b6\u5728\u4e0d\u540cIoU\u9608\u503c\u4e0b\u7684\u51c6\u786e\u7387\u6bd4\u57fa\u7ebf\u6a21\u578b\u63d0\u5347\u4e865%\u523030%\u3002", "conclusion": "Chain-of-Caption\u6846\u67b6\u901a\u8fc7\u6709\u6548\u7ed3\u5408\u591a\u79cd\u89c6\u89c9\u548c\u6587\u672c\u4e0a\u4e0b\u6587\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6307\u4ee3\u8868\u8fbe\u7406\u89e3\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\uff0c\u4e14\u65e0\u9700\u989d\u5916\u8bad\u7ec3\uff0c\u4e3aREC\u4efb\u52a1\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.07090", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07090", "abs": "https://arxiv.org/abs/2602.07090", "authors": ["Yu-Che Tsai", "Hsiang Hsiao", "Kuan-Yu Chen", "Shou-De Lin"], "title": "Concept-Aware Privacy Mechanisms for Defending Embedding Inversion Attacks", "comment": null, "summary": "Text embeddings enable numerous NLP applications but face severe privacy risks from embedding inversion attacks, which can expose sensitive attributes or reconstruct raw text. Existing differential privacy defenses assume uniform sensitivity across embedding dimensions, leading to excessive noise and degraded utility. We propose SPARSE, a user-centric framework for concept-specific privacy protection in text embeddings. SPARSE combines (1) differentiable mask learning to identify privacy-sensitive dimensions for user-defined concepts, and (2) the Mahalanobis mechanism that applies elliptical noise calibrated by dimension sensitivity. Unlike traditional spherical noise injection, SPARSE selectively perturbs privacy-sensitive dimensions while preserving non-sensitive semantics. Evaluated across six datasets with three embedding models and attack scenarios, SPARSE consistently reduces privacy leakage while achieving superior downstream performance compared to state-of-the-art DP methods.", "AI": {"tldr": "SPARSE\u662f\u4e00\u4e2a\u7528\u6237\u4e2d\u5fc3\u7684\u6587\u672c\u5d4c\u5165\u9690\u79c1\u4fdd\u62a4\u6846\u67b6\uff0c\u901a\u8fc7\u53ef\u5fae\u5206\u63a9\u7801\u5b66\u4e60\u548c\u692d\u5706\u566a\u58f0\u6ce8\u5165\uff0c\u9488\u5bf9\u7528\u6237\u5b9a\u4e49\u7684\u6982\u5ff5\u9009\u62e9\u6027\u6270\u52a8\u9690\u79c1\u654f\u611f\u7ef4\u5ea6\uff0c\u5728\u4fdd\u62a4\u9690\u79c1\u7684\u540c\u65f6\u4fdd\u6301\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u3002", "motivation": "\u6587\u672c\u5d4c\u5165\u9762\u4e34\u4e25\u91cd\u7684\u9690\u79c1\u98ce\u9669\uff0c\u7279\u522b\u662f\u5d4c\u5165\u53cd\u8f6c\u653b\u51fb\u53ef\u80fd\u66b4\u9732\u654f\u611f\u5c5e\u6027\u6216\u91cd\u5efa\u539f\u59cb\u6587\u672c\u3002\u73b0\u6709\u7684\u5dee\u5206\u9690\u79c1\u9632\u5fa1\u65b9\u6cd5\u5047\u8bbe\u6240\u6709\u5d4c\u5165\u7ef4\u5ea6\u5177\u6709\u5747\u5300\u654f\u611f\u6027\uff0c\u5bfc\u81f4\u566a\u58f0\u8fc7\u5927\u548c\u6548\u7528\u4e0b\u964d\u3002", "method": "SPARSE\u7ed3\u5408\u4e86\u4e24\u79cd\u6280\u672f\uff1a(1) \u53ef\u5fae\u5206\u63a9\u7801\u5b66\u4e60\uff0c\u7528\u4e8e\u8bc6\u522b\u7528\u6237\u5b9a\u4e49\u6982\u5ff5\u7684\u9690\u79c1\u654f\u611f\u7ef4\u5ea6\uff1b(2) Mahalanobis\u673a\u5236\uff0c\u6839\u636e\u7ef4\u5ea6\u654f\u611f\u6027\u6821\u51c6\u692d\u5706\u566a\u58f0\u6ce8\u5165\u3002\u4e0e\u4f20\u7edf\u7403\u5f62\u566a\u58f0\u6ce8\u5165\u4e0d\u540c\uff0cSPARSE\u9009\u62e9\u6027\u6270\u52a8\u9690\u79c1\u654f\u611f\u7ef4\u5ea6\uff0c\u540c\u65f6\u4fdd\u7559\u975e\u654f\u611f\u8bed\u4e49\u3002", "result": "\u5728\u516d\u4e2a\u6570\u636e\u96c6\u3001\u4e09\u79cd\u5d4c\u5165\u6a21\u578b\u548c\u591a\u79cd\u653b\u51fb\u573a\u666f\u4e0b\u7684\u8bc4\u4f30\u8868\u660e\uff0cSPARSE\u80fd\u6301\u7eed\u51cf\u5c11\u9690\u79c1\u6cc4\u9732\uff0c\u540c\u65f6\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u5dee\u5206\u9690\u79c1\u65b9\u6cd5\u3002", "conclusion": "SPARSE\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u6548\u7684\u6982\u5ff5\u7279\u5b9a\u9690\u79c1\u4fdd\u62a4\u6846\u67b6\uff0c\u901a\u8fc7\u7ef4\u5ea6\u654f\u611f\u7684\u566a\u58f0\u6ce8\u5165\uff0c\u5728\u6587\u672c\u5d4c\u5165\u4e2d\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u9690\u79c1-\u6548\u7528\u6743\u8861\u3002"}}
{"id": "2602.08262", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.08262", "abs": "https://arxiv.org/abs/2602.08262", "authors": ["Guoqi Yu", "Xiaowei Hu", "Angelica I. Aviles-Rivero", "Anqi Qiu", "Shujun Wang"], "title": "Moving Beyond Functional Connectivity: Time-Series Modeling for fMRI-Based Brain Disorder Classification", "comment": "This paper has been accepted by IEEE Transactions on Medical Imaging", "summary": "Functional magnetic resonance imaging (fMRI) enables non-invasive brain disorder classification by capturing blood-oxygen-level-dependent (BOLD) signals. However, most existing methods rely on functional connectivity (FC) via Pearson correlation, which reduces 4D BOLD signals to static 2D matrices, discarding temporal dynamics and capturing only linear inter-regional relationships. In this work, we benchmark state-of-the-art temporal models (e.g., time-series models such as PatchTST, TimesNet, and TimeMixer) on raw BOLD signals across five public datasets. Results show these models consistently outperform traditional FC-based approaches, highlighting the value of directly modeling temporal information such as cycle-like oscillatory fluctuations and drift-like slow baseline trends. Building on this insight, we propose DeCI, a simple yet effective framework that integrates two key principles: (i) Cycle and Drift Decomposition to disentangle cycle and drift within each ROI (Region of Interest); and (ii) Channel-Independence to model each ROI separately, improving robustness and reducing overfitting. Extensive experiments demonstrate that DeCI achieves superior classification accuracy and generalization compared to both FC-based and temporal baselines. Our findings advocate for a shift toward end-to-end temporal modeling in fMRI analysis to better capture complex brain dynamics. The code is available at https://github.com/Levi-Ackman/DeCI.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51faDeCI\u6846\u67b6\uff0c\u901a\u8fc7\u5468\u671f-\u6f02\u79fb\u5206\u89e3\u548c\u901a\u9053\u72ec\u7acb\u6027\u5efa\u6a21\u539f\u59cbBOLD\u4fe1\u53f7\uff0c\u5728fMRI\u8111\u75be\u75c5\u5206\u7c7b\u4e2d\u4f18\u4e8e\u4f20\u7edf\u529f\u80fd\u8fde\u63a5\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709fMRI\u8111\u75be\u75c5\u5206\u7c7b\u65b9\u6cd5\u5927\u591a\u57fa\u4e8ePearson\u76f8\u5173\u6027\u7684\u529f\u80fd\u8fde\u63a5\uff0c\u5c064D BOLD\u4fe1\u53f7\u7b80\u5316\u4e3a\u9759\u60012D\u77e9\u9635\uff0c\u4e22\u5931\u4e86\u65f6\u95f4\u52a8\u6001\u4fe1\u606f\u4e14\u53ea\u80fd\u6355\u6349\u7ebf\u6027\u5173\u7cfb\u3002", "method": "\u63d0\u51faDeCI\u6846\u67b6\uff1a1) \u5468\u671f-\u6f02\u79fb\u5206\u89e3\uff1a\u5c06\u6bcf\u4e2aROI\u7684BOLD\u4fe1\u53f7\u5206\u89e3\u4e3a\u5468\u671f\u6027\u548c\u6f02\u79fb\u6210\u5206\uff1b2) \u901a\u9053\u72ec\u7acb\u6027\uff1a\u72ec\u7acb\u5efa\u6a21\u6bcf\u4e2aROI\uff0c\u63d0\u9ad8\u9c81\u68d2\u6027\u548c\u51cf\u5c11\u8fc7\u62df\u5408\u3002", "result": "\u5728\u4e94\u4e2a\u516c\u5171\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cDeCI\u5728\u5206\u7c7b\u51c6\u786e\u7387\u548c\u6cdb\u5316\u80fd\u529b\u4e0a\u4f18\u4e8e\u4f20\u7edfFC\u65b9\u6cd5\u548c\u73b0\u6709\u65f6\u95f4\u5e8f\u5217\u6a21\u578b\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u5728fMRI\u5206\u6790\u4e2d\u5e94\u91c7\u7528\u7aef\u5230\u7aef\u7684\u65f6\u95f4\u5efa\u6a21\u65b9\u6cd5\uff0c\u4ee5\u66f4\u597d\u5730\u6355\u6349\u590d\u6742\u7684\u5927\u8111\u52a8\u6001\u3002DeCI\u6846\u67b6\u4e3a\u8fd9\u4e00\u65b9\u5411\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.08277", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08277", "abs": "https://arxiv.org/abs/2602.08277", "authors": ["Xiangbo Gao", "Renjie Li", "Xinghao Chen", "Yuheng Wu", "Suofei Feng", "Qing Yin", "Zhengzhong Tu"], "title": "PISCO: Precise Video Instance Insertion with Sparse Control", "comment": null, "summary": "The landscape of AI video generation is undergoing a pivotal shift: moving beyond general generation - which relies on exhaustive prompt-engineering and \"cherry-picking\" - towards fine-grained, controllable generation and high-fidelity post-processing. In professional AI-assisted filmmaking, it is crucial to perform precise, targeted modifications. A cornerstone of this transition is video instance insertion, which requires inserting a specific instance into existing footage while maintaining scene integrity. Unlike traditional video editing, this task demands several requirements: precise spatial-temporal placement, physically consistent scene interaction, and the faithful preservation of original dynamics - all achieved under minimal user effort. In this paper, we propose PISCO, a video diffusion model for precise video instance insertion with arbitrary sparse keyframe control. PISCO allows users to specify a single keyframe, start-and-end keyframes, or sparse keyframes at arbitrary timestamps, and automatically propagates object appearance, motion, and interaction. To address the severe distribution shift induced by sparse conditioning in pretrained video diffusion models, we introduce Variable-Information Guidance for robust conditioning and Distribution-Preserving Temporal Masking to stabilize temporal generation, together with geometry-aware conditioning for realistic scene adaptation. We further construct PISCO-Bench, a benchmark with verified instance annotations and paired clean background videos, and evaluate performance using both reference-based and reference-free perceptual metrics. Experiments demonstrate that PISCO consistently outperforms strong inpainting and video editing baselines under sparse control, and exhibits clear, monotonic performance improvements as additional control signals are provided. Project page: xiangbogaobarry.github.io/PISCO.", "AI": {"tldr": "PISCO\u662f\u4e00\u4e2a\u89c6\u9891\u6269\u6563\u6a21\u578b\uff0c\u7528\u4e8e\u901a\u8fc7\u4efb\u610f\u7a00\u758f\u5173\u952e\u5e27\u63a7\u5236\u5b9e\u73b0\u7cbe\u786e\u7684\u89c6\u9891\u5b9e\u4f8b\u63d2\u5165\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u89c6\u9891\u7f16\u8f91\u4e2d\u7a7a\u95f4-\u65f6\u95f4\u5b9a\u4f4d\u3001\u7269\u7406\u4e00\u81f4\u6027\u573a\u666f\u4ea4\u4e92\u548c\u539f\u59cb\u52a8\u6001\u4fdd\u6301\u7684\u6311\u6218\u3002", "motivation": "AI\u89c6\u9891\u751f\u6210\u6b63\u4ece\u4f9d\u8d56\u5927\u91cf\u63d0\u793a\u5de5\u7a0b\u548c\"\u6311\u9009\"\u7684\u901a\u7528\u751f\u6210\uff0c\u8f6c\u5411\u7ec6\u7c92\u5ea6\u53ef\u63a7\u751f\u6210\u548c\u9ad8\u4fdd\u771f\u540e\u5904\u7406\u3002\u5728\u4e13\u4e1aAI\u8f85\u52a9\u7535\u5f71\u5236\u4f5c\u4e2d\uff0c\u9700\u8981\u8fdb\u884c\u7cbe\u786e\u3001\u6709\u9488\u5bf9\u6027\u7684\u4fee\u6539\uff0c\u89c6\u9891\u5b9e\u4f8b\u63d2\u5165\u662f\u8fd9\u4e00\u8f6c\u53d8\u7684\u5173\u952e\uff0c\u9700\u8981\u5728\u4fdd\u6301\u573a\u666f\u5b8c\u6574\u6027\u7684\u540c\u65f6\u5c06\u7279\u5b9a\u5b9e\u4f8b\u63d2\u5165\u73b0\u6709\u7d20\u6750\u3002", "method": "PISCO\u91c7\u7528\u89c6\u9891\u6269\u6563\u6a21\u578b\uff0c\u652f\u6301\u5355\u5173\u952e\u5e27\u3001\u8d77\u59cb-\u7ed3\u675f\u5173\u952e\u5e27\u6216\u4efb\u610f\u65f6\u95f4\u6233\u7684\u7a00\u758f\u5173\u952e\u5e27\u63a7\u5236\u3002\u4e3a\u89e3\u51b3\u9884\u8bad\u7ec3\u89c6\u9891\u6269\u6563\u6a21\u578b\u4e2d\u7a00\u758f\u6761\u4ef6\u5f15\u8d77\u7684\u4e25\u91cd\u5206\u5e03\u504f\u79fb\uff0c\u5f15\u5165\u4e86\u53d8\u91cf\u4fe1\u606f\u5f15\u5bfc\u4ee5\u5b9e\u73b0\u9c81\u68d2\u6761\u4ef6\uff0c\u4ee5\u53ca\u5206\u5e03\u4fdd\u6301\u65f6\u95f4\u63a9\u7801\u4ee5\u7a33\u5b9a\u65f6\u95f4\u751f\u6210\uff0c\u540c\u65f6\u4f7f\u7528\u51e0\u4f55\u611f\u77e5\u6761\u4ef6\u5b9e\u73b0\u771f\u5b9e\u573a\u666f\u9002\u5e94\u3002", "result": "\u6784\u5efa\u4e86PISCO-Bench\u57fa\u51c6\u6d4b\u8bd5\u96c6\uff0c\u5305\u542b\u5df2\u9a8c\u8bc1\u7684\u5b9e\u4f8b\u6807\u6ce8\u548c\u914d\u5bf9\u7684\u5e72\u51c0\u80cc\u666f\u89c6\u9891\u3002\u5b9e\u9a8c\u8868\u660e\uff0cPISCO\u5728\u7a00\u758f\u63a7\u5236\u4e0b\u59cb\u7ec8\u4f18\u4e8e\u5f3a\u5927\u7684\u4fee\u590d\u548c\u89c6\u9891\u7f16\u8f91\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u4e14\u5728\u63d0\u4f9b\u989d\u5916\u63a7\u5236\u4fe1\u53f7\u65f6\u8868\u73b0\u51fa\u6e05\u6670\u3001\u5355\u8c03\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "PISCO\u901a\u8fc7\u7a00\u758f\u5173\u952e\u5e27\u63a7\u5236\u5b9e\u73b0\u4e86\u7cbe\u786e\u7684\u89c6\u9891\u5b9e\u4f8b\u63d2\u5165\uff0c\u89e3\u51b3\u4e86AI\u89c6\u9891\u751f\u6210\u5411\u53ef\u63a7\u751f\u6210\u548c\u9ad8\u8d28\u91cf\u540e\u5904\u7406\u8f6c\u53d8\u4e2d\u7684\u5173\u952e\u6280\u672f\u6311\u6218\uff0c\u4e3a\u4e13\u4e1aAI\u8f85\u52a9\u7535\u5f71\u5236\u4f5c\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.08337", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.08337", "abs": "https://arxiv.org/abs/2602.08337", "authors": ["Sheng Yan", "Yong Wang", "Xin Du", "Junsong Yuan", "Mengyuan Liu"], "title": "Language-Guided Transformer Tokenizer for Human Motion Generation", "comment": null, "summary": "In this paper, we focus on motion discrete tokenization, which converts raw motion into compact discrete tokens--a process proven crucial for efficient motion generation. In this paradigm, increasing the number of tokens is a common approach to improving motion reconstruction quality, but more tokens make it more difficult for generative models to learn. To maintain high reconstruction quality while reducing generation complexity, we propose leveraging language to achieve efficient motion tokenization, which we term Language-Guided Tokenization (LG-Tok). LG-Tok aligns natural language with motion at the tokenization stage, yielding compact, high-level semantic representations. This approach not only strengthens both tokenization and detokenization but also simplifies the learning of generative models. Furthermore, existing tokenizers predominantly adopt convolutional architectures, whose local receptive fields struggle to support global language guidance. To this end, we propose a Transformer-based Tokenizer that leverages attention mechanisms to enable effective alignment between language and motion. Additionally, we design a language-drop scheme, in which language conditions are randomly removed during training, enabling the detokenizer to support language-free guidance during generation. On the HumanML3D and Motion-X generation benchmarks, LG-Tok achieves Top-1 scores of 0.542 and 0.582, outperforming state-of-the-art methods (MARDM: 0.500 and 0.528), and with FID scores of 0.057 and 0.088, respectively, versus 0.114 and 0.147. LG-Tok-mini uses only half the tokens while maintaining competitive performance (Top-1: 0.521/0.588, FID: 0.085/0.071), validating the efficiency of our semantic representations.", "AI": {"tldr": "LG-Tok\uff1a\u4e00\u79cd\u8bed\u8a00\u5f15\u5bfc\u7684\u8fd0\u52a8\u79bb\u6563\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7Transformer\u67b6\u6784\u5b9e\u73b0\u8bed\u8a00\u4e0e\u8fd0\u52a8\u7684\u5bf9\u9f50\uff0c\u5728\u4fdd\u6301\u9ad8\u8d28\u91cf\u91cd\u5efa\u7684\u540c\u65f6\u51cf\u5c11\u751f\u6210\u590d\u6742\u5ea6", "motivation": "\u4f20\u7edf\u8fd0\u52a8\u79bb\u6563\u5316\u65b9\u6cd5\u901a\u8fc7\u589e\u52a0token\u6570\u91cf\u6765\u63d0\u9ad8\u91cd\u5efa\u8d28\u91cf\uff0c\u4f46\u8fd9\u4f1a\u589e\u52a0\u751f\u6210\u6a21\u578b\u7684\u590d\u6742\u5ea6\u3002\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u4fdd\u6301\u9ad8\u8d28\u91cf\u91cd\u5efa\u53c8\u80fd\u964d\u4f4e\u751f\u6210\u590d\u6742\u5ea6\u7684tokenization\u65b9\u6cd5", "method": "\u63d0\u51fa\u8bed\u8a00\u5f15\u5bfc\u7684tokenization\uff08LG-Tok\uff09\uff0c\u4f7f\u7528Transformer\u67b6\u6784\u5b9e\u73b0\u8bed\u8a00\u4e0e\u8fd0\u52a8\u7684\u5bf9\u9f50\uff0c\u8bbe\u8ba1\u8bed\u8a00\u4e22\u5f03\u65b9\u6848\u4f7fdetokenizer\u652f\u6301\u65e0\u8bed\u8a00\u5f15\u5bfc\u7684\u751f\u6210", "result": "\u5728HumanML3D\u548cMotion-X\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cLG-Tok\u83b7\u5f97Top-1\u5206\u65700.542\u548c0.582\uff0c\u4f18\u4e8eSOTA\u65b9\u6cd5\uff08MARDM\uff1a0.500\u548c0.528\uff09\uff1bFID\u5206\u6570\u5206\u522b\u4e3a0.057\u548c0.088\uff0c\u4f18\u4e8e0.114\u548c0.147\u3002LG-Tok-mini\u4ec5\u4f7f\u7528\u4e00\u534atoken\u4ecd\u4fdd\u6301\u7ade\u4e89\u529b", "conclusion": "LG-Tok\u901a\u8fc7\u8bed\u8a00\u5f15\u5bfc\u5b9e\u73b0\u4e86\u7d27\u51d1\u3001\u9ad8\u8bed\u4e49\u7684\u8fd0\u52a8\u8868\u793a\uff0c\u5728\u4fdd\u6301\u9ad8\u8d28\u91cf\u91cd\u5efa\u7684\u540c\u65f6\u964d\u4f4e\u4e86\u751f\u6210\u590d\u6742\u5ea6\uff0c\u4e3a\u9ad8\u6548\u8fd0\u52a8\u751f\u6210\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2602.07107", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07107", "abs": "https://arxiv.org/abs/2602.07107", "authors": ["Shang Liu", "Hanyu Pei", "Zeyan Liu"], "title": "ShallowJail: Steering Jailbreaks against Large Language Models", "comment": null, "summary": "Large Language Models(LLMs) have been successful in numerous fields. Alignment has usually been applied to prevent them from harmful purposes. However, aligned LLMs remain vulnerable to jailbreak attacks that deliberately mislead them into producing harmful outputs. Existing jailbreaks are either black-box, using carefully crafted, unstealthy prompts, or white-box, requiring resource-intensive computation. In light of these challenges, we introduce ShallowJail, a novel attack that exploits shallow alignment in LLMs. ShallowJail can misguide LLMs' responses by manipulating the initial tokens during inference. Through extensive experiments, we demonstrate the effectiveness of~\\shallow, which substantially degrades the safety of state-of-the-art LLM responses.", "AI": {"tldr": "ShallowJail\u662f\u4e00\u79cd\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u6d45\u5c42\u5bf9\u9f50\u6f0f\u6d1e\u7684\u65b0\u578b\u8d8a\u72f1\u653b\u51fb\u65b9\u6cd5\uff0c\u901a\u8fc7\u64cd\u7eb5\u63a8\u7406\u8fc7\u7a0b\u4e2d\u7684\u521d\u59cb\u4ee4\u724c\u6765\u8bef\u5bfc\u6a21\u578b\u4ea7\u751f\u6709\u5bb3\u8f93\u51fa", "motivation": "\u73b0\u6709\u8d8a\u72f1\u653b\u51fb\u8981\u4e48\u662f\u9ed1\u76d2\u65b9\u6cd5\uff08\u4f7f\u7528\u7cbe\u5fc3\u8bbe\u8ba1\u4f46\u4e0d\u9690\u853d\u7684\u63d0\u793a\uff09\uff0c\u8981\u4e48\u662f\u767d\u76d2\u65b9\u6cd5\uff08\u9700\u8981\u5927\u91cf\u8ba1\u7b97\u8d44\u6e90\uff09\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u6709\u6548\u4e14\u9690\u853d\u7684\u653b\u51fb\u65b9\u5f0f", "method": "ShallowJail\u901a\u8fc7\u5229\u7528LLMs\u7684\u6d45\u5c42\u5bf9\u9f50\u6f0f\u6d1e\uff0c\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u64cd\u7eb5\u521d\u59cb\u4ee4\u724c\u6765\u8bef\u5bfc\u6a21\u578b\u54cd\u5e94", "result": "\u5b9e\u9a8c\u8bc1\u660eShallowJail\u80fd\u663e\u8457\u964d\u4f4e\u6700\u5148\u8fdbLLM\u7684\u5b89\u5168\u6027\uff0c\u6709\u6548\u8bef\u5bfc\u6a21\u578b\u4ea7\u751f\u6709\u5bb3\u8f93\u51fa", "conclusion": "ShallowJail\u63ed\u793a\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u6d45\u5c42\u5bf9\u9f50\u7684\u8106\u5f31\u6027\uff0c\u4e3a\u6a21\u578b\u5b89\u5168\u9632\u62a4\u63d0\u51fa\u4e86\u65b0\u7684\u6311\u6218"}}
{"id": "2602.08342", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08342", "abs": "https://arxiv.org/abs/2602.08342", "authors": ["Jie Zhang", "Xingtong Yu", "Yuan Fang", "Rudi Stouffs", "Zdravko Trivic"], "title": "UrbanGraphEmbeddings: Learning and Evaluating Spatially Grounded Multimodal Embeddings for Urban Science", "comment": null, "summary": "Learning transferable multimodal embeddings for urban environments is challenging because urban understanding is inherently spatial, yet existing datasets and benchmarks lack explicit alignment between street-view images and urban structure. We introduce UGData, a spatially grounded dataset that anchors street-view images to structured spatial graphs and provides graph-aligned supervision via spatial reasoning paths and spatial context captions, exposing distance, directionality, connectivity, and neighborhood context beyond image content. Building on UGData, we propose UGE, a two-stage training strategy that progressively and stably aligns images, text, and spatial structures by combining instruction-guided contrastive learning with graph-based spatial encoding. We finally introduce UGBench, a comprehensive benchmark to evaluate how spatially grounded embeddings support diverse urban understanding tasks -- including geolocation ranking, image retrieval, urban perception, and spatial grounding. We develop UGE on multiple state-of-the-art VLM backbones, including Qwen2-VL, Qwen2.5-VL, Phi-3-Vision, and LLaVA1.6-Mistral, and train fixed-dimensional spatial embeddings with LoRA tuning. UGE built upon Qwen2.5-VL-7B backbone achieves up to 44% improvement in image retrieval and 30% in geolocation ranking on training cities, and over 30% and 22% gains respectively on held-out cities, demonstrating the effectiveness of explicit spatial grounding for spatially intensive urban tasks.", "AI": {"tldr": "UGData\u6570\u636e\u96c6\u5c06\u8857\u666f\u56fe\u50cf\u4e0e\u7a7a\u95f4\u56fe\u5bf9\u9f50\uff0cUGE\u8bad\u7ec3\u7b56\u7565\u5bf9\u9f50\u56fe\u50cf\u3001\u6587\u672c\u548c\u7a7a\u95f4\u7ed3\u6784\uff0cUGBench\u8bc4\u4f30\u7a7a\u95f4\u5d4c\u5165\u5728\u591a\u79cd\u57ce\u5e02\u7406\u89e3\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u5728Qwen2.5-VL-7B\u4e0a\u5b9e\u73b0\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u57ce\u5e02\u7406\u89e3\u672c\u8d28\u4e0a\u662f\u7a7a\u95f4\u6027\u7684\uff0c\u4f46\u73b0\u6709\u6570\u636e\u96c6\u548c\u57fa\u51c6\u7f3a\u4e4f\u8857\u666f\u56fe\u50cf\u4e0e\u57ce\u5e02\u7ed3\u6784\u4e4b\u95f4\u7684\u663e\u5f0f\u5bf9\u9f50\uff0c\u8fd9\u9650\u5236\u4e86\u53ef\u8fc1\u79fb\u591a\u6a21\u6001\u5d4c\u5165\u7684\u5b66\u4e60\u3002", "method": "1) \u5f15\u5165UGData\u6570\u636e\u96c6\uff0c\u5c06\u8857\u666f\u56fe\u50cf\u951a\u5b9a\u5230\u7ed3\u6784\u5316\u7a7a\u95f4\u56fe\uff0c\u901a\u8fc7\u7a7a\u95f4\u63a8\u7406\u8def\u5f84\u548c\u7a7a\u95f4\u4e0a\u4e0b\u6587\u63cf\u8ff0\u63d0\u4f9b\u56fe\u5bf9\u9f50\u76d1\u7763\uff1b2) \u63d0\u51faUGE\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff0c\u7ed3\u5408\u6307\u4ee4\u5f15\u5bfc\u5bf9\u6bd4\u5b66\u4e60\u548c\u57fa\u4e8e\u56fe\u7684\u7a7a\u95f4\u7f16\u7801\uff0c\u9010\u6b65\u7a33\u5b9a\u5730\u5bf9\u9f50\u56fe\u50cf\u3001\u6587\u672c\u548c\u7a7a\u95f4\u7ed3\u6784\uff1b3) \u4f7f\u7528LoRA\u8c03\u4f18\u5728\u591a\u4e2a\u5148\u8fdbVLM\u9aa8\u5e72\u4e0a\u8bad\u7ec3\u56fa\u5b9a\u7ef4\u5ea6\u7a7a\u95f4\u5d4c\u5165\u3002", "result": "\u57fa\u4e8eQwen2.5-VL-7B\u9aa8\u5e72\u7684UGE\u5728\u8bad\u7ec3\u57ce\u5e02\u4e0a\u56fe\u50cf\u68c0\u7d22\u63d0\u534744%\uff0c\u5730\u7406\u4f4d\u7f6e\u6392\u540d\u63d0\u534730%\uff1b\u5728\u672a\u89c1\u57ce\u5e02\u4e0a\u5206\u522b\u83b7\u5f97\u8d85\u8fc730%\u548c22%\u7684\u589e\u76ca\uff0c\u8bc1\u660e\u4e86\u663e\u5f0f\u7a7a\u95f4\u63a5\u5730\u5bf9\u7a7a\u95f4\u5bc6\u96c6\u578b\u57ce\u5e02\u4efb\u52a1\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u901a\u8fc7\u663e\u5f0f\u7a7a\u95f4\u63a5\u5730\uff08UGData\u6570\u636e\u96c6\uff09\u3001\u6e10\u8fdb\u5f0f\u5bf9\u9f50\u7b56\u7565\uff08UGE\uff09\u548c\u5168\u9762\u8bc4\u4f30\u57fa\u51c6\uff08UGBench\uff09\uff0c\u80fd\u591f\u6709\u6548\u5b66\u4e60\u53ef\u8fc1\u79fb\u7684\u57ce\u5e02\u591a\u6a21\u6001\u5d4c\u5165\uff0c\u663e\u8457\u63d0\u5347\u7a7a\u95f4\u5bc6\u96c6\u578b\u57ce\u5e02\u7406\u89e3\u4efb\u52a1\u7684\u6027\u80fd\u3002"}}
{"id": "2602.08346", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.08346", "abs": "https://arxiv.org/abs/2602.08346", "authors": ["Yujin Zhou", "Pengcheng Wen", "Jiale Chen", "Boqin Yin", "Han Zhu", "Jiaming Ji", "Juntao Dai", "Chi-Min Chan", "Sirui Han"], "title": "What, Whether and How? Unveiling Process Reward Models for Thinking with Images Reasoning", "comment": null, "summary": "The rapid advancement of Large Vision Language Models (LVLMs) has demonstrated excellent abilities in various visual tasks. Building upon these developments, the thinking with images paradigm has emerged, enabling models to dynamically edit and re-encode visual information at each reasoning step, mirroring human visual processing. However, this paradigm introduces significant challenges as diverse errors may occur during reasoning processes. This necessitates Process Reward Models (PRMs) for distinguishing positive and negative reasoning steps, yet existing benchmarks for PRMs are predominantly text-centric and lack comprehensive assessment under this paradigm. To address these gaps, this work introduces the first comprehensive benchmark specifically designed for evaluating PRMs under the thinking with images paradigm. Our main contributions are: (1) Through extensive analysis of reasoning trajectories and guided search experiments with PRMs, we define 7 fine-grained error types and demonstrate both the necessity for specialized PRMs and the potential for improvement. (2) We construct a comprehensive benchmark comprising 1,206 manually annotated thinking with images reasoning trajectories spanning 4 categories and 16 subcategories for fine-grained evaluation of PRMs. (3) Our experimental analysis reveals that current LVLMs fall short as effective PRMs, exhibiting limited capabilities in visual reasoning process evaluation with significant performance disparities across error types, positive evaluation bias, and sensitivity to reasoning step positions. These findings demonstrate the effectiveness of our benchmark and establish crucial foundations for advancing PRMs in LVLMs.", "AI": {"tldr": "\u8be5\u7814\u7a76\u9488\u5bf9\u5927\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\"\u56fe\u50cf\u601d\u7ef4\"\u8303\u5f0f\uff0c\u9996\u6b21\u6784\u5efa\u4e86\u4e13\u95e8\u8bc4\u4f30\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\u7684\u7efc\u5408\u57fa\u51c6\uff0c\u5305\u542b1,206\u6761\u4eba\u5de5\u6807\u6ce8\u7684\u63a8\u7406\u8f68\u8ff9\uff0c\u5b9a\u4e49\u4e867\u79cd\u7ec6\u7c92\u5ea6\u9519\u8bef\u7c7b\u578b\uff0c\u5e76\u53d1\u73b0\u5f53\u524dLVLMs\u4f5c\u4e3aPRMs\u5b58\u5728\u660e\u663e\u4e0d\u8db3\u3002", "motivation": "\u968f\u7740\u5927\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u53d1\u5c55\uff0c\"\u56fe\u50cf\u601d\u7ef4\"\u8303\u5f0f\u4f7f\u6a21\u578b\u80fd\u591f\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u52a8\u6001\u7f16\u8f91\u548c\u91cd\u65b0\u7f16\u7801\u89c6\u89c9\u4fe1\u606f\u3002\u7136\u800c\uff0c\u8fd9\u79cd\u8303\u5f0f\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u4f1a\u4ea7\u751f\u5404\u79cd\u9519\u8bef\uff0c\u9700\u8981\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\u6765\u533a\u5206\u6b63\u8d1f\u63a8\u7406\u6b65\u9aa4\u3002\u73b0\u6709\u7684PRM\u57fa\u51c6\u4e3b\u8981\u662f\u6587\u672c\u4e2d\u5fc3\u7684\uff0c\u7f3a\u4e4f\u9488\u5bf9\u8fd9\u4e00\u8303\u5f0f\u7684\u5168\u9762\u8bc4\u4f30\u3002", "method": "1) \u901a\u8fc7\u5206\u6790\u63a8\u7406\u8f68\u8ff9\u548cPRMs\u5f15\u5bfc\u641c\u7d22\u5b9e\u9a8c\uff0c\u5b9a\u4e497\u79cd\u7ec6\u7c92\u5ea6\u9519\u8bef\u7c7b\u578b\uff1b2) \u6784\u5efa\u5305\u542b1,206\u6761\u4eba\u5de5\u6807\u6ce8\u7684\"\u56fe\u50cf\u601d\u7ef4\"\u63a8\u7406\u8f68\u8ff9\u7684\u57fa\u51c6\uff0c\u6db5\u76d64\u4e2a\u7c7b\u522b\u548c16\u4e2a\u5b50\u7c7b\u522b\uff1b3) \u8fdb\u884c\u5b9e\u9a8c\u5206\u6790\uff0c\u8bc4\u4f30\u5f53\u524dLVLMs\u4f5c\u4e3aPRMs\u7684\u6027\u80fd\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u5f53\u524dLVLMs\u4f5c\u4e3aPRMs\u6548\u679c\u4e0d\u4f73\uff1a\u5728\u89c6\u89c9\u63a8\u7406\u8fc7\u7a0b\u8bc4\u4f30\u4e2d\u80fd\u529b\u6709\u9650\uff0c\u5728\u4e0d\u540c\u9519\u8bef\u7c7b\u578b\u95f4\u8868\u73b0\u5dee\u5f02\u663e\u8457\uff0c\u5b58\u5728\u6b63\u5411\u8bc4\u4f30\u504f\u89c1\uff0c\u5bf9\u63a8\u7406\u6b65\u9aa4\u4f4d\u7f6e\u654f\u611f\u3002\u8fd9\u4e9b\u53d1\u73b0\u8bc1\u660e\u4e86\u6240\u6784\u5efa\u57fa\u51c6\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\"\u56fe\u50cf\u601d\u7ef4\"\u8303\u5f0f\u4e0b\u7684\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\u8bc4\u4f30\u63d0\u4f9b\u4e86\u9996\u4e2a\u7efc\u5408\u57fa\u51c6\uff0c\u63ed\u793a\u4e86\u5f53\u524dLVLMs\u4f5c\u4e3aPRMs\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u63a8\u8fdbLVLMs\u4e2dPRMs\u7684\u53d1\u5c55\u5960\u5b9a\u4e86\u91cd\u8981\u57fa\u7840\u3002"}}
{"id": "2602.08388", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.08388", "abs": "https://arxiv.org/abs/2602.08388", "authors": ["Shuo Zhang", "Wenzhuo Wu", "Huayu Zhang", "Jiarong Cheng", "Xianghao Zang", "Chao Ban", "Hao Sun", "Zhongjiang He", "Tianwei Cao", "Kongming Liang", "Zhanyu Ma"], "title": "Geometric Image Editing via Effects-Sensitive In-Context Inpainting with Diffusion Transformers", "comment": null, "summary": "Recent advances in diffusion models have significantly improved image editing. However, challenges persist in handling geometric transformations, such as translation, rotation, and scaling, particularly in complex scenes. Existing approaches suffer from two main limitations: (1) difficulty in achieving accurate geometric editing of object translation, rotation, and scaling; (2) inadequate modeling of intricate lighting and shadow effects, leading to unrealistic results. To address these issues, we propose GeoEdit, a framework that leverages in-context generation through a diffusion transformer module, which integrates geometric transformations for precise object edits. Moreover, we introduce Effects-Sensitive Attention, which enhances the modeling of intricate lighting and shadow effects for improved realism. To further support training, we construct RS-Objects, a large-scale geometric editing dataset containing over 120,000 high-quality image pairs, enabling the model to learn precise geometric editing while generating realistic lighting and shadows. Extensive experiments on public benchmarks demonstrate that GeoEdit consistently outperforms state-of-the-art methods in terms of visual quality, geometric accuracy, and realism.", "AI": {"tldr": "GeoEdit\u662f\u4e00\u4e2a\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u56fe\u50cf\u7f16\u8f91\u6846\u67b6\uff0c\u901a\u8fc7\u6269\u6563transformer\u6a21\u5757\u5b9e\u73b0\u7cbe\u786e\u7684\u51e0\u4f55\u53d8\u6362\u7f16\u8f91\uff0c\u5e76\u5f15\u5165\u6548\u679c\u654f\u611f\u6ce8\u610f\u529b\u673a\u5236\u6765\u6539\u5584\u5149\u7167\u548c\u9634\u5f71\u6548\u679c\uff0c\u5728\u5927\u578b\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u540e\u663e\u8457\u63d0\u5347\u4e86\u56fe\u50cf\u7f16\u8f91\u7684\u51e0\u4f55\u7cbe\u5ea6\u548c\u771f\u5b9e\u611f\u3002", "motivation": "\u73b0\u6709\u6269\u6563\u6a21\u578b\u5728\u56fe\u50cf\u7f16\u8f91\u4e2d\u5904\u7406\u51e0\u4f55\u53d8\u6362\uff08\u5982\u5e73\u79fb\u3001\u65cb\u8f6c\u3001\u7f29\u653e\uff09\u65f6\u5b58\u5728\u56f0\u96be\uff0c\u7279\u522b\u662f\u5728\u590d\u6742\u573a\u666f\u4e2d\u3002\u4e3b\u8981\u95ee\u9898\u5305\u62ec\uff1a1\uff09\u96be\u4ee5\u5b9e\u73b0\u51c6\u786e\u7684\u51e0\u4f55\u7f16\u8f91\uff1b2\uff09\u5bf9\u590d\u6742\u5149\u7167\u548c\u9634\u5f71\u6548\u679c\u5efa\u6a21\u4e0d\u8db3\uff0c\u5bfc\u81f4\u7ed3\u679c\u4e0d\u771f\u5b9e\u3002", "method": "\u63d0\u51faGeoEdit\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a1\uff09\u57fa\u4e8e\u6269\u6563transformer\u7684\u4e0a\u4e0b\u6587\u751f\u6210\u6a21\u5757\uff0c\u96c6\u6210\u51e0\u4f55\u53d8\u6362\u5b9e\u73b0\u7cbe\u786e\u5bf9\u8c61\u7f16\u8f91\uff1b2\uff09\u6548\u679c\u654f\u611f\u6ce8\u610f\u529b\u673a\u5236\uff0c\u589e\u5f3a\u590d\u6742\u5149\u7167\u548c\u9634\u5f71\u6548\u679c\u7684\u5efa\u6a21\u3002\u6b64\u5916\u6784\u5efa\u4e86\u5305\u542b12\u4e07\u9ad8\u8d28\u91cf\u56fe\u50cf\u5bf9\u7684\u5927\u89c4\u6a21\u51e0\u4f55\u7f16\u8f91\u6570\u636e\u96c6RS-Objects\u7528\u4e8e\u8bad\u7ec3\u3002", "result": "\u5728\u516c\u5f00\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cGeoEdit\u5728\u89c6\u89c9\u8d28\u91cf\u3001\u51e0\u4f55\u7cbe\u5ea6\u548c\u771f\u5b9e\u611f\u65b9\u9762\u6301\u7eed\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002", "conclusion": "GeoEdit\u901a\u8fc7\u521b\u65b0\u7684\u6269\u6563transformer\u67b6\u6784\u548c\u6548\u679c\u654f\u611f\u6ce8\u610f\u529b\u673a\u5236\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u56fe\u50cf\u51e0\u4f55\u7f16\u8f91\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7f16\u8f91\u7ed3\u679c\u7684\u51e0\u4f55\u51c6\u786e\u6027\u548c\u89c6\u89c9\u771f\u5b9e\u611f\u3002"}}
{"id": "2602.08395", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.08395", "abs": "https://arxiv.org/abs/2602.08395", "authors": ["Jianfeng Liang", "Shaocheng Shen", "Botao Xu", "Qiang Hu", "Xiaoyun Zhang"], "title": "D$^2$-VR: Degradation-Robust and Distilled Video Restoration with Synergistic Optimization Strategy", "comment": null, "summary": "The integration of diffusion priors with temporal alignment has emerged as a transformative paradigm for video restoration, delivering fantastic perceptual quality, yet the practical deployment of such frameworks is severely constrained by prohibitive inference latency and temporal instability when confronted with complex real-world degradations. To address these limitations, we propose \\textbf{D$^2$-VR}, a single-image diffusion-based video-restoration framework with low-step inference. To obtain precise temporal guidance under severe degradation, we first design a Degradation-Robust Flow Alignment (DRFA) module that leverages confidence-aware attention to filter unreliable motion cues. We then incorporate an adversarial distillation paradigm to compress the diffusion sampling trajectory into a rapid few-step regime. Finally, a synergistic optimization strategy is devised to harmonize perceptual quality with rigorous temporal consistency. Extensive experiments demonstrate that D$^2$-VR achieves state-of-the-art performance while accelerating the sampling process by \\textbf{12$\\times$}", "AI": {"tldr": "D\u00b2-VR\uff1a\u4e00\u79cd\u57fa\u4e8e\u5355\u56fe\u50cf\u6269\u6563\u7684\u89c6\u9891\u4fee\u590d\u6846\u67b6\uff0c\u901a\u8fc7\u9000\u5316\u9c81\u68d2\u6d41\u5bf9\u9f50\u548c\u5bf9\u6297\u84b8\u998f\u5b9e\u73b012\u500d\u52a0\u901f\uff0c\u5728\u4fdd\u6301\u611f\u77e5\u8d28\u91cf\u7684\u540c\u65f6\u63d0\u5347\u65f6\u95f4\u7a33\u5b9a\u6027", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6269\u6563\u5148\u9a8c\u548c\u65f6\u95f4\u5bf9\u9f50\u7684\u89c6\u9891\u4fee\u590d\u65b9\u6cd5\u867d\u7136\u80fd\u63d0\u4f9b\u51fa\u8272\u7684\u611f\u77e5\u8d28\u91cf\uff0c\u4f46\u5728\u9762\u5bf9\u590d\u6742\u771f\u5b9e\u4e16\u754c\u9000\u5316\u65f6\u5b58\u5728\u63a8\u7406\u5ef6\u8fdf\u9ad8\u548c\u65f6\u95f4\u4e0d\u7a33\u5b9a\u7684\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u90e8\u7f72", "method": "1) \u8bbe\u8ba1\u9000\u5316\u9c81\u68d2\u6d41\u5bf9\u9f50\u6a21\u5757\uff0c\u5229\u7528\u7f6e\u4fe1\u611f\u77e5\u6ce8\u610f\u529b\u8fc7\u6ee4\u4e0d\u53ef\u9760\u8fd0\u52a8\u7ebf\u7d22\uff1b2) \u91c7\u7528\u5bf9\u6297\u84b8\u998f\u8303\u5f0f\u5c06\u6269\u6563\u91c7\u6837\u8f68\u8ff9\u538b\u7f29\u5230\u5feb\u901f\u5c11\u6b65\u673a\u5236\uff1b3) \u8bbe\u8ba1\u534f\u540c\u4f18\u5316\u7b56\u7565\u5e73\u8861\u611f\u77e5\u8d28\u91cf\u548c\u65f6\u95f4\u4e00\u81f4\u6027", "result": "D\u00b2-VR\u5728\u4fdd\u6301\u6700\u5148\u8fdb\u6027\u80fd\u7684\u540c\u65f6\uff0c\u5c06\u91c7\u6837\u8fc7\u7a0b\u52a0\u901f12\u500d\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u590d\u6742\u9000\u5316\u4e0b\u7684\u65f6\u95f4\u4e0d\u7a33\u5b9a\u95ee\u9898", "conclusion": "\u8be5\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u6269\u6563\u57fa\u89c6\u9891\u4fee\u590d\u65b9\u6cd5\u7684\u5ef6\u8fdf\u548c\u7a33\u5b9a\u6027\u95ee\u9898\uff0c\u4e3a\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2602.08397", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.08397", "abs": "https://arxiv.org/abs/2602.08397", "authors": ["Chiara Lena", "Davide Milesi", "Alessandro Casella", "Luca Carlini", "Joseph C. Norton", "James Martin", "Bruno Scaglioni", "Keith L. Obstein", "Roberto De Sire", "Marco Spadaccini", "Cesare Hassan", "Pietro Valdastri", "Elena De Momi"], "title": "RealSynCol: a high-fidelity synthetic colon dataset for 3D reconstruction applications", "comment": null, "summary": "Deep learning has the potential to improve colonoscopy by enabling 3D reconstruction of the colon, providing a comprehensive view of mucosal surfaces and lesions, and facilitating the identification of unexplored areas. However, the development of robust methods is limited by the scarcity of large-scale ground truth data. We propose RealSynCol, a highly realistic synthetic dataset designed to replicate the endoscopic environment. Colon geometries extracted from 10 CT scans were imported into a virtual environment that closely mimics intraoperative conditions and rendered with realistic vascular textures. The resulting dataset comprises 28\\,130 frames, paired with ground truth depth maps, optical flow, 3D meshes, and camera trajectories. A benchmark study was conducted to evaluate the available synthetic colon datasets for the tasks of depth and pose estimation. Results demonstrate that the high realism and variability of RealSynCol significantly enhance generalization performance on clinical images, proving it to be a powerful tool for developing deep learning algorithms to support endoscopic diagnosis.", "AI": {"tldr": "RealSynCol\u662f\u4e00\u4e2a\u9ad8\u5ea6\u903c\u771f\u7684\u5408\u6210\u7ed3\u80a0\u955c\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u89e3\u51b3\u6df1\u5ea6\u5b66\u4e60\u5728\u7ed3\u80a0\u955c3D\u91cd\u5efa\u4e2d\u7f3a\u4e4f\u5927\u89c4\u6a21\u771f\u5b9e\u6570\u636e\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5728\u4e34\u5e8a\u56fe\u50cf\u4e0a\u7684\u6cdb\u5316\u6027\u80fd\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u53ef\u4ee5\u6539\u8fdb\u7ed3\u80a0\u955c\u68c0\u67e5\uff0c\u901a\u8fc73D\u91cd\u5efa\u63d0\u4f9b\u5168\u9762\u7684\u9ecf\u819c\u8868\u9762\u548c\u75c5\u53d8\u89c6\u56fe\uff0c\u5e76\u5e2e\u52a9\u8bc6\u522b\u672a\u63a2\u7d22\u533a\u57df\u3002\u4f46\u73b0\u6709\u65b9\u6cd5\u7684\u53d1\u5c55\u53d7\u5230\u5927\u89c4\u6a21\u771f\u5b9e\u6570\u636e\u7a00\u7f3a\u7684\u9650\u5236\u3002", "method": "\u4ece10\u4e2aCT\u626b\u63cf\u4e2d\u63d0\u53d6\u7ed3\u80a0\u51e0\u4f55\u7ed3\u6784\uff0c\u5bfc\u5165\u5230\u6a21\u62df\u672f\u4e2d\u6761\u4ef6\u7684\u865a\u62df\u73af\u5883\u4e2d\uff0c\u4f7f\u7528\u903c\u771f\u7684\u8840\u7ba1\u7eb9\u7406\u8fdb\u884c\u6e32\u67d3\u3002\u751f\u6210\u4e8628,130\u5e27\u56fe\u50cf\uff0c\u914d\u6709\u6df1\u5ea6\u56fe\u3001\u5149\u6d41\u30013D\u7f51\u683c\u548c\u76f8\u673a\u8f68\u8ff9\u7b49\u771f\u5b9e\u6807\u6ce8\u3002", "result": "\u57fa\u51c6\u7814\u7a76\u8868\u660e\uff0cRealSynCol\u7684\u9ad8\u771f\u5b9e\u611f\u548c\u591a\u6837\u6027\u663e\u8457\u63d0\u5347\u4e86\u5728\u4e34\u5e8a\u56fe\u50cf\u4e0a\u7684\u6cdb\u5316\u6027\u80fd\uff0c\u8bc1\u660e\u5b83\u662f\u5f00\u53d1\u652f\u6301\u5185\u955c\u8bca\u65ad\u7684\u6df1\u5ea6\u5b66\u4e60\u7b97\u6cd5\u7684\u6709\u529b\u5de5\u5177\u3002", "conclusion": "RealSynCol\u662f\u4e00\u4e2a\u9ad8\u5ea6\u903c\u771f\u7684\u5408\u6210\u7ed3\u80a0\u955c\u6570\u636e\u96c6\uff0c\u80fd\u591f\u6709\u6548\u89e3\u51b3\u771f\u5b9e\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u4e3a\u5f00\u53d1\u7ed3\u80a0\u955c3D\u91cd\u5efa\u548c\u75c5\u53d8\u8bc6\u522b\u7684\u6df1\u5ea6\u5b66\u4e60\u7b97\u6cd5\u63d0\u4f9b\u4e86\u91cd\u8981\u8d44\u6e90\u3002"}}
{"id": "2602.08430", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.08430", "abs": "https://arxiv.org/abs/2602.08430", "authors": ["Qiang Wang"], "title": "Understanding and Optimizing Attention-Based Sparse Matching for Diverse Local Features", "comment": null, "summary": "We revisit the problem of training attention-based sparse image matching models for various local features. We first identify one critical design choice that has been previously overlooked, which significantly impacts the performance of the LightGlue model. We then investigate the role of detectors and descriptors within the transformer-based matching framework, finding that detectors, rather than descriptors, are often the primary cause for performance difference. Finally, we propose a novel approach to fine-tune existing image matching models using keypoints from a diverse set of detectors, resulting in a universal, detector-agnostic model. When deployed as a zero-shot matcher for novel detectors, the resulting model achieves or exceeds the accuracy of models specifically trained for those features. Our findings offer valuable insights for the deployment of transformer-based matching models and the future design of local features.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u6ce8\u610f\u529b\u7a00\u758f\u56fe\u50cf\u5339\u914d\u6a21\u578b\u7684\u5173\u952e\u8bbe\u8ba1\u9009\u62e9\uff0c\u53d1\u73b0\u68c0\u6d4b\u5668\u6bd4\u63cf\u8ff0\u7b26\u5bf9\u6027\u80fd\u5f71\u54cd\u66f4\u5927\uff0c\u63d0\u51fa\u7528\u591a\u79cd\u68c0\u6d4b\u5668\u5173\u952e\u70b9\u5fae\u8c03\u73b0\u6709\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u521b\u5efa\u901a\u7528\u68c0\u6d4b\u5668\u65e0\u5173\u7684\u5339\u914d\u6a21\u578b\u3002", "motivation": "\u91cd\u65b0\u5ba1\u89c6\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u7a00\u758f\u56fe\u50cf\u5339\u914d\u6a21\u578b\u8bad\u7ec3\u95ee\u9898\uff0c\u8bc6\u522b\u5148\u524d\u88ab\u5ffd\u89c6\u7684\u5173\u952e\u8bbe\u8ba1\u9009\u62e9\uff0c\u63a2\u7a76\u68c0\u6d4b\u5668\u548c\u63cf\u8ff0\u7b26\u5728\u57fa\u4e8etransformer\u7684\u5339\u914d\u6846\u67b6\u4e2d\u7684\u4f5c\u7528\uff0c\u65e8\u5728\u5f00\u53d1\u901a\u7528\u3001\u68c0\u6d4b\u5668\u65e0\u5173\u7684\u5339\u914d\u6a21\u578b\u3002", "method": "\u9996\u5148\u8bc6\u522b\u5f71\u54cdLightGlue\u6a21\u578b\u6027\u80fd\u7684\u5173\u952e\u8bbe\u8ba1\u9009\u62e9\uff1b\u7136\u540e\u7814\u7a76\u68c0\u6d4b\u5668\u548c\u63cf\u8ff0\u7b26\u5728transformer\u5339\u914d\u6846\u67b6\u4e2d\u7684\u4f5c\u7528\uff1b\u6700\u540e\u63d0\u51fa\u4f7f\u7528\u591a\u79cd\u68c0\u6d4b\u5668\u5173\u952e\u70b9\u5fae\u8c03\u73b0\u6709\u56fe\u50cf\u5339\u914d\u6a21\u578b\u7684\u65b0\u65b9\u6cd5\u3002", "result": "\u53d1\u73b0\u68c0\u6d4b\u5668\uff08\u800c\u975e\u63cf\u8ff0\u7b26\uff09\u901a\u5e38\u662f\u6027\u80fd\u5dee\u5f02\u7684\u4e3b\u8981\u539f\u56e0\uff1b\u63d0\u51fa\u7684\u901a\u7528\u68c0\u6d4b\u5668\u65e0\u5173\u6a21\u578b\u5728\u4f5c\u4e3a\u96f6\u6837\u672c\u5339\u914d\u5668\u7528\u4e8e\u65b0\u68c0\u6d4b\u5668\u65f6\uff0c\u8fbe\u5230\u6216\u8d85\u8fc7\u4e13\u95e8\u4e3a\u8fd9\u4e9b\u7279\u5f81\u8bad\u7ec3\u7684\u6a21\u578b\u7684\u51c6\u786e\u6027\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a\u57fa\u4e8etransformer\u7684\u5339\u914d\u6a21\u578b\u90e8\u7f72\u548c\u672a\u6765\u5c40\u90e8\u7279\u5f81\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u89c1\u89e3\uff0c\u8868\u660e\u901a\u8fc7\u9002\u5f53\u7684\u5fae\u8c03\u65b9\u6cd5\u53ef\u4ee5\u521b\u5efa\u901a\u7528\u7684\u68c0\u6d4b\u5668\u65e0\u5173\u5339\u914d\u6a21\u578b\u3002"}}
{"id": "2602.08439", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.08439", "abs": "https://arxiv.org/abs/2602.08439", "authors": ["Yuhao Dong", "Shulin Tian", "Shuai Liu", "Shuangrui Ding", "Yuhang Zang", "Xiaoyi Dong", "Yuhang Cao", "Jiaqi Wang", "Ziwei Liu"], "title": "Demo-ICL: In-Context Learning for Procedural Video Knowledge Acquisition", "comment": null, "summary": "Despite the growing video understanding capabilities of recent Multimodal Large Language Models (MLLMs), existing video benchmarks primarily assess understanding based on models' static, internal knowledge, rather than their ability to learn and adapt from dynamic, novel contexts from few examples. To bridge this gap, we present Demo-driven Video In-Context Learning, a novel task focused on learning from in-context demonstrations to answer questions about the target videos. Alongside this, we propose Demo-ICL-Bench, a challenging benchmark designed to evaluate demo-driven video in-context learning capabilities. Demo-ICL-Bench is constructed from 1200 instructional YouTube videos with associated questions, from which two types of demonstrations are derived: (i) summarizing video subtitles for text demonstration; and (ii) corresponding instructional videos as video demonstrations. To effectively tackle this new challenge, we develop Demo-ICL, an MLLM with a two-stage training strategy: video-supervised fine-tuning and information-assisted direct preference optimization, jointly enhancing the model's ability to learn from in-context examples. Extensive experiments with state-of-the-art MLLMs confirm the difficulty of Demo-ICL-Bench, demonstrate the effectiveness of Demo-ICL, and thereby unveil future research directions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Demo\u9a71\u52a8\u7684\u89c6\u9891\u4e0a\u4e0b\u6587\u5b66\u4e60\u65b0\u4efb\u52a1\u548cDemo-ICL-Bench\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4ece\u5c11\u91cf\u793a\u4f8b\u4e2d\u5b66\u4e60\u52a8\u6001\u65b0\u77e5\u8bc6\u7684\u80fd\u529b\uff0c\u5e76\u5f00\u53d1\u4e86Demo-ICL\u6a21\u578b\u6765\u89e3\u51b3\u8fd9\u4e00\u6311\u6218\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u57fa\u51c6\u4e3b\u8981\u8bc4\u4f30\u6a21\u578b\u57fa\u4e8e\u9759\u6001\u5185\u90e8\u77e5\u8bc6\u7406\u89e3\u89c6\u9891\u7684\u80fd\u529b\uff0c\u800c\u975e\u4ece\u52a8\u6001\u65b0\u4e0a\u4e0b\u6587\u4e2d\u901a\u8fc7\u5c11\u91cf\u793a\u4f8b\u5b66\u4e60\u548c\u9002\u5e94\u7684\u80fd\u529b\uff0c\u8fd9\u9650\u5236\u4e86\u6a21\u578b\u7684\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002", "method": "\u63d0\u51fa\u4e86Demo-ICL-Bench\u57fa\u51c6\uff0c\u5305\u542b1200\u4e2a\u6559\u5b66YouTube\u89c6\u9891\u53ca\u76f8\u5173\u95ee\u9898\uff0c\u63d0\u4f9b\u6587\u672c\u548c\u89c6\u9891\u4e24\u79cd\u6f14\u793a\u7c7b\u578b\uff1b\u5f00\u53d1\u4e86Demo-ICL\u6a21\u578b\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff1a\u89c6\u9891\u76d1\u7763\u5fae\u8c03\u548c\u4fe1\u606f\u8f85\u52a9\u76f4\u63a5\u504f\u597d\u4f18\u5316\u3002", "result": "\u5b9e\u9a8c\u8868\u660eDemo-ICL-Bench\u5bf9\u73b0\u6709\u6700\u5148\u8fdbMLLM\u5177\u6709\u6311\u6218\u6027\uff0c\u800cDemo-ICL\u6a21\u578b\u5728\u8be5\u57fa\u51c6\u4e0a\u8868\u73b0\u51fa\u6709\u6548\u6027\uff0c\u63ed\u793a\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "conclusion": "\u672c\u6587\u586b\u8865\u4e86\u89c6\u9891\u4e0a\u4e0b\u6587\u5b66\u4e60\u8bc4\u4f30\u7684\u7a7a\u767d\uff0c\u63d0\u51fa\u7684\u4efb\u52a1\u3001\u57fa\u51c6\u548c\u6a21\u578b\u4e3a\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4ece\u52a8\u6001\u4e0a\u4e0b\u6587\u4e2d\u5b66\u4e60\u65b0\u77e5\u8bc6\u7684\u80fd\u529b\u8bc4\u4f30\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2602.08462", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.08462", "abs": "https://arxiv.org/abs/2602.08462", "authors": ["Yiyang Cao", "Yunze Deng", "Ziyu Lin", "Bin Feng", "Xinggang Wang", "Wenyu Liu", "Dandan Zheng", "Jingdong Chen"], "title": "TriC-Motion: Tri-Domain Causal Modeling Grounded Text-to-Motion Generation", "comment": null, "summary": "Text-to-motion generation, a rapidly evolving field in computer vision, aims to produce realistic and text-aligned motion sequences. Current methods primarily focus on spatial-temporal modeling or independent frequency domain analysis, lacking a unified framework for joint optimization across spatial, temporal, and frequency domains. This limitation hinders the model's ability to leverage information from all domains simultaneously, leading to suboptimal generation quality. Additionally, in motion generation frameworks, motion-irrelevant cues caused by noise are often entangled with features that contribute positively to generation, thereby leading to motion distortion. To address these issues, we propose Tri-Domain Causal Text-to-Motion Generation (TriC-Motion), a novel diffusion-based framework integrating spatial-temporal-frequency-domain modeling with causal intervention. TriC-Motion includes three core modeling modules for domain-specific modeling, namely Temporal Motion Encoding, Spatial Topology Modeling, and Hybrid Frequency Analysis. After comprehensive modeling, a Score-guided Tri-domain Fusion module integrates valuable information from the triple domains, simultaneously ensuring temporal consistency, spatial topology, motion trends, and dynamics. Moreover, the Causality-based Counterfactual Motion Disentangler is meticulously designed to expose motion-irrelevant cues to eliminate noise, disentangling the real modeling contributions of each domain for superior generation. Extensive experimental results validate that TriC-Motion achieves superior performance compared to state-of-the-art methods, attaining an outstanding R@1 of 0.612 on the HumanML3D dataset. These results demonstrate its capability to generate high-fidelity, coherent, diverse, and text-aligned motion sequences. Code is available at: https://caoyiyang1105.github.io/TriC-Motion/.", "AI": {"tldr": "TriC-Motion\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u57fa\u4e8e\u6269\u6563\u7684\u6587\u672c\u5230\u52a8\u4f5c\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u65f6\u7a7a\u9891\u57df\u5efa\u6a21\u4e0e\u56e0\u679c\u5e72\u9884\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u8054\u5408\u4f18\u5316\u548c\u566a\u58f0\u89e3\u8026\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u5f53\u524d\u6587\u672c\u5230\u52a8\u4f5c\u751f\u6210\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u65f6\u7a7a\u5efa\u6a21\u6216\u72ec\u7acb\u7684\u9891\u57df\u5206\u6790\uff0c\u7f3a\u4e4f\u8de8\u7a7a\u95f4\u3001\u65f6\u95f4\u548c\u9891\u7387\u57df\u7684\u8054\u5408\u4f18\u5316\u6846\u67b6\uff0c\u8fd9\u9650\u5236\u4e86\u6a21\u578b\u540c\u65f6\u5229\u7528\u6240\u6709\u57df\u4fe1\u606f\u7684\u80fd\u529b\uff0c\u5bfc\u81f4\u751f\u6210\u8d28\u91cf\u4e0d\u7406\u60f3\u3002\u6b64\u5916\uff0c\u52a8\u4f5c\u751f\u6210\u6846\u67b6\u4e2d\u7531\u566a\u58f0\u5f15\u8d77\u7684\u52a8\u4f5c\u65e0\u5173\u7ebf\u7d22\u5e38\u5e38\u4e0e\u5bf9\u751f\u6210\u6709\u79ef\u6781\u8d21\u732e\u7684\u7279\u5f81\u7ea0\u7f20\u5728\u4e00\u8d77\uff0c\u5bfc\u81f4\u52a8\u4f5c\u5931\u771f\u3002", "method": "\u63d0\u51faTri-Domain Causal Text-to-Motion Generation (TriC-Motion)\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u5efa\u6a21\u6a21\u5757\uff1a\u65f6\u95f4\u52a8\u4f5c\u7f16\u7801\u3001\u7a7a\u95f4\u62d3\u6251\u5efa\u6a21\u548c\u6df7\u5408\u9891\u7387\u5206\u6790\u3002\u901a\u8fc7Score-guided Tri-domain Fusion\u6a21\u5757\u6574\u5408\u4e09\u4e2a\u57df\u7684\u6709\u4ef7\u503c\u4fe1\u606f\uff0c\u540c\u65f6\u786e\u4fdd\u65f6\u95f4\u4e00\u81f4\u6027\u3001\u7a7a\u95f4\u62d3\u6251\u3001\u52a8\u4f5c\u8d8b\u52bf\u548c\u52a8\u6001\u7279\u6027\u3002\u6b64\u5916\uff0c\u8bbe\u8ba1\u4e86\u57fa\u4e8e\u56e0\u679c\u5173\u7cfb\u7684\u53cd\u4e8b\u5b9e\u52a8\u4f5c\u89e3\u8026\u5668\u6765\u66b4\u9732\u52a8\u4f5c\u65e0\u5173\u7ebf\u7d22\u4ee5\u6d88\u9664\u566a\u58f0\u3002", "result": "\u5728HumanML3D\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u51fa\u8272\u7684\u6027\u80fd\uff0cR@1\u8fbe\u52300.612\uff0c\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u6846\u67b6\u80fd\u591f\u751f\u6210\u9ad8\u4fdd\u771f\u3001\u8fde\u8d2f\u3001\u591a\u6837\u4e14\u4e0e\u6587\u672c\u5bf9\u9f50\u7684\u52a8\u4f5c\u5e8f\u5217\u3002", "conclusion": "TriC-Motion\u901a\u8fc7\u6574\u5408\u65f6\u7a7a\u9891\u57df\u5efa\u6a21\u4e0e\u56e0\u679c\u5e72\u9884\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u6587\u672c\u5230\u52a8\u4f5c\u751f\u6210\u4e2d\u7684\u8054\u5408\u4f18\u5316\u548c\u566a\u58f0\u89e3\u8026\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u52a8\u4f5c\u751f\u6210\uff0c\u4e3a\u76f8\u5173\u9886\u57df\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.08479", "categories": ["cs.CV", "cs.AI", "cs.ET", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.08479", "abs": "https://arxiv.org/abs/2602.08479", "authors": ["Alif Rizqullah Mahdi", "Mahdi Rezaei", "Natasha Merat"], "title": "Gesture Matters: Pedestrian Gesture Recognition for AVs Through Skeleton Pose Evaluation", "comment": "9th International Conference on Instrumentation, Control, and Automation (ICA)", "summary": "Gestures are a key component of non-verbal communication in traffic, often helping pedestrian-to-driver interactions when formal traffic rules may be insufficient. This problem becomes more apparent when autonomous vehicles (AVs) struggle to interpret such gestures. In this study, we present a gesture classification framework using 2D pose estimation applied to real-world video sequences from the WIVW dataset. We categorise gestures into four primary classes (Stop, Go, Thank & Greet, and No Gesture) and extract 76 static and dynamic features from normalised keypoints. Our analysis demonstrates that hand position and movement velocity are especially discriminative in distinguishing between gesture classes, achieving a classification accuracy score of 87%. These findings not only improve the perceptual capabilities of AV systems but also contribute to the broader understanding of pedestrian behaviour in traffic contexts.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u57fa\u4e8e2D\u59ff\u6001\u4f30\u8ba1\u7684\u624b\u52bf\u5206\u7c7b\u6846\u67b6\uff0c\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u8bc6\u522b\u884c\u4eba\u624b\u52bf\uff0c\u5728\u771f\u5b9e\u4e16\u754c\u89c6\u9891\u6570\u636e\u4e0a\u8fbe\u523087%\u7684\u5206\u7c7b\u51c6\u786e\u7387\u3002", "motivation": "\u624b\u52bf\u662f\u4ea4\u901a\u4e2d\u975e\u8bed\u8a00\u4ea4\u6d41\u7684\u5173\u952e\u7ec4\u6210\u90e8\u5206\uff0c\u6709\u52a9\u4e8e\u884c\u4eba\u4e0e\u9a7e\u9a76\u5458\u4e92\u52a8\uff0c\u4f46\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u96be\u4ee5\u89e3\u91ca\u8fd9\u4e9b\u624b\u52bf\u3002\u5f53\u6b63\u5f0f\u4ea4\u901a\u89c4\u5219\u4e0d\u8db3\u65f6\uff0c\u8fd9\u4e2a\u95ee\u9898\u66f4\u52a0\u660e\u663e\u3002", "method": "\u4f7f\u75282D\u59ff\u6001\u4f30\u8ba1\u6280\u672f\u5904\u7406WIVW\u6570\u636e\u96c6\u7684\u771f\u5b9e\u4e16\u754c\u89c6\u9891\u5e8f\u5217\uff0c\u5c06\u624b\u52bf\u5206\u4e3a\u56db\u7c7b\uff08\u505c\u6b62\u3001\u901a\u884c\u3001\u611f\u8c22\u4e0e\u95ee\u5019\u3001\u65e0\u624b\u52bf\uff09\uff0c\u4ece\u5f52\u4e00\u5316\u7684\u5173\u952e\u70b9\u4e2d\u63d0\u53d676\u4e2a\u9759\u6001\u548c\u52a8\u6001\u7279\u5f81\u3002", "result": "\u5206\u6790\u8868\u660e\u624b\u90e8\u4f4d\u7f6e\u548c\u79fb\u52a8\u901f\u5ea6\u5728\u533a\u5206\u624b\u52bf\u7c7b\u522b\u65b9\u9762\u7279\u522b\u5177\u6709\u5224\u522b\u529b\uff0c\u5206\u7c7b\u51c6\u786e\u7387\u8fbe\u523087%\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u4e0d\u4ec5\u63d0\u5347\u4e86\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u611f\u77e5\u80fd\u529b\uff0c\u8fd8\u6709\u52a9\u4e8e\u66f4\u5e7f\u6cdb\u5730\u7406\u89e3\u4ea4\u901a\u73af\u5883\u4e2d\u7684\u884c\u4eba\u884c\u4e3a\u3002"}}
{"id": "2602.08491", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.08491", "abs": "https://arxiv.org/abs/2602.08491", "authors": ["Keonvin Park", "Aditya Pal", "Jin Hong Mok"], "title": "Enhanced Food Category Recognition under Illumination-Induced Domain Shift", "comment": null, "summary": "Visual food recognition systems deployed in real-world environments, such as automated conveyor-belt inspection, are highly sensitive to domain shifts caused by illumination changes. While recent studies have shown that lighting variations can significantly distort food perception by both humans and AI, existing works are often limited to single food categories or controlled settings, and most public food datasets lack explicit illumination annotations.\n  In this work, we investigate illumination-induced domain shift in multi-class food category recognition using two widely adopted datasets, Food-101 and Fruits-360. We demonstrate substantial accuracy degradation under cross-dataset evaluation due to mismatched visual conditions. To address this challenge, we construct synthetic illumination-augmented datasets by systematically varying light temperature and intensity, enabling controlled robustness analysis without additional labels.\n  We further evaluate cross-dataset transfer learning and domain generalization, with a focus on illumination-sensitive target categories such as apple-based classes. Experimental results show that illumination-aware augmentation significantly improves recognition robustness under domain shift while preserving real-time performance. Our findings highlight the importance of illumination robustness and provide practical insights for deploying reliable food recognition systems in real-world inspection scenarios.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u8ba8\u4e86\u5149\u7167\u53d8\u5316\u5bf9\u591a\u7c7b\u522b\u98df\u7269\u8bc6\u522b\u7cfb\u7edf\u7684\u5f71\u54cd\uff0c\u901a\u8fc7\u5408\u6210\u5149\u7167\u589e\u5f3a\u6570\u636e\u96c6\u548c\u8de8\u6570\u636e\u96c6\u8bc4\u4f30\uff0c\u63d0\u51fa\u4e86\u63d0\u5347\u5149\u7167\u9c81\u68d2\u6027\u7684\u65b9\u6cd5\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u89c6\u89c9\u98df\u7269\u8bc6\u522b\u7cfb\u7edf\uff08\u5982\u81ea\u52a8\u4f20\u9001\u5e26\u68c0\u6d4b\uff09\u5bf9\u5149\u7167\u53d8\u5316\u5f15\u8d77\u7684\u9886\u57df\u504f\u79fb\u975e\u5e38\u654f\u611f\u3002\u73b0\u6709\u7814\u7a76\u901a\u5e38\u5c40\u9650\u4e8e\u5355\u4e00\u98df\u7269\u7c7b\u522b\u6216\u53d7\u63a7\u73af\u5883\uff0c\u4e14\u5927\u591a\u6570\u516c\u5171\u98df\u7269\u6570\u636e\u96c6\u7f3a\u4e4f\u660e\u786e\u7684\u5149\u7167\u6807\u6ce8\uff0c\u56e0\u6b64\u9700\u8981\u7814\u7a76\u5149\u7167\u53d8\u5316\u5bf9\u591a\u7c7b\u522b\u98df\u7269\u8bc6\u522b\u7684\u5f71\u54cd\u3002", "method": "\u4f7f\u7528Food-101\u548cFruits-360\u4e24\u4e2a\u5e7f\u6cdb\u91c7\u7528\u7684\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u7cfb\u7edf\u53d8\u5316\u5149\u6e29\u548c\u5f3a\u5ea6\u6784\u5efa\u5408\u6210\u5149\u7167\u589e\u5f3a\u6570\u636e\u96c6\uff0c\u8fdb\u884c\u8de8\u6570\u636e\u96c6\u8bc4\u4f30\u3002\u540c\u65f6\u8bc4\u4f30\u8de8\u6570\u636e\u96c6\u8fc1\u79fb\u5b66\u4e60\u548c\u9886\u57df\u6cdb\u5316\u65b9\u6cd5\uff0c\u7279\u522b\u5173\u6ce8\u5bf9\u5149\u7167\u654f\u611f\u7684\u82f9\u679c\u7c7b\u98df\u7269\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u7531\u4e8e\u89c6\u89c9\u6761\u4ef6\u4e0d\u5339\u914d\uff0c\u8de8\u6570\u636e\u96c6\u8bc4\u4f30\u5bfc\u81f4\u51c6\u786e\u7387\u663e\u8457\u4e0b\u964d\u3002\u5149\u7167\u611f\u77e5\u589e\u5f3a\u663e\u8457\u63d0\u9ad8\u4e86\u5728\u9886\u57df\u504f\u79fb\u4e0b\u7684\u8bc6\u522b\u9c81\u68d2\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u5b9e\u65f6\u6027\u80fd\u3002", "conclusion": "\u5149\u7167\u9c81\u68d2\u6027\u5bf9\u4e8e\u90e8\u7f72\u53ef\u9760\u7684\u73b0\u5b9e\u4e16\u754c\u98df\u7269\u8bc6\u522b\u7cfb\u7edf\u81f3\u5173\u91cd\u8981\u3002\u5149\u7167\u611f\u77e5\u589e\u5f3a\u65b9\u6cd5\u80fd\u6709\u6548\u63d0\u5347\u7cfb\u7edf\u5728\u5149\u7167\u53d8\u5316\u73af\u5883\u4e2d\u7684\u6027\u80fd\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u5b9e\u7528\u89c1\u89e3\u3002"}}
{"id": "2602.08524", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.08524", "abs": "https://arxiv.org/abs/2602.08524", "authors": ["Linger Deng", "Yuliang Liu", "Wenwen Yu", "Zujia Zhang", "Jianzhong Ju", "Zhenbo Luo", "Xiang Bai"], "title": "GeoFocus: Blending Efficient Global-to-Local Perception for Multimodal Geometry Problem-Solving", "comment": null, "summary": "Geometry problem-solving remains a significant challenge for Large Multimodal Models (LMMs), requiring not only global shape recognition but also attention to intricate local relationships related to geometric theory. To address this, we propose GeoFocus, a novel framework comprising two core modules. 1) Critical Local Perceptor, which automatically identifies and emphasizes critical local structure (e.g., angles, parallel lines, comparative distances) through thirteen theory-based perception templates, boosting critical local feature coverage by 61% compared to previous methods. 2) VertexLang, a compact topology formal language, encodes global figures through vertex coordinates and connectivity relations. By replacing bulky code-based encodings, VertexLang reduces global perception training time by 20% while improving topology recognition accuracy. When evaluated in Geo3K, GeoQA, and FormalGeo7K, GeoFocus achieves a 4.7% accuracy improvement over leading specialized models and demonstrates superior robustness in MATHVERSE under diverse visual conditions. Project Page -- https://github.com/dle666/GeoFocus", "AI": {"tldr": "GeoFocus\u662f\u4e00\u4e2a\u89e3\u51b3\u51e0\u4f55\u95ee\u9898\u7684\u591a\u6a21\u6001\u6a21\u578b\u6846\u67b6\uff0c\u901a\u8fc7\u5173\u952e\u5c40\u90e8\u611f\u77e5\u5668\u548c\u9876\u70b9\u8bed\u8a00\u7f16\u7801\uff0c\u663e\u8457\u63d0\u5347\u4e86\u51e0\u4f55\u95ee\u9898\u6c42\u89e3\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "motivation": "\u51e0\u4f55\u95ee\u9898\u6c42\u89e3\u5bf9\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u5177\u6709\u6311\u6218\u6027\uff0c\u9700\u8981\u5168\u5c40\u5f62\u72b6\u8bc6\u522b\u548c\u5c40\u90e8\u51e0\u4f55\u5173\u7cfb\u7684\u7cbe\u7ec6\u5173\u6ce8\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u5173\u952e\u5c40\u90e8\u7279\u5f81\u8986\u76d6\u548c\u5168\u5c40\u62d3\u6251\u7f16\u7801\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u63d0\u51faGeoFocus\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u6a21\u5757\uff1a1) \u5173\u952e\u5c40\u90e8\u611f\u77e5\u5668\uff0c\u901a\u8fc713\u4e2a\u57fa\u4e8e\u7406\u8bba\u7684\u611f\u77e5\u6a21\u677f\u81ea\u52a8\u8bc6\u522b\u548c\u5f3a\u8c03\u5173\u952e\u5c40\u90e8\u7ed3\u6784\uff1b2) VertexLang\uff0c\u4e00\u79cd\u7d27\u51d1\u7684\u62d3\u6251\u5f62\u5f0f\u8bed\u8a00\uff0c\u901a\u8fc7\u9876\u70b9\u5750\u6807\u548c\u8fde\u63a5\u5173\u7cfb\u7f16\u7801\u5168\u5c40\u56fe\u5f62\u3002", "result": "\u5728Geo3K\u3001GeoQA\u548cFormalGeo7K\u6570\u636e\u96c6\u4e0a\uff0cGeoFocus\u6bd4\u9886\u5148\u7684\u4e13\u7528\u6a21\u578b\u51c6\u786e\u7387\u63d0\u53474.7%\uff1b\u5173\u952e\u5c40\u90e8\u7279\u5f81\u8986\u76d6\u63d0\u534761%\uff1b\u5168\u5c40\u611f\u77e5\u8bad\u7ec3\u65f6\u95f4\u51cf\u5c1120%\uff1b\u5728MATHVERSE\u4e2d\u5c55\u73b0\u51fa\u4f18\u8d8a\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "GeoFocus\u901a\u8fc7\u7ed3\u5408\u5c40\u90e8\u611f\u77e5\u589e\u5f3a\u548c\u9ad8\u6548\u5168\u5c40\u7f16\u7801\uff0c\u663e\u8457\u63d0\u5347\u4e86\u51e0\u4f55\u95ee\u9898\u6c42\u89e3\u80fd\u529b\uff0c\u4e3a\u591a\u6a21\u6001\u51e0\u4f55\u63a8\u7406\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.08528", "categories": ["cs.CV", "math.OC"], "pdf": "https://arxiv.org/pdf/2602.08528", "abs": "https://arxiv.org/abs/2602.08528", "authors": ["Chuyang Wu", "Samuli Siltanen"], "title": "Automatic regularization parameter choice for tomography using a double model approach", "comment": null, "summary": "Image reconstruction in X-ray tomography is an ill-posed inverse problem, particularly with limited available data. Regularization is thus essential, but its effectiveness hinges on the choice of a regularization parameter that balances data fidelity against a priori information. We present a novel method for automatic parameter selection based on the use of two distinct computational discretizations of the same problem. A feedback control algorithm dynamically adjusts the regularization strength, driving an iterative reconstruction toward the smallest parameter that yields sufficient similarity between reconstructions on the two grids. The effectiveness of the proposed approach is demonstrated using real tomographic data.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53cc\u7f51\u683c\u79bb\u6563\u5316\u7684\u81ea\u52a8\u6b63\u5219\u5316\u53c2\u6570\u9009\u62e9\u65b9\u6cd5\uff0c\u901a\u8fc7\u53cd\u9988\u63a7\u5236\u7b97\u6cd5\u52a8\u6001\u8c03\u6574\u6b63\u5219\u5316\u5f3a\u5ea6\uff0c\u5728X\u5c04\u7ebf\u65ad\u5c42\u626b\u63cf\u56fe\u50cf\u91cd\u5efa\u4e2d\u5b9e\u73b0\u53c2\u6570\u81ea\u52a8\u4f18\u5316\u3002", "motivation": "X\u5c04\u7ebf\u65ad\u5c42\u626b\u63cf\u56fe\u50cf\u91cd\u5efa\u662f\u4e00\u4e2a\u75c5\u6001\u9006\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u6570\u636e\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u3002\u6b63\u5219\u5316\u662f\u5fc5\u8981\u7684\uff0c\u4f46\u5176\u6548\u679c\u4f9d\u8d56\u4e8e\u6b63\u5219\u5316\u53c2\u6570\u7684\u9009\u62e9\uff0c\u8be5\u53c2\u6570\u9700\u8981\u5728\u6570\u636e\u4fdd\u771f\u5ea6\u548c\u5148\u9a8c\u4fe1\u606f\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\u3002\u76ee\u524d\u7f3a\u4e4f\u6709\u6548\u7684\u81ea\u52a8\u53c2\u6570\u9009\u62e9\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u81ea\u52a8\u53c2\u6570\u9009\u62e9\u65b9\u6cd5\uff0c\u57fa\u4e8e\u540c\u4e00\u95ee\u9898\u7684\u4e24\u79cd\u4e0d\u540c\u8ba1\u7b97\u79bb\u6563\u5316\u3002\u4f7f\u7528\u53cd\u9988\u63a7\u5236\u7b97\u6cd5\u52a8\u6001\u8c03\u6574\u6b63\u5219\u5316\u5f3a\u5ea6\uff0c\u9a71\u52a8\u8fed\u4ee3\u91cd\u5efa\u8d8b\u5411\u4e8e\u80fd\u591f\u4f7f\u4e24\u4e2a\u7f51\u683c\u4e0a\u7684\u91cd\u5efa\u7ed3\u679c\u8fbe\u5230\u8db3\u591f\u76f8\u4f3c\u5ea6\u7684\u6700\u5c0f\u53c2\u6570\u503c\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u5b9e\u9645\u65ad\u5c42\u626b\u63cf\u6570\u636e\u4e0a\u5f97\u5230\u4e86\u9a8c\u8bc1\uff0c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u53cc\u7f51\u683c\u79bb\u6563\u5316\u7ed3\u5408\u53cd\u9988\u63a7\u5236\u7b97\u6cd5\u7684\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5b9e\u73b0\u6b63\u5219\u5316\u53c2\u6570\u7684\u81ea\u52a8\u9009\u62e9\uff0c\u89e3\u51b3\u4e86X\u5c04\u7ebf\u65ad\u5c42\u626b\u63cf\u56fe\u50cf\u91cd\u5efa\u4e2d\u7684\u53c2\u6570\u9009\u62e9\u96be\u9898\uff0c\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u8868\u73b0\u51fa\u826f\u597d\u6548\u679c\u3002"}}
{"id": "2602.08531", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.08531", "abs": "https://arxiv.org/abs/2602.08531", "authors": ["Anastasiia Kornilova", "Ivan Moskalenko", "Arabella Gromova", "Gonzalo Ferrer", "Alexander Menshchikov"], "title": "Thegra: Graph-based SLAM for Thermal Imagery", "comment": null, "summary": "Thermal imaging provides a practical sensing modality for visual SLAM in visually degraded environments such as low illumination, smoke, or adverse weather. However, thermal imagery often exhibits low texture, low contrast, and high noise, complicating feature-based SLAM. In this work, we propose a sparse monocular graph-based SLAM system for thermal imagery that leverages general-purpose learned features -- the SuperPoint detector and LightGlue matcher, trained on large-scale visible-spectrum data to improve cross-domain generalization. To adapt these components to thermal data, we introduce a preprocessing pipeline to enhance input suitability and modify core SLAM modules to handle sparse and outlier-prone feature matches. We further incorporate keypoint confidence scores from SuperPoint into a confidence-weighted factor graph to improve estimation robustness. Evaluations on public thermal datasets demonstrate that the proposed system achieves reliable performance without requiring dataset-specific training or fine-tuning a desired feature detector, given the scarcity of quality thermal data. Code will be made available upon publication.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7a00\u758f\u5355\u76ee\u56fe\u7684\u70ed\u6210\u50cfSLAM\u7cfb\u7edf\uff0c\u5229\u7528\u53ef\u89c1\u5149\u8c31\u8bad\u7ec3\u7684\u901a\u7528\u5b66\u4e60\u7279\u5f81\uff08SuperPoint\u68c0\u6d4b\u5668\u548cLightGlue\u5339\u914d\u5668\uff09\uff0c\u901a\u8fc7\u9884\u5904\u7406\u548c\u7f6e\u4fe1\u5ea6\u52a0\u6743\u56e0\u5b50\u56fe\u63d0\u9ad8\u5728\u4f4e\u7eb9\u7406\u70ed\u56fe\u50cf\u4e0a\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u70ed\u6210\u50cf\u5728\u89c6\u89c9\u9000\u5316\u73af\u5883\uff08\u5982\u4f4e\u5149\u7167\u3001\u70df\u96fe\u3001\u6076\u52a3\u5929\u6c14\uff09\u4e2d\u5177\u6709\u5b9e\u7528\u4ef7\u503c\uff0c\u4f46\u70ed\u56fe\u50cf\u901a\u5e38\u5177\u6709\u4f4e\u7eb9\u7406\u3001\u4f4e\u5bf9\u6bd4\u5ea6\u548c\u9ad8\u566a\u58f0\u7684\u7279\u70b9\uff0c\u8fd9\u4f7f\u5f97\u57fa\u4e8e\u7279\u5f81\u7684SLAM\u53d8\u5f97\u590d\u6742\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u70ed\u56fe\u50cf\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u4e14\u7f3a\u4e4f\u9ad8\u8d28\u91cf\u7684\u70ed\u6210\u50cf\u8bad\u7ec3\u6570\u636e\u3002", "method": "1. \u4f7f\u7528\u5728\u53ef\u89c1\u5149\u8c31\u5927\u89c4\u6a21\u6570\u636e\u4e0a\u8bad\u7ec3\u7684SuperPoint\u7279\u5f81\u68c0\u6d4b\u5668\u548cLightGlue\u5339\u914d\u5668\uff0c\u5229\u7528\u5176\u8de8\u57df\u6cdb\u5316\u80fd\u529b\uff1b2. \u5f15\u5165\u9884\u5904\u7406\u7ba1\u9053\u589e\u5f3a\u70ed\u56fe\u50cf\u8f93\u5165\u9002\u5e94\u6027\uff1b3. \u4fee\u6539\u6838\u5fc3SLAM\u6a21\u5757\u4ee5\u5904\u7406\u7a00\u758f\u548c\u5f02\u5e38\u503c\u8f83\u591a\u7684\u7279\u5f81\u5339\u914d\uff1b4. \u5c06SuperPoint\u7684\u5173\u952e\u70b9\u7f6e\u4fe1\u5ea6\u5206\u6570\u6574\u5408\u5230\u7f6e\u4fe1\u5ea6\u52a0\u6743\u56e0\u5b50\u56fe\u4e2d\uff0c\u63d0\u9ad8\u4f30\u8ba1\u9c81\u68d2\u6027\u3002", "result": "\u5728\u516c\u5f00\u70ed\u6210\u50cf\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u7cfb\u7edf\u5b9e\u73b0\u4e86\u53ef\u9760\u7684\u6027\u80fd\uff0c\u65e0\u9700\u8fdb\u884c\u6570\u636e\u96c6\u7279\u5b9a\u7684\u8bad\u7ec3\u6216\u5fae\u8c03\u7279\u5f81\u68c0\u6d4b\u5668\uff0c\u8fd9\u5728\u9ad8\u8d28\u91cf\u70ed\u6570\u636e\u7a00\u7f3a\u7684\u60c5\u51b5\u4e0b\u5c24\u4e3a\u91cd\u8981\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u6709\u6548\u7684\u70ed\u6210\u50cfSLAM\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528\u53ef\u89c1\u5149\u8c31\u8bad\u7ec3\u7684\u901a\u7528\u5b66\u4e60\u7279\u5f81\u548c\u4e13\u95e8\u7684\u9884\u5904\u7406\u4e0e\u4f18\u5316\u6280\u672f\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u70ed\u56fe\u50cf\u4f4e\u7eb9\u7406\u3001\u4f4e\u5bf9\u6bd4\u5ea6\u5e26\u6765\u7684\u6311\u6218\uff0c\u4e3a\u89c6\u89c9\u9000\u5316\u73af\u5883\u4e0b\u7684\u5b9a\u4f4d\u4e0e\u5efa\u56fe\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.08540", "categories": ["cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2602.08540", "abs": "https://arxiv.org/abs/2602.08540", "authors": ["He Wu", "Xia Yan", "Yanghui Xu", "Liegang Xia", "Jiazhou Chen"], "title": "TIBR4D: Tracing-Guided Iterative Boundary Refinement for Efficient 4D Gaussian Segmentation", "comment": "13 pages, 6 figures, 4 tables", "summary": "Object-level segmentation in dynamic 4D Gaussian scenes remains challenging due to complex motion, occlusions, and ambiguous boundaries. In this paper, we present an efficient learning-free 4D Gaussian segmentation framework that lifts video segmentation masks to 4D spaces, whose core is a two-stage iterative boundary refinement, TIBR4D. The first stage is an Iterative Gaussian Instance Tracing (IGIT) at the temporal segment level. It progressively refines Gaussian-to-instance probabilities through iterative tracing, and extracts corresponding Gaussian point clouds that better handle occlusions and preserve completeness of object structures compared to existing one-shot threshold-based methods. The second stage is a frame-wise Gaussian Rendering Range Control (RCC) via suppressing highly uncertain Gaussians near object boundaries while retaining their core contributions for more accurate boundaries. Furthermore, a temporal segmentation merging strategy is proposed for IGIT to balance identity consistency and dynamic awareness. Longer segments enforce stronger multi-frame constraints for stable identities, while shorter segments allow identity changes to be captured promptly. Experiments on HyperNeRF and Neu3D demonstrate that our method produces accurate object Gaussian point clouds with clearer boundaries and higher efficiency compared to SOTA methods.", "AI": {"tldr": "\u63d0\u51faTIBR4D\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u8fed\u4ee3\u8fb9\u754c\u7cbe\u70bc\u5b9e\u73b0\u52a8\u60014D\u9ad8\u65af\u573a\u666f\u7684\u65e0\u5b66\u4e60\u5bf9\u8c61\u5206\u5272\uff0c\u63d0\u5347\u8fb9\u754c\u7cbe\u5ea6\u548c\u5904\u7406\u906e\u6321\u80fd\u529b\u3002", "motivation": "\u52a8\u60014D\u9ad8\u65af\u573a\u666f\u4e2d\u7684\u5bf9\u8c61\u7ea7\u5206\u5272\u9762\u4e34\u590d\u6742\u8fd0\u52a8\u3001\u906e\u6321\u548c\u6a21\u7cca\u8fb9\u754c\u7684\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u5904\u7406\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u8fed\u4ee3\u8fb9\u754c\u7cbe\u70bc\uff1a1) IGIT\u9636\u6bb5\u5728\u65f6\u95f4\u7247\u6bb5\u7ea7\u8fed\u4ee3\u8ffd\u8e2a\u9ad8\u65af\u5b9e\u4f8b\uff0c\u7cbe\u70bc\u6982\u7387\u5e76\u63d0\u53d6\u70b9\u4e91\uff1b2) RCC\u9636\u6bb5\u901a\u8fc7\u6291\u5236\u8fb9\u754c\u9644\u8fd1\u4e0d\u786e\u5b9a\u9ad8\u65af\u5b9e\u73b0\u5e27\u7ea7\u6e32\u67d3\u8303\u56f4\u63a7\u5236\uff1b\u7ed3\u5408\u65f6\u95f4\u5206\u5272\u5408\u5e76\u7b56\u7565\u5e73\u8861\u8eab\u4efd\u4e00\u81f4\u6027\u548c\u52a8\u6001\u611f\u77e5\u3002", "result": "\u5728HyperNeRF\u548cNeu3D\u6570\u636e\u96c6\u4e0a\u5b9e\u9a8c\u8868\u660e\uff0c\u76f8\u6bd4SOTA\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u80fd\u751f\u6210\u8fb9\u754c\u66f4\u6e05\u6670\u3001\u7cbe\u5ea6\u66f4\u9ad8\u7684\u5bf9\u8c61\u9ad8\u65af\u70b9\u4e91\uff0c\u4e14\u6548\u7387\u66f4\u9ad8\u3002", "conclusion": "TIBR4D\u6846\u67b6\u901a\u8fc7\u8fed\u4ee3\u8fb9\u754c\u7cbe\u70bc\u6709\u6548\u89e3\u51b3\u4e86\u52a8\u60014D\u9ad8\u65af\u573a\u666f\u4e2d\u7684\u5bf9\u8c61\u5206\u5272\u95ee\u9898\uff0c\u5728\u8fb9\u754c\u7cbe\u5ea6\u3001\u906e\u6321\u5904\u7406\u548c\u6548\u7387\u65b9\u9762\u5747\u6709\u663e\u8457\u63d0\u5347\u3002"}}
{"id": "2602.08550", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.MM", "eess.IV"], "pdf": "https://arxiv.org/pdf/2602.08550", "abs": "https://arxiv.org/abs/2602.08550", "authors": ["Shih-Fang Chen", "Jun-Cheng Chen", "I-Hong Jhuo", "Yen-Yu Lin"], "title": "GOT-Edit: Geometry-Aware Generic Object Tracking via Online Model Editing", "comment": "ICLR 2026. This is a preprint version. The camera-ready version will be updated soon", "summary": "Human perception for effective object tracking in a 2D video stream arises from the implicit use of prior 3D knowledge combined with semantic reasoning. In contrast, most generic object tracking (GOT) methods primarily rely on 2D features of the target and its surroundings while neglecting 3D geometric cues, which makes them susceptible to partial occlusion, distractors, and variations in geometry and appearance. To address this limitation, we introduce GOT-Edit, an online cross-modality model editing approach that integrates geometry-aware cues into a generic object tracker from a 2D video stream. Our approach leverages features from a pre-trained Visual Geometry Grounded Transformer to enable geometric cue inference from only a few 2D images. To tackle the challenge of seamlessly combining geometry and semantics, GOT-Edit performs online model editing with null-space constrained updates that incorporate geometric information while preserving semantic discrimination, yielding consistently better performance across diverse scenarios. Extensive experiments on multiple GOT benchmarks demonstrate that GOT-Edit achieves superior robustness and accuracy, particularly under occlusion and clutter, establishing a new paradigm for combining 2D semantics with 3D geometric reasoning for generic object tracking.", "AI": {"tldr": "GOT-Edit\uff1a\u4e00\u79cd\u5728\u7ebf\u8de8\u6a21\u6001\u6a21\u578b\u7f16\u8f91\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u51e0\u4f55\u611f\u77e5\u7ebf\u7d22\u6574\u5408\u5230\u901a\u7528\u76ee\u6807\u8ddf\u8e2a\u5668\u4e2d\uff0c\u7ed3\u54082D\u8bed\u4e49\u4e0e3D\u51e0\u4f55\u63a8\u7406\u63d0\u5347\u8ddf\u8e2a\u6027\u80fd", "motivation": "\u4eba\u7c7b\u611f\u77e5\u57282D\u89c6\u9891\u6d41\u4e2d\u8fdb\u884c\u6709\u6548\u76ee\u6807\u8ddf\u8e2a\u65f6\uff0c\u4f1a\u9690\u5f0f\u4f7f\u7528\u5148\u9a8c3D\u77e5\u8bc6\u7ed3\u5408\u8bed\u4e49\u63a8\u7406\u3002\u800c\u5927\u591a\u6570\u901a\u7528\u76ee\u6807\u8ddf\u8e2a\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u76ee\u6807\u76842D\u7279\u5f81\u53ca\u5176\u5468\u56f4\u73af\u5883\uff0c\u5ffd\u7565\u4e863D\u51e0\u4f55\u7ebf\u7d22\uff0c\u8fd9\u4f7f\u5f97\u5b83\u4eec\u5bb9\u6613\u53d7\u5230\u90e8\u5206\u906e\u6321\u3001\u5e72\u6270\u7269\u4ee5\u53ca\u51e0\u4f55\u548c\u5916\u89c2\u53d8\u5316\u7684\u5f71\u54cd\u3002", "method": "GOT-Edit\u91c7\u7528\u5728\u7ebf\u8de8\u6a21\u6001\u6a21\u578b\u7f16\u8f91\u65b9\u6cd5\uff0c\u4ece\u9884\u8bad\u7ec3\u7684\u89c6\u89c9\u51e0\u4f55\u57fa\u7840Transformer\u4e2d\u63d0\u53d6\u7279\u5f81\uff0c\u4ec5\u4ece\u5c11\u91cf2D\u56fe\u50cf\u63a8\u65ad\u51e0\u4f55\u7ebf\u7d22\u3002\u901a\u8fc7\u96f6\u7a7a\u95f4\u7ea6\u675f\u66f4\u65b0\u8fdb\u884c\u5728\u7ebf\u6a21\u578b\u7f16\u8f91\uff0c\u65e0\u7f1d\u7ed3\u5408\u51e0\u4f55\u548c\u8bed\u4e49\u4fe1\u606f\uff0c\u5728\u4fdd\u6301\u8bed\u4e49\u533a\u5206\u80fd\u529b\u7684\u540c\u65f6\u878d\u5165\u51e0\u4f55\u4fe1\u606f\u3002", "result": "\u5728\u591a\u4e2a\u901a\u7528\u76ee\u6807\u8ddf\u8e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cGOT-Edit\u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u9c81\u68d2\u6027\u548c\u51c6\u786e\u6027\uff0c\u7279\u522b\u662f\u5728\u906e\u6321\u548c\u6742\u4e71\u573a\u666f\u4e0b\uff0c\u4e3a\u7ed3\u54082D\u8bed\u4e49\u4e0e3D\u51e0\u4f55\u63a8\u7406\u7684\u901a\u7528\u76ee\u6807\u8ddf\u8e2a\u5efa\u7acb\u4e86\u65b0\u8303\u5f0f\u3002", "conclusion": "GOT-Edit\u901a\u8fc7\u5728\u7ebf\u6a21\u578b\u7f16\u8f91\u5c06\u51e0\u4f55\u611f\u77e5\u7ebf\u7d22\u6574\u5408\u5230\u901a\u7528\u76ee\u6807\u8ddf\u8e2a\u5668\u4e2d\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5ffd\u75653D\u51e0\u4f55\u7ebf\u7d22\u7684\u95ee\u9898\uff0c\u5728\u906e\u6321\u548c\u6742\u4e71\u573a\u666f\u4e0b\u8868\u73b0\u51fa\u8272\uff0c\u4e3a2D\u8bed\u4e49\u4e0e3D\u51e0\u4f55\u63a8\u7406\u7684\u7ed3\u5408\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\u3002"}}
{"id": "2602.08582", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.08582", "abs": "https://arxiv.org/abs/2602.08582", "authors": ["Melany Yang", "Yuhang Yu", "Diwang Weng", "Jinwei Chen", "Wei Dong"], "title": "SemiNFT: Learning to Transfer Presets from Imitation to Appreciation via Hybrid-Sample Reinforcement Learning", "comment": null, "summary": "Photorealistic color retouching plays a vital role in visual content creation, yet manual retouching remains inaccessible to non-experts due to its reliance on specialized expertise. Reference-based methods offer a promising alternative by transferring the preset color of a reference image to a source image. However, these approaches often operate as novice learners, performing global color mappings derived from pixel-level statistics, without a true understanding of semantic context or human aesthetics. To address this issue, we propose SemiNFT, a Diffusion Transformer (DiT)-based retouching framework that mirrors the trajectory of human artistic training: beginning with rigid imitation and evolving into intuitive creation. Specifically, SemiNFT is first taught with paired triplets to acquire basic structural preservation and color mapping skills, and then advanced to reinforcement learning (RL) on unpaired data to cultivate nuanced aesthetic perception. Crucially, during the RL stage, to prevent catastrophic forgetting of old skills, we design a hybrid online-offline reward mechanism that anchors aesthetic exploration with structural review. % experiments Extensive experiments show that SemiNFT not only outperforms state-of-the-art methods on standard preset transfer benchmarks but also demonstrates remarkable intelligence in zero-shot tasks, such as black-and-white photo colorization and cross-domain (anime-to-photo) preset transfer. These results confirm that SemiNFT transcends simple statistical matching and achieves a sophisticated level of aesthetic comprehension. Our project can be found at https://melanyyang.github.io/SemiNFT/.", "AI": {"tldr": "SemiNFT\u662f\u4e00\u4e2a\u57fa\u4e8e\u6269\u6563Transformer\u7684\u53c2\u8003\u5f0f\u8272\u5f69\u4fee\u56fe\u6846\u67b6\uff0c\u901a\u8fc7\u6a21\u4eff\u4eba\u7c7b\u827a\u672f\u8bad\u7ec3\u8f68\u8ff9\uff08\u4ece\u521a\u6027\u6a21\u4eff\u5230\u76f4\u89c9\u521b\u4f5c\uff09\uff0c\u7ed3\u5408\u914d\u5bf9\u6570\u636e\u5b66\u4e60\u548c\u65e0\u914d\u5bf9\u5f3a\u5316\u5b66\u4e60\uff0c\u5b9e\u73b0\u8bed\u4e49\u611f\u77e5\u7684\u8272\u5f69\u8fc1\u79fb\u3002", "motivation": "\u5f53\u524d\u53c2\u8003\u5f0f\u8272\u5f69\u4fee\u56fe\u65b9\u6cd5\u4e3b\u8981\u57fa\u4e8e\u50cf\u7d20\u7ea7\u7edf\u8ba1\u7684\u5168\u5c40\u8272\u5f69\u6620\u5c04\uff0c\u7f3a\u4e4f\u5bf9\u8bed\u4e49\u4e0a\u4e0b\u6587\u548c\u4eba\u7c7b\u7f8e\u5b66\u7684\u771f\u6b63\u7406\u89e3\uff0c\u65e0\u6cd5\u5b9e\u73b0\u4e13\u4e1a\u7ea7\u7684\u4fee\u56fe\u6548\u679c\u3002", "method": "SemiNFT\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\uff1a1\uff09\u4f7f\u7528\u914d\u5bf9\u4e09\u5143\u7ec4\u5b66\u4e60\u57fa\u672c\u7ed3\u6784\u4fdd\u6301\u548c\u8272\u5f69\u6620\u5c04\u6280\u80fd\uff1b2\uff09\u5728\u65e0\u914d\u5bf9\u6570\u636e\u4e0a\u8fdb\u884c\u5f3a\u5316\u5b66\u4e60\u4ee5\u57f9\u517b\u7ec6\u81f4\u7684\u7f8e\u5b66\u611f\u77e5\uff0c\u5e76\u8bbe\u8ba1\u6df7\u5408\u5728\u7ebf-\u79bb\u7ebf\u5956\u52b1\u673a\u5236\u9632\u6b62\u707e\u96be\u6027\u9057\u5fd8\u3002", "result": "SemiNFT\u5728\u6807\u51c6\u9884\u8bbe\u8fc1\u79fb\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u5728\u9ed1\u767d\u7167\u7247\u7740\u8272\u548c\u8de8\u57df\uff08\u52a8\u6f2b\u5230\u7167\u7247\uff09\u9884\u8bbe\u8fc1\u79fb\u7b49\u96f6\u6837\u672c\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u8bc1\u5b9e\u5176\u8d85\u8d8a\u4e86\u7b80\u5355\u7684\u7edf\u8ba1\u5339\u914d\u3002", "conclusion": "SemiNFT\u901a\u8fc7\u6a21\u4eff\u4eba\u7c7b\u827a\u672f\u5b66\u4e60\u8f68\u8ff9\uff0c\u5b9e\u73b0\u4e86\u5bf9\u7f8e\u5b66\u7406\u89e3\u7684\u66f4\u9ad8\u5c42\u6b21\uff0c\u4e3a\u53c2\u8003\u5f0f\u8272\u5f69\u4fee\u56fe\u63d0\u4f9b\u4e86\u66f4\u667a\u80fd\u3001\u8bed\u4e49\u611f\u77e5\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.08613", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.08613", "abs": "https://arxiv.org/abs/2602.08613", "authors": ["Wei Gao", "Wenxu Gao", "Xingming Mu", "Changhao Peng", "Ge Li"], "title": "Overview and Comparison of AVS Point Cloud Compression Standard", "comment": "3 figures, 3 tables", "summary": "Point cloud is a prevalent 3D data representation format with significant application values in immersive media, autonomous driving, digital heritage protection, etc. However, the large data size of point clouds poses challenges to transmission and storage, which influences the wide deployments. Therefore, point cloud compression plays a crucial role in practical applications for both human and machine perception optimization. To this end, the Moving Picture Experts Group (MPEG) has established two standards for point cloud compression, including Geometry-based Point Cloud Compression (G-PCC) and Video-based Point Cloud Compression (V-PCC). In the meantime, the Audio Video coding Standard (AVS) Workgroup of China also have launched and completed the development for its first generation point cloud compression standard, namely AVS PCC. This new standardization effort has adopted many new coding tools and techniques, which are different from the other counterpart standards. This paper reviews the AVS PCC standard from two perspectives, i.e., the related technologies and performance comparisons.", "AI": {"tldr": "\u672c\u6587\u56de\u987e\u4e86\u4e2d\u56fdAVS\u5de5\u4f5c\u7ec4\u5f00\u53d1\u7684\u7b2c\u4e00\u4ee3\u70b9\u4e91\u538b\u7f29\u6807\u51c6AVS PCC\uff0c\u4ece\u6280\u672f\u7279\u70b9\u548c\u6027\u80fd\u6bd4\u8f83\u4e24\u4e2a\u89d2\u5ea6\u8fdb\u884c\u5206\u6790\uff0c\u5c55\u793a\u4e86\u8be5\u6807\u51c6\u4e0eMPEG\u7684G-PCC\u548cV-PCC\u6807\u51c6\u7684\u4e0d\u540c\u4e4b\u5904\u3002", "motivation": "\u70b9\u4e91\u4f5c\u4e3a\u91cd\u8981\u76843D\u6570\u636e\u8868\u793a\u683c\u5f0f\uff0c\u5728\u6c89\u6d78\u5f0f\u5a92\u4f53\u3001\u81ea\u52a8\u9a7e\u9a76\u3001\u6570\u5b57\u9057\u4ea7\u4fdd\u62a4\u7b49\u9886\u57df\u6709\u5e7f\u6cdb\u5e94\u7528\u4ef7\u503c\uff0c\u4f46\u5176\u5927\u6570\u636e\u91cf\u7ed9\u4f20\u8f93\u548c\u5b58\u50a8\u5e26\u6765\u6311\u6218\u3002\u4e3a\u4f18\u5316\u4eba\u7c7b\u548c\u673a\u5668\u611f\u77e5\uff0c\u70b9\u4e91\u538b\u7f29\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u56e0\u6b64\u9700\u8981\u5236\u5b9a\u76f8\u5173\u6807\u51c6\u3002", "method": "\u672c\u6587\u91c7\u7528\u7efc\u8ff0\u5206\u6790\u65b9\u6cd5\uff0c\u4ece\u4e24\u4e2a\u89d2\u5ea6\u56de\u987eAVS PCC\u6807\u51c6\uff1a1\uff09\u76f8\u5173\u6280\u672f\u7279\u70b9\uff0c\u5206\u6790AVS PCC\u91c7\u7528\u7684\u65b0\u7f16\u7801\u5de5\u5177\u548c\u6280\u672f\uff1b2\uff09\u6027\u80fd\u6bd4\u8f83\uff0c\u5c06AVS PCC\u4e0eMPEG\u7684G-PCC\u548cV-PCC\u6807\u51c6\u8fdb\u884c\u5bf9\u6bd4\u3002", "result": "AVS PCC\u4f5c\u4e3a\u4e2d\u56fd\u81ea\u4e3b\u5f00\u53d1\u7684\u7b2c\u4e00\u4ee3\u70b9\u4e91\u538b\u7f29\u6807\u51c6\uff0c\u91c7\u7528\u4e86\u4e0eMPEG\u6807\u51c6\u4e0d\u540c\u7684\u65b0\u7f16\u7801\u5de5\u5177\u548c\u6280\u672f\uff0c\u5f62\u6210\u4e86\u5177\u6709\u81ea\u8eab\u7279\u8272\u7684\u6807\u51c6\u5316\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "AVS PCC\u6807\u51c6\u7684\u5f00\u53d1\u5b8c\u6210\u586b\u8865\u4e86\u4e2d\u56fd\u5728\u70b9\u4e91\u538b\u7f29\u6807\u51c6\u9886\u57df\u7684\u7a7a\u767d\uff0c\u4e3a\u70b9\u4e91\u6570\u636e\u7684\u4f20\u8f93\u548c\u5b58\u50a8\u63d0\u4f9b\u4e86\u65b0\u7684\u6807\u51c6\u5316\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u70b9\u4e91\u6280\u672f\u5728\u5404\u9886\u57df\u7684\u5e7f\u6cdb\u5e94\u7528\u3002"}}
{"id": "2602.08615", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.08615", "abs": "https://arxiv.org/abs/2602.08615", "authors": ["Kfir Goldberg", "Elad Richardson", "Yael Vinker"], "title": "Inspiration Seeds: Learning Non-Literal Visual Combinations for Generative Exploration", "comment": "Project page available at https://inspirationseedspaper.github.io/InspirationSeeds/", "summary": "While generative models have become powerful tools for image synthesis, they are typically optimized for executing carefully crafted textual prompts, offering limited support for the open-ended visual exploration that often precedes idea formation. In contrast, designers frequently draw inspiration from loosely connected visual references, seeking emergent connections that spark new ideas. We propose Inspiration Seeds, a generative framework that shifts image generation from final execution to exploratory ideation. Given two input images, our model produces diverse, visually coherent compositions that reveal latent relationships between inputs, without relying on user-specified text prompts. Our approach is feed-forward, trained on synthetic triplets of decomposed visual aspects derived entirely through visual means: we use CLIP Sparse Autoencoders to extract editing directions in CLIP latent space and isolate concept pairs. By removing the reliance on language and enabling fast, intuitive recombination, our method supports visual ideation at the early and ambiguous stages of creative work.", "AI": {"tldr": "\u63d0\u51faInspiration Seeds\u6846\u67b6\uff0c\u5c06\u56fe\u50cf\u751f\u6210\u4ece\u6700\u7ec8\u6267\u884c\u8f6c\u5411\u63a2\u7d22\u6027\u6784\u601d\uff0c\u901a\u8fc7\u89c6\u89c9\u8f93\u5165\u800c\u975e\u6587\u672c\u63d0\u793a\u751f\u6210\u591a\u6837\u7ec4\u5408\uff0c\u63ed\u793a\u8f93\u5165\u56fe\u50cf\u95f4\u7684\u6f5c\u5728\u5173\u7cfb", "motivation": "\u5f53\u524d\u751f\u6210\u6a21\u578b\u4e3b\u8981\u9488\u5bf9\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u6587\u672c\u63d0\u793a\u8fdb\u884c\u4f18\u5316\uff0c\u7f3a\u4e4f\u5bf9\u5f00\u653e\u5f0f\u89c6\u89c9\u63a2\u7d22\u7684\u652f\u6301\uff0c\u800c\u8bbe\u8ba1\u5e08\u7ecf\u5e38\u4ece\u677e\u6563\u8fde\u63a5\u7684\u89c6\u89c9\u53c2\u8003\u4e2d\u5bfb\u627e\u7075\u611f\uff0c\u5bfb\u6c42\u6fc0\u53d1\u65b0\u60f3\u6cd5\u7684\u6d8c\u73b0\u8fde\u63a5", "method": "\u4f7f\u7528CLIP\u7a00\u758f\u81ea\u7f16\u7801\u5668\u63d0\u53d6CLIP\u6f5c\u5728\u7a7a\u95f4\u4e2d\u7684\u7f16\u8f91\u65b9\u5411\uff0c\u901a\u8fc7\u7eaf\u89c6\u89c9\u624b\u6bb5\u5206\u89e3\u89c6\u89c9\u65b9\u9762\uff0c\u751f\u6210\u5408\u6210\u4e09\u5143\u7ec4\u8fdb\u884c\u8bad\u7ec3\uff0c\u5b9e\u73b0\u65e0\u9700\u7528\u6237\u6307\u5b9a\u6587\u672c\u63d0\u793a\u7684\u524d\u9988\u751f\u6210", "result": "\u6a21\u578b\u80fd\u591f\u4ece\u4e24\u4e2a\u8f93\u5165\u56fe\u50cf\u751f\u6210\u591a\u6837\u4e14\u89c6\u89c9\u8fde\u8d2f\u7684\u7ec4\u5408\uff0c\u63ed\u793a\u8f93\u5165\u4e4b\u95f4\u7684\u6f5c\u5728\u5173\u7cfb\uff0c\u652f\u6301\u5feb\u901f\u76f4\u89c2\u7684\u89c6\u89c9\u91cd\u7ec4", "conclusion": "\u8be5\u65b9\u6cd5\u6446\u8131\u4e86\u5bf9\u8bed\u8a00\u7684\u4f9d\u8d56\uff0c\u5b9e\u73b0\u4e86\u5feb\u901f\u76f4\u89c2\u7684\u89c6\u89c9\u91cd\u7ec4\uff0c\u652f\u6301\u521b\u610f\u5de5\u4f5c\u65e9\u671f\u548c\u6a21\u7cca\u9636\u6bb5\u7684\u89c6\u89c9\u6784\u601d"}}
{"id": "2602.08620", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.08620", "abs": "https://arxiv.org/abs/2602.08620", "authors": ["Siyu Liu", "Chujie Qin", "Hubery Yin", "Qixin Yan", "Zheng-Peng Duan", "Chen Li", "Jing Lyu", "Chun-Le Guo", "Chongyi Li"], "title": "Improving Reconstruction of Representation Autoencoder", "comment": null, "summary": "Recent work leverages Vision Foundation Models as image encoders to boost the generative performance of latent diffusion models (LDMs), as their semantic feature distributions are easy to learn. However, such semantic features often lack low-level information (\\eg, color and texture), leading to degraded reconstruction fidelity, which has emerged as a primary bottleneck in further scaling LDMs. To address this limitation, we propose LV-RAE, a representation autoencoder that augments semantic features with missing low-level information, enabling high-fidelity reconstruction while remaining highly aligned with the semantic distribution. We further observe that the resulting high-dimensional, information-rich latent make decoders sensitive to latent perturbations, causing severe artifacts when decoding generated latent and consequently degrading generation quality. Our analysis suggests that this sensitivity primarily stems from excessive decoder responses along directions off the data manifold. Building on these insights, we propose fine-tuning the decoder to increase its robustness and smoothing the generated latent via controlled noise injection, thereby enhancing generation quality. Experiments demonstrate that LV-RAE significantly improves reconstruction fidelity while preserving the semantic abstraction and achieving strong generative quality. Our code is available at https://github.com/modyu-liu/LVRAE.", "AI": {"tldr": "LV-RAE\u662f\u4e00\u79cd\u8868\u793a\u81ea\u7f16\u7801\u5668\uff0c\u901a\u8fc7\u589e\u5f3a\u8bed\u4e49\u7279\u5f81\u7684\u4f4e\u7ea7\u4fe1\u606f\u6765\u63d0\u5347\u6f5c\u5728\u6269\u6563\u6a21\u578b\u7684\u91cd\u5efa\u4fdd\u771f\u5ea6\uff0c\u540c\u65f6\u901a\u8fc7\u589e\u5f3a\u89e3\u7801\u5668\u9c81\u68d2\u6027\u548c\u5e73\u6ed1\u6f5c\u5728\u7a7a\u95f4\u6765\u6539\u5584\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f7f\u7528\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u4f5c\u4e3a\u56fe\u50cf\u7f16\u7801\u5668\u6765\u63d0\u5347\u6f5c\u5728\u6269\u6563\u6a21\u578b\u7684\u751f\u6210\u6027\u80fd\uff0c\u4f46\u8fd9\u4e9b\u8bed\u4e49\u7279\u5f81\u7f3a\u4e4f\u4f4e\u7ea7\u4fe1\u606f\uff08\u5982\u989c\u8272\u548c\u7eb9\u7406\uff09\uff0c\u5bfc\u81f4\u91cd\u5efa\u4fdd\u771f\u5ea6\u4e0b\u964d\uff0c\u6210\u4e3a\u8fdb\u4e00\u6b65\u6269\u5c55LDMs\u7684\u4e3b\u8981\u74f6\u9888\u3002", "method": "\u63d0\u51faLV-RAE\u8868\u793a\u81ea\u7f16\u7801\u5668\uff0c\u901a\u8fc7\u589e\u5f3a\u8bed\u4e49\u7279\u5f81\u7684\u4f4e\u7ea7\u4fe1\u606f\u6765\u5b9e\u73b0\u9ad8\u4fdd\u771f\u91cd\u5efa\uff1b\u540c\u65f6\u901a\u8fc7\u5fae\u8c03\u89e3\u7801\u5668\u589e\u5f3a\u5176\u9c81\u68d2\u6027\uff0c\u5e76\u901a\u8fc7\u53d7\u63a7\u566a\u58f0\u6ce8\u5165\u5e73\u6ed1\u751f\u6210\u7684\u6f5c\u5728\u8868\u793a\u3002", "result": "\u5b9e\u9a8c\u8868\u660eLV-RAE\u663e\u8457\u63d0\u9ad8\u4e86\u91cd\u5efa\u4fdd\u771f\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u8bed\u4e49\u62bd\u8c61\u80fd\u529b\uff0c\u5e76\u5b9e\u73b0\u4e86\u5f3a\u5927\u7684\u751f\u6210\u8d28\u91cf\u3002", "conclusion": "LV-RAE\u901a\u8fc7\u589e\u5f3a\u8bed\u4e49\u7279\u5f81\u7684\u4f4e\u7ea7\u4fe1\u606f\u548c\u6539\u5584\u89e3\u7801\u5668\u9c81\u68d2\u6027\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u6f5c\u5728\u6269\u6563\u6a21\u578b\u4e2d\u91cd\u5efa\u4fdd\u771f\u5ea6\u4e0e\u751f\u6210\u8d28\u91cf\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\u3002"}}
{"id": "2602.08626", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.08626", "abs": "https://arxiv.org/abs/2602.08626", "authors": ["Alexis Marouani", "Oriane Sim\u00e9oni", "Herv\u00e9 J\u00e9gou", "Piotr Bojanowski", "Huy V. Vo"], "title": "Revisiting [CLS] and Patch Token Interaction in Vision Transformers", "comment": "To be published as a conference paper at ICLR 2026", "summary": "Vision Transformers have emerged as powerful, scalable and versatile representation learners. To capture both global and local features, a learnable [CLS] class token is typically prepended to the input sequence of patch tokens. Despite their distinct nature, both token types are processed identically throughout the model. In this work, we investigate the friction between global and local feature learning under different pre-training strategies by analyzing the interactions between class and patch tokens. Our analysis reveals that standard normalization layers introduce an implicit differentiation between these token types. Building on this insight, we propose specialized processing paths that selectively disentangle the computational flow of class and patch tokens, particularly within normalization layers and early query-key-value projections. This targeted specialization leads to significantly improved patch representation quality for dense prediction tasks. Our experiments demonstrate segmentation performance gains of over 2 mIoU points on standard benchmarks, while maintaining strong classification accuracy. The proposed modifications introduce only an 8% increase in parameters, with no additional computational overhead. Through comprehensive ablations, we provide insights into which architectural components benefit most from specialization and how our approach generalizes across model scales and learning frameworks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u901a\u8fc7\u4e13\u95e8\u5904\u7406\u8def\u5f84\u5206\u79bb\u7c7b\u522btoken\u548c\u8865\u4e01token\u7684\u8ba1\u7b97\u6d41\uff0c\u7279\u522b\u662f\u6807\u51c6\u5316\u5c42\u548c\u65e9\u671fQKV\u6295\u5f71\uff0c\u663e\u8457\u63d0\u5347\u5bc6\u96c6\u9884\u6d4b\u4efb\u52a1\u7684\u8865\u4e01\u8868\u793a\u8d28\u91cf\uff0c\u5728\u5206\u5272\u4efb\u52a1\u4e0a\u83b7\u5f97\u8d85\u8fc72 mIoU\u7684\u6027\u80fd\u63d0\u5347\uff0c\u540c\u65f6\u4fdd\u6301\u5f3a\u5206\u7c7b\u7cbe\u5ea6\u3002", "motivation": "Vision Transformers\u4e2d\uff0c\u53ef\u5b66\u4e60\u7684[CLS]\u7c7b\u522btoken\u548c\u8865\u4e01token\u867d\u7136\u6027\u8d28\u4e0d\u540c\uff0c\u4f46\u5728\u6574\u4e2a\u6a21\u578b\u4e2d\u91c7\u7528\u76f8\u540c\u7684\u5904\u7406\u65b9\u5f0f\u3002\u4f5c\u8005\u7814\u7a76\u4e86\u4e0d\u540c\u9884\u8bad\u7ec3\u7b56\u7565\u4e0b\u5168\u5c40\u548c\u5c40\u90e8\u7279\u5f81\u5b66\u4e60\u4e4b\u95f4\u7684\u51b2\u7a81\uff0c\u53d1\u73b0\u6807\u51c6\u6807\u51c6\u5316\u5c42\u5728\u8fd9\u4e24\u79cdtoken\u7c7b\u578b\u4e4b\u95f4\u5f15\u5165\u4e86\u9690\u5f0f\u533a\u5206\u3002", "method": "\u57fa\u4e8e\u5206\u6790\u53d1\u73b0\uff0c\u63d0\u51fa\u4e13\u95e8\u7684\u5904\u7406\u8def\u5f84\uff0c\u9009\u62e9\u6027\u5730\u89e3\u8026\u7c7b\u522btoken\u548c\u8865\u4e01token\u7684\u8ba1\u7b97\u6d41\uff0c\u7279\u522b\u662f\u5728\u6807\u51c6\u5316\u5c42\u548c\u65e9\u671f\u67e5\u8be2-\u952e-\u503c\u6295\u5f71\u4e2d\u3002\u8fd9\u79cd\u9488\u5bf9\u6027\u4e13\u95e8\u5316\u663e\u8457\u63d0\u5347\u4e86\u5bc6\u96c6\u9884\u6d4b\u4efb\u52a1\u7684\u8865\u4e01\u8868\u793a\u8d28\u91cf\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5206\u5272\u6027\u80fd\u63d0\u5347\u8d85\u8fc72 mIoU\u70b9\uff0c\u540c\u65f6\u4fdd\u6301\u5f3a\u5206\u7c7b\u7cbe\u5ea6\u3002\u63d0\u51fa\u7684\u4fee\u6539\u4ec5\u589e\u52a08%\u7684\u53c2\u6570\uff0c\u6ca1\u6709\u989d\u5916\u7684\u8ba1\u7b97\u5f00\u9500\u3002\u901a\u8fc7\u5168\u9762\u6d88\u878d\u7814\u7a76\uff0c\u63d0\u4f9b\u4e86\u54ea\u4e9b\u67b6\u6784\u7ec4\u4ef6\u4ece\u4e13\u95e8\u5316\u4e2d\u83b7\u76ca\u6700\u591a\u4ee5\u53ca\u8be5\u65b9\u6cd5\u5982\u4f55\u8de8\u6a21\u578b\u89c4\u6a21\u548c\u5b66\u6846\u67b6\u6cdb\u5316\u7684\u89c1\u89e3\u3002", "conclusion": "\u901a\u8fc7\u4e13\u95e8\u5904\u7406\u8def\u5f84\u5206\u79bb\u7c7b\u522btoken\u548c\u8865\u4e01token\u7684\u8ba1\u7b97\u6d41\uff0c\u7279\u522b\u662f\u5728\u6807\u51c6\u5316\u5c42\u548c\u65e9\u671fQKV\u6295\u5f71\u4e2d\uff0c\u53ef\u4ee5\u663e\u8457\u63d0\u5347Vision Transformers\u5728\u5bc6\u96c6\u9884\u6d4b\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u5206\u7c7b\u80fd\u529b\uff0c\u4e14\u8ba1\u7b97\u5f00\u9500\u5f88\u5c0f\u3002"}}
{"id": "2602.08652", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.08652", "abs": "https://arxiv.org/abs/2602.08652", "authors": ["Oskar Thaeter", "Tanja Niedermair", "Johannes Raffler", "Ralf Huss", "Peter J. Sch\u00fcffler"], "title": "Deep Learning-Based Fixation Type Prediction for Quality Assurance in Digital Pathology", "comment": "17 pages, 8 figures, 7 tables", "summary": "Accurate annotation of fixation type is a critical step in slide preparation for pathology laboratories. However, this manual process is prone to\n  errors, impacting downstream analyses and diagnostic accuracy. Existing methods for verifying formalin-fixed, paraffin-embedded (FFPE), and frozen\n  section (FS) fixation types typically require full-resolution whole-slide images (WSIs), limiting scalability for high-throughput quality control.\n  We propose a deep-learning model to predict fixation types using low-resolution, pre-scan thumbnail images. The model was trained on WSIs from\n  the TUM Institute of Pathology (n=1,200, Leica GT450DX) and evaluated on a class-balanced subset of The Cancer Genome Atlas dataset (TCGA, n=8,800,\n  Leica AT2), as well as on class-balanced datasets from Augsburg (n=695 [392 FFPE, 303 FS], Philips UFS) and Regensburg (n=202, 3DHISTECH P1000).\n  Our model achieves an AUROC of 0.88 on TCGA, outperforming comparable pre-scan methods by 4.8%. It also achieves AUROCs of 0.72 on Regensburg and\n  Augsburg slides, underscoring challenges related to scanner-induced domain shifts. Furthermore, the model processes each slide in 21 ms, $400\\times$\n  faster than existing high-magnification, full-resolution methods, enabling rapid, high-throughput processing.\n  This approach provides an efficient solution for detecting labelling errors without relying on high-magnification scans, offering a valuable tool for\n  quality control in high-throughput pathology workflows. Future work will improve and evaluate the model's generalisation to additional scanner\n  types. Our findings suggest that this method can increase accuracy and efficiency in digital pathology workflows and may be extended to other\n  low-resolution slide annotations.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u4f7f\u7528\u4f4e\u5206\u8fa8\u7387\u9884\u626b\u63cf\u7f29\u7565\u56fe\u9884\u6d4b\u75c5\u7406\u5207\u7247\u56fa\u5b9a\u7c7b\u578b\uff0c\u76f8\u6bd4\u4f20\u7edf\u9ad8\u5206\u8fa8\u7387\u65b9\u6cd5\u5904\u7406\u901f\u5ea6\u5feb400\u500d\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u6709\u6548", "motivation": "\u75c5\u7406\u5207\u7247\u56fa\u5b9a\u7c7b\u578b\uff08FFPE\u548cFS\uff09\u7684\u624b\u52a8\u6807\u6ce8\u5bb9\u6613\u51fa\u9519\uff0c\u5f71\u54cd\u4e0b\u6e38\u5206\u6790\u548c\u8bca\u65ad\u51c6\u786e\u6027\u3002\u73b0\u6709\u9a8c\u8bc1\u65b9\u6cd5\u9700\u8981\u5168\u5206\u8fa8\u7387\u5168\u73bb\u7247\u56fe\u50cf\uff0c\u9650\u5236\u4e86\u9ad8\u901a\u91cf\u8d28\u91cf\u63a7\u5236\u7684\u53ef\u6269\u5c55\u6027\u3002", "method": "\u5f00\u53d1\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u5229\u7528\u4f4e\u5206\u8fa8\u7387\u9884\u626b\u63cf\u7f29\u7565\u56fe\u50cf\u9884\u6d4b\u56fa\u5b9a\u7c7b\u578b\u3002\u6a21\u578b\u5728TUM\u75c5\u7406\u7814\u7a76\u6240\u76841200\u5f20WSI\u4e0a\u8bad\u7ec3\uff0c\u5728TCGA\u6570\u636e\u96c6\uff088800\u5f20\uff09\u4ee5\u53caAugsburg\uff08695\u5f20\uff09\u548cRegensburg\uff08202\u5f20\uff09\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u3002", "result": "\u6a21\u578b\u5728TCGA\u6570\u636e\u96c6\u4e0aAUROC\u8fbe\u52300.88\uff0c\u6bd4\u540c\u7c7b\u9884\u626b\u63cf\u65b9\u6cd5\u63d0\u53474.8%\u3002\u5728Regensburg\u548cAugsburg\u6570\u636e\u96c6\u4e0aAUROC\u4e3a0.72\uff0c\u663e\u793a\u626b\u63cf\u4eea\u5f15\u8d77\u7684\u57df\u504f\u79fb\u6311\u6218\u3002\u6bcf\u5f20\u5207\u7247\u5904\u7406\u65f6\u95f4\u4ec521\u6beb\u79d2\uff0c\u6bd4\u73b0\u6709\u9ad8\u500d\u7387\u5168\u5206\u8fa8\u7387\u65b9\u6cd5\u5feb400\u500d\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u9ad8\u901a\u91cf\u75c5\u7406\u5de5\u4f5c\u6d41\u7a0b\u63d0\u4f9b\u4e86\u65e0\u9700\u9ad8\u500d\u7387\u626b\u63cf\u7684\u6807\u7b7e\u9519\u8bef\u68c0\u6d4b\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002\u672a\u6765\u5de5\u4f5c\u5c06\u6539\u8fdb\u6a21\u578b\u5bf9\u4e0d\u540c\u626b\u63cf\u4eea\u7c7b\u578b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u8be5\u65b9\u6cd5\u53ef\u63d0\u9ad8\u6570\u5b57\u75c5\u7406\u5de5\u4f5c\u6d41\u7a0b\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\uff0c\u5e76\u53ef\u6269\u5c55\u5230\u5176\u4ed6\u4f4e\u5206\u8fa8\u7387\u5207\u7247\u6807\u6ce8\u4efb\u52a1\u3002"}}
{"id": "2602.08661", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.08661", "abs": "https://arxiv.org/abs/2602.08661", "authors": ["Yi Dao", "Lankai Zhang", "Hao Liu", "Haiwei Zhang", "Wenbo Wang"], "title": "WiFlow: A Lightweight WiFi-based Continuous Human Pose Estimation Network with Spatio-Temporal Feature Decoupling", "comment": null, "summary": "Human pose estimation is fundamental to intelligent perception in the Internet of Things (IoT), enabling applications ranging from smart healthcare to human-computer interaction. While WiFi-based methods have gained traction, they often struggle with continuous motion and high computational overhead. This work presents WiFlow, a novel framework for continuous human pose estimation using WiFi signals. Unlike vision-based approaches such as two-dimensional deep residual networks that treat Channel State Information (CSI) as images, WiFlow employs an encoder-decoder architecture. The encoder captures spatio-temporal features of CSI using temporal and asymmetric convolutions, preserving the original sequential structure of signals. It then refines keypoint features of human bodies to be tracked and capture their structural dependencies via axial attention. The decoder subsequently maps the encoded high-dimensional features into keypoint coordinates. Trained on a self-collected dataset of 360,000 synchronized CSI-pose samples from 5 subjects performing continuous sequences of 8 daily activities, WiFlow achieves a Percentage of Correct Keypoints (PCK) of 97.00% at a threshold of 20% (PCK@20) and 99.48% at PCK@50, with a mean per-joint position error of 0.008m. With only 4.82M parameters, WiFlow significantly reduces model complexity and computational cost, establishing a new performance baseline for practical WiFi-based human pose estimation. Our code and datasets are available at https://github.com/DY2434/WiFlow-WiFi-Pose-Estimation-with-Spatio-Temporal-Decoupling.git.", "AI": {"tldr": "WiFlow\u662f\u4e00\u4e2a\u57fa\u4e8eWiFi\u4fe1\u53f7\u7684\u8fde\u7eed\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1\u6846\u67b6\uff0c\u91c7\u7528\u7f16\u7801\u5668-\u89e3\u7801\u5668\u67b6\u6784\uff0c\u901a\u8fc7\u65f6\u7a7a\u7279\u5f81\u63d0\u53d6\u548c\u8f74\u5411\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5728\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u59ff\u6001\u4f30\u8ba1\u3002", "motivation": "\u73b0\u6709WiFi\u59ff\u6001\u4f30\u8ba1\u65b9\u6cd5\u5728\u5904\u7406\u8fde\u7eed\u8fd0\u52a8\u65f6\u5b58\u5728\u56f0\u96be\u4e14\u8ba1\u7b97\u5f00\u9500\u5927\uff0c\u800c\u89c6\u89c9\u65b9\u6cd5\u5c06CSI\u89c6\u4e3a\u56fe\u50cf\u5904\u7406\u53ef\u80fd\u4e22\u5931\u4fe1\u53f7\u539f\u59cb\u65f6\u5e8f\u7ed3\u6784\u3002\u9700\u8981\u4e00\u79cd\u80fd\u5904\u7406\u8fde\u7eed\u8fd0\u52a8\u3001\u4fdd\u7559\u4fe1\u53f7\u65f6\u5e8f\u7279\u6027\u4e14\u8ba1\u7b97\u9ad8\u6548\u7684WiFi\u59ff\u6001\u4f30\u8ba1\u65b9\u6cd5\u3002", "method": "\u63d0\u51faWiFlow\u6846\u67b6\uff0c\u91c7\u7528\u7f16\u7801\u5668-\u89e3\u7801\u5668\u67b6\u6784\uff1a\u7f16\u7801\u5668\u4f7f\u7528\u65f6\u5e8f\u548c\u975e\u5bf9\u79f0\u5377\u79ef\u6355\u83b7CSI\u7684\u65f6\u7a7a\u7279\u5f81\uff0c\u4fdd\u7559\u4fe1\u53f7\u539f\u59cb\u5e8f\u5217\u7ed3\u6784\uff1b\u901a\u8fc7\u8f74\u5411\u6ce8\u610f\u529b\u673a\u5236\u7cbe\u70bc\u4eba\u4f53\u5173\u952e\u70b9\u7279\u5f81\u5e76\u6355\u83b7\u7ed3\u6784\u4f9d\u8d56\u5173\u7cfb\uff1b\u89e3\u7801\u5668\u5c06\u7f16\u7801\u7684\u9ad8\u7ef4\u7279\u5f81\u6620\u5c04\u4e3a\u5173\u952e\u70b9\u5750\u6807\u3002", "result": "\u5728\u81ea\u6536\u96c6\u768436\u4e07\u540c\u6b65CSI-\u59ff\u6001\u6837\u672c\u6570\u636e\u96c6\u4e0a\uff0cWiFlow\u5728PCK@20\u8fbe\u523097.00%\uff0cPCK@50\u8fbe\u523099.48%\uff0c\u5e73\u5747\u5173\u8282\u4f4d\u7f6e\u8bef\u5dee\u4e3a0.008\u7c73\u3002\u6a21\u578b\u4ec5\u9700482\u4e07\u53c2\u6570\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u590d\u6742\u5ea6\u548c\u8ba1\u7b97\u6210\u672c\u3002", "conclusion": "WiFlow\u4e3a\u5b9e\u7528\u7684WiFi\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1\u5efa\u7acb\u4e86\u65b0\u7684\u6027\u80fd\u57fa\u51c6\uff0c\u5728\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u7684\u540c\u65f6\u5927\u5e45\u964d\u4f4e\u4e86\u6a21\u578b\u590d\u6742\u5ea6\u548c\u8ba1\u7b97\u5f00\u9500\uff0c\u4e3a\u7269\u8054\u7f51\u667a\u80fd\u611f\u77e5\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.08682", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.08682", "abs": "https://arxiv.org/abs/2602.08682", "authors": ["Ying Guo", "Qijun Gan", "Yifu Zhang", "Jinlai Liu", "Yifei Hu", "Pan Xie", "Dongjun Qian", "Yu Zhang", "Ruiqi Li", "Yuqi Zhang", "Ruibiao Lu", "Xiaofeng Mei", "Bo Han", "Xiang Yin", "Bingyue Peng", "Zehuan Yuan"], "title": "ALIVE: Animate Your World with Lifelike Audio-Video Generation", "comment": null, "summary": "Video generation is rapidly evolving towards unified audio-video generation. In this paper, we present ALIVE, a generation model that adapts a pretrained Text-to-Video (T2V) model to Sora-style audio-video generation and animation. In particular, the model unlocks the Text-to-Video&Audio (T2VA) and Reference-to-Video&Audio (animation) capabilities compared to the T2V foundation models. To support the audio-visual synchronization and reference animation, we augment the popular MMDiT architecture with a joint audio-video branch which includes TA-CrossAttn for temporally-aligned cross-modal fusion and UniTemp-RoPE for precise audio-visual alignment. Meanwhile, a comprehensive data pipeline consisting of audio-video captioning, quality control, etc., is carefully designed to collect high-quality finetuning data. Additionally, we introduce a new benchmark to perform a comprehensive model test and comparison. After continue pretraining and finetuning on million-level high-quality data, ALIVE demonstrates outstanding performance, consistently outperforming open-source models and matching or surpassing state-of-the-art commercial solutions. With detailed recipes and benchmarks, we hope ALIVE helps the community develop audio-video generation models more efficiently. Official page: https://github.com/FoundationVision/Alive.", "AI": {"tldr": "ALIVE\u662f\u4e00\u4e2a\u97f3\u9891-\u89c6\u9891\u751f\u6210\u6a21\u578b\uff0c\u901a\u8fc7\u6539\u9020\u9884\u8bad\u7ec3\u7684\u6587\u672c\u5230\u89c6\u9891\u6a21\u578b\uff0c\u5b9e\u73b0\u4e86\u7c7b\u4f3cSora\u7684\u97f3\u9891\u89c6\u9891\u751f\u6210\u548c\u52a8\u753b\u529f\u80fd\uff0c\u652f\u6301\u6587\u672c\u5230\u97f3\u89c6\u9891\u548c\u53c2\u8003\u52a8\u753b\u751f\u6210\u3002", "motivation": "\u89c6\u9891\u751f\u6210\u6b63\u5728\u5feb\u901f\u5411\u7edf\u4e00\u7684\u97f3\u9891-\u89c6\u9891\u751f\u6210\u53d1\u5c55\uff0c\u73b0\u6709\u6587\u672c\u5230\u89c6\u9891\u57fa\u7840\u6a21\u578b\u7f3a\u4e4f\u97f3\u9891\u751f\u6210\u548c\u52a8\u753b\u80fd\u529b\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u540c\u65f6\u5904\u7406\u97f3\u9891\u548c\u89c6\u9891\u7684\u751f\u6210\u6a21\u578b\u3002", "method": "1. \u57fa\u4e8eMMDiT\u67b6\u6784\u6269\u5c55\u8054\u5408\u97f3\u9891-\u89c6\u9891\u5206\u652f\uff0c\u5305\u542bTA-CrossAttn\u7528\u4e8e\u65f6\u95f4\u5bf9\u9f50\u7684\u8de8\u6a21\u6001\u878d\u5408\u548cUniTemp-RoPE\u7528\u4e8e\u7cbe\u786e\u7684\u97f3\u9891-\u89c6\u89c9\u5bf9\u9f50\uff1b2. \u8bbe\u8ba1\u5168\u9762\u7684\u6570\u636e\u7ba1\u9053\uff0c\u5305\u62ec\u97f3\u9891-\u89c6\u9891\u5b57\u5e55\u3001\u8d28\u91cf\u63a7\u5236\u7b49\uff0c\u6536\u96c6\u9ad8\u8d28\u91cf\u5fae\u8c03\u6570\u636e\uff1b3. \u5728\u767e\u4e07\u7ea7\u522b\u9ad8\u8d28\u91cf\u6570\u636e\u4e0a\u8fdb\u884c\u6301\u7eed\u9884\u8bad\u7ec3\u548c\u5fae\u8c03\u3002", "result": "ALIVE\u8868\u73b0\u51fa\u8272\uff0c\u5728\u6027\u80fd\u4e0a\u4e00\u81f4\u4f18\u4e8e\u5f00\u6e90\u6a21\u578b\uff0c\u5339\u914d\u6216\u8d85\u8d8a\u4e86\u6700\u5148\u8fdb\u7684\u5546\u4e1a\u89e3\u51b3\u65b9\u6848\u3002\u6a21\u578b\u89e3\u9501\u4e86\u6587\u672c\u5230\u97f3\u89c6\u9891\u548c\u53c2\u8003\u52a8\u753b\u80fd\u529b\u3002", "conclusion": "ALIVE\u901a\u8fc7\u8be6\u7ec6\u7684\u6280\u672f\u65b9\u6848\u548c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5e2e\u52a9\u793e\u533a\u66f4\u9ad8\u6548\u5730\u5f00\u53d1\u97f3\u9891-\u89c6\u9891\u751f\u6210\u6a21\u578b\uff0c\u63a8\u52a8\u4e86\u7edf\u4e00\u7684\u97f3\u9891-\u89c6\u9891\u751f\u6210\u6280\u672f\u7684\u53d1\u5c55\u3002"}}
{"id": "2602.08683", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.08683", "abs": "https://arxiv.org/abs/2602.08683", "authors": ["Feilong Tang", "Xiang An", "Yunyao Yan", "Yin Xie", "Bin Qin", "Kaicheng Yang", "Yifei Shen", "Yuanhan Zhang", "Chunyuan Li", "Shikun Feng", "Changrui Chen", "Huajie Tan", "Ming Hu", "Manyuan Zhang", "Bo Li", "Ziyong Feng", "Ziwei Liu", "Zongyuan Ge", "Jiankang Deng"], "title": "OneVision-Encoder: Codec-Aligned Sparsity as a Foundational Principle for Multimodal Intelligence", "comment": null, "summary": "Hypothesis. Artificial general intelligence is, at its core, a compression problem. Effective compression demands resonance: deep learning scales best when its architecture aligns with the fundamental structure of the data. These are the fundamental principles. Yet, modern vision architectures have strayed from these truths: visual signals are highly redundant, while discriminative information, the surprise, is sparse. Current models process dense pixel grids uniformly, wasting vast compute on static background rather than focusing on the predictive residuals that define motion and meaning. We argue that to solve visual understanding, we must align our architectures with the information-theoretic principles of video, i.e., Codecs.\n  Method. OneVision-Encoder encodes video by compressing predictive visual structure into semantic meaning. By adopting Codec Patchification, OV-Encoder abandons uniform computation to focus exclusively on the 3.1%-25% of regions rich in signal entropy. To unify spatial and temporal reasoning under irregular token layouts, OneVision-Encoder employs a shared 3D RoPE and is trained with a large-scale cluster discrimination objective over more than one million semantic concepts, jointly capturing object permanence and motion dynamics.\n  Evidence. The results validate our core hypothesis: efficiency and accuracy are not a trade-off; they are positively correlated. When integrated into LLM, it consistently outperforms strong vision backbones such as Qwen3-ViT and SigLIP2 across 16 image, video, and document understanding benchmarks, despite using substantially fewer visual tokens and pretraining data. Notably, on video understanding tasks, OV-Encoder achieves an average improvement of 4.1% over Qwen3-ViT. Codec-aligned, patch-level sparsity is a foundational principle, enabling OV-Encoder as a scalable engine for next-generation visual generalists.", "AI": {"tldr": "OneVision-Encoder\u63d0\u51fa\u5c06\u89c6\u9891\u538b\u7f29\u4f5c\u4e3aAGI\u7684\u6838\u5fc3\u95ee\u9898\uff0c\u901a\u8fc7Codec Patchification\u53ea\u5904\u7406\u4fe1\u53f7\u71b5\u4e30\u5bcc\u7684\u533a\u57df\uff083.1%-25%\uff09\uff0c\u5728\u51cf\u5c11\u89c6\u89c9token\u548c\u6570\u636e\u91cf\u7684\u60c5\u51b5\u4e0b\uff0c\u572816\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u73b0\u6709\u89c6\u89c9\u9aa8\u5e72\u6a21\u578b\u3002", "motivation": "\u73b0\u4ee3\u89c6\u89c9\u67b6\u6784\u504f\u79bb\u4e86\u4fe1\u606f\u8bba\u57fa\u672c\u539f\u5219\uff1a\u89c6\u89c9\u4fe1\u53f7\u9ad8\u5ea6\u5197\u4f59\uff0c\u800c\u5224\u522b\u4fe1\u606f\u7a00\u758f\u3002\u5f53\u524d\u6a21\u578b\u5747\u5300\u5904\u7406\u5bc6\u96c6\u50cf\u7d20\u7f51\u683c\uff0c\u6d6a\u8d39\u5927\u91cf\u8ba1\u7b97\u5728\u9759\u6001\u80cc\u666f\u4e0a\uff0c\u800c\u4e0d\u662f\u5173\u6ce8\u5b9a\u4e49\u8fd0\u52a8\u548c\u610f\u4e49\u7684\u9884\u6d4b\u6b8b\u5dee\u3002\u9700\u8981\u4f7f\u67b6\u6784\u4e0e\u89c6\u9891\u7684\u4fe1\u606f\u8bba\u539f\u5219\uff08\u5373\u7f16\u89e3\u7801\u5668\uff09\u5bf9\u9f50\u3002", "method": "\u91c7\u7528Codec Patchification\uff0c\u653e\u5f03\u5747\u5300\u8ba1\u7b97\uff0c\u53ea\u5173\u6ce8\u4fe1\u53f7\u71b5\u4e30\u5bcc\u7684\u533a\u57df\uff083.1%-25%\uff09\u3002\u4f7f\u7528\u5171\u4eab3D RoPE\u7edf\u4e00\u7a7a\u95f4\u548c\u65f6\u95f4\u63a8\u7406\uff0c\u901a\u8fc7\u8d85\u8fc7100\u4e07\u4e2a\u8bed\u4e49\u6982\u5ff5\u7684\u5927\u89c4\u6a21\u805a\u7c7b\u5224\u522b\u76ee\u6807\u8fdb\u884c\u8bad\u7ec3\uff0c\u8054\u5408\u6355\u6349\u5bf9\u8c61\u6301\u4e45\u6027\u548c\u8fd0\u52a8\u52a8\u6001\u3002", "result": "\u5728\u96c6\u6210\u5230LLM\u540e\uff0c\u572816\u4e2a\u56fe\u50cf\u3001\u89c6\u9891\u548c\u6587\u6863\u7406\u89e3\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4e00\u81f4\u4f18\u4e8eQwen3-ViT\u548cSigLIP2\u7b49\u5f3a\u89c6\u89c9\u9aa8\u5e72\u6a21\u578b\uff0c\u5c3d\u7ba1\u4f7f\u7528\u4e86\u663e\u8457\u66f4\u5c11\u7684\u89c6\u89c9token\u548c\u9884\u8bad\u7ec3\u6570\u636e\u3002\u5728\u89c6\u9891\u7406\u89e3\u4efb\u52a1\u4e0a\uff0c\u5e73\u5747\u6bd4Qwen3-ViT\u63d0\u53474.1%\u3002", "conclusion": "\u6548\u7387\u4e0e\u51c6\u786e\u6027\u4e0d\u662f\u6743\u8861\u5173\u7cfb\uff0c\u800c\u662f\u6b63\u76f8\u5173\u7684\u3002\u7f16\u89e3\u7801\u5668\u5bf9\u9f50\u7684\u8865\u4e01\u7ea7\u7a00\u758f\u6027\u662f\u57fa\u672c\u539f\u5219\uff0c\u4f7fOV-Encoder\u6210\u4e3a\u4e0b\u4e00\u4ee3\u89c6\u89c9\u901a\u7528\u6a21\u578b\u7684\u6269\u5c55\u5f15\u64ce\u3002"}}
{"id": "2602.08711", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.08711", "abs": "https://arxiv.org/abs/2602.08711", "authors": ["Linli Yao", "Yuancheng Wei", "Yaojie Zhang", "Lei Li", "Xinlong Chen", "Feifan Song", "Ziyue Wang", "Kun Ouyang", "Yuanxin Liu", "Lingpeng Kong", "Qi Liu", "Pengfei Wan", "Kun Gai", "Yuanxing Zhang", "Xu Sun"], "title": "TimeChat-Captioner: Scripting Multi-Scene Videos with Time-Aware and Structural Audio-Visual Captions", "comment": null, "summary": "This paper proposes Omni Dense Captioning, a novel task designed to generate continuous, fine-grained, and structured audio-visual narratives with explicit timestamps. To ensure dense semantic coverage, we introduce a six-dimensional structural schema to create \"script-like\" captions, enabling readers to vividly imagine the video content scene by scene, akin to a cinematographic screenplay. To facilitate research, we construct OmniDCBench, a high-quality, human-annotated benchmark, and propose SodaM, a unified metric that evaluates time-aware detailed descriptions while mitigating scene boundary ambiguity. Furthermore, we construct a training dataset, TimeChatCap-42K, and present TimeChat-Captioner-7B, a strong baseline trained via SFT and GRPO with task-specific rewards. Extensive experiments demonstrate that TimeChat-Captioner-7B achieves state-of-the-art performance, surpassing Gemini-2.5-Pro, while its generated dense descriptions significantly boost downstream capabilities in audio-visual reasoning (DailyOmni and WorldSense) and temporal grounding (Charades-STA). All datasets, models, and code will be made publicly available at https://github.com/yaolinli/TimeChat-Captioner.", "AI": {"tldr": "\u63d0\u51faOmni Dense Captioning\u65b0\u4efb\u52a1\uff0c\u6784\u5efa\u9ad8\u8d28\u91cf\u57fa\u51c6\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u6307\u6807\uff0c\u5f00\u53d1TimeChat-Captioner-7B\u6a21\u578b\uff0c\u5728\u591a\u4e2a\u4e0b\u6e38\u4efb\u52a1\u4e0a\u53d6\u5f97SOTA\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u63cf\u8ff0\u4efb\u52a1\u901a\u5e38\u751f\u6210\u7b80\u77ed\u3001\u79bb\u6563\u7684\u6587\u672c\uff0c\u7f3a\u4e4f\u8fde\u7eed\u3001\u7ec6\u7c92\u5ea6\u3001\u7ed3\u6784\u5316\u7684\u89c6\u542c\u53d9\u4e8b\u80fd\u529b\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u751f\u6210\u7c7b\u4f3c\u7535\u5f71\u5267\u672c\u7684\u8be6\u7ec6\u63cf\u8ff0\uff0c\u8ba9\u8bfb\u8005\u80fd\u591f\u9010\u573a\u666f\u751f\u52a8\u60f3\u8c61\u89c6\u9891\u5185\u5bb9\u7684\u65b9\u6cd5\u3002", "method": "1. \u63d0\u51fa\u516d\u7ef4\u7ed3\u6784\u6a21\u5f0f\u521b\u5efa\"\u811a\u672c\u5f0f\"\u63cf\u8ff0\uff1b2. \u6784\u5efa\u9ad8\u8d28\u91cf\u4eba\u5de5\u6807\u6ce8\u57fa\u51c6OmniDCBench\uff1b3. \u63d0\u51faSodaM\u7edf\u4e00\u8bc4\u4f30\u6307\u6807\uff1b4. \u6784\u5efa\u8bad\u7ec3\u6570\u636e\u96c6TimeChatCap-42K\uff1b5. \u5f00\u53d1TimeChat-Captioner-7B\u6a21\u578b\uff0c\u91c7\u7528SFT\u548cGRPO\u8bad\u7ec3\uff0c\u914d\u5408\u4efb\u52a1\u7279\u5b9a\u5956\u52b1\u3002", "result": "TimeChat-Captioner-7B\u5728\u5bc6\u96c6\u63cf\u8ff0\u751f\u6210\u4e0a\u8d85\u8d8aGemini-2.5-Pro\uff0c\u8fbe\u5230SOTA\u6027\u80fd\u3002\u751f\u6210\u7684\u5bc6\u96c6\u63cf\u8ff0\u663e\u8457\u63d0\u5347\u4e0b\u6e38\u4efb\u52a1\u80fd\u529b\uff1a\u89c6\u542c\u63a8\u7406\uff08DailyOmni\u548cWorldSense\uff09\u548c\u65f6\u95f4\u5b9a\u4f4d\uff08Charades-STA\uff09\u3002", "conclusion": "Omni Dense Captioning\u662f\u4e00\u4e2a\u6709\u524d\u666f\u7684\u65b0\u4efb\u52a1\uff0c\u63d0\u51fa\u7684\u57fa\u51c6\u3001\u6307\u6807\u548c\u6a21\u578b\u4e3a\u8fde\u7eed\u3001\u7ec6\u7c92\u5ea6\u3001\u7ed3\u6784\u5316\u89c6\u542c\u53d9\u4e8b\u751f\u6210\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e0b\u6e38\u89c6\u542c\u7406\u89e3\u4efb\u52a1\u6027\u80fd\u3002"}}
{"id": "2602.08713", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.08713", "abs": "https://arxiv.org/abs/2602.08713", "authors": ["Lachin Naghashyar", "Hunar Batra", "Ashkan Khakzar", "Philip Torr", "Ronald Clark", "Christian Schroeder de Witt", "Constantin Venhoff"], "title": "Towards Understanding Multimodal Fine-Tuning: Spatial Features", "comment": null, "summary": "Contemporary Vision-Language Models (VLMs) achieve strong performance on a wide range of tasks by pairing a vision encoder with a pre-trained language model, fine-tuned for visual-text inputs. Yet despite these gains, it remains unclear how language backbone representations adapt during multimodal training and when vision-specific capabilities emerge. In this work, we present the first mechanistic analysis of VLM adaptation. Using stage-wise model diffing, a technique that isolates representational changes introduced during multimodal fine-tuning, we reveal how a language model learns to \"see\". We first identify vision-preferring features that emerge or reorient during fine-tuning. We then show that a selective subset of these features reliably encodes spatial relations, revealed through controlled shifts to spatial prompts. Finally, we trace the causal activation of these features to a small group of attention heads. Our findings show that stage-wise model diffing reveals when and where spatially grounded multimodal features arise. It also provides a clearer view of modality fusion by showing how visual grounding reshapes features that were previously text-only. This methodology enhances the interpretability of multimodal training and provides a foundation for understanding and refining how pretrained language models acquire vision-grounded capabilities.", "AI": {"tldr": "\u8be5\u7814\u7a76\u9996\u6b21\u5bf9\u89c6\u89c9\u8bed\u8a00\u6a21\u578b(VLMs)\u8fdb\u884c\u673a\u5236\u5206\u6790\uff0c\u901a\u8fc7\u9636\u6bb5\u5f0f\u6a21\u578b\u5dee\u5f02\u6280\u672f\u63ed\u793a\u8bed\u8a00\u6a21\u578b\u5982\u4f55\u5b66\u4e60\"\u770b\"\u7684\u80fd\u529b\uff0c\u8bc6\u522b\u51fa\u5728\u5fae\u8c03\u8fc7\u7a0b\u4e2d\u51fa\u73b0\u7684\u89c6\u89c9\u504f\u597d\u7279\u5f81\u53ca\u5176\u7a7a\u95f4\u5173\u7cfb\u7f16\u7801\u673a\u5236\u3002", "motivation": "\u5c3d\u7ba1\u73b0\u4ee3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u5404\u79cd\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u8bed\u8a00\u4e3b\u5e72\u8868\u793a\u5728\u591a\u6a21\u6001\u8bad\u7ec3\u4e2d\u5982\u4f55\u9002\u5e94\u4ee5\u53ca\u89c6\u89c9\u7279\u5b9a\u80fd\u529b\u4f55\u65f6\u51fa\u73b0\u4ecd\u4e0d\u6e05\u695a\u3002\u7814\u7a76\u8005\u5e0c\u671b\u63ed\u793a\u8bed\u8a00\u6a21\u578b\u5b66\u4e60\u89c6\u89c9\u80fd\u529b\u7684\u673a\u5236\u3002", "method": "\u91c7\u7528\u9636\u6bb5\u5f0f\u6a21\u578b\u5dee\u5f02\u6280\u672f\uff0c\u8be5\u6280\u672f\u80fd\u591f\u9694\u79bb\u591a\u6a21\u6001\u5fae\u8c03\u8fc7\u7a0b\u4e2d\u5f15\u5165\u7684\u8868\u5f81\u53d8\u5316\u3002\u901a\u8fc7\u8bc6\u522b\u5728\u5fae\u8c03\u8fc7\u7a0b\u4e2d\u51fa\u73b0\u6216\u91cd\u65b0\u5b9a\u5411\u7684\u89c6\u89c9\u504f\u597d\u7279\u5f81\uff0c\u5206\u6790\u8fd9\u4e9b\u7279\u5f81\u5982\u4f55\u7f16\u7801\u7a7a\u95f4\u5173\u7cfb\uff0c\u5e76\u8ffd\u8e2a\u8fd9\u4e9b\u7279\u5f81\u7684\u56e0\u679c\u6fc0\u6d3b\u5230\u7279\u5b9a\u7684\u6ce8\u610f\u529b\u5934\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff1a1) \u8bc6\u522b\u51fa\u5728\u5fae\u8c03\u8fc7\u7a0b\u4e2d\u51fa\u73b0\u6216\u91cd\u65b0\u5b9a\u5411\u7684\u89c6\u89c9\u504f\u597d\u7279\u5f81\uff1b2) \u8fd9\u4e9b\u7279\u5f81\u7684\u4e00\u4e2a\u9009\u62e9\u6027\u5b50\u96c6\u53ef\u9760\u5730\u7f16\u7801\u7a7a\u95f4\u5173\u7cfb\uff0c\u901a\u8fc7\u5bf9\u7a7a\u95f4\u63d0\u793a\u7684\u53d7\u63a7\u504f\u79fb\u63ed\u793a\uff1b3) \u8fd9\u4e9b\u7279\u5f81\u7684\u56e0\u679c\u6fc0\u6d3b\u53ef\u8ffd\u6eaf\u5230\u4e00\u5c0f\u7fa4\u6ce8\u610f\u529b\u5934\u3002", "conclusion": "\u9636\u6bb5\u5f0f\u6a21\u578b\u5dee\u5f02\u6280\u672f\u63ed\u793a\u4e86\u4f55\u65f6\u4f55\u5730\u51fa\u73b0\u7a7a\u95f4\u57fa\u7840\u7684\u591a\u6a21\u6001\u7279\u5f81\uff0c\u4e3a\u7406\u89e3\u6a21\u6001\u878d\u5408\u63d0\u4f9b\u4e86\u66f4\u6e05\u6670\u7684\u89c6\u89d2\uff0c\u5c55\u793a\u4e86\u89c6\u89c9\u57fa\u7840\u5982\u4f55\u91cd\u5851\u5148\u524d\u4ec5\u7528\u4e8e\u6587\u672c\u7684\u7279\u5f81\u3002\u8be5\u65b9\u6cd5\u589e\u5f3a\u4e86\u591a\u6a21\u6001\u8bad\u7ec3\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u4e3a\u7406\u89e3\u548c\u6539\u8fdb\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u83b7\u53d6\u89c6\u89c9\u57fa\u7840\u80fd\u529b\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2602.08717", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08717", "abs": "https://arxiv.org/abs/2602.08717", "authors": ["Farnaz Khun Jush", "Grit Werner", "Mark Klemens", "Matthias Lenga"], "title": "Zero-shot System for Automatic Body Region Detection for Volumetric CT and MR Images", "comment": "8 pages, 5 figures, 5 tables", "summary": "Reliable identification of anatomical body regions is a prerequisite for many automated medical imaging workflows, yet existing solutions remain heavily dependent on unreliable DICOM metadata. Current solutions mainly use supervised learning, which limits their applicability in many real-world scenarios. In this work, we investigate whether body region detection in volumetric CT and MR images can be achieved in a fully zero-shot manner by using knowledge embedded in large pre-trained foundation models. We propose and systematically evaluate three training-free pipelines: (1) a segmentation-driven rule-based system leveraging pre-trained multi-organ segmentation models, (2) a Multimodal Large Language Model (MLLM) guided by radiologist-defined rules, and (3) a segmentation-aware MLLM that combines visual input with explicit anatomical evidence. All methods are evaluated on 887 heterogeneous CT and MR scans with manually verified anatomical region labels. The segmentation-driven rule-based approach achieves the strongest and most consistent performance, with weighted F1-scores of 0.947 (CT) and 0.914 (MR), demonstrating robustness across modalities and atypical scan coverage. The MLLM performs competitively in visually distinctive regions, while the segmentation-aware MLLM reveals fundamental limitations.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5728\u96f6\u6837\u672c\u6761\u4ef6\u4e0b\u4f7f\u7528\u9884\u8bad\u7ec3\u57fa\u7840\u6a21\u578b\u8fdb\u884cCT\u548cMR\u56fe\u50cf\u89e3\u5256\u533a\u57df\u68c0\u6d4b\u7684\u65b9\u6cd5\uff0c\u63d0\u51fa\u4e86\u4e09\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u65b9\u6cd5\uff0c\u5176\u4e2d\u57fa\u4e8e\u5206\u5272\u7684\u89c4\u5219\u7cfb\u7edf\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u5f53\u524d\u533b\u5b66\u5f71\u50cf\u89e3\u5256\u533a\u57df\u8bc6\u522b\u4e3b\u8981\u4f9d\u8d56\u4e0d\u53ef\u9760\u7684DICOM\u5143\u6570\u636e\u6216\u76d1\u7763\u5b66\u4e60\uff0c\u9650\u5236\u4e86\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528\u3002\u672c\u6587\u65e8\u5728\u63a2\u7d22\u662f\u5426\u53ef\u4ee5\u5229\u7528\u9884\u8bad\u7ec3\u57fa\u7840\u6a21\u578b\u7684\u77e5\u8bc6\uff0c\u5728\u96f6\u6837\u672c\u6761\u4ef6\u4e0b\u5b9e\u73b0\u53ef\u9760\u7684\u89e3\u5256\u533a\u57df\u68c0\u6d4b\u3002", "method": "\u63d0\u51fa\u4e86\u4e09\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u6d41\u7a0b\uff1a1\uff09\u57fa\u4e8e\u9884\u8bad\u7ec3\u591a\u5668\u5b98\u5206\u5272\u6a21\u578b\u7684\u5206\u5272\u9a71\u52a8\u89c4\u5219\u7cfb\u7edf\uff1b2\uff09\u57fa\u4e8e\u653e\u5c04\u79d1\u533b\u751f\u5b9a\u4e49\u89c4\u5219\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff1b3\uff09\u7ed3\u5408\u89c6\u89c9\u8f93\u5165\u548c\u89e3\u5256\u8bc1\u636e\u7684\u5206\u5272\u611f\u77e5\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u3002\u5728887\u4e2aCT\u548cMR\u626b\u63cf\u4e0a\u8fdb\u884c\u4e86\u7cfb\u7edf\u8bc4\u4f30\u3002", "result": "\u5206\u5272\u9a71\u52a8\u7684\u89c4\u5219\u65b9\u6cd5\u8868\u73b0\u6700\u4f73\u4e14\u6700\u4e00\u81f4\uff0c\u52a0\u6743F1\u5206\u6570\u5206\u522b\u4e3a0.947\uff08CT\uff09\u548c0.914\uff08MR\uff09\uff0c\u5728\u4e0d\u540c\u6a21\u6001\u548c\u975e\u5178\u578b\u626b\u63cf\u8986\u76d6\u4e0b\u90fd\u8868\u73b0\u51fa\u9c81\u68d2\u6027\u3002\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u89c6\u89c9\u7279\u5f81\u660e\u663e\u7684\u533a\u57df\u8868\u73b0\u6709\u7ade\u4e89\u529b\uff0c\u800c\u5206\u5272\u611f\u77e5\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u663e\u793a\u51fa\u57fa\u672c\u5c40\u9650\u6027\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u57fa\u7840\u6a21\u578b\u7684\u77e5\u8bc6\u53ef\u4ee5\u5728\u96f6\u6837\u672c\u6761\u4ef6\u4e0b\u5b9e\u73b0\u53ef\u9760\u7684\u89e3\u5256\u533a\u57df\u68c0\u6d4b\uff0c\u5176\u4e2d\u57fa\u4e8e\u5206\u5272\u7684\u89c4\u5219\u65b9\u6cd5\u662f\u6700\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4e3a\u533b\u5b66\u5f71\u50cf\u5de5\u4f5c\u6d41\u7a0b\u63d0\u4f9b\u4e86\u4e0d\u4f9d\u8d56DICOM\u5143\u6570\u636e\u7684\u53ef\u9760\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2602.08725", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.08725", "abs": "https://arxiv.org/abs/2602.08725", "authors": ["Yongwen Lai", "Chaoqun Wang", "Shaobo Min"], "title": "FusionEdit: Semantic Fusion and Attention Modulation for Training-Free Image Editing", "comment": "Accepted by ICASSP 2026", "summary": "Text-guided image editing aims to modify specific regions according to the target prompt while preserving the identity of the source image. Recent methods exploit explicit binary masks to constrain editing, but hard mask boundaries introduce artifacts and reduce editability. To address these issues, we propose FusionEdit, a training-free image editing framework that achieves precise and controllable edits. First, editing and preserved regions are automatically identified by measuring semantic discrepancies between source and target prompts. To mitigate boundary artifacts, FusionEdit performs distance-aware latent fusion along region boundaries to yield the soft and accurate mask, and employs a total variation loss to enforce smooth transitions, obtaining natural editing results. Second, FusionEdit leverages AdaIN-based modulation within DiT attention layers to perform a statistical attention fusion in the editing region, enhancing editability while preserving global consistency with the source image. Extensive experiments demonstrate that our FusionEdit significantly outperforms state-of-the-art methods. Code is available at \\href{https://github.com/Yvan1001/FusionEdit}{https://github.com/Yvan1001/FusionEdit}.", "AI": {"tldr": "FusionEdit\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684\u6587\u672c\u5f15\u5bfc\u56fe\u50cf\u7f16\u8f91\u6846\u67b6\uff0c\u901a\u8fc7\u8f6f\u63a9\u7801\u878d\u5408\u548c\u7edf\u8ba1\u6ce8\u610f\u529b\u878d\u5408\u5b9e\u73b0\u7cbe\u786e\u53ef\u63a7\u7684\u7f16\u8f91\uff0c\u89e3\u51b3\u4e86\u786c\u63a9\u7801\u8fb9\u754c\u5e26\u6765\u7684\u4f2a\u5f71\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f7f\u7528\u663e\u5f0f\u4e8c\u503c\u63a9\u7801\u7ea6\u675f\u7f16\u8f91\uff0c\u4f46\u786c\u63a9\u7801\u8fb9\u754c\u4f1a\u5f15\u5165\u4f2a\u5f71\u5e76\u964d\u4f4e\u53ef\u7f16\u8f91\u6027\u3002\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u7cbe\u786e\u63a7\u5236\u7f16\u8f91\u533a\u57df\u53c8\u80fd\u4fdd\u6301\u56fe\u50cf\u81ea\u7136\u8fc7\u6e21\u7684\u65b9\u6cd5\u3002", "method": "1) \u901a\u8fc7\u6d4b\u91cf\u6e90\u63d0\u793a\u8bcd\u548c\u76ee\u6807\u63d0\u793a\u8bcd\u8bed\u4e49\u5dee\u5f02\u81ea\u52a8\u8bc6\u522b\u7f16\u8f91\u548c\u4fdd\u7559\u533a\u57df\uff1b2) \u6cbf\u8fb9\u754c\u8fdb\u884c\u8ddd\u79bb\u611f\u77e5\u7684\u6f5c\u5728\u878d\u5408\u751f\u6210\u8f6f\u63a9\u7801\uff0c\u5e76\u4f7f\u7528\u603b\u53d8\u5dee\u635f\u5931\u786e\u4fdd\u5e73\u6ed1\u8fc7\u6e21\uff1b3) \u5728DiT\u6ce8\u610f\u529b\u5c42\u4e2d\u4f7f\u7528AdaIN\u8c03\u5236\u8fdb\u884c\u7edf\u8ba1\u6ce8\u610f\u529b\u878d\u5408\uff0c\u589e\u5f3a\u53ef\u7f16\u8f91\u6027\u540c\u65f6\u4fdd\u6301\u5168\u5c40\u4e00\u81f4\u6027\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cFusionEdit\u5728\u6587\u672c\u5f15\u5bfc\u56fe\u50cf\u7f16\u8f91\u4efb\u52a1\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u80fd\u591f\u5b9e\u73b0\u66f4\u7cbe\u786e\u3001\u66f4\u81ea\u7136\u7684\u7f16\u8f91\u6548\u679c\u3002", "conclusion": "FusionEdit\u901a\u8fc7\u8f6f\u63a9\u7801\u878d\u5408\u548c\u7edf\u8ba1\u6ce8\u610f\u529b\u878d\u5408\uff0c\u89e3\u51b3\u4e86\u786c\u63a9\u7801\u8fb9\u754c\u5e26\u6765\u7684\u4f2a\u5f71\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u7cbe\u786e\u53ef\u63a7\u7684\u56fe\u50cf\u7f16\u8f91\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u7f16\u8f91\u7684\u81ea\u7136\u6027\u548c\u5168\u5c40\u4e00\u81f4\u6027\u3002"}}
{"id": "2602.08727", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08727", "abs": "https://arxiv.org/abs/2602.08727", "authors": ["Johannes Thalhammer", "Tina Dorosti", "Sebastian Peterhansl", "Daniela Pfeiffer", "Franz Pfeiffer", "Florian Schaff"], "title": "Artifact Reduction in Undersampled 3D Cone-Beam CTs using a Hybrid 2D-3D CNN Framework", "comment": null, "summary": "Undersampled CT volumes minimize acquisition time and radiation exposure but introduce artifacts degrading image quality and diagnostic utility. Reducing these artifacts is critical for high-quality imaging. We propose a computationally efficient hybrid deep-learning framework that combines the strengths of 2D and 3D models. First, a 2D U-Net operates on individual slices of undersampled CT volumes to extract feature maps. These slice-wise feature maps are then stacked across the volume and used as input to a 3D decoder, which utilizes contextual information across slices to predict an artifact-free 3D CT volume. The proposed two-stage approach balances the computational efficiency of 2D processing with the volumetric consistency provided by 3D modeling. The results show substantial improvements in inter-slice consistency in coronal and sagittal direction with low computational overhead. This hybrid framework presents a robust and efficient solution for high-quality 3D CT image post-processing. The code of this project can be found on github: https://github.com/J-3TO/2D-3DCNN_sparseview/.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u54082D\u548c3D\u6a21\u578b\u7684\u6df7\u5408\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u6b20\u91c7\u6837CT\u4f53\u79ef\u4e2d\u53bb\u9664\u4f2a\u5f71\uff0c\u5728\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u7684\u540c\u65f6\u63d0\u9ad8\u56fe\u50cf\u8d28\u91cf\u548c\u8bca\u65ad\u4ef7\u503c\u3002", "motivation": "\u6b20\u91c7\u6837CT\u626b\u63cf\u867d\u7136\u51cf\u5c11\u4e86\u91c7\u96c6\u65f6\u95f4\u548c\u8f90\u5c04\u66b4\u9732\uff0c\u4f46\u4f1a\u5f15\u5165\u4f2a\u5f71\uff0c\u964d\u4f4e\u56fe\u50cf\u8d28\u91cf\u548c\u8bca\u65ad\u6548\u7528\u3002\u6d88\u9664\u8fd9\u4e9b\u4f2a\u5f71\u5bf9\u4e8e\u9ad8\u8d28\u91cf\u6210\u50cf\u81f3\u5173\u91cd\u8981\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u6df7\u5408\u6846\u67b6\uff1a\u9996\u5148\u4f7f\u75282D U-Net\u5904\u7406\u6b20\u91c7\u6837CT\u4f53\u79ef\u7684\u5355\u4e2a\u5207\u7247\u63d0\u53d6\u7279\u5f81\u56fe\uff0c\u7136\u540e\u5c06\u8fd9\u4e9b\u5207\u7247\u7279\u5f81\u56fe\u5806\u53e0\u6210\u4f53\u79ef\uff0c\u8f93\u5165\u52303D\u89e3\u7801\u5668\u4e2d\uff0c\u5229\u7528\u8de8\u5207\u7247\u4e0a\u4e0b\u6587\u4fe1\u606f\u9884\u6d4b\u65e0\u4f2a\u5f71\u76843D CT\u4f53\u79ef\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u51a0\u72b6\u9762\u548c\u77e2\u72b6\u9762\u65b9\u5411\u4e0a\u663e\u8457\u63d0\u9ad8\u4e86\u5207\u7247\u95f4\u4e00\u81f4\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u8f83\u4f4e\u7684\u8ba1\u7b97\u5f00\u9500\uff0c\u4e3a\u9ad8\u8d28\u91cf3D CT\u56fe\u50cf\u540e\u5904\u7406\u63d0\u4f9b\u4e86\u7a33\u5065\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "\u63d0\u51fa\u7684\u6df7\u5408\u6846\u67b6\u5e73\u8861\u4e862D\u5904\u7406\u7684\u8ba1\u7b97\u6548\u7387\u548c3D\u5efa\u6a21\u7684\u4f53\u79ef\u4e00\u81f4\u6027\uff0c\u662f\u9ad8\u8d28\u91cf3D CT\u56fe\u50cf\u540e\u5904\u7406\u7684\u7a33\u5065\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.08730", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.08730", "abs": "https://arxiv.org/abs/2602.08730", "authors": ["Shanshan Wang", "Ziying Feng", "Xiaozheng Shen", "Xun Yang", "Pichao Wang", "Zhenwei He", "Xingyi Zhang"], "title": "Closing the Confusion Loop: CLIP-Guided Alignment for Source-Free Domain Adaptation", "comment": null, "summary": "Source-Free Domain Adaptation (SFDA) tackles the problem of adapting a pre-trained source model to an unlabeled target domain without accessing any source data, which is quite suitable for the field of data security. Although recent advances have shown that pseudo-labeling strategies can be effective, they often fail in fine-grained scenarios due to subtle inter-class similarities. A critical but underexplored issue is the presence of asymmetric and dynamic class confusion, where visually similar classes are unequally and inconsistently misclassified by the source model. Existing methods typically ignore such confusion patterns, leading to noisy pseudo-labels and poor target discrimination. To address this, we propose CLIP-Guided Alignment(CGA), a novel framework that explicitly models and mitigates class confusion in SFDA. Generally, our method consists of three parts: (1) MCA: detects first directional confusion pairs by analyzing the predictions of the source model in the target domain; (2) MCC: leverages CLIP to construct confusion-aware textual prompts (e.g. a truck that looks like a bus), enabling more context-sensitive pseudo-labeling; and (3) FAM: builds confusion-guided feature banks for both CLIP and the source model and aligns them using contrastive learning to reduce ambiguity in the representation space. Extensive experiments on various datasets demonstrate that CGA consistently outperforms state-of-the-art SFDA methods, with especially notable gains in confusion-prone and fine-grained scenarios. Our results highlight the importance of explicitly modeling inter-class confusion for effective source-free adaptation. Our code can be find at https://github.com/soloiro/CGA", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faCLIP\u5f15\u5bfc\u5bf9\u9f50(CGA)\u6846\u67b6\uff0c\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u548c\u7f13\u89e3\u7c7b\u522b\u6df7\u6dc6\u6765\u89e3\u51b3\u6e90\u81ea\u7531\u57df\u9002\u5e94\u4e2d\u7684\u4f2a\u6807\u7b7e\u566a\u58f0\u95ee\u9898\uff0c\u5728\u7ec6\u7c92\u5ea6\u573a\u666f\u4e0b\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u6e90\u81ea\u7531\u57df\u9002\u5e94\u4e2d\uff0c\u73b0\u6709\u4f2a\u6807\u7b7e\u7b56\u7565\u5728\u7ec6\u7c92\u5ea6\u573a\u666f\u4e0b\u56e0\u7c7b\u522b\u95f4\u76f8\u4f3c\u6027\u800c\u5931\u6548\uff0c\u7279\u522b\u662f\u5b58\u5728\u4e0d\u5bf9\u79f0\u548c\u52a8\u6001\u7684\u7c7b\u522b\u6df7\u6dc6\u95ee\u9898\uff0c\u5bfc\u81f4\u4f2a\u6807\u7b7e\u566a\u58f0\u548c\u76ee\u6807\u5224\u522b\u80fd\u529b\u5dee\u3002", "method": "CGA\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1aMCA\u68c0\u6d4b\u76ee\u6807\u57df\u4e2d\u7684\u65b9\u5411\u6027\u6df7\u6dc6\u5bf9\uff1bMCC\u5229\u7528CLIP\u6784\u5efa\u6df7\u6dc6\u611f\u77e5\u7684\u6587\u672c\u63d0\u793a\uff1bFAM\u5efa\u7acb\u6df7\u6dc6\u5f15\u5bfc\u7684\u7279\u5f81\u5e93\u5e76\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u5bf9\u9f50CLIP\u548c\u6e90\u6a21\u578b\u7684\u8868\u793a\u7a7a\u95f4\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cCGA\u6301\u7eed\u8d85\u8d8a\u6700\u5148\u8fdb\u7684SFDA\u65b9\u6cd5\uff0c\u5728\u6613\u6df7\u6dc6\u548c\u7ec6\u7c92\u5ea6\u573a\u666f\u4e0b\u53d6\u5f97\u7279\u522b\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "\u663e\u5f0f\u5efa\u6a21\u7c7b\u522b\u95f4\u6df7\u6dc6\u5bf9\u4e8e\u6709\u6548\u7684\u6e90\u81ea\u7531\u57df\u9002\u5e94\u81f3\u5173\u91cd\u8981\uff0cCGA\u6846\u67b6\u901a\u8fc7\u7ed3\u5408CLIP\u7684\u8bed\u4e49\u7406\u89e3\u548c\u6df7\u6dc6\u611f\u77e5\u7b56\u7565\uff0c\u6210\u529f\u7f13\u89e3\u4e86\u4f2a\u6807\u7b7e\u566a\u58f0\u95ee\u9898\u3002"}}
{"id": "2602.08735", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.08735", "abs": "https://arxiv.org/abs/2602.08735", "authors": ["Masanari Oi", "Koki Maeda", "Ryuto Koike", "Daisuke Oba", "Nakamasa Inoue", "Naoaki Okazaki"], "title": "From Correspondence to Actions: Human-Like Multi-Image Spatial Reasoning in Multi-modal Large Language Models", "comment": null, "summary": "While multimodal large language models (MLLMs) have made substantial progress in single-image spatial reasoning, multi-image spatial reasoning, which requires integration of information from multiple viewpoints, remains challenging. Cognitive studies suggest that humans address such tasks through two mechanisms: cross-view correspondence, which identifies regions across different views that correspond to the same physical locations, and stepwise viewpoint transformation, which composes relative viewpoint changes sequentially. However, existing studies incorporate these mechanisms only partially and often implicitly, without explicit supervision for both. We propose Human-Aware Training for Cross-view correspondence and viewpoint cHange (HATCH), a training framework with two complementary objectives: (1) Patch-Level Spatial Alignment, which encourages patch representations to align across views for spatially corresponding regions, and (2) Action-then-Answer Reasoning, which requires the model to generate explicit viewpoint transition actions before predicting the final answer. Experiments on three benchmarks demonstrate that HATCH consistently outperforms baselines of comparable size by a clear margin and achieves competitive results against much larger models, while preserving single-image reasoning capabilities.", "AI": {"tldr": "HATCH\u8bad\u7ec3\u6846\u67b6\u901a\u8fc7\u8865\u4e01\u7ea7\u7a7a\u95f4\u5bf9\u9f50\u548c\u884c\u52a8-\u7b54\u6848\u63a8\u7406\uff0c\u63d0\u5347\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u591a\u56fe\u50cf\u7a7a\u95f4\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u5728\u4fdd\u6301\u5355\u56fe\u50cf\u63a8\u7406\u80fd\u529b\u7684\u540c\u65f6\uff0c\u663e\u8457\u4f18\u4e8e\u540c\u89c4\u6a21\u57fa\u7ebf\u6a21\u578b\u3002", "motivation": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5355\u56fe\u50cf\u7a7a\u95f4\u63a8\u7406\u65b9\u9762\u53d6\u5f97\u8fdb\u5c55\uff0c\u4f46\u5728\u9700\u8981\u6574\u5408\u591a\u4e2a\u89c6\u89d2\u4fe1\u606f\u7684\u591a\u56fe\u50cf\u7a7a\u95f4\u63a8\u7406\u65b9\u9762\u4ecd\u9762\u4e34\u6311\u6218\u3002\u8ba4\u77e5\u7814\u7a76\u8868\u660e\u4eba\u7c7b\u901a\u8fc7\u8de8\u89c6\u56fe\u5bf9\u5e94\u548c\u9010\u6b65\u89c6\u89d2\u53d8\u6362\u4e24\u79cd\u673a\u5236\u89e3\u51b3\u6b64\u7c7b\u4efb\u52a1\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u4ec5\u90e8\u5206\u4e14\u9690\u5f0f\u5730\u878d\u5165\u8fd9\u4e9b\u673a\u5236\uff0c\u7f3a\u4e4f\u5bf9\u4e24\u8005\u7684\u663e\u5f0f\u76d1\u7763\u3002", "method": "\u63d0\u51faHATCH\u8bad\u7ec3\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u4e92\u8865\u76ee\u6807\uff1a1) \u8865\u4e01\u7ea7\u7a7a\u95f4\u5bf9\u9f50 - \u9f13\u52b1\u4e0d\u540c\u89c6\u56fe\u4e2d\u7a7a\u95f4\u5bf9\u5e94\u533a\u57df\u7684\u8865\u4e01\u8868\u793a\u5bf9\u9f50\uff1b2) \u884c\u52a8-\u7b54\u6848\u63a8\u7406 - \u8981\u6c42\u6a21\u578b\u5728\u9884\u6d4b\u6700\u7ec8\u7b54\u6848\u524d\u751f\u6210\u663e\u5f0f\u7684\u89c6\u89d2\u8f6c\u6362\u884c\u52a8\u3002", "result": "\u5728\u4e09\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cHATCH\u59cb\u7ec8\u4ee5\u660e\u663e\u4f18\u52bf\u4f18\u4e8e\u540c\u89c4\u6a21\u57fa\u7ebf\u6a21\u578b\uff0c\u5e76\u4e0e\u66f4\u5927\u6a21\u578b\u53d6\u5f97\u7ade\u4e89\u6027\u7ed3\u679c\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u5355\u56fe\u50cf\u63a8\u7406\u80fd\u529b\u3002", "conclusion": "HATCH\u6846\u67b6\u901a\u8fc7\u663e\u5f0f\u76d1\u7763\u8de8\u89c6\u56fe\u5bf9\u5e94\u548c\u9010\u6b65\u89c6\u89d2\u53d8\u6362\uff0c\u6709\u6548\u63d0\u5347\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u591a\u56fe\u50cf\u7a7a\u95f4\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\uff0c\u4e3a\u591a\u89c6\u89d2\u7a7a\u95f4\u7406\u89e3\u63d0\u4f9b\u4e86\u7cfb\u7edf\u5316\u8bad\u7ec3\u65b9\u6cd5\u3002"}}
{"id": "2602.08749", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.08749", "abs": "https://arxiv.org/abs/2602.08749", "authors": ["Carmine Zaccagnino", "Fabio Quattrini", "Enis Simsar", "Marta Tintor\u00e9 Gazulla", "Rita Cucchiara", "Alessio Tonioni", "Silvia Cascianelli"], "title": "Shifting the Breaking Point of Flow Matching for Multi-Instance Editing", "comment": null, "summary": "Flow matching models have recently emerged as an efficient alternative to diffusion, especially for text-guided image generation and editing, offering faster inference through continuous-time dynamics. However, existing flow-based editors predominantly support global or single-instruction edits and struggle with multi-instance scenarios, where multiple parts of a reference input must be edited independently without semantic interference. We identify this limitation as a consequence of globally conditioned velocity fields and joint attention mechanisms, which entangle concurrent edits. To address this issue, we introduce Instance-Disentangled Attention, a mechanism that partitions joint attention operations, enforcing binding between instance-specific textual instructions and spatial regions during velocity field estimation. We evaluate our approach on both natural image editing and a newly introduced benchmark of text-dense infographics with region-level editing instructions. Experimental results demonstrate that our approach promotes edit disentanglement and locality while preserving global output coherence, enabling single-pass, instance-level editing.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aInstance-Disentangled Attention\u7684\u65b0\u673a\u5236\uff0c\u7528\u4e8e\u89e3\u51b3\u73b0\u6709\u57fa\u4e8e\u6d41\u7684\u56fe\u50cf\u7f16\u8f91\u6a21\u578b\u5728\u591a\u5b9e\u4f8b\u7f16\u8f91\u573a\u666f\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u80fd\u591f\u5b9e\u73b0\u5355\u6b21\u63a8\u7406\u4e2d\u7684\u5b9e\u4f8b\u7ea7\u7f16\u8f91\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6d41\u7684\u56fe\u50cf\u7f16\u8f91\u6a21\u578b\u4e3b\u8981\u652f\u6301\u5168\u5c40\u6216\u5355\u6307\u4ee4\u7f16\u8f91\uff0c\u4f46\u5728\u591a\u5b9e\u4f8b\u573a\u666f\u4e2d\u5b58\u5728\u5c40\u9650\u6027\uff0c\u5373\u5f53\u9700\u8981\u5bf9\u53c2\u8003\u8f93\u5165\u7684\u591a\u4e2a\u90e8\u5206\u8fdb\u884c\u72ec\u7acb\u7f16\u8f91\u65f6\uff0c\u4f1a\u4ea7\u751f\u8bed\u4e49\u5e72\u6270\u3002\u8fd9\u79cd\u5c40\u9650\u6027\u6e90\u4e8e\u5168\u5c40\u6761\u4ef6\u5316\u7684\u901f\u5ea6\u573a\u548c\u8054\u5408\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5bfc\u81f4\u5e76\u53d1\u7f16\u8f91\u76f8\u4e92\u7ea0\u7f20\u3002", "method": "\u63d0\u51fa\u4e86Instance-Disentangled Attention\u673a\u5236\uff0c\u8be5\u673a\u5236\u901a\u8fc7\u5212\u5206\u8054\u5408\u6ce8\u610f\u529b\u64cd\u4f5c\uff0c\u5728\u901f\u5ea6\u573a\u4f30\u8ba1\u8fc7\u7a0b\u4e2d\u5f3a\u5236\u5efa\u7acb\u5b9e\u4f8b\u7279\u5b9a\u6587\u672c\u6307\u4ee4\u4e0e\u7a7a\u95f4\u533a\u57df\u4e4b\u95f4\u7684\u7ed1\u5b9a\u5173\u7cfb\uff0c\u4ece\u800c\u5b9e\u73b0\u7f16\u8f91\u7684\u89e3\u8026\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u81ea\u7136\u56fe\u50cf\u7f16\u8f91\u548c\u6587\u672c\u5bc6\u96c6\u4fe1\u606f\u56fe\u7f16\u8f91\u4efb\u52a1\u4e2d\u90fd\u80fd\u4fc3\u8fdb\u7f16\u8f91\u7684\u89e3\u8026\u548c\u5c40\u90e8\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u5168\u5c40\u8f93\u51fa\u7684\u8fde\u8d2f\u6027\uff0c\u5b9e\u73b0\u4e86\u5355\u6b21\u63a8\u7406\u4e2d\u7684\u5b9e\u4f8b\u7ea7\u7f16\u8f91\u3002", "conclusion": "Instance-Disentangled Attention\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86\u57fa\u4e8e\u6d41\u7684\u56fe\u50cf\u7f16\u8f91\u6a21\u578b\u5728\u591a\u5b9e\u4f8b\u7f16\u8f91\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u5b9e\u73b0\u4e86\u66f4\u7cbe\u7ec6\u7684\u5b9e\u4f8b\u7ea7\u7f16\u8f91\u63a7\u5236\uff0c\u4e3a\u6587\u672c\u5f15\u5bfc\u7684\u56fe\u50cf\u751f\u6210\u548c\u7f16\u8f91\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.08753", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.08753", "abs": "https://arxiv.org/abs/2602.08753", "authors": ["Tianyu Sun", "Zhoujie Fu", "Bang Zhang", "Guosheng Lin"], "title": "MVAnimate: Enhancing Character Animation with Multi-View Optimization", "comment": null, "summary": "The demand for realistic and versatile character animation has surged, driven by its wide-ranging applications in various domains. However, the animation generation algorithms modeling human pose with 2D or 3D structures all face various problems, including low-quality output content and training data deficiency, preventing the related algorithms from generating high-quality animation videos. Therefore, we introduce MVAnimate, a novel framework that synthesizes both 2D and 3D information of dynamic figures based on multi-view prior information, to enhance the generated video quality. Our approach leverages multi-view prior information to produce temporally consistent and spatially coherent animation outputs, demonstrating improvements over existing animation methods. Our MVAnimate also optimizes the multi-view videos of the target character, enhancing the video quality from different views. Experimental results on diverse datasets highlight the robustness of our method in handling various motion patterns and appearances.", "AI": {"tldr": "MVAnimate\u662f\u4e00\u4e2a\u5229\u7528\u591a\u89c6\u89d2\u5148\u9a8c\u4fe1\u606f\u751f\u6210\u9ad8\u8d28\u91cf2D\u548c3D\u89d2\u8272\u52a8\u753b\u7684\u65b0\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u52a8\u753b\u751f\u6210\u7b97\u6cd5\u8f93\u51fa\u8d28\u91cf\u4f4e\u548c\u8bad\u7ec3\u6570\u636e\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e2D\u62163D\u7ed3\u6784\u5efa\u6a21\u4eba\u4f53\u59ff\u6001\u7684\u52a8\u753b\u751f\u6210\u7b97\u6cd5\u9762\u4e34\u8f93\u51fa\u8d28\u91cf\u4f4e\u548c\u8bad\u7ec3\u6570\u636e\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u65e0\u6cd5\u751f\u6210\u9ad8\u8d28\u91cf\u52a8\u753b\u89c6\u9891\u3002\u9700\u8981\u4e00\u79cd\u65b0\u65b9\u6cd5\u6765\u63d0\u5347\u52a8\u753b\u751f\u6210\u8d28\u91cf\u3002", "method": "MVAnimate\u662f\u4e00\u4e2a\u65b0\u9896\u6846\u67b6\uff0c\u57fa\u4e8e\u591a\u89c6\u89d2\u5148\u9a8c\u4fe1\u606f\u5408\u6210\u52a8\u6001\u4eba\u7269\u76842D\u548c3D\u4fe1\u606f\u3002\u8be5\u65b9\u6cd5\u5229\u7528\u591a\u89c6\u89d2\u5148\u9a8c\u4fe1\u606f\u751f\u6210\u65f6\u95f4\u4e00\u81f4\u548c\u7a7a\u95f4\u8fde\u8d2f\u7684\u52a8\u753b\u8f93\u51fa\uff0c\u5e76\u4f18\u5316\u76ee\u6807\u89d2\u8272\u7684\u591a\u89c6\u89d2\u89c6\u9891\u8d28\u91cf\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u6837\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u80fd\u591f\u5904\u7406\u5404\u79cd\u8fd0\u52a8\u6a21\u5f0f\u548c\u5916\u89c2\uff0c\u76f8\u6bd4\u73b0\u6709\u52a8\u753b\u65b9\u6cd5\u6709\u663e\u8457\u6539\u8fdb\u3002", "conclusion": "MVAnimate\u901a\u8fc7\u6574\u5408\u591a\u89c6\u89d2\u4fe1\u606f\u6709\u6548\u63d0\u5347\u4e86\u52a8\u753b\u751f\u6210\u8d28\u91cf\uff0c\u4e3a\u9ad8\u8d28\u91cf\u89d2\u8272\u52a8\u753b\u751f\u6210\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.08775", "categories": ["cs.CV", "cs.CG"], "pdf": "https://arxiv.org/pdf/2602.08775", "abs": "https://arxiv.org/abs/2602.08775", "authors": ["Vineet Kumar Rakesh", "Ahana Bhattacharjee", "Soumya Mazumdar", "Tapas Samanta", "Hemendra Kumar Pandey", "Amitabha Das", "Sarbajit Pal"], "title": "VedicTHG: Symbolic Vedic Computation for Low-Resource Talking-Head Generation in Educational Avatars", "comment": null, "summary": "Talking-head avatars are increasingly adopted in educational technology to deliver content with social presence and improved engagement. However, many recent talking-head generation (THG) methods rely on GPU-centric neural rendering, large training sets, or high-capacity diffusion models, which limits deployment in offline or resource-constrained learning environments. A deterministic and CPU-oriented THG framework is described, termed Symbolic Vedic Computation, that converts speech to a time-aligned phoneme stream, maps phonemes to a compact viseme inventory, and produces smooth viseme trajectories through symbolic coarticulation inspired by Vedic sutra Urdhva Tiryakbhyam. A lightweight 2D renderer performs region-of-interest (ROI) warping and mouth compositing with stabilization to support real-time synthesis on commodity CPUs. Experiments report synchronization accuracy, temporal stability, and identity consistency under CPU-only execution, alongside benchmarking against representative CPU-feasible baselines. Results indicate that acceptable lip-sync quality can be achieved while substantially reducing computational load and latency, supporting practical educational avatars on low-end hardware. GitHub: https://vineetkumarrakesh.github.io/vedicthg", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7b26\u53f7\u5420\u9640\u8ba1\u7b97\u7684\u8f7b\u91cf\u7ea7\u8bf4\u8bdd\u5934\u751f\u6210\u6846\u67b6\uff0c\u53ef\u5728CPU\u4e0a\u5b9e\u65f6\u8fd0\u884c\uff0c\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u7684\u6559\u80b2\u73af\u5883", "motivation": "\u5f53\u524d\u8bf4\u8bdd\u5934\u751f\u6210\u65b9\u6cd5\u4f9d\u8d56GPU\u6e32\u67d3\u3001\u5927\u91cf\u8bad\u7ec3\u6570\u636e\u6216\u9ad8\u5bb9\u91cf\u6269\u6563\u6a21\u578b\uff0c\u96be\u4ee5\u5728\u79bb\u7ebf\u6216\u8d44\u6e90\u53d7\u9650\u7684\u5b66\u4e60\u73af\u5883\u4e2d\u90e8\u7f72\uff0c\u9700\u8981\u4e00\u79cd\u8f7b\u91cf\u7ea7\u3001CPU\u53cb\u597d\u7684\u89e3\u51b3\u65b9\u6848", "method": "\u91c7\u7528\u7b26\u53f7\u5420\u9640\u8ba1\u7b97\u65b9\u6cd5\uff1a1) \u8bed\u97f3\u8f6c\u65f6\u95f4\u5bf9\u9f50\u97f3\u7d20\u6d41\uff1b2) \u97f3\u7d20\u6620\u5c04\u5230\u7d27\u51d1\u89c6\u7d20\u5e93\uff1b3) \u57fa\u4e8e\u5420\u9640\u7ecfUrdhva Tiryakbhyam\u539f\u7406\u7684\u7b26\u53f7\u534f\u540c\u53d1\u97f3\u751f\u6210\u5e73\u6ed1\u89c6\u7d20\u8f68\u8ff9\uff1b4) \u8f7b\u91cf\u7ea72D\u6e32\u67d3\u5668\u8fdb\u884cROI\u626d\u66f2\u548c\u5634\u90e8\u5408\u6210", "result": "\u5728\u4ec5CPU\u6267\u884c\u4e0b\u5b9e\u73b0\u4e86\u53ef\u63a5\u53d7\u7684\u5507\u540c\u6b65\u8d28\u91cf\uff0c\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u8d1f\u8f7d\u548c\u5ef6\u8fdf\uff0c\u652f\u6301\u5728\u4f4e\u7aef\u786c\u4ef6\u4e0a\u8fd0\u884c\u6559\u80b2\u7528\u865a\u62df\u5f62\u8c61", "conclusion": "\u63d0\u51fa\u7684\u7b26\u53f7\u5420\u9640\u8ba1\u7b97\u6846\u67b6\u4e3a\u8d44\u6e90\u53d7\u9650\u73af\u5883\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u7684\u8bf4\u8bdd\u5934\u751f\u6210\u89e3\u51b3\u65b9\u6848\uff0c\u5e73\u8861\u4e86\u8d28\u91cf\u4e0e\u8ba1\u7b97\u6548\u7387\uff0c\u652f\u6301\u6559\u80b2\u865a\u62df\u5f62\u8c61\u5728\u4f4e\u7aef\u786c\u4ef6\u4e0a\u7684\u90e8\u7f72"}}
{"id": "2602.08792", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.08792", "abs": "https://arxiv.org/abs/2602.08792", "authors": ["Hao Dong", "Eleni Chatzi", "Olga Fink"], "title": "Multimodal Learning for Arcing Detection in Pantograph-Catenary Systems", "comment": null, "summary": "The pantograph-catenary interface is essential for ensuring uninterrupted and reliable power delivery in electrified rail systems. However, electrical arcing at this interface poses serious risks, including accelerated wear of contact components, degraded system performance, and potential service disruptions. Detecting arcing events at the pantograph-catenary interface is challenging due to their transient nature, noisy operating environment, data scarcity, and the difficulty of distinguishing arcs from other similar transient phenomena. To address these challenges, we propose a novel multimodal framework that combines high-resolution image data with force measurements to more accurately and robustly detect arcing events. First, we construct two arcing detection datasets comprising synchronized visual and force measurements. One dataset is built from data provided by the Swiss Federal Railways (SBB), and the other is derived from publicly available videos of arcing events in different railway systems and synthetic force data that mimic the characteristics observed in the real dataset. Leveraging these datasets, we propose MultiDeepSAD, an extension of the DeepSAD algorithm for multiple modalities with a new loss formulation. Additionally, we introduce tailored pseudo-anomaly generation techniques specific to each data type, such as synthetic arc-like artifacts in images and simulated force irregularities, to augment training data and improve the discriminative ability of the model. Through extensive experiments and ablation studies, we demonstrate that our framework significantly outperforms baseline approaches, exhibiting enhanced sensitivity to real arcing events even under domain shifts and limited availability of real arcing observations.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u548c\u529b\u6d4b\u91cf\u7684\u591a\u6a21\u6001\u6846\u67b6\uff0c\u7528\u4e8e\u66f4\u51c6\u786e\u3001\u9c81\u68d2\u5730\u68c0\u6d4b\u53d7\u7535\u5f13-\u63a5\u89e6\u7f51\u63a5\u53e3\u7684\u7535\u5f27\u4e8b\u4ef6\uff0c\u89e3\u51b3\u4e86\u7535\u5f27\u68c0\u6d4b\u4e2d\u7684\u77ac\u6001\u6027\u3001\u566a\u58f0\u73af\u5883\u3001\u6570\u636e\u7a00\u7f3a\u7b49\u6311\u6218\u3002", "motivation": "\u53d7\u7535\u5f13-\u63a5\u89e6\u7f51\u63a5\u53e3\u7684\u7535\u5f27\u73b0\u8c61\u5bf9\u94c1\u8def\u4f9b\u7535\u7cfb\u7edf\u6784\u6210\u4e25\u91cd\u98ce\u9669\uff0c\u5305\u62ec\u52a0\u901f\u63a5\u89e6\u90e8\u4ef6\u78e8\u635f\u3001\u7cfb\u7edf\u6027\u80fd\u4e0b\u964d\u548c\u670d\u52a1\u4e2d\u65ad\u3002\u7535\u5f27\u68c0\u6d4b\u9762\u4e34\u77ac\u6001\u6027\u3001\u566a\u58f0\u73af\u5883\u3001\u6570\u636e\u7a00\u7f3a\u4ee5\u53ca\u96be\u4ee5\u533a\u5206\u7535\u5f27\u4e0e\u5176\u4ed6\u77ac\u6001\u73b0\u8c61\u7b49\u6311\u6218\u3002", "method": "\u6784\u5efa\u4e86\u4e24\u4e2a\u7535\u5f27\u68c0\u6d4b\u6570\u636e\u96c6\uff08\u745e\u58eb\u8054\u90a6\u94c1\u8def\u6570\u636e\u548c\u516c\u5f00\u89c6\u9891+\u5408\u6210\u529b\u6570\u636e\uff09\uff0c\u63d0\u51fa\u4e86MultiDeepSAD\u591a\u6a21\u6001\u6269\u5c55\u7b97\u6cd5\uff0c\u5e76\u9488\u5bf9\u6bcf\u79cd\u6570\u636e\u7c7b\u578b\u8bbe\u8ba1\u4e86\u4e13\u95e8\u7684\u4f2a\u5f02\u5e38\u751f\u6210\u6280\u672f\uff08\u5982\u56fe\u50cf\u4e2d\u7684\u5408\u6210\u7535\u5f27\u4f2a\u5f71\u548c\u6a21\u62df\u529b\u4e0d\u89c4\u5219\u6027\uff09\u3002", "result": "\u901a\u8fc7\u5927\u91cf\u5b9e\u9a8c\u548c\u6d88\u878d\u7814\u7a76\uff0c\u8bc1\u660e\u8be5\u6846\u67b6\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5373\u4f7f\u5728\u57df\u504f\u79fb\u548c\u771f\u5b9e\u7535\u5f27\u89c2\u6d4b\u6570\u636e\u6709\u9650\u7684\u60c5\u51b5\u4e0b\uff0c\u5bf9\u771f\u5b9e\u7535\u5f27\u4e8b\u4ef6\u4e5f\u8868\u73b0\u51fa\u589e\u5f3a\u7684\u654f\u611f\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u591a\u6a21\u6001\u6846\u67b6\u7ed3\u5408\u89c6\u89c9\u548c\u529b\u6d4b\u91cf\uff0c\u80fd\u591f\u66f4\u51c6\u786e\u3001\u9c81\u68d2\u5730\u68c0\u6d4b\u53d7\u7535\u5f13-\u63a5\u89e6\u7f51\u63a5\u53e3\u7684\u7535\u5f27\u4e8b\u4ef6\uff0c\u4e3a\u89e3\u51b3\u94c1\u8def\u4f9b\u7535\u7cfb\u7edf\u4e2d\u7684\u7535\u5f27\u68c0\u6d4b\u6311\u6218\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\u3002"}}
{"id": "2602.08794", "categories": ["cs.CV", "cs.SD"], "pdf": "https://arxiv.org/pdf/2602.08794", "abs": "https://arxiv.org/abs/2602.08794", "authors": ["SII-OpenMOSS Team", ":", "Donghua Yu", "Mingshu Chen", "Qi Chen", "Qi Luo", "Qianyi Wu", "Qinyuan Cheng", "Ruixiao Li", "Tianyi Liang", "Wenbo Zhang", "Wenming Tu", "Xiangyu Peng", "Yang Gao", "Yanru Huo", "Ying Zhu", "Yinze Luo", "Yiyang Zhang", "Yuerong Song", "Zhe Xu", "Zhiyu Zhang", "Chenchen Yang", "Cheng Chang", "Chushu Zhou", "Hanfu Chen", "Hongnan Ma", "Jiaxi Li", "Jingqi Tong", "Junxi Liu", "Ke Chen", "Shimin Li", "Songlin Wang", "Wei Jiang", "Zhaoye Fei", "Zhiyuan Ning", "Chunguo Li", "Chenhui Li", "Ziwei He", "Zengfeng Huang", "Xie Chen", "Xipeng Qiu"], "title": "MOVA: Towards Scalable and Synchronized Video-Audio Generation", "comment": "Technical report for MOVA (open-source video-audio generation model). 38 pages, 10 figures, 22 tables. Project page: https://mosi.cn/models/mova Code: https://github.com/OpenMOSS/MOVA Models: https://huggingface.co/collections/OpenMOSS-Team/mova. Qinyuan Cheng and Tianyi Liang are project leader. Xie Chen and Xipeng Qiu are corresponding authors", "summary": "Audio is indispensable for real-world video, yet generation models have largely overlooked audio components. Current approaches to producing audio-visual content often rely on cascaded pipelines, which increase cost, accumulate errors, and degrade overall quality. While systems such as Veo 3 and Sora 2 emphasize the value of simultaneous generation, joint multimodal modeling introduces unique challenges in architecture, data, and training. Moreover, the closed-source nature of existing systems limits progress in the field. In this work, we introduce MOVA (MOSS Video and Audio), an open-source model capable of generating high-quality, synchronized audio-visual content, including realistic lip-synced speech, environment-aware sound effects, and content-aligned music. MOVA employs a Mixture-of-Experts (MoE) architecture, with a total of 32B parameters, of which 18B are active during inference. It supports IT2VA (Image-Text to Video-Audio) generation task. By releasing the model weights and code, we aim to advance research and foster a vibrant community of creators. The released codebase features comprehensive support for efficient inference, LoRA fine-tuning, and prompt enhancement.", "AI": {"tldr": "MOVA\u662f\u4e00\u4e2a\u5f00\u6e90\u7684\u97f3\u9891-\u89c6\u9891\u8054\u5408\u751f\u6210\u6a21\u578b\uff0c\u91c7\u7528\u6df7\u5408\u4e13\u5bb6\u67b6\u6784\uff0c\u80fd\u591f\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u540c\u6b65\u7684\u89c6\u542c\u5185\u5bb9\uff0c\u5305\u62ec\u5507\u8bed\u540c\u6b65\u7684\u8bed\u97f3\u3001\u73af\u5883\u611f\u77e5\u97f3\u6548\u548c\u5185\u5bb9\u5bf9\u9f50\u7684\u97f3\u4e50\u3002", "motivation": "\u5f53\u524d\u97f3\u9891-\u89c6\u9891\u751f\u6210\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u7ea7\u8054\u7ba1\u9053\uff0c\u5bfc\u81f4\u6210\u672c\u589e\u52a0\u3001\u9519\u8bef\u7d2f\u79ef\u548c\u8d28\u91cf\u4e0b\u964d\u3002\u73b0\u6709\u7cfb\u7edf\u5982Veo 3\u548cSora 2\u867d\u7136\u5f3a\u8c03\u540c\u65f6\u751f\u6210\u7684\u91cd\u8981\u6027\uff0c\u4f46\u8054\u5408\u591a\u6a21\u6001\u5efa\u6a21\u9762\u4e34\u67b6\u6784\u3001\u6570\u636e\u548c\u8bad\u7ec3\u65b9\u9762\u7684\u6311\u6218\uff0c\u4e14\u95ed\u6e90\u6027\u8d28\u9650\u5236\u4e86\u8be5\u9886\u57df\u7684\u53d1\u5c55\u3002", "method": "\u91c7\u7528\u6df7\u5408\u4e13\u5bb6\u67b6\u6784\uff0c\u603b\u53c2\u6570\u91cf320\u4ebf\uff0c\u63a8\u7406\u65f6\u6fc0\u6d3b180\u4ebf\u53c2\u6570\u3002\u652f\u6301\u56fe\u50cf-\u6587\u672c\u5230\u89c6\u9891-\u97f3\u9891\u7684\u751f\u6210\u4efb\u52a1\uff0c\u5177\u5907\u9ad8\u6548\u63a8\u7406\u3001LoRA\u5fae\u8c03\u548c\u63d0\u793a\u589e\u5f3a\u7b49\u529f\u80fd\u3002", "result": "\u5f00\u53d1\u51fa\u80fd\u591f\u751f\u6210\u9ad8\u8d28\u91cf\u540c\u6b65\u89c6\u542c\u5185\u5bb9\u7684\u5f00\u6e90\u6a21\u578b\uff0c\u5305\u62ec\u771f\u5b9e\u7684\u5507\u8bed\u540c\u6b65\u8bed\u97f3\u3001\u73af\u5883\u611f\u77e5\u97f3\u6548\u548c\u5185\u5bb9\u5bf9\u9f50\u97f3\u4e50\u3002\u901a\u8fc7\u5f00\u6e90\u6a21\u578b\u6743\u91cd\u548c\u4ee3\u7801\u4fc3\u8fdb\u7814\u7a76\u548c\u521b\u4f5c\u8005\u793e\u533a\u53d1\u5c55\u3002", "conclusion": "MOVA\u4f5c\u4e3a\u5f00\u6e90\u97f3\u9891-\u89c6\u9891\u8054\u5408\u751f\u6210\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u901a\u8fc7\u6df7\u5408\u4e13\u5bb6\u67b6\u6784\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u540c\u6b65\u89c6\u542c\u5185\u5bb9\u751f\u6210\uff0c\u6709\u671b\u63a8\u52a8\u8be5\u9886\u57df\u7684\u7814\u7a76\u548c\u5e94\u7528\u53d1\u5c55\u3002"}}
{"id": "2602.08797", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08797", "abs": "https://arxiv.org/abs/2602.08797", "authors": ["Jiaming Liu", "Cheng Ding", "Daoqiang Zhang"], "title": "Addressing data annotation scarcity in Brain Tumor Segmentation on 3D MRI scan Using a Semi-Supervised Teacher-Student Framework", "comment": "10 pages, 7 figures. Submitted to IEEE Journal of Biomedical and Health Informatics (JBHI)", "summary": "Accurate brain tumor segmentation from MRI is limited by expensive annotations and data heterogeneity across scanners and sites. We propose a semi-supervised teacher-student framework that combines an uncertainty-aware pseudo-labeling teacher with a progressive, confidence-based curriculum for the student. The teacher produces probabilistic masks and per-pixel uncertainty; unlabeled scans are ranked by image-level confidence and introduced in stages, while a dual-loss objective trains the student to learn from high-confidence regions and unlearn low-confidence ones. Agreement-based refinement further improves pseudo-label quality. On BraTS 2021, validation DSC increased from 0.393 (10% data) to 0.872 (100%), with the largest gains in early stages, demonstrating data efficiency. The teacher reached a validation DSC of 0.922, and the student surpassed the teacher on tumor subregions (e.g., NCR/NET 0.797 and Edema 0.980); notably, the student recovered the Enhancing class (DSC 0.620) where the teacher failed. These results show that confidence-driven curricula and selective unlearning provide robust segmentation under limited supervision and noisy pseudo-labels.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u534a\u76d1\u7763\u5e08\u751f\u6846\u67b6\uff0c\u7ed3\u5408\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u4f2a\u6807\u7b7e\u6559\u5e08\u548c\u57fa\u4e8e\u7f6e\u4fe1\u5ea6\u7684\u6e10\u8fdb\u8bfe\u7a0b\u5b66\u4e60\uff0c\u7528\u4e8eMRI\u8111\u80bf\u7624\u5206\u5272\uff0c\u5728\u6709\u9650\u6807\u6ce8\u4e0b\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "MRI\u8111\u80bf\u7624\u5206\u5272\u9762\u4e34\u6807\u6ce8\u6210\u672c\u9ad8\u548c\u6570\u636e\u5f02\u8d28\u6027\uff08\u4e0d\u540c\u626b\u63cf\u4eea\u548c\u7ad9\u70b9\uff09\u7684\u6311\u6218\uff0c\u9700\u8981\u5f00\u53d1\u5728\u6709\u9650\u76d1\u7763\u4e0b\u9c81\u68d2\u7684\u5206\u5272\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u5e08\u751f\u6846\u67b6\uff1a\u6559\u5e08\u6a21\u578b\u751f\u6210\u6982\u7387\u63a9\u7801\u548c\u9010\u50cf\u7d20\u4e0d\u786e\u5b9a\u6027\uff1b\u57fa\u4e8e\u56fe\u50cf\u7ea7\u7f6e\u4fe1\u5ea6\u5bf9\u672a\u6807\u6ce8\u626b\u63cf\u6392\u5e8f\u5e76\u5206\u9636\u6bb5\u5f15\u5165\uff1b\u5b66\u751f\u6a21\u578b\u4f7f\u7528\u53cc\u635f\u5931\u76ee\u6807\uff0c\u5b66\u4e60\u9ad8\u7f6e\u4fe1\u533a\u57df\u5e76\"\u9057\u5fd8\"\u4f4e\u7f6e\u4fe1\u533a\u57df\uff1b\u901a\u8fc7\u4e00\u81f4\u6027\u7cbe\u70bc\u63d0\u5347\u4f2a\u6807\u7b7e\u8d28\u91cf\u3002", "result": "\u5728BraTS 2021\u4e0a\uff0c\u9a8c\u8bc1\u96c6DSC\u4ece0.393\uff0810%\u6570\u636e\uff09\u63d0\u5347\u52300.872\uff08100%\uff09\uff0c\u65e9\u671f\u9636\u6bb5\u63d0\u5347\u6700\u5927\uff1b\u6559\u5e08\u6a21\u578bDSC\u8fbe0.922\uff0c\u5b66\u751f\u5728\u80bf\u7624\u4e9a\u533a\u57df\u8d85\u8d8a\u6559\u5e08\uff08NCR/NET 0.797\uff0cEdema 0.980\uff09\uff0c\u7279\u522b\u662f\u5728\u6559\u5e08\u5931\u8d25\u7684\u589e\u5f3a\u7c7b\u522b\u4e0a\u6062\u590d\u6027\u80fd\uff08DSC 0.620\uff09\u3002", "conclusion": "\u7f6e\u4fe1\u5ea6\u9a71\u52a8\u7684\u8bfe\u7a0b\u5b66\u4e60\u548c\u9009\u62e9\u6027\u9057\u5fd8\u80fd\u591f\u5728\u6709\u9650\u76d1\u7763\u548c\u566a\u58f0\u4f2a\u6807\u7b7e\u4e0b\u63d0\u4f9b\u9c81\u68d2\u7684\u5206\u5272\u6027\u80fd\uff0c\u5c55\u793a\u4e86\u6570\u636e\u6548\u7387\u548c\u6a21\u578b\u6539\u8fdb\u6f5c\u529b\u3002"}}
{"id": "2602.08820", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.08820", "abs": "https://arxiv.org/abs/2602.08820", "authors": ["Hao Yang", "Zhiyu Tan", "Jia Gong", "Luozheng Qin", "Hesen Chen", "Xiaomeng Yang", "Yuqing Sun", "Yuetan Lin", "Mengping Yang", "Hao Li"], "title": "Omni-Video 2: Scaling MLLM-Conditioned Diffusion for Unified Video Generation and Editing", "comment": "Technical Report, Project: https://howellyoung-s.github.io/Omni-Video2-project/", "summary": "We present Omni-Video 2, a scalable and computationally efficient model that connects pretrained multimodal large-language models (MLLMs) with video diffusion models for unified video generation and editing. Our key idea is to exploit the understanding and reasoning capabilities of MLLMs to produce explicit target captions to interpret user instructions. In this way, the rich contextual representations from the understanding model are directly used to guide the generative process, thereby improving performance on complex and compositional editing. Moreover, a lightweight adapter is developed to inject multimodal conditional tokens into pretrained text-to-video diffusion models, allowing maximum reuse of their powerful generative priors in a parameter-efficient manner. Benefiting from these designs, we scale up Omni-Video 2 to a 14B video diffusion model on meticulously curated training data with quality, supporting high quality text-to-video generation and various video editing tasks such as object removal, addition, background change, complex motion editing, \\emph{etc.} We evaluate the performance of Omni-Video 2 on the FiVE benchmark for fine-grained video editing and the VBench benchmark for text-to-video generation. The results demonstrate its superior ability to follow complex compositional instructions in video editing, while also achieving competitive or superior quality in video generation tasks.", "AI": {"tldr": "Omni-Video 2\u662f\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u9ad8\u6548\u89c6\u9891\u751f\u6210\u4e0e\u7f16\u8f91\u6a21\u578b\uff0c\u901a\u8fc7\u8fde\u63a5\u9884\u8bad\u7ec3\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u548c\u89c6\u9891\u6269\u6563\u6a21\u578b\uff0c\u5229\u7528MLLM\u7684\u7406\u89e3\u80fd\u529b\u751f\u6210\u660e\u786e\u7684\u76ee\u6807\u63cf\u8ff0\u6765\u6307\u5bfc\u751f\u6210\u8fc7\u7a0b\uff0c\u5b9e\u73b0\u9ad8\u8d28\u91cf\u7684\u89c6\u9891\u751f\u6210\u548c\u590d\u6742\u7f16\u8f91\u4efb\u52a1\u3002", "motivation": "\u5f53\u524d\u89c6\u9891\u751f\u6210\u548c\u7f16\u8f91\u9762\u4e34\u590d\u6742\u7ec4\u5408\u6307\u4ee4\u7406\u89e3\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u9700\u8981\u66f4\u6709\u6548\u5730\u5229\u7528\u591a\u6a21\u6001\u7406\u89e3\u80fd\u529b\u6765\u6307\u5bfc\u751f\u6210\u8fc7\u7a0b\uff0c\u540c\u65f6\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u548c\u6a21\u578b\u53ef\u6269\u5c55\u6027\u3002", "method": "1. \u5229\u7528\u9884\u8bad\u7ec3\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u7406\u89e3\u548c\u63a8\u7406\u80fd\u529b\u751f\u6210\u660e\u786e\u7684\u76ee\u6807\u63cf\u8ff0\u6765\u89e3\u91ca\u7528\u6237\u6307\u4ee4\uff1b2. \u5f00\u53d1\u8f7b\u91cf\u7ea7\u9002\u914d\u5668\u5c06\u591a\u6a21\u6001\u6761\u4ef6\u6807\u8bb0\u6ce8\u5165\u9884\u8bad\u7ec3\u6587\u672c\u5230\u89c6\u9891\u6269\u6563\u6a21\u578b\uff1b3. \u5728\u7cbe\u5fc3\u7b56\u5212\u7684\u8bad\u7ec3\u6570\u636e\u4e0a\u6269\u5c55\u523014B\u53c2\u6570\u89c4\u6a21\u3002", "result": "\u5728FiVE\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5c55\u793a\u4e86\u5904\u7406\u590d\u6742\u7ec4\u5408\u6307\u4ee4\u7684\u5353\u8d8a\u80fd\u529b\uff0c\u5728VBench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u7ade\u4e89\u6027\u6216\u66f4\u4f18\u7684\u89c6\u9891\u751f\u6210\u8d28\u91cf\uff0c\u652f\u6301\u9ad8\u8d28\u91cf\u6587\u672c\u5230\u89c6\u9891\u751f\u6210\u548c\u5404\u79cd\u89c6\u9891\u7f16\u8f91\u4efb\u52a1\u3002", "conclusion": "Omni-Video 2\u901a\u8fc7\u6709\u6548\u8fde\u63a5\u591a\u6a21\u6001\u7406\u89e3\u548c\u751f\u6210\u6a21\u578b\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u3001\u53ef\u6269\u5c55\u7684\u89c6\u9891\u751f\u6210\u4e0e\u7f16\u8f91\uff0c\u5728\u590d\u6742\u7ec4\u5408\u6307\u4ee4\u7406\u89e3\u548c\u751f\u6210\u8d28\u91cf\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2602.08822", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.08822", "abs": "https://arxiv.org/abs/2602.08822", "authors": ["Yao Pu", "Yiming Shi", "Zhenxi Zhang", "Peixin Yu", "Yitao Zhuang", "Xiang Wang", "Hongzhao Chen", "Jing Cai", "Ge Ren"], "title": "Any-to-All MRI Synthesis: A Unified Foundation Model for Nasopharyngeal Carcinoma and Its Downstream Applications", "comment": null, "summary": "Magnetic resonance imaging (MRI) is essential for nasopharyngeal carcinoma (NPC) radiotherapy (RT), but practical constraints, such as patient discomfort, long scan times, and high costs often lead to incomplete modalities in clinical practice, compromising RT planning accuracy. Traditional MRI synthesis methods are modality-specific, limited in anatomical adaptability, and lack clinical interpretability-failing to meet NPC's RT needs. Here, we developed a unified foundation model integrating contrastive visual representation learning and vision-language alignment (VLA) to enable any-to-all MRI synthesis. The model uses a contrastive encoder for modality-invariant representations and a CLIP-based text-informed decoder for semantically consistent synthesis, supporting any-to-all MRI synthesis via one unified foundation model. Trained on 40,825 images from 13 institutions, it achieves consistently high performance (average SSIM 0.90, PSNR 27) across 26 internal/external validation sites (15,748 images), with superior synthesis fidelity and robustness to noise and domain shifts. Meanwhile, its unified representation enhances downstream RT-relevant tasks (e.g., segmentation). This work advances digital medicine solutions for NPC care by leveraging foundation models to bridge technical synthesis and clinical utility.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u57fa\u7840\u6a21\u578b\uff0c\u901a\u8fc7\u5bf9\u6bd4\u89c6\u89c9\u8868\u793a\u5b66\u4e60\u548c\u89c6\u89c9\u8bed\u8a00\u5bf9\u9f50\uff0c\u5b9e\u73b0\u4efb\u610f\u5230\u6240\u6709MRI\u5e8f\u5217\u5408\u6210\uff0c\u63d0\u5347\u9f3b\u54bd\u764c\u653e\u7597\u89c4\u5212\u51c6\u786e\u6027\u3002", "motivation": "\u4e34\u5e8a\u5b9e\u8df5\u4e2d\uff0c\u7531\u4e8e\u60a3\u8005\u4e0d\u9002\u3001\u626b\u63cf\u65f6\u95f4\u957f\u548c\u6210\u672c\u9ad8\u7b49\u9650\u5236\uff0c\u9f3b\u54bd\u764c\u653e\u7597\u5e38\u9762\u4e34MRI\u6a21\u6001\u4e0d\u5b8c\u6574\u7684\u95ee\u9898\uff0c\u5f71\u54cd\u653e\u7597\u89c4\u5212\u51c6\u786e\u6027\u3002\u4f20\u7edfMRI\u5408\u6210\u65b9\u6cd5\u5b58\u5728\u6a21\u6001\u7279\u5f02\u6027\u3001\u89e3\u5256\u9002\u5e94\u6027\u6709\u9650\u548c\u7f3a\u4e4f\u4e34\u5e8a\u53ef\u89e3\u91ca\u6027\u7b49\u5c40\u9650\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u57fa\u7840\u6a21\u578b\uff0c\u6574\u5408\u5bf9\u6bd4\u89c6\u89c9\u8868\u793a\u5b66\u4e60\u548c\u89c6\u89c9\u8bed\u8a00\u5bf9\u9f50\u3002\u4f7f\u7528\u5bf9\u6bd4\u7f16\u7801\u5668\u83b7\u53d6\u6a21\u6001\u4e0d\u53d8\u8868\u793a\uff0c\u57fa\u4e8eCLIP\u7684\u6587\u672c\u4fe1\u606f\u89e3\u7801\u5668\u5b9e\u73b0\u8bed\u4e49\u4e00\u81f4\u7684\u5408\u6210\uff0c\u652f\u6301\u901a\u8fc7\u5355\u4e00\u6a21\u578b\u5b9e\u73b0\u4efb\u610f\u5230\u6240\u6709MRI\u5e8f\u5217\u5408\u6210\u3002", "result": "\u572813\u4e2a\u673a\u6784\u768440,825\u5f20\u56fe\u50cf\u4e0a\u8bad\u7ec3\uff0c\u572826\u4e2a\u5185\u90e8/\u5916\u90e8\u9a8c\u8bc1\u7ad9\u70b9\uff0815,748\u5f20\u56fe\u50cf\uff09\u4e0a\u83b7\u5f97\u4e00\u81f4\u9ad8\u6027\u80fd\uff08\u5e73\u5747SSIM 0.90\uff0cPSNR 27\uff09\uff0c\u5408\u6210\u4fdd\u771f\u5ea6\u9ad8\uff0c\u5bf9\u566a\u58f0\u548c\u57df\u504f\u79fb\u5177\u6709\u9c81\u68d2\u6027\u3002\u7edf\u4e00\u8868\u793a\u8fd8\u589e\u5f3a\u4e86\u653e\u7597\u76f8\u5173\u4e0b\u6e38\u4efb\u52a1\uff08\u5982\u5206\u5272\uff09\u7684\u6027\u80fd\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u901a\u8fc7\u5229\u7528\u57fa\u7840\u6a21\u578b\u6865\u63a5\u6280\u672f\u5408\u6210\u548c\u4e34\u5e8a\u5b9e\u7528\u6027\uff0c\u63a8\u8fdb\u4e86\u9f3b\u54bd\u764c\u62a4\u7406\u7684\u6570\u5b57\u533b\u5b66\u89e3\u51b3\u65b9\u6848\uff0c\u4e3a\u4e34\u5e8a\u5b9e\u8df5\u63d0\u4f9b\u4e86\u66f4\u5b8c\u6574\u3001\u51c6\u786e\u7684MRI\u4fe1\u606f\u652f\u6301\u3002"}}
{"id": "2602.08828", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.08828", "abs": "https://arxiv.org/abs/2602.08828", "authors": ["Hao Tan", "Jun Lan", "Senyuan Shi", "Zichang Tan", "Zijian Yu", "Huijia Zhu", "Weiqiang Wang", "Jun Wan", "Zhen Lei"], "title": "VideoVeritas: AI-Generated Video Detection via Perception Pretext Reinforcement Learning", "comment": "Project: https://github.com/EricTan7/VideoVeritas", "summary": "The growing capability of video generation poses escalating security risks, making reliable detection increasingly essential. In this paper, we introduce VideoVeritas, a framework that integrates fine-grained perception and fact-based reasoning. We observe that while current multi-modal large language models (MLLMs) exhibit strong reasoning capacity, their granular perception ability remains limited. To mitigate this, we introduce Joint Preference Alignment and Perception Pretext Reinforcement Learning (PPRL). Specifically, rather than directly optimizing for detection task, we adopt general spatiotemporal grounding and self-supervised object counting in the RL stage, enhancing detection performance with simple perception pretext tasks. To facilitate robust evaluation, we further introduce MintVid, a light yet high-quality dataset containing 3K videos from 9 state-of-the-art generators, along with a real-world collected subset that has factual errors in content. Experimental results demonstrate that existing methods tend to bias towards either superficial reasoning or mechanical analysis, while VideoVeritas achieves more balanced performance across diverse benchmarks.", "AI": {"tldr": "VideoVeritas\u6846\u67b6\u901a\u8fc7\u8054\u5408\u504f\u597d\u5bf9\u9f50\u548c\u611f\u77e5\u9884\u6587\u672c\u5f3a\u5316\u5b66\u4e60\uff0c\u7ed3\u5408\u7ec6\u7c92\u5ea6\u611f\u77e5\u4e0e\u4e8b\u5b9e\u63a8\u7406\uff0c\u63d0\u5347\u89c6\u9891\u751f\u6210\u5185\u5bb9\u7684\u68c0\u6d4b\u80fd\u529b\uff0c\u5e76\u5728\u65b0\u6570\u636e\u96c6MintVid\u4e0a\u9a8c\u8bc1\u4e86\u5176\u5e73\u8861\u6027\u80fd\u3002", "motivation": "\u89c6\u9891\u751f\u6210\u80fd\u529b\u7684\u4e0d\u65ad\u589e\u5f3a\u5e26\u6765\u4e86\u65e5\u76ca\u589e\u957f\u7684\u5b89\u5168\u98ce\u9669\uff0c\u9700\u8981\u53ef\u9760\u7684\u68c0\u6d4b\u65b9\u6cd5\u3002\u5f53\u524d\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u867d\u7136\u63a8\u7406\u80fd\u529b\u5f3a\uff0c\u4f46\u7ec6\u7c92\u5ea6\u611f\u77e5\u80fd\u529b\u6709\u9650\uff0c\u9700\u8981\u6539\u8fdb\u3002", "method": "\u63d0\u51faVideoVeritas\u6846\u67b6\uff0c\u96c6\u6210\u7ec6\u7c92\u5ea6\u611f\u77e5\u548c\u57fa\u4e8e\u4e8b\u5b9e\u7684\u63a8\u7406\u3002\u5f15\u5165\u8054\u5408\u504f\u597d\u5bf9\u9f50\u548c\u611f\u77e5\u9884\u6587\u672c\u5f3a\u5316\u5b66\u4e60(PPRL)\uff0c\u901a\u8fc7\u901a\u7528\u65f6\u7a7a\u5b9a\u4f4d\u548c\u81ea\u76d1\u7763\u7269\u4f53\u8ba1\u6570\u7b49\u611f\u77e5\u9884\u6587\u672c\u4efb\u52a1\u589e\u5f3a\u68c0\u6d4b\u6027\u80fd\u3002\u521b\u5efaMintVid\u6570\u636e\u96c6\uff0c\u5305\u542b9\u4e2a\u6700\u5148\u8fdb\u751f\u6210\u5668\u76843K\u89c6\u9891\u548c\u5177\u6709\u4e8b\u5b9e\u9519\u8bef\u7684\u5185\u5bb9\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u73b0\u6709\u65b9\u6cd5\u503e\u5411\u4e8e\u504f\u5411\u8868\u9762\u63a8\u7406\u6216\u673a\u68b0\u5206\u6790\uff0c\u800cVideoVeritas\u5728\u591a\u6837\u5316\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u66f4\u5e73\u8861\u7684\u6027\u80fd\u8868\u73b0\u3002", "conclusion": "VideoVeritas\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u7ec6\u7c92\u5ea6\u611f\u77e5\u548c\u4e8b\u5b9e\u63a8\u7406\uff0c\u6709\u6548\u63d0\u5347\u4e86\u89c6\u9891\u751f\u6210\u5185\u5bb9\u7684\u68c0\u6d4b\u80fd\u529b\uff0c\u4e3a\u89e3\u51b3\u89c6\u9891\u751f\u6210\u5b89\u5168\u98ce\u9669\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.08861", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.08861", "abs": "https://arxiv.org/abs/2602.08861", "authors": ["Xiangtian Zheng", "Zishuo Wang", "Yuxin Peng"], "title": "TiFRe: Text-guided Video Frame Reduction for Efficient Video Multi-modal Large Language Models", "comment": null, "summary": "With the rapid development of Large Language Models (LLMs), Video Multi-Modal Large Language Models (Video MLLMs) have achieved remarkable performance in video-language tasks such as video understanding and question answering. However, Video MLLMs face high computational costs, particularly in processing numerous video frames as input, which leads to significant attention computation overhead. A straightforward approach to reduce computational costs is to decrease the number of input video frames. However, simply selecting key frames at a fixed frame rate (FPS) often overlooks valuable information in non-key frames, resulting in notable performance degradation. To address this, we propose Text-guided Video Frame Reduction (TiFRe), a framework that reduces input frames while preserving essential video information. TiFRe uses a Text-guided Frame Sampling (TFS) strategy to select key frames based on user input, which is processed by an LLM to generate a CLIP-style prompt. Pre-trained CLIP encoders calculate the semantic similarity between the prompt and each frame, selecting the most relevant frames as key frames. To preserve video semantics, TiFRe employs a Frame Matching and Merging (FMM) mechanism, which integrates non-key frame information into the selected key frames, minimizing information loss. Experiments show that TiFRe effectively reduces computational costs while improving performance on video-language tasks.", "AI": {"tldr": "TiFRe\u662f\u4e00\u4e2a\u6587\u672c\u5f15\u5bfc\u7684\u89c6\u9891\u5e27\u7f29\u51cf\u6846\u67b6\uff0c\u901a\u8fc7\u667a\u80fd\u9009\u62e9\u5173\u952e\u5e27\u5e76\u5408\u5e76\u975e\u5173\u952e\u5e27\u4fe1\u606f\u6765\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u89c6\u9891\u8bed\u4e49\u5b8c\u6574\u6027\u3002", "motivation": "\u89c6\u9891\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u5927\u91cf\u89c6\u9891\u5e27\u65f6\u9762\u4e34\u9ad8\u8ba1\u7b97\u6210\u672c\u95ee\u9898\uff0c\u4f20\u7edf\u56fa\u5b9a\u5e27\u7387\u7684\u5173\u952e\u5e27\u9009\u62e9\u65b9\u6cd5\u4f1a\u4e22\u5931\u975e\u5173\u952e\u5e27\u7684\u91cd\u8981\u4fe1\u606f\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002", "method": "\u63d0\u51fa\u6587\u672c\u5f15\u5bfc\u7684\u89c6\u9891\u5e27\u7f29\u51cf\u6846\u67b6\uff0c\u5305\u542b\u6587\u672c\u5f15\u5bfc\u7684\u5e27\u91c7\u6837\u7b56\u7565\uff08\u4f7f\u7528LLM\u751f\u6210CLIP\u98ce\u683c\u63d0\u793a\uff0c\u901a\u8fc7CLIP\u7f16\u7801\u5668\u8ba1\u7b97\u8bed\u4e49\u76f8\u4f3c\u5ea6\u9009\u62e9\u5173\u952e\u5e27\uff09\u548c\u5e27\u5339\u914d\u4e0e\u5408\u5e76\u673a\u5236\uff08\u5c06\u975e\u5173\u952e\u5e27\u4fe1\u606f\u6574\u5408\u5230\u5173\u952e\u5e27\u4e2d\uff09\u3002", "result": "\u5b9e\u9a8c\u8868\u660eTiFRe\u80fd\u6709\u6548\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\uff0c\u540c\u65f6\u5728\u89c6\u9891\u8bed\u8a00\u4efb\u52a1\u4e0a\u63d0\u5347\u6027\u80fd\u3002", "conclusion": "TiFRe\u901a\u8fc7\u667a\u80fd\u5e27\u9009\u62e9\u548c\u8bed\u4e49\u4fdd\u7559\u673a\u5236\uff0c\u89e3\u51b3\u4e86\u89c6\u9891MLLMs\u8ba1\u7b97\u6210\u672c\u9ad8\u7684\u95ee\u9898\uff0c\u5728\u51cf\u5c11\u8f93\u5165\u5e27\u6570\u7684\u540c\u65f6\u4fdd\u6301\u4e86\u89c6\u9891\u7406\u89e3\u80fd\u529b\u3002"}}
{"id": "2602.08909", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.08909", "abs": "https://arxiv.org/abs/2602.08909", "authors": ["Zhendong Wang", "Cihan Ruan", "Jingchuan Xiao", "Chuqing Shi", "Wei Jiang", "Wei Wang", "Wenjie Liu", "Nam Ling"], "title": "Analysis of Converged 3D Gaussian Splatting Solutions: Density Effects and Prediction Limit", "comment": null, "summary": "We investigate what structure emerges in 3D Gaussian Splatting (3DGS) solutions from standard multi-view optimization. We term these Rendering-Optimal References (RORs) and analyze their statistical properties, revealing stable patterns: mixture-structured scales and bimodal radiance across diverse scenes. To understand what determines these parameters, we apply learnability probes by training predictors to reconstruct RORs from point clouds without rendering supervision. Our analysis uncovers fundamental density-stratification. Dense regions exhibit geometry-correlated parameters amenable to render-free prediction, while sparse regions show systematic failure across architectures. We formalize this through variance decomposition, demonstrating that visibility heterogeneity creates covariance-dominated coupling between geometric and appearance parameters in sparse regions. This reveals the dual character of RORs: geometric primitives where point clouds suffice, and view synthesis primitives where multi-view constraints are essential. We provide density-aware strategies that improve training robustness and discuss architectural implications for systems that adaptively balance feed-forward prediction and rendering-based refinement.", "AI": {"tldr": "\u8bba\u6587\u5206\u6790\u4e863D\u9ad8\u65af\u6cfc\u6e85\uff083DGS\uff09\u6807\u51c6\u591a\u89c6\u56fe\u4f18\u5316\u4e2d\u51fa\u73b0\u7684\u7ed3\u6784\u6a21\u5f0f\uff0c\u79f0\u4e3a\u6e32\u67d3\u6700\u4f18\u53c2\u8003\uff08RORs\uff09\uff0c\u63ed\u793a\u4e86\u5176\u7edf\u8ba1\u7279\u6027\uff0c\u5e76\u901a\u8fc7\u53ef\u5b66\u4e60\u6027\u63a2\u9488\u53d1\u73b0\u5bc6\u5ea6\u5206\u5c42\u73b0\u8c61\uff1a\u5bc6\u96c6\u533a\u57df\u53c2\u6570\u53ef\u9884\u6d4b\uff0c\u7a00\u758f\u533a\u57df\u9700\u8981\u591a\u89c6\u56fe\u7ea6\u675f\u3002", "motivation": "\u7814\u7a763D\u9ad8\u65af\u6cfc\u6e85\uff083DGS\uff09\u5728\u591a\u89c6\u56fe\u4f18\u5316\u4e2d\u5f62\u6210\u7684\u7ed3\u6784\u6a21\u5f0f\uff0c\u7406\u89e3\u8fd9\u4e9b\u6e32\u67d3\u6700\u4f18\u53c2\u8003\uff08RORs\uff09\u7684\u7edf\u8ba1\u7279\u6027\uff0c\u63a2\u7a76\u51b3\u5b9a\u8fd9\u4e9b\u53c2\u6570\u7684\u56e0\u7d20\uff0c\u4e3a\u6539\u8fdb\u8bad\u7ec3\u9c81\u68d2\u6027\u548c\u7cfb\u7edf\u67b6\u6784\u63d0\u4f9b\u7406\u8bba\u57fa\u7840\u3002", "method": "1) \u5206\u67903DGS\u6807\u51c6\u591a\u89c6\u56fe\u4f18\u5316\u4ea7\u751f\u7684RORs\u7edf\u8ba1\u7279\u6027\uff1b2) \u4f7f\u7528\u53ef\u5b66\u4e60\u6027\u63a2\u9488\u8bad\u7ec3\u9884\u6d4b\u5668\u4ece\u70b9\u4e91\u91cd\u5efaRORs\uff1b3) \u901a\u8fc7\u65b9\u5dee\u5206\u89e3\u5206\u6790\u5bc6\u5ea6\u5206\u5c42\u73b0\u8c61\uff1b4) \u63d0\u51fa\u5bc6\u5ea6\u611f\u77e5\u7b56\u7565\u6539\u8fdb\u8bad\u7ec3\u9c81\u68d2\u6027\u3002", "result": "\u53d1\u73b0RORs\u5177\u6709\u7a33\u5b9a\u7684\u7edf\u8ba1\u6a21\u5f0f\uff1a\u6df7\u5408\u7ed3\u6784\u7684\u5c3a\u5ea6\u548c\u53cc\u5cf0\u8f90\u5c04\u5206\u5e03\uff1b\u63ed\u793a\u5bc6\u5ea6\u5206\u5c42\u73b0\u8c61\uff1a\u5bc6\u96c6\u533a\u57df\u53c2\u6570\u4e0e\u51e0\u4f55\u76f8\u5173\u53ef\u9884\u6d4b\uff0c\u7a00\u758f\u533a\u57df\u53c2\u6570\u53d7\u53ef\u89c1\u6027\u5f02\u8d28\u6027\u5f71\u54cd\u9700\u8981\u591a\u89c6\u56fe\u7ea6\u675f\uff1b\u901a\u8fc7\u65b9\u5dee\u5206\u89e3\u8bc1\u660e\u7a00\u758f\u533a\u57df\u51e0\u4f55\u4e0e\u5916\u89c2\u53c2\u6570\u5b58\u5728\u534f\u65b9\u5dee\u4e3b\u5bfc\u7684\u8026\u5408\u3002", "conclusion": "RORs\u5177\u6709\u53cc\u91cd\u7279\u6027\uff1a\u5728\u5bc6\u96c6\u533a\u57df\u8868\u73b0\u4e3a\u51e0\u4f55\u57fa\u5143\uff08\u70b9\u4e91\u8db3\u591f\u9884\u6d4b\uff09\uff0c\u5728\u7a00\u758f\u533a\u57df\u8868\u73b0\u4e3a\u89c6\u56fe\u5408\u6210\u57fa\u5143\uff08\u9700\u8981\u591a\u89c6\u56fe\u7ea6\u675f\uff09\uff1b\u8fd9\u4e3a\u81ea\u9002\u5e94\u5e73\u8861\u524d\u9988\u9884\u6d4b\u548c\u6e32\u67d3\u4f18\u5316\u7684\u7cfb\u7edf\u67b6\u6784\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u3002"}}
{"id": "2602.08958", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.08958", "abs": "https://arxiv.org/abs/2602.08958", "authors": ["Weihan Luo", "Lily Goli", "Sherwin Bahmani", "Felix Taubner", "Andrea Tagliasacchi", "David B. Lindell"], "title": "Grow with the Flow: 4D Reconstruction of Growing Plants with Gaussian Flow Fields", "comment": "Project page: https://weihanluo.ca/growflow/", "summary": "Modeling the time-varying 3D appearance of plants during their growth poses unique challenges: unlike many dynamic scenes, plants generate new geometry over time as they expand, branch, and differentiate. Recent motion modeling techniques are ill-suited to this problem setting. For example, deformation fields cannot introduce new geometry, and 4D Gaussian splatting constrains motion to a linear trajectory in space and time and cannot track the same set of Gaussians over time. Here, we introduce a 3D Gaussian flow field representation that models plant growth as a time-varying derivative over Gaussian parameters -- position, scale, orientation, color, and opacity -- enabling nonlinear and continuous-time growth dynamics. To initialize a sufficient set of Gaussian primitives, we reconstruct the mature plant and learn a process of reverse growth, effectively simulating the plant's developmental history in reverse. Our approach achieves superior image quality and geometric accuracy compared to prior methods on multi-view timelapse datasets of plant growth, providing a new approach for appearance modeling of growing 3D structures.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u690d\u7269\u751f\u957f\u5efa\u6a21\u76843D\u9ad8\u65af\u6d41\u573a\u8868\u793a\u65b9\u6cd5\uff0c\u901a\u8fc7\u53cd\u5411\u751f\u957f\u8fc7\u7a0b\u521d\u59cb\u5316\u9ad8\u65af\u57fa\u5143\uff0c\u5b9e\u73b0\u4e86\u8fde\u7eed\u65f6\u95f4\u7684\u975e\u7ebf\u6027\u751f\u957f\u52a8\u6001\u5efa\u6a21\u3002", "motivation": "\u690d\u7269\u751f\u957f\u5efa\u6a21\u9762\u4e34\u72ec\u7279\u6311\u6218\uff1a\u690d\u7269\u4f1a\u968f\u65f6\u95f4\u4ea7\u751f\u65b0\u51e0\u4f55\u7ed3\u6784\uff0c\u800c\u73b0\u6709\u52a8\u6001\u573a\u666f\u5efa\u6a21\u65b9\u6cd5\uff08\u5982\u53d8\u5f62\u573a\u30014D\u9ad8\u65af\u6e85\u5c04\uff09\u65e0\u6cd5\u5904\u7406\u8fd9\u79cd\u51e0\u4f55\u751f\u6210\u95ee\u9898\uff0c\u9700\u8981\u4e13\u95e8\u9488\u5bf9\u690d\u7269\u751f\u957f\u7279\u6027\u7684\u65b0\u8868\u793a\u65b9\u6cd5\u3002", "method": "\u5f15\u51653D\u9ad8\u65af\u6d41\u573a\u8868\u793a\uff0c\u5c06\u690d\u7269\u751f\u957f\u5efa\u6a21\u4e3a\u9ad8\u65af\u53c2\u6570\uff08\u4f4d\u7f6e\u3001\u5c3a\u5ea6\u3001\u65b9\u5411\u3001\u989c\u8272\u3001\u4e0d\u900f\u660e\u5ea6\uff09\u968f\u65f6\u95f4\u53d8\u5316\u7684\u5bfc\u6570\uff1b\u901a\u8fc7\u91cd\u5efa\u6210\u719f\u690d\u7269\u5e76\u5b66\u4e60\u53cd\u5411\u751f\u957f\u8fc7\u7a0b\u6765\u521d\u59cb\u5316\u8db3\u591f\u7684\u9ad8\u65af\u57fa\u5143\u3002", "result": "\u5728\u690d\u7269\u751f\u957f\u7684\u591a\u89c6\u89d2\u65f6\u5e8f\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u76f8\u6bd4\u5148\u524d\u65b9\u6cd5\u83b7\u5f97\u4e86\u66f4\u4f18\u7684\u56fe\u50cf\u8d28\u91cf\u548c\u51e0\u4f55\u7cbe\u5ea6\uff0c\u4e3a\u751f\u957f\u4e2d\u76843D\u7ed3\u6784\u5916\u89c2\u5efa\u6a21\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u76843D\u9ad8\u65af\u6d41\u573a\u8868\u793a\u80fd\u591f\u6709\u6548\u5efa\u6a21\u690d\u7269\u751f\u957f\u8fc7\u7a0b\u4e2d\u7684\u975e\u7ebf\u6027\u8fde\u7eed\u65f6\u95f4\u52a8\u6001\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u5904\u7406\u51e0\u4f55\u751f\u6210\u7684\u95ee\u9898\uff0c\u4e3a\u690d\u7269\u751f\u957f\u5efa\u6a21\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2602.08961", "categories": ["cs.CV", "cs.AI", "cs.CG", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.08961", "abs": "https://arxiv.org/abs/2602.08961", "authors": ["Ruijie Zhu", "Jiahao Lu", "Wenbo Hu", "Xiaoguang Han", "Jianfei Cai", "Ying Shan", "Chuanxia Zheng"], "title": "MotionCrafter: Dense Geometry and Motion Reconstruction with a 4D VAE", "comment": "Project page: https://ruijiezhu94.github.io/MotionCrafter_Page", "summary": "We introduce MotionCrafter, a video diffusion-based framework that jointly reconstructs 4D geometry and estimates dense motion from a monocular video. The core of our method is a novel joint representation of dense 3D point maps and 3D scene flows in a shared coordinate system, and a novel 4D VAE to effectively learn this representation. Unlike prior work that forces the 3D value and latents to align strictly with RGB VAE latents-despite their fundamentally different distributions-we show that such alignment is unnecessary and leads to suboptimal performance. Instead, we introduce a new data normalization and VAE training strategy that better transfers diffusion priors and greatly improves reconstruction quality. Extensive experiments across multiple datasets demonstrate that MotionCrafter achieves state-of-the-art performance in both geometry reconstruction and dense scene flow estimation, delivering 38.64% and 25.0% improvements in geometry and motion reconstruction, respectively, all without any post-optimization. Project page: https://ruijiezhu94.github.io/MotionCrafter_Page", "AI": {"tldr": "MotionCrafter\u662f\u4e00\u4e2a\u57fa\u4e8e\u89c6\u9891\u6269\u6563\u7684\u6846\u67b6\uff0c\u80fd\u591f\u4ece\u5355\u76ee\u89c6\u9891\u4e2d\u8054\u5408\u91cd\u5efa4D\u51e0\u4f55\u5e76\u4f30\u8ba1\u5bc6\u96c6\u8fd0\u52a8\uff0c\u901a\u8fc7\u65b0\u7684\u8054\u5408\u8868\u793a\u548c4D VAE\u5b9e\u73b0\uff0c\u65e0\u9700\u540e\u4f18\u5316\u5373\u53ef\u8fbe\u5230SOTA\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5f3a\u52363D\u503c\u548c\u6f5c\u5728\u53d8\u91cf\u4e0eRGB VAE\u6f5c\u5728\u53d8\u91cf\u4e25\u683c\u5bf9\u9f50\uff0c\u5c3d\u7ba1\u5b83\u4eec\u7684\u5206\u5e03\u6839\u672c\u4e0d\u540c\uff0c\u8fd9\u5bfc\u81f4\u6b21\u4f18\u6027\u80fd\u3002\u9700\u8981\u4e00\u79cd\u66f4\u597d\u7684\u65b9\u6cd5\u6765\u4ece\u5355\u76ee\u89c6\u9891\u4e2d\u8054\u5408\u91cd\u5efa4D\u51e0\u4f55\u548c\u4f30\u8ba1\u5bc6\u96c6\u8fd0\u52a8\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u8054\u5408\u8868\u793a\u65b9\u6cd5\uff0c\u5c06\u5bc6\u96c63D\u70b9\u56fe\u548c3D\u573a\u666f\u6d41\u5728\u5171\u4eab\u5750\u6807\u7cfb\u4e2d\u8868\u793a\uff0c\u5e76\u8bbe\u8ba1\u4e86\u65b0\u76844D VAE\u6765\u6709\u6548\u5b66\u4e60\u8fd9\u79cd\u8868\u793a\u3002\u5f15\u5165\u4e86\u65b0\u7684\u6570\u636e\u5f52\u4e00\u5316\u548cVAE\u8bad\u7ec3\u7b56\u7565\uff0c\u66f4\u597d\u5730\u4f20\u9012\u6269\u6563\u5148\u9a8c\uff0c\u65e0\u9700\u5f3a\u5236\u4e0eRGB VAE\u6f5c\u5728\u53d8\u91cf\u5bf9\u9f50\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cMotionCrafter\u5728\u51e0\u4f55\u91cd\u5efa\u548c\u5bc6\u96c6\u573a\u666f\u6d41\u4f30\u8ba1\u65b9\u9762\u90fd\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u51e0\u4f55\u91cd\u5efa\u63d0\u534738.64%\uff0c\u8fd0\u52a8\u91cd\u5efa\u63d0\u534725.0%\uff0c\u4e14\u65e0\u9700\u4efb\u4f55\u540e\u4f18\u5316\u3002", "conclusion": "MotionCrafter\u901a\u8fc7\u521b\u65b0\u7684\u8054\u5408\u8868\u793a\u548c4D VAE\u8bbe\u8ba1\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u4ece\u5355\u76ee\u89c6\u9891\u4e2d\u8054\u5408\u91cd\u5efa4D\u51e0\u4f55\u548c\u4f30\u8ba1\u5bc6\u96c6\u8fd0\u52a8\u7684\u95ee\u9898\uff0c\u8bc1\u660e\u4e86\u5f3a\u5236\u4e0eRGB VAE\u6f5c\u5728\u53d8\u91cf\u5bf9\u9f50\u662f\u4e0d\u5fc5\u8981\u7684\uff0c\u5e76\u663e\u8457\u63d0\u5347\u4e86\u91cd\u5efa\u8d28\u91cf\u3002"}}
{"id": "2602.08971", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.08971", "abs": "https://arxiv.org/abs/2602.08971", "authors": ["Yu Shang", "Zhuohang Li", "Yiding Ma", "Weikang Su", "Xin Jin", "Ziyou Wang", "Xin Zhang", "Yinzhou Tang", "Chen Gao", "Wei Wu", "Xihui Liu", "Dhruv Shah", "Zhaoxiang Zhang", "Zhibo Chen", "Jun Zhu", "Yonghong Tian", "Tat-Seng Chua", "Wenwu Zhu", "Yong Li"], "title": "WorldArena: A Unified Benchmark for Evaluating Perception and Functional Utility of Embodied World Models", "comment": null, "summary": "While world models have emerged as a cornerstone of embodied intelligence by enabling agents to reason about environmental dynamics through action-conditioned prediction, their evaluation remains fragmented. Current evaluation of embodied world models has largely focused on perceptual fidelity (e.g., video generation quality), overlooking the functional utility of these models in downstream decision-making tasks. In this work, we introduce WorldArena, a unified benchmark designed to systematically evaluate embodied world models across both perceptual and functional dimensions. WorldArena assesses models through three dimensions: video perception quality, measured with 16 metrics across six sub-dimensions; embodied task functionality, which evaluates world models as data engines, policy evaluators, and action planners integrating with subjective human evaluation. Furthermore, we propose EWMScore, a holistic metric integrating multi-dimensional performance into a single interpretable index. Through extensive experiments on 14 representative models, we reveal a significant perception-functionality gap, showing that high visual quality does not necessarily translate into strong embodied task capability. WorldArena benchmark with the public leaderboard is released at https://worldarena.ai, providing a framework for tracking progress toward truly functional world models in embodied AI.", "AI": {"tldr": "WorldArena\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u7cfb\u7edf\u8bc4\u4f30\u5177\u8eab\u4e16\u754c\u6a21\u578b\u5728\u611f\u77e5\u548c\u529f\u80fd\u4e24\u4e2a\u7ef4\u5ea6\u4e0a\u7684\u8868\u73b0\uff0c\u63ed\u793a\u4e86\u611f\u77e5\u8d28\u91cf\u4e0e\u529f\u80fd\u6548\u7528\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "motivation": "\u5f53\u524d\u5bf9\u5177\u8eab\u4e16\u754c\u6a21\u578b\u7684\u8bc4\u4f30\u4e3b\u8981\u5173\u6ce8\u611f\u77e5\u4fdd\u771f\u5ea6\uff08\u5982\u89c6\u9891\u751f\u6210\u8d28\u91cf\uff09\uff0c\u800c\u5ffd\u89c6\u4e86\u8fd9\u4e9b\u6a21\u578b\u5728\u4e0b\u6e38\u51b3\u7b56\u4efb\u52a1\u4e2d\u7684\u529f\u80fd\u6548\u7528\u3002\u8fd9\u79cd\u788e\u7247\u5316\u7684\u8bc4\u4f30\u65b9\u5f0f\u65e0\u6cd5\u5168\u9762\u8861\u91cf\u4e16\u754c\u6a21\u578b\u7684\u5b9e\u9645\u4ef7\u503c\u3002", "method": "\u63d0\u51fa\u4e86WorldArena\u57fa\u51c6\u6d4b\u8bd5\uff0c\u901a\u8fc7\u4e09\u4e2a\u7ef4\u5ea6\u8bc4\u4f30\u6a21\u578b\uff1a1\uff09\u89c6\u9891\u611f\u77e5\u8d28\u91cf\uff08\u4f7f\u752816\u4e2a\u6307\u6807\u5728\u516d\u4e2a\u5b50\u7ef4\u5ea6\u4e0a\u6d4b\u91cf\uff09\uff1b2\uff09\u5177\u8eab\u4efb\u52a1\u529f\u80fd\uff08\u8bc4\u4f30\u4e16\u754c\u6a21\u578b\u4f5c\u4e3a\u6570\u636e\u5f15\u64ce\u3001\u7b56\u7565\u8bc4\u4f30\u5668\u548c\u52a8\u4f5c\u89c4\u5212\u5668\uff0c\u5e76\u7ed3\u5408\u4e3b\u89c2\u4eba\u7c7b\u8bc4\u4f30\uff09\uff1b3\uff09EWMScore\u7efc\u5408\u6307\u6807\uff0c\u5c06\u591a\u7ef4\u6027\u80fd\u6574\u5408\u4e3a\u5355\u4e00\u53ef\u89e3\u91ca\u6307\u6570\u3002", "result": "\u901a\u8fc7\u5bf914\u4e2a\u4ee3\u8868\u6027\u6a21\u578b\u7684\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u63ed\u793a\u4e86\u663e\u8457\u7684\u611f\u77e5-\u529f\u80fd\u5dee\u8ddd\uff0c\u8868\u660e\u9ad8\u89c6\u89c9\u8d28\u91cf\u5e76\u4e0d\u4e00\u5b9a\u8f6c\u5316\u4e3a\u5f3a\u5927\u7684\u5177\u8eab\u4efb\u52a1\u80fd\u529b\u3002", "conclusion": "WorldArena\u57fa\u51c6\u6d4b\u8bd5\u4e3a\u8ffd\u8e2a\u5177\u8eabAI\u4e2d\u771f\u6b63\u529f\u80fd\u6027\u4e16\u754c\u6a21\u578b\u7684\u8fdb\u5c55\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6846\u67b6\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u8be5\u9886\u57df\u5411\u66f4\u5b9e\u7528\u7684\u65b9\u5411\u53d1\u5c55\u3002"}}
{"id": "2602.09014", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.09014", "abs": "https://arxiv.org/abs/2602.09014", "authors": ["Zihan Yang", "Shuyuan Tu", "Licheng Zhang", "Qi Dai", "Yu-Gang Jiang", "Zuxuan Wu"], "title": "ArcFlow: Unleashing 2-Step Text-to-Image Generation via High-Precision Non-Linear Flow Distillation", "comment": null, "summary": "Diffusion models have achieved remarkable generation quality, but they suffer from significant inference cost due to their reliance on multiple sequential denoising steps, motivating recent efforts to distill this inference process into a few-step regime. However, existing distillation methods typically approximate the teacher trajectory by using linear shortcuts, which makes it difficult to match its constantly changing tangent directions as velocities evolve across timesteps, thereby leading to quality degradation. To address this limitation, we propose ArcFlow, a few-step distillation framework that explicitly employs non-linear flow trajectories to approximate pre-trained teacher trajectories. Concretely, ArcFlow parameterizes the velocity field underlying the inference trajectory as a mixture of continuous momentum processes. This enables ArcFlow to capture velocity evolution and extrapolate coherent velocities to form a continuous non-linear trajectory within each denoising step. Importantly, this parameterization admits an analytical integration of this non-linear trajectory, which circumvents numerical discretization errors and results in high-precision approximation of the teacher trajectory. To train this parameterization into a few-step generator, we implement ArcFlow via trajectory distillation on pre-trained teacher models using lightweight adapters. This strategy ensures fast, stable convergence while preserving generative diversity and quality. Built on large-scale models (Qwen-Image-20B and FLUX.1-dev), ArcFlow only fine-tunes on less than 5% of original parameters and achieves a 40x speedup with 2 NFEs over the original multi-step teachers without significant quality degradation. Experiments on benchmarks show the effectiveness of ArcFlow both qualitatively and quantitatively.", "AI": {"tldr": "ArcFlow\u63d0\u51fa\u4e86\u4e00\u79cd\u975e\u7ebf\u6027\u8f68\u8ff9\u84b8\u998f\u6846\u67b6\uff0c\u901a\u8fc7\u53c2\u6570\u5316\u8fde\u7eed\u52a8\u91cf\u8fc7\u7a0b\u6765\u903c\u8fd1\u6559\u5e08\u6a21\u578b\u7684\u63a8\u7406\u8f68\u8ff9\uff0c\u5b9e\u73b0\u4ec5\u97002\u6b65\u63a8\u7406\u5c31\u80fd\u8fbe\u523040\u500d\u52a0\u901f\uff0c\u540c\u65f6\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u867d\u7136\u751f\u6210\u8d28\u91cf\u4f18\u79c0\uff0c\u4f46\u9700\u8981\u5927\u91cf\u987a\u5e8f\u53bb\u566a\u6b65\u9aa4\u5bfc\u81f4\u63a8\u7406\u6210\u672c\u9ad8\u6602\u3002\u73b0\u6709\u84b8\u998f\u65b9\u6cd5\u4f7f\u7528\u7ebf\u6027\u6377\u5f84\u903c\u8fd1\u6559\u5e08\u8f68\u8ff9\uff0c\u96be\u4ee5\u5339\u914d\u968f\u65f6\u95f4\u6b65\u53d8\u5316\u7684\u5207\u7ebf\u65b9\u5411\uff0c\u5bfc\u81f4\u8d28\u91cf\u4e0b\u964d\u3002", "method": "ArcFlow\u5c06\u63a8\u7406\u8f68\u8ff9\u7684\u5e95\u5c42\u901f\u5ea6\u573a\u53c2\u6570\u5316\u4e3a\u8fde\u7eed\u52a8\u91cf\u8fc7\u7a0b\u7684\u6df7\u5408\uff0c\u80fd\u591f\u6355\u6349\u901f\u5ea6\u6f14\u5316\u5e76\u5916\u63a8\u8fde\u8d2f\u901f\u5ea6\u5f62\u6210\u8fde\u7eed\u975e\u7ebf\u6027\u8f68\u8ff9\u3002\u8be5\u53c2\u6570\u5316\u5141\u8bb8\u5bf9\u975e\u7ebf\u6027\u8f68\u8ff9\u8fdb\u884c\u89e3\u6790\u79ef\u5206\uff0c\u907f\u514d\u6570\u503c\u79bb\u6563\u8bef\u5dee\u3002\u901a\u8fc7\u8f7b\u91cf\u7ea7\u9002\u914d\u5668\u5728\u9884\u8bad\u7ec3\u6559\u5e08\u6a21\u578b\u4e0a\u8fdb\u884c\u8f68\u8ff9\u84b8\u998f\u8bad\u7ec3\u3002", "result": "\u5728Qwen-Image-20B\u548cFLUX.1-dev\u7b49\u5927\u89c4\u6a21\u6a21\u578b\u4e0a\uff0cArcFlow\u4ec5\u5fae\u8c03\u4e0d\u52305%\u7684\u539f\u59cb\u53c2\u6570\uff0c\u4f7f\u75282\u6b65\u63a8\u7406\u5c31\u5b9e\u73b0\u4e8640\u500d\u52a0\u901f\uff0c\u4e14\u6ca1\u6709\u663e\u8457\u7684\u8d28\u91cf\u4e0b\u964d\u3002\u57fa\u51c6\u6d4b\u8bd5\u5728\u5b9a\u6027\u548c\u5b9a\u91cf\u4e0a\u90fd\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\u3002", "conclusion": "ArcFlow\u901a\u8fc7\u975e\u7ebf\u6027\u8f68\u8ff9\u84b8\u998f\u6210\u529f\u89e3\u51b3\u4e86\u6269\u6563\u6a21\u578b\u63a8\u7406\u52a0\u901f\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u7684\u540c\u65f6\u5927\u5e45\u51cf\u5c11\u63a8\u7406\u6b65\u9aa4\uff0c\u4e3a\u9ad8\u6548\u6269\u6563\u6a21\u578b\u90e8\u7f72\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.09016", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.09016", "abs": "https://arxiv.org/abs/2602.09016", "authors": ["Hao Phung", "Hadar Averbuch-Elor"], "title": "Raster2Seq: Polygon Sequence Generation for Floorplan Reconstruction", "comment": "Code: https://anonymous.4open.science/r/Raster2Seq-BE73/", "summary": "Reconstructing a structured vector-graphics representation from a rasterized floorplan image is typically an important prerequisite for computational tasks involving floorplans such as automated understanding or CAD workflows. However, existing techniques struggle in faithfully generating the structure and semantics conveyed by complex floorplans that depict large indoor spaces with many rooms and a varying numbers of polygon corners. To this end, we propose Raster2Seq, framing floorplan reconstruction as a sequence-to-sequence task in which floorplan elements--such as rooms, windows, and doors--are represented as labeled polygon sequences that jointly encode geometry and semantics. Our approach introduces an autoregressive decoder that learns to predict the next corner conditioned on image features and previously generated corners using guidance from learnable anchors. These anchors represent spatial coordinates in image space, hence allowing for effectively directing the attention mechanism to focus on informative image regions. By embracing the autoregressive mechanism, our method offers flexibility in the output format, enabling for efficiently handling complex floorplans with numerous rooms and diverse polygon structures. Our method achieves state-of-the-art performance on standard benchmarks such as Structure3D, CubiCasa5K, and Raster2Graph, while also demonstrating strong generalization to more challenging datasets like WAFFLE, which contain diverse room structures and complex geometric variations.", "AI": {"tldr": "Raster2Seq\uff1a\u5c06\u6805\u683c\u5316\u5e73\u9762\u56fe\u91cd\u5efa\u4e3a\u5e8f\u5217\u5230\u5e8f\u5217\u4efb\u52a1\uff0c\u901a\u8fc7\u81ea\u56de\u5f52\u89e3\u7801\u5668\u9884\u6d4b\u591a\u8fb9\u5f62\u89d2\u70b9\uff0c\u5b9e\u73b0\u590d\u6742\u5e73\u9762\u56fe\u7684\u7ed3\u6784\u548c\u8bed\u4e49\u91cd\u5efa", "motivation": "\u73b0\u6709\u6280\u672f\u5728\u91cd\u5efa\u590d\u6742\u5e73\u9762\u56fe\u65f6\u96be\u4ee5\u51c6\u786e\u751f\u6210\u7ed3\u6784\u548c\u8bed\u4e49\u4fe1\u606f\uff0c\u7279\u522b\u662f\u5904\u7406\u5305\u542b\u5927\u91cf\u623f\u95f4\u548c\u4e0d\u540c\u591a\u8fb9\u5f62\u89d2\u70b9\u6570\u91cf\u7684\u5ba4\u5185\u7a7a\u95f4\u5e73\u9762\u56fe", "method": "\u5c06\u5e73\u9762\u56fe\u91cd\u5efa\u5b9a\u4e49\u4e3a\u5e8f\u5217\u5230\u5e8f\u5217\u4efb\u52a1\uff0c\u5c06\u623f\u95f4\u3001\u95e8\u7a97\u7b49\u5143\u7d20\u8868\u793a\u4e3a\u5e26\u6807\u7b7e\u7684\u591a\u8fb9\u5f62\u5e8f\u5217\uff1b\u5f15\u5165\u81ea\u56de\u5f52\u89e3\u7801\u5668\uff0c\u57fa\u4e8e\u56fe\u50cf\u7279\u5f81\u548c\u5df2\u751f\u6210\u89d2\u70b9\u9884\u6d4b\u4e0b\u4e00\u4e2a\u89d2\u70b9\uff1b\u4f7f\u7528\u53ef\u5b66\u4e60\u951a\u70b9\u5f15\u5bfc\u6ce8\u610f\u529b\u673a\u5236\u805a\u7126\u4fe1\u606f\u4e30\u5bcc\u7684\u56fe\u50cf\u533a\u57df", "result": "\u5728Structure3D\u3001CubiCasa5K\u548cRaster2Graph\u7b49\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5728\u5305\u542b\u591a\u6837\u5316\u623f\u95f4\u7ed3\u6784\u548c\u590d\u6742\u51e0\u4f55\u53d8\u5316\u7684WAFFLE\u6570\u636e\u96c6\u4e0a\u4e5f\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b", "conclusion": "Raster2Seq\u901a\u8fc7\u81ea\u56de\u5f52\u5e8f\u5217\u5efa\u6a21\u65b9\u6cd5\uff0c\u80fd\u591f\u7075\u6d3b\u5904\u7406\u590d\u6742\u5e73\u9762\u56fe\uff0c\u6709\u6548\u91cd\u5efa\u51e0\u4f55\u7ed3\u6784\u548c\u8bed\u4e49\u4fe1\u606f\uff0c\u4e3a\u81ea\u52a8\u5316\u7406\u89e3\u548cCAD\u5de5\u4f5c\u6d41\u7a0b\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u57fa\u7840"}}
{"id": "2602.09022", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.09022", "abs": "https://arxiv.org/abs/2602.09022", "authors": ["Zehan Wang", "Tengfei Wang", "Haiyu Zhang", "Xuhui Zuo", "Junta Wu", "Haoyuan Wang", "Wenqiang Sun", "Zhenwei Wang", "Chenjie Cao", "Hengshuang Zhao", "Chunchao Guo", "Zhou Zhao"], "title": "WorldCompass: Reinforcement Learning for Long-Horizon World Models", "comment": "Project page: \\url{https://3d-models.hunyuan.tencent.com/world/}", "summary": "This work presents WorldCompass, a novel Reinforcement Learning (RL) post-training framework for the long-horizon, interactive video-based world models, enabling them to explore the world more accurately and consistently based on interaction signals. To effectively \"steer\" the world model's exploration, we introduce three core innovations tailored to the autoregressive video generation paradigm: 1) Clip-level rollout Strategy: We generate and evaluate multiple samples at a single target clip, which significantly boosts rollout efficiency and provides fine-grained reward signals. 2) Complementary Reward Functions: We design reward functions for both interaction-following accuracy and visual quality, which provide direct supervision and effectively suppress reward-hacking behaviors. 3) Efficient RL Algorithm: We employ the negative-aware fine-tuning strategy coupled with various efficiency optimizations to efficiently and effectively enhance model capacity. Evaluations on the SoTA open-source world model, WorldPlay, demonstrate that WorldCompass significantly improves interaction accuracy and visual fidelity across various scenarios.", "AI": {"tldr": "WorldCompass\u662f\u4e00\u4e2a\u7528\u4e8e\u957f\u65f6\u7a0b\u4ea4\u4e92\u5f0f\u89c6\u9891\u4e16\u754c\u6a21\u578b\u7684\u5f3a\u5316\u5b66\u4e60\u540e\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u526a\u8f91\u7ea7\u7b56\u7565\u3001\u4e92\u8865\u5956\u52b1\u51fd\u6570\u548c\u9ad8\u6548RL\u7b97\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u6a21\u578b\u63a2\u7d22\u51c6\u786e\u6027\u548c\u89c6\u89c9\u8d28\u91cf", "motivation": "\u73b0\u6709\u7684\u957f\u65f6\u7a0b\u4ea4\u4e92\u5f0f\u89c6\u9891\u4e16\u754c\u6a21\u578b\u5728\u63a2\u7d22\u4e16\u754c\u65f6\u7f3a\u4e4f\u51c6\u786e\u6027\u548c\u4e00\u81f4\u6027\uff0c\u9700\u8981\u4e00\u79cd\u6709\u6548\u7684\u540e\u8bad\u7ec3\u6846\u67b6\u6765\u57fa\u4e8e\u4ea4\u4e92\u4fe1\u53f7\u5f15\u5bfc\u6a21\u578b\u66f4\u51c6\u786e\u5730\u63a2\u7d22\u4e16\u754c", "method": "\u63d0\u51fa\u4e86\u4e09\u4e2a\u6838\u5fc3\u521b\u65b0\uff1a1)\u526a\u8f91\u7ea7\u7b56\u7565\uff1a\u5728\u5355\u4e2a\u76ee\u6807\u526a\u8f91\u751f\u6210\u548c\u8bc4\u4f30\u591a\u4e2a\u6837\u672c\uff0c\u63d0\u9ad8\u6548\u7387\u5e76\u63d0\u4f9b\u7ec6\u7c92\u5ea6\u5956\u52b1\u4fe1\u53f7\uff1b2)\u4e92\u8865\u5956\u52b1\u51fd\u6570\uff1a\u8bbe\u8ba1\u4ea4\u4e92\u8ddf\u968f\u51c6\u786e\u6027\u548c\u89c6\u89c9\u8d28\u91cf\u7684\u5956\u52b1\u51fd\u6570\uff0c\u6291\u5236\u5956\u52b1\u9ed1\u5ba2\u884c\u4e3a\uff1b3)\u9ad8\u6548RL\u7b97\u6cd5\uff1a\u91c7\u7528\u8d1f\u611f\u77e5\u5fae\u8c03\u7b56\u7565\u7ed3\u5408\u591a\u79cd\u6548\u7387\u4f18\u5316", "result": "\u5728SoTA\u5f00\u6e90\u4e16\u754c\u6a21\u578bWorldPlay\u4e0a\u7684\u8bc4\u4f30\u8868\u660e\uff0cWorldCompass\u5728\u5404\u79cd\u573a\u666f\u4e0b\u663e\u8457\u63d0\u9ad8\u4e86\u4ea4\u4e92\u51c6\u786e\u6027\u548c\u89c6\u89c9\u4fdd\u771f\u5ea6", "conclusion": "WorldCompass\u662f\u4e00\u4e2a\u6709\u6548\u7684RL\u540e\u8bad\u7ec3\u6846\u67b6\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347\u957f\u65f6\u7a0b\u4ea4\u4e92\u5f0f\u89c6\u9891\u4e16\u754c\u6a21\u578b\u7684\u63a2\u7d22\u51c6\u786e\u6027\u548c\u89c6\u89c9\u8d28\u91cf\uff0c\u4e3a\u89c6\u9891\u751f\u6210\u4e16\u754c\u6a21\u578b\u63d0\u4f9b\u4e86\u65b0\u7684\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u65b9\u6cd5"}}
{"id": "2602.09024", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.09024", "abs": "https://arxiv.org/abs/2602.09024", "authors": ["Qihang Yu", "Qihao Liu", "Ju He", "Xinyang Zhang", "Yang Liu", "Liang-Chieh Chen", "Xi Chen"], "title": "Autoregressive Image Generation with Masked Bit Modeling", "comment": "SOTA discrete visual generation defeats diffusion models with 0.99 FID score, project page is available at https://bar-gen.github.io/", "summary": "This paper challenges the dominance of continuous pipelines in visual generation. We systematically investigate the performance gap between discrete and continuous methods. Contrary to the belief that discrete tokenizers are intrinsically inferior, we demonstrate that the disparity arises primarily from the total number of bits allocated in the latent space (i.e., the compression ratio). We show that scaling up the codebook size effectively bridges this gap, allowing discrete tokenizers to match or surpass their continuous counterparts. However, existing discrete generation methods struggle to capitalize on this insight, suffering from performance degradation or prohibitive training costs with scaled codebook. To address this, we propose masked Bit AutoRegressive modeling (BAR), a scalable framework that supports arbitrary codebook sizes. By equipping an autoregressive transformer with a masked bit modeling head, BAR predicts discrete tokens through progressively generating their constituent bits. BAR achieves a new state-of-the-art gFID of 0.99 on ImageNet-256, outperforming leading methods across both continuous and discrete paradigms, while significantly reducing sampling costs and converging faster than prior continuous approaches. Project page is available at https://bar-gen.github.io/", "AI": {"tldr": "\u672c\u6587\u6311\u6218\u4e86\u89c6\u89c9\u751f\u6210\u4e2d\u8fde\u7eed\u7ba1\u9053\u7684\u7edf\u6cbb\u5730\u4f4d\uff0c\u901a\u8fc7\u7814\u7a76\u53d1\u73b0\u79bb\u6563\u4e0e\u8fde\u7eed\u65b9\u6cd5\u7684\u6027\u80fd\u5dee\u8ddd\u4e3b\u8981\u6e90\u4e8e\u6f5c\u5728\u7a7a\u95f4\u7684\u6bd4\u7279\u5206\u914d\uff0c\u63d0\u51fa\u901a\u8fc7\u6269\u5927\u7801\u672c\u89c4\u6a21\u6765\u5f25\u5408\u5dee\u8ddd\uff0c\u5e76\u8bbe\u8ba1\u4e86BAR\u6846\u67b6\u5b9e\u73b0\u8fd9\u4e00\u76ee\u6807\u3002", "motivation": "\u6311\u6218\u89c6\u89c9\u751f\u6210\u9886\u57df\u8fde\u7eed\u7ba1\u9053\u7684\u4e3b\u5bfc\u5730\u4f4d\uff0c\u7cfb\u7edf\u7814\u7a76\u79bb\u6563\u4e0e\u8fde\u7eed\u65b9\u6cd5\u7684\u6027\u80fd\u5dee\u8ddd\uff0c\u63ed\u793a\u79bb\u6563\u6807\u8bb0\u5668\u5e76\u975e\u672c\u8d28\u52a3\u4e8e\u8fde\u7eed\u65b9\u6cd5\uff0c\u800c\u662f\u53d7\u9650\u4e8e\u6f5c\u5728\u7a7a\u95f4\u7684\u6bd4\u7279\u5206\u914d\u3002", "method": "\u63d0\u51fa\u63a9\u7801\u6bd4\u7279\u81ea\u56de\u5f52\u5efa\u6a21(BAR)\u6846\u67b6\uff0c\u901a\u8fc7\u4e3a\u81ea\u56de\u5f52\u53d8\u6362\u5668\u914d\u5907\u63a9\u7801\u6bd4\u7279\u5efa\u6a21\u5934\uff0c\u652f\u6301\u4efb\u610f\u7801\u672c\u89c4\u6a21\uff0c\u901a\u8fc7\u9010\u6b65\u751f\u6210\u6bd4\u7279\u6765\u9884\u6d4b\u79bb\u6563\u6807\u8bb0\u3002", "result": "BAR\u5728ImageNet-256\u4e0a\u5b9e\u73b0\u4e860.99\u7684gFID\u65b0SOTA\uff0c\u8d85\u8d8a\u4e86\u8fde\u7eed\u548c\u79bb\u6563\u8303\u5f0f\u7684\u9886\u5148\u65b9\u6cd5\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u91c7\u6837\u6210\u672c\uff0c\u6536\u655b\u901f\u5ea6\u6bd4\u4e4b\u524d\u7684\u8fde\u7eed\u65b9\u6cd5\u66f4\u5feb\u3002", "conclusion": "\u79bb\u6563\u6807\u8bb0\u5668\u5728\u89c6\u89c9\u751f\u6210\u4e2d\u5177\u6709\u4e0e\u8fde\u7eed\u65b9\u6cd5\u76f8\u5f53\u751a\u81f3\u66f4\u4f18\u7684\u6f5c\u529b\uff0c\u5173\u952e\u5728\u4e8e\u6f5c\u5728\u7a7a\u95f4\u7684\u6bd4\u7279\u5206\u914d\uff1bBAR\u6846\u67b6\u6210\u529f\u5b9e\u73b0\u4e86\u8fd9\u4e00\u6f5c\u529b\uff0c\u4e3a\u89c6\u89c9\u751f\u6210\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u65b0\u8303\u5f0f\u3002"}}
