{"id": "2602.20303", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.20303", "abs": "https://arxiv.org/abs/2602.20303", "authors": ["Joyanta Jyoti Mondal"], "title": "Multilevel Determinants of Overweight and Obesity Among U.S. Children Aged 10-17: Comparative Evaluation of Statistical and Machine Learning Approaches Using the 2021 National Survey of Children's Health", "comment": null, "summary": "Background: Childhood and adolescent overweight and obesity remain major public health concerns in the United States and are shaped by behavioral, household, and community factors. Their joint predictive structure at the population level remains incompletely characterized. Objectives: The study aims to identify multilevel predictors of overweight and obesity among U.S. adolescents and compare the predictive performance, calibration, and subgroup equity of statistical, machine-learning, and deep-learning models. Data and Methods: We analyze 18,792 children aged 10-17 years from the 2021 National Survey of Children's Health. Overweight/obesity is defined using BMI categories. Predictors included diet, physical activity, sleep, parental stress, socioeconomic conditions, adverse experiences, and neighborhood characteristics. Models include logistic regression, random forest, gradient boosting, XGBoost, LightGBM, multilayer perceptron, and TabNet. Performance is evaluated using AUC, accuracy, precision, recall, F1 score, and Brier score. Results: Discrimination range from 0.66 to 0.79. Logistic regression, gradient boosting, and MLP showed the most stable balance of discrimination and calibration. Boosting and deep learning modestly improve recall and F1 score. No model was uniformly superior. Performance disparities across race and poverty groups persist across algorithms. Conclusion: Increased model complexity yields limited gains over logistic regression. Predictors consistently span behavioral, household, and neighborhood domains. Persistent subgroup disparities indicate the need for improved data quality and equity-focused surveillance rather than greater algorithmic complexity.", "AI": {"tldr": "\u7814\u7a76\u6bd4\u8f83\u4e86\u7edf\u8ba1\u3001\u673a\u5668\u5b66\u4e60\u548c\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u9884\u6d4b\u7f8e\u56fd\u9752\u5c11\u5e74\u8d85\u91cd/\u80a5\u80d6\u65b9\u9762\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u903b\u8f91\u56de\u5f52\u3001\u68af\u5ea6\u63d0\u5347\u548c\u591a\u5c42\u611f\u77e5\u5668\u8868\u73b0\u6700\u7a33\u5b9a\uff0c\u6a21\u578b\u590d\u6742\u6027\u589e\u52a0\u5e26\u6765\u7684\u6539\u8fdb\u6709\u9650\uff0c\u4e0d\u540c\u79cd\u65cf\u548c\u8d2b\u56f0\u7fa4\u4f53\u95f4\u7684\u6027\u80fd\u5dee\u5f02\u6301\u7eed\u5b58\u5728\u3002", "motivation": "\u513f\u7ae5\u548c\u9752\u5c11\u5e74\u8d85\u91cd/\u80a5\u80d6\u662f\u7f8e\u56fd\u4e3b\u8981\u7684\u516c\u5171\u536b\u751f\u95ee\u9898\uff0c\u53d7\u884c\u4e3a\u3001\u5bb6\u5ead\u548c\u793e\u533a\u56e0\u7d20\u5171\u540c\u5f71\u54cd\u3002\u76ee\u524d\u5bf9\u8fd9\u4e9b\u591a\u5c42\u6b21\u9884\u6d4b\u56e0\u7d20\u5728\u4eba\u7fa4\u5c42\u9762\u7684\u8054\u5408\u9884\u6d4b\u7ed3\u6784\u4e86\u89e3\u4e0d\u8db3\uff0c\u9700\u8981\u6bd4\u8f83\u4e0d\u540c\u9884\u6d4b\u6a21\u578b\u7684\u6027\u80fd\u3001\u6821\u51c6\u548c\u4e9a\u7ec4\u516c\u5e73\u6027\u3002", "method": "\u5206\u67902021\u5e74\u5168\u56fd\u513f\u7ae5\u5065\u5eb7\u8c03\u67e5\u4e2d18,792\u540d10-17\u5c81\u513f\u7ae5\u6570\u636e\u3002\u4f7f\u7528BMI\u5b9a\u4e49\u8d85\u91cd/\u80a5\u80d6\u3002\u9884\u6d4b\u56e0\u7d20\u5305\u62ec\u996e\u98df\u3001\u4f53\u80b2\u6d3b\u52a8\u3001\u7761\u7720\u3001\u7236\u6bcd\u538b\u529b\u3001\u793e\u4f1a\u7ecf\u6d4e\u6761\u4ef6\u3001\u4e0d\u826f\u7ecf\u5386\u548c\u793e\u533a\u7279\u5f81\u3002\u6bd4\u8f83\u4e86\u903b\u8f91\u56de\u5f52\u3001\u968f\u673a\u68ee\u6797\u3001\u68af\u5ea6\u63d0\u5347\u3001XGBoost\u3001LightGBM\u3001\u591a\u5c42\u611f\u77e5\u5668\u548cTabNet\u7b49\u6a21\u578b\u3002\u4f7f\u7528AUC\u3001\u51c6\u786e\u7387\u3001\u7cbe\u786e\u7387\u3001\u53ec\u56de\u7387\u3001F1\u5206\u6570\u548cBrier\u5206\u6570\u8bc4\u4f30\u6027\u80fd\u3002", "result": "\u6a21\u578b\u533a\u5206\u5ea6\u57280.66\u52300.79\u4e4b\u95f4\u3002\u903b\u8f91\u56de\u5f52\u3001\u68af\u5ea6\u63d0\u5347\u548c\u591a\u5c42\u611f\u77e5\u5668\u5728\u533a\u5206\u5ea6\u548c\u6821\u51c6\u65b9\u9762\u8868\u73b0\u6700\u7a33\u5b9a\u3002\u63d0\u5347\u65b9\u6cd5\u548c\u6df1\u5ea6\u5b66\u4e60\u5728\u53ec\u56de\u7387\u548cF1\u5206\u6570\u4e0a\u6709\u9002\u5ea6\u6539\u8fdb\u3002\u6ca1\u6709\u6a21\u578b\u5728\u6240\u6709\u65b9\u9762\u90fd\u8868\u73b0\u6700\u4f18\u3002\u4e0d\u540c\u79cd\u65cf\u548c\u8d2b\u56f0\u7fa4\u4f53\u95f4\u7684\u6027\u80fd\u5dee\u5f02\u5728\u6240\u6709\u7b97\u6cd5\u4e2d\u6301\u7eed\u5b58\u5728\u3002", "conclusion": "\u589e\u52a0\u6a21\u578b\u590d\u6742\u6027\u5bf9\u903b\u8f91\u56de\u5f52\u7684\u6539\u8fdb\u6709\u9650\u3002\u9884\u6d4b\u56e0\u7d20\u6301\u7eed\u6db5\u76d6\u884c\u4e3a\u3001\u5bb6\u5ead\u548c\u793e\u533a\u9886\u57df\u3002\u6301\u7eed\u7684\u4e9a\u7ec4\u5dee\u5f02\u8868\u660e\u9700\u8981\u6539\u8fdb\u6570\u636e\u8d28\u91cf\u548c\u5173\u6ce8\u516c\u5e73\u6027\u7684\u76d1\u6d4b\uff0c\u800c\u4e0d\u662f\u589e\u52a0\u7b97\u6cd5\u590d\u6742\u6027\u3002"}}
{"id": "2601.12815", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.MA"], "pdf": "https://arxiv.org/pdf/2601.12815", "abs": "https://arxiv.org/abs/2601.12815", "authors": ["Zhaolu Kang", "Junhao Gong", "Qingxi Chen", "Hao Zhang", "Jiaxin Liu", "Rong Fu", "Zhiyuan Feng", "Yuan Wang", "Simon Fong", "Kaiyue Zhou"], "title": "Multimodal Multi-Agent Empowered Legal Judgment Prediction", "comment": "Accepted to the IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP) 2026", "summary": "Legal Judgment Prediction (LJP) aims to predict the outcomes of legal cases based on factual descriptions, serving as a fundamental task to advance the development of legal systems. Traditional methods often rely on statistical analyses or role-based simulations but face challenges with multiple allegations, diverse evidence, and lack adaptability. In this paper, we introduce JurisMMA, a novel framework for LJP that effectively decomposes trial tasks, standardizes processes, and organizes them into distinct stages. Furthermore, we build JurisMM, a large dataset with over 100,000 recent Chinese judicial records, including both text and multimodal video-text data, enabling comprehensive evaluation. Experiments on JurisMM and the benchmark LawBench validate our framework's effectiveness. These results indicate that our framework is effective not only for LJP but also for a broader range of legal applications, offering new perspectives for the development of future legal methods and datasets.", "AI": {"tldr": "\u63d0\u51faJurisMMA\u6846\u67b6\u7528\u4e8e\u6cd5\u5f8b\u5224\u51b3\u9884\u6d4b\uff0c\u901a\u8fc7\u4efb\u52a1\u5206\u89e3\u548c\u6d41\u7a0b\u6807\u51c6\u5316\u5904\u7406\u591a\u6307\u63a7\u3001\u591a\u6837\u5316\u8bc1\u636e\u95ee\u9898\uff0c\u5e76\u6784\u5efa\u5305\u542b10\u4e07+\u4e2d\u6587\u53f8\u6cd5\u8bb0\u5f55\u7684JurisMM\u6570\u636e\u96c6\u8fdb\u884c\u9a8c\u8bc1\u3002", "motivation": "\u4f20\u7edf\u6cd5\u5f8b\u5224\u51b3\u9884\u6d4b\u65b9\u6cd5\u5728\u5904\u7406\u591a\u6307\u63a7\u3001\u591a\u6837\u5316\u8bc1\u636e\u65f6\u9762\u4e34\u6311\u6218\u4e14\u7f3a\u4e4f\u9002\u5e94\u6027\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u6846\u67b6\u6765\u63a8\u8fdb\u6cd5\u5f8b\u7cfb\u7edf\u53d1\u5c55\u3002", "method": "\u63d0\u51faJurisMMA\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u89e3\u5ba1\u5224\u4efb\u52a1\u3001\u6807\u51c6\u5316\u6d41\u7a0b\u5e76\u7ec4\u7ec7\u4e3a\u4e0d\u540c\u9636\u6bb5\uff0c\u540c\u65f6\u6784\u5efa\u5305\u542b\u6587\u672c\u548c\u591a\u6a21\u6001\u89c6\u9891-\u6587\u672c\u6570\u636e\u7684JurisMM\u5927\u89c4\u6a21\u6570\u636e\u96c6\u3002", "result": "\u5728JurisMM\u548cLawBench\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u6709\u6548\u6027\uff0c\u8868\u660e\u8be5\u6846\u67b6\u4e0d\u4ec5\u9002\u7528\u4e8e\u6cd5\u5f8b\u5224\u51b3\u9884\u6d4b\uff0c\u8fd8\u53ef\u6269\u5c55\u5230\u66f4\u5e7f\u6cdb\u7684\u6cd5\u5f8b\u5e94\u7528\u3002", "conclusion": "JurisMMA\u6846\u67b6\u4e3a\u6cd5\u5f8b\u5224\u51b3\u9884\u6d4b\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u540c\u65f6\u6784\u5efa\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4e3a\u672a\u6765\u6cd5\u5f8b\u65b9\u6cd5\u548c\u6570\u636e\u96c6\u53d1\u5c55\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002"}}
{"id": "2602.20164", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.20164", "abs": "https://arxiv.org/abs/2602.20164", "authors": ["Sachin Gopal Wani", "Eric Page", "Ajay Dholakia", "David Ellison"], "title": "Benchmarking Distilled Language Models: Performance and Efficiency in Resource-Constrained Settings", "comment": "16 pages, 5 figures, accepted at the the 2025 TPCTC Conference", "summary": "Knowledge distillation offers a transformative pathway to developing powerful, yet efficient, small language models (SLMs) suitable for resource-constrained environments. In this paper, we benchmark the performance and computational cost of distilled models against their vanilla and proprietary counterparts, providing a quantitative analysis of their efficiency. Our results demonstrate that distillation creates a superior performance-tocompute curve. We find that creating a distilled 8B model is over 2,000 times more compute-efficient than training its vanilla counterpart, while achieving reasoning capabilities on par with, or even exceeding, standard models ten times its size. These findings validate distillation not just as a compression technique, but as a primary strategy for building state-of-the-art, accessible AI", "AI": {"tldr": "\u77e5\u8bc6\u84b8\u998f\u80fd\u521b\u5efa\u6bd4\u4f20\u7edf\u8bad\u7ec3\u66f4\u9ad8\u6548\u7684\u5c0f\u8bed\u8a00\u6a21\u578b\uff0c8B\u84b8\u998f\u6a21\u578b\u7684\u8ba1\u7b97\u6548\u7387\u6bd4\u666e\u901a\u8bad\u7ec3\u9ad82000\u500d\u4ee5\u4e0a\uff0c\u63a8\u7406\u80fd\u529b\u53ef\u4e0e10\u500d\u5927\u5c0f\u7684\u6807\u51c6\u6a21\u578b\u5ab2\u7f8e\u751a\u81f3\u8d85\u8d8a", "motivation": "\u4e3a\u8d44\u6e90\u53d7\u9650\u73af\u5883\u5f00\u53d1\u5f3a\u5927\u800c\u9ad8\u6548\u7684\u5c0f\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u77e5\u8bc6\u84b8\u998f\u6280\u672f\u5b9e\u73b0\u6a21\u578b\u538b\u7f29\u548c\u6027\u80fd\u63d0\u5347\uff0c\u9a8c\u8bc1\u84b8\u998f\u4f5c\u4e3a\u6784\u5efa\u5148\u8fdb\u53ef\u8bbf\u95eeAI\u7684\u4e3b\u8981\u7b56\u7565", "method": "\u5bf9\u84b8\u998f\u6a21\u578b\u4e0e\u666e\u901a\u6a21\u578b\u53ca\u4e13\u6709\u6a21\u578b\u8fdb\u884c\u6027\u80fd\u4e0e\u8ba1\u7b97\u6210\u672c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u63d0\u4f9b\u6548\u7387\u7684\u5b9a\u91cf\u5206\u6790\uff0c\u6bd4\u8f83\u4e0d\u540c\u89c4\u6a21\u6a21\u578b\u7684\u8ba1\u7b97\u6548\u7387\u548c\u63a8\u7406\u80fd\u529b", "result": "\u84b8\u998f\u521b\u9020\u4e86\u66f4\u4f18\u7684\u6027\u80fd-\u8ba1\u7b97\u66f2\u7ebf\uff0c8B\u84b8\u998f\u6a21\u578b\u7684\u8ba1\u7b97\u6548\u7387\u6bd4\u666e\u901a\u8bad\u7ec3\u9ad82000\u500d\u4ee5\u4e0a\uff0c\u63a8\u7406\u80fd\u529b\u4e0e10\u500d\u5927\u5c0f\u7684\u6807\u51c6\u6a21\u578b\u76f8\u5f53\u751a\u81f3\u8d85\u8d8a", "conclusion": "\u77e5\u8bc6\u84b8\u998f\u4e0d\u4ec5\u662f\u538b\u7f29\u6280\u672f\uff0c\u66f4\u662f\u6784\u5efa\u5148\u8fdb\u3001\u53ef\u8bbf\u95eeAI\u7684\u4e3b\u8981\u7b56\u7565\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347\u5c0f\u8bed\u8a00\u6a21\u578b\u7684\u6548\u7387\u548c\u6027\u80fd"}}
{"id": "2602.20291", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.20291", "abs": "https://arxiv.org/abs/2602.20291", "authors": ["Valentin Bonas", "Martin Sinnona", "Viviana Siless", "Emmanuel Iarussi"], "title": "De-rendering, Reasoning, and Repairing Charts with Vision-Language Models", "comment": null, "summary": "Data visualizations are central to scientific communication, journalism, and everyday decision-making, yet they are frequently prone to errors that can distort interpretation or mislead audiences. Rule-based visualization linters can flag violations, but they miss context and do not suggest meaningful design changes. Directly querying general-purpose LLMs about visualization quality is unreliable: lacking training to follow visualization design principles, they often produce inconsistent or incorrect feedback. In this work, we introduce a framework that combines chart de-rendering, automated analysis, and iterative improvement to deliver actionable, interpretable feedback on visualization design. Our system reconstructs the structure of a chart from an image, identifies design flaws using vision-language reasoning, and proposes concrete modifications supported by established principles in visualization research. Users can selectively apply these improvements and re-render updated figures, creating a feedback loop that promotes both higher-quality visualizations and the development of visualization literacy. In our evaluation on 1,000 charts from the Chart2Code benchmark, the system generated 10,452 design recommendations, which clustered into 10 coherent categories (e.g., axis formatting, color accessibility, legend consistency). These results highlight the promise of LLM-driven recommendation systems for delivering structured, principle-based feedback on visualization design, opening the door to more intelligent and accessible authoring tools.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u4e2a\u7ed3\u5408\u56fe\u8868\u53cd\u6e32\u67d3\u3001\u81ea\u52a8\u5206\u6790\u548c\u8fed\u4ee3\u6539\u8fdb\u7684\u6846\u67b6\uff0c\u4e3a\u53ef\u89c6\u5316\u8bbe\u8ba1\u63d0\u4f9b\u53ef\u64cd\u4f5c\u3001\u53ef\u89e3\u91ca\u7684\u53cd\u9988\uff0c\u901a\u8fc7LLM\u9a71\u52a8\u7684\u63a8\u8350\u7cfb\u7edf\u751f\u6210\u57fa\u4e8e\u53ef\u89c6\u5316\u539f\u5219\u7684\u7ed3\u6784\u5316\u8bbe\u8ba1\u5efa\u8bae\u3002", "motivation": "\u6570\u636e\u53ef\u89c6\u5316\u5728\u79d1\u5b66\u4f20\u64ad\u3001\u65b0\u95fb\u548c\u65e5\u5e38\u51b3\u7b56\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5e38\u5b58\u5728\u9519\u8bef\u5bfc\u81f4\u8bef\u89e3\u3002\u57fa\u4e8e\u89c4\u5219\u7684\u68c0\u67e5\u5668\u7f3a\u4e4f\u4e0a\u4e0b\u6587\u4e14\u65e0\u6cd5\u63d0\u4f9b\u6709\u610f\u4e49\u7684\u6539\u8fdb\u5efa\u8bae\uff0c\u800c\u901a\u7528LLM\u76f4\u63a5\u67e5\u8be2\u53ef\u89c6\u5316\u8d28\u91cf\u4e0d\u53ef\u9760\uff0c\u56e0\u4e3a\u5b83\u4eec\u7f3a\u4e4f\u9075\u5faa\u53ef\u89c6\u5316\u8bbe\u8ba1\u539f\u5219\u7684\u8bad\u7ec3\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u7ed3\u5408\u56fe\u8868\u53cd\u6e32\u67d3\u3001\u81ea\u52a8\u5206\u6790\u548c\u8fed\u4ee3\u6539\u8fdb\u7684\u6846\u67b6\uff1a1) \u4ece\u56fe\u50cf\u91cd\u5efa\u56fe\u8868\u7ed3\u6784\uff1b2) \u4f7f\u7528\u89c6\u89c9\u8bed\u8a00\u63a8\u7406\u8bc6\u522b\u8bbe\u8ba1\u7f3a\u9677\uff1b3) \u57fa\u4e8e\u53ef\u89c6\u5316\u7814\u7a76\u539f\u5219\u63d0\u51fa\u5177\u4f53\u4fee\u6539\u5efa\u8bae\uff1b4) \u7528\u6237\u53ef\u9009\u62e9\u5e94\u7528\u6539\u8fdb\u5e76\u91cd\u65b0\u6e32\u67d3\uff0c\u5f62\u6210\u53cd\u9988\u5faa\u73af\u3002", "result": "\u5728Chart2Code\u57fa\u51c6\u76841,000\u4e2a\u56fe\u8868\u4e0a\u8bc4\u4f30\uff0c\u7cfb\u7edf\u751f\u6210\u4e8610,452\u4e2a\u8bbe\u8ba1\u5efa\u8bae\uff0c\u805a\u7c7b\u4e3a10\u4e2a\u8fde\u8d2f\u7c7b\u522b\uff08\u5982\u8f74\u683c\u5f0f\u5316\u3001\u989c\u8272\u53ef\u8bbf\u95ee\u6027\u3001\u56fe\u4f8b\u4e00\u81f4\u6027\uff09\u3002\u8fd9\u4e9b\u7ed3\u679c\u5c55\u793a\u4e86LLM\u9a71\u52a8\u63a8\u8350\u7cfb\u7edf\u5728\u63d0\u4f9b\u7ed3\u6784\u5316\u3001\u57fa\u4e8e\u539f\u5219\u7684\u53ef\u89c6\u5316\u8bbe\u8ba1\u53cd\u9988\u65b9\u9762\u7684\u6f5c\u529b\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u53ef\u89c6\u5316\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u53ef\u64cd\u4f5c\u3001\u53ef\u89e3\u91ca\u7684\u53cd\u9988\uff0c\u4fc3\u8fdb\u4e86\u66f4\u9ad8\u8d28\u91cf\u7684\u53ef\u89c6\u5316\u548c\u53ef\u89c6\u5316\u7d20\u517b\u7684\u53d1\u5c55\uff0c\u4e3a\u66f4\u667a\u80fd\u3001\u66f4\u6613\u7528\u7684\u521b\u4f5c\u5de5\u5177\u6253\u5f00\u4e86\u5927\u95e8\u3002"}}
{"id": "2602.20334", "categories": ["cs.SE", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.20334", "abs": "https://arxiv.org/abs/2602.20334", "authors": ["Chengjie Lu", "Jiahui Wu", "Shaukat Ali", "Malaika Din Hashmi", "Sebastian Mathias Thomle Mason", "Francois Picard", "Mikkel Labori Olsen", "Thomas Peyrucain"], "title": "UAMTERS: Uncertainty-Aware Mutation Analysis for DL-enabled Robotic Software", "comment": "23 pages, 6 figures, 7 tables", "summary": "Self-adaptive robots adjust their behaviors in response to unpredictable environmental changes. These robots often incorporate deep learning (DL) components into their software to support functionality such as perception, decision-making, and control, enhancing autonomy and self-adaptability. However, the inherent uncertainty of DL-enabled software makes it challenging to ensure its dependability in dynamic environments. Consequently, test generation techniques have been developed to test robot software, and classical mutation analysis injects faults into the software to assess the test suite's effectiveness in detecting the resulting failures. However, there is a lack of mutation analysis techniques to assess the effectiveness under the uncertainty inherent to DL-enabled software. To this end, we propose UAMTERS, an uncertainty-aware mutation analysis framework that introduces uncertainty-aware mutation operators to explicitly inject stochastic uncertainty into DL-enabled robotic software, simulating uncertainty in its behavior. We further propose mutation score metrics to quantify a test suite's ability to detect failures under varying levels of uncertainty. We evaluate UAMTERS across three robotic case studies, demonstrating that UAMTERS more effectively distinguishes test suite quality and captures uncertainty-induced failures in DL-enabled software.", "AI": {"tldr": "UAMTERS\u662f\u4e00\u4e2a\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7684\u53d8\u5f02\u5206\u6790\u6846\u67b6\uff0c\u4e13\u95e8\u9488\u5bf9\u6df1\u5ea6\u5b66\u4e60\u673a\u5668\u4eba\u8f6f\u4ef6\uff0c\u901a\u8fc7\u6ce8\u5165\u968f\u673a\u4e0d\u786e\u5b9a\u6027\u6765\u8bc4\u4f30\u6d4b\u8bd5\u5957\u4ef6\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7684\u6709\u6548\u6027\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u7ec4\u4ef6\u4e3a\u81ea\u9002\u5e94\u673a\u5668\u4eba\u63d0\u4f9b\u4e86\u611f\u77e5\u3001\u51b3\u7b56\u548c\u63a7\u5236\u529f\u80fd\uff0c\u4f46\u5176\u56fa\u6709\u7684\u4e0d\u786e\u5b9a\u6027\u4f7f\u5f97\u5728\u52a8\u6001\u73af\u5883\u4e2d\u786e\u4fdd\u8f6f\u4ef6\u53ef\u9760\u6027\u53d8\u5f97\u56f0\u96be\u3002\u73b0\u6709\u7684\u53d8\u5f02\u5206\u6790\u6280\u672f\u7f3a\u4e4f\u9488\u5bf9\u6df1\u5ea6\u5b66\u4e60\u8f6f\u4ef6\u4e0d\u786e\u5b9a\u6027\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u63d0\u51faUAMTERS\u6846\u67b6\uff0c\u5305\u542b\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7684\u53d8\u5f02\u7b97\u5b50\u6765\u663e\u5f0f\u6ce8\u5165\u968f\u673a\u4e0d\u786e\u5b9a\u6027\u5230\u6df1\u5ea6\u5b66\u4e60\u673a\u5668\u4eba\u8f6f\u4ef6\u4e2d\uff0c\u6a21\u62df\u884c\u4e3a\u4e0d\u786e\u5b9a\u6027\uff0c\u5e76\u63d0\u51fa\u53d8\u5f02\u5206\u6570\u6307\u6807\u6765\u91cf\u5316\u6d4b\u8bd5\u5957\u4ef6\u5728\u4e0d\u540c\u4e0d\u786e\u5b9a\u6027\u6c34\u5e73\u4e0b\u68c0\u6d4b\u6545\u969c\u7684\u80fd\u529b\u3002", "result": "\u5728\u4e09\u4e2a\u673a\u5668\u4eba\u6848\u4f8b\u7814\u7a76\u4e2d\u8bc4\u4f30UAMTERS\uff0c\u8bc1\u660e\u5176\u80fd\u66f4\u6709\u6548\u5730\u533a\u5206\u6d4b\u8bd5\u5957\u4ef6\u8d28\u91cf\uff0c\u5e76\u6355\u83b7\u6df1\u5ea6\u5b66\u4e60\u8f6f\u4ef6\u4e2d\u7531\u4e0d\u786e\u5b9a\u6027\u5f15\u8d77\u7684\u6545\u969c\u3002", "conclusion": "UAMTERS\u6846\u67b6\u4e3a\u6df1\u5ea6\u5b66\u4e60\u673a\u5668\u4eba\u8f6f\u4ef6\u7684\u53ef\u9760\u6027\u8bc4\u4f30\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u5de5\u5177\uff0c\u80fd\u591f\u66f4\u597d\u5730\u8bc4\u4f30\u6d4b\u8bd5\u5957\u4ef6\u5728\u4e0d\u786e\u5b9a\u6027\u73af\u5883\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2602.20312", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.20312", "abs": "https://arxiv.org/abs/2602.20312", "authors": ["Guodong Chen", "Huanshuo Dong", "Mallesham Dasari"], "title": "N4MC: Neural 4D Mesh Compression", "comment": null, "summary": "We present N4MC, the first 4D neural compression framework to efficiently compress time-varying mesh sequences by exploiting their temporal redundancy. Unlike prior neural mesh compression methods that treat each mesh frame independently, N4MC takes inspiration from inter-frame compression in 2D video codecs, and learns motion compensation in long mesh sequences. Specifically, N4MC converts consecutive irregular mesh frames into regular 4D tensors to provide a uniform and compact representation. These tensors are then condensed using an auto-decoder, which captures both spatial and temporal correlations for redundancy removal. To enhance temporal coherence, we introduce a transformer-based interpolation model that predicts intermediate mesh frames conditioned on latent embeddings derived from tracked volume centers, eliminating motion ambiguities. Extensive evaluations show that N4MC outperforms state-of-the-art in rate-distortion performance, while enabling real-time decoding of 4D mesh sequences. The implementation of our method is available at: https://github.com/frozzzen3/N4MC.", "AI": {"tldr": "N4MC\u662f\u9996\u4e2a4D\u795e\u7ecf\u538b\u7f29\u6846\u67b6\uff0c\u901a\u8fc7\u5229\u7528\u65f6\u95f4\u5197\u4f59\u6765\u9ad8\u6548\u538b\u7f29\u65f6\u53d8\u7f51\u683c\u5e8f\u5217\uff0c\u91c7\u7528\u7c7b\u4f3c2D\u89c6\u9891\u7f16\u89e3\u7801\u5668\u7684\u5e27\u95f4\u538b\u7f29\u601d\u60f3\uff0c\u5728\u957f\u7f51\u683c\u5e8f\u5217\u4e2d\u5b66\u4e60\u8fd0\u52a8\u8865\u507f\u3002", "motivation": "\u73b0\u6709\u795e\u7ecf\u7f51\u683c\u538b\u7f29\u65b9\u6cd5\u72ec\u7acb\u5904\u7406\u6bcf\u4e2a\u7f51\u683c\u5e27\uff0c\u5ffd\u7565\u4e86\u65f6\u95f4\u5197\u4f59\u3002\u53d72D\u89c6\u9891\u7f16\u89e3\u7801\u5668\u4e2d\u5e27\u95f4\u538b\u7f29\u7684\u542f\u53d1\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u5229\u7528\u7f51\u683c\u5e8f\u5217\u65f6\u95f4\u76f8\u5173\u6027\u7684\u538b\u7f29\u65b9\u6cd5\u3002", "method": "1) \u5c06\u8fde\u7eed\u7684\u4e0d\u89c4\u5219\u7f51\u683c\u5e27\u8f6c\u6362\u4e3a\u89c4\u5219\u76844D\u5f20\u91cf\uff1b2) \u4f7f\u7528\u81ea\u52a8\u89e3\u7801\u5668\u538b\u7f29\u5f20\u91cf\uff0c\u6355\u83b7\u65f6\u7a7a\u76f8\u5173\u6027\uff1b3) \u5f15\u5165\u57fa\u4e8eTransformer\u7684\u63d2\u503c\u6a21\u578b\uff0c\u901a\u8fc7\u8ddf\u8e2a\u4f53\u79ef\u4e2d\u5fc3\u7684\u6f5c\u5728\u5d4c\u5165\u9884\u6d4b\u4e2d\u95f4\u7f51\u683c\u5e27\uff0c\u6d88\u9664\u8fd0\u52a8\u6a21\u7cca\u3002", "result": "N4MC\u5728\u7387\u5931\u771f\u6027\u80fd\u4e0a\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u540c\u65f6\u80fd\u591f\u5b9e\u65f6\u89e3\u78014D\u7f51\u683c\u5e8f\u5217\u3002\u4ee3\u7801\u5df2\u5f00\u6e90\u3002", "conclusion": "N4MC\u6210\u529f\u5c55\u793a\u4e86\u5229\u7528\u65f6\u95f4\u5197\u4f59\u8fdb\u884c4D\u7f51\u683c\u5e8f\u5217\u538b\u7f29\u7684\u6709\u6548\u6027\uff0c\u4e3a\u65f6\u53d8\u7f51\u683c\u6570\u636e\u7684\u9ad8\u6548\u538b\u7f29\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2602.20422", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.20422", "abs": "https://arxiv.org/abs/2602.20422", "authors": ["Hanping Zhang", "Yuhong Guo"], "title": "Diffusion Modulation via Environment Mechanism Modeling for Planning", "comment": null, "summary": "Diffusion models have shown promising capabilities in trajectory generation for planning in offline reinforcement learning (RL). However, conventional diffusion-based planning methods often fail to account for the fact that generating trajectories in RL requires unique consistency between transitions to ensure coherence in real environments. This oversight can result in considerable discrepancies between the generated trajectories and the underlying mechanisms of a real environment. To address this problem, we propose a novel diffusion-based planning method, termed as Diffusion Modulation via Environment Mechanism Modeling (DMEMM). DMEMM modulates diffusion model training by incorporating key RL environment mechanisms, particularly transition dynamics and reward functions. Experimental results demonstrate that DMEMM achieves state-of-the-art performance for planning with offline reinforcement learning.", "AI": {"tldr": "DMEMM\u662f\u4e00\u79cd\u65b0\u7684\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u89c4\u5212\u65b9\u6cd5\uff0c\u901a\u8fc7\u5efa\u6a21\u73af\u5883\u673a\u5236\uff08\u8f6c\u79fb\u52a8\u6001\u548c\u5956\u52b1\u51fd\u6570\uff09\u6765\u8c03\u5236\u6269\u6563\u6a21\u578b\u8bad\u7ec3\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u6269\u6563\u89c4\u5212\u65b9\u6cd5\u5728\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e2d\u8f68\u8ff9\u751f\u6210\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u89c4\u5212\u65b9\u6cd5\u5728\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e2d\u751f\u6210\u8f68\u8ff9\u65f6\uff0c\u5f80\u5f80\u5ffd\u7565\u4e86\u8f68\u8ff9\u4e2d\u8f6c\u79fb\u4e4b\u95f4\u7684\u4e00\u81f4\u6027\u8981\u6c42\uff0c\u5bfc\u81f4\u751f\u6210\u7684\u8f68\u8ff9\u4e0e\u771f\u5b9e\u73af\u5883\u673a\u5236\u5b58\u5728\u663e\u8457\u5dee\u5f02\u3002", "method": "\u63d0\u51faDMEMM\u65b9\u6cd5\uff0c\u901a\u8fc7\u5efa\u6a21\u73af\u5883\u7684\u5173\u952e\u673a\u5236\uff08\u7279\u522b\u662f\u8f6c\u79fb\u52a8\u6001\u548c\u5956\u52b1\u51fd\u6570\uff09\u6765\u8c03\u5236\u6269\u6563\u6a21\u578b\u7684\u8bad\u7ec3\u8fc7\u7a0b\uff0c\u786e\u4fdd\u751f\u6210\u7684\u8f68\u8ff9\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u5177\u6709\u4e00\u81f4\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cDMEMM\u5728\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u7684\u89c4\u5212\u4efb\u52a1\u4e2d\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u5c06\u73af\u5883\u673a\u5236\u5efa\u6a21\u878d\u5165\u6269\u6563\u6a21\u578b\u8bad\u7ec3\uff0cDMEMM\u80fd\u591f\u751f\u6210\u66f4\u7b26\u5408\u771f\u5b9e\u73af\u5883\u4e00\u81f4\u6027\u7684\u8f68\u8ff9\uff0c\u4ece\u800c\u63d0\u5347\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e2d\u89c4\u5212\u7684\u6548\u679c\u3002"}}
{"id": "2602.20300", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.20300", "abs": "https://arxiv.org/abs/2602.20300", "authors": ["William Watson", "Nicole Cho", "Sumitra Ganesh", "Manuela Veloso"], "title": "What Makes a Good Query? Measuring the Impact of Human-Confusing Linguistic Features on LLM Performance", "comment": "EACL 2026 Findings", "summary": "Large Language Model (LLM) hallucinations are usually treated as defects of the model or its decoding strategy. Drawing on classical linguistics, we argue that a query's form can also shape a listener's (and model's) response. We operationalize this insight by constructing a 22-dimension query feature vector covering clause complexity, lexical rarity, and anaphora, negation, answerability, and intention grounding, all known to affect human comprehension. Using 369,837 real-world queries, we ask: Are there certain types of queries that make hallucination more likely? A large-scale analysis reveals a consistent \"risk landscape\": certain features such as deep clause nesting and underspecification align with higher hallucination propensity. In contrast, clear intention grounding and answerability align with lower hallucination rates. Others, including domain specificity, show mixed, dataset- and model-dependent effects. Thus, these findings establish an empirically observable query-feature representation correlated with hallucination risk, paving the way for guided query rewriting and future intervention studies.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u67e5\u8be2\u8bed\u53e5\u7684\u7279\u5b9a\u7279\u5f81\uff08\u5982\u4ece\u53e5\u5d4c\u5957\u6df1\u5ea6\u3001\u6307\u4ee3\u4e0d\u660e\u786e\u7b49\uff09\u4e0eLLM\u5e7b\u89c9\u98ce\u9669\u76f8\u5173\uff0c\u800c\u610f\u56fe\u660e\u786e\u548c\u53ef\u56de\u7b54\u6027\u5219\u964d\u4f4e\u5e7b\u89c9\u6982\u7387", "motivation": "\u4f20\u7edf\u4e0a\u5c06LLM\u5e7b\u89c9\u89c6\u4e3a\u6a21\u578b\u6216\u89e3\u7801\u7b56\u7565\u7684\u7f3a\u9677\uff0c\u4f46\u672c\u6587\u4ece\u8bed\u8a00\u5b66\u89d2\u5ea6\u63a2\u8ba8\u67e5\u8be2\u8bed\u53e5\u7684\u5f62\u5f0f\u7279\u5f81\u5982\u4f55\u5f71\u54cd\u6a21\u578b\u54cd\u5e94\u548c\u5e7b\u89c9\u6982\u7387", "method": "\u6784\u5efa22\u7ef4\u67e5\u8be2\u7279\u5f81\u5411\u91cf\uff0c\u6db5\u76d6\u4ece\u53e5\u590d\u6742\u5ea6\u3001\u8bcd\u6c47\u7a00\u6709\u5ea6\u3001\u6307\u4ee3\u3001\u5426\u5b9a\u3001\u53ef\u56de\u7b54\u6027\u548c\u610f\u56fe\u57fa\u7840\u7b49\u8bed\u8a00\u5b66\u7279\u5f81\uff0c\u4f7f\u7528369,837\u4e2a\u771f\u5b9e\u4e16\u754c\u67e5\u8be2\u8fdb\u884c\u5927\u89c4\u6a21\u5206\u6790", "result": "\u53d1\u73b0\u4e00\u81f4\u7684\"\u98ce\u9669\u56fe\u8c31\"\uff1a\u67d0\u4e9b\u7279\u5f81\uff08\u5982\u6df1\u5c42\u4ece\u53e5\u5d4c\u5957\u548c\u6307\u4ee3\u4e0d\u660e\u786e\uff09\u4e0e\u66f4\u9ad8\u7684\u5e7b\u89c9\u503e\u5411\u76f8\u5173\uff0c\u800c\u6e05\u6670\u7684\u610f\u56fe\u57fa\u7840\u548c\u53ef\u56de\u7b54\u6027\u5219\u4e0e\u8f83\u4f4e\u7684\u5e7b\u89c9\u7387\u76f8\u5173\uff1b\u5176\u4ed6\u7279\u5f81\uff08\u5982\u9886\u57df\u7279\u5f02\u6027\uff09\u5219\u5448\u73b0\u6df7\u5408\u3001\u6570\u636e\u96c6\u548c\u6a21\u578b\u4f9d\u8d56\u6548\u5e94", "conclusion": "\u5efa\u7acb\u4e86\u4e0e\u5e7b\u89c9\u98ce\u9669\u76f8\u5173\u7684\u53ef\u89c2\u5bdf\u67e5\u8be2\u7279\u5f81\u8868\u793a\uff0c\u4e3a\u5f15\u5bfc\u6027\u67e5\u8be2\u91cd\u5199\u548c\u672a\u6765\u5e72\u9884\u7814\u7a76\u94fa\u5e73\u4e86\u9053\u8def"}}
{"id": "2602.20328", "categories": ["cs.CV", "eess.IV", "math.OC"], "pdf": "https://arxiv.org/pdf/2602.20328", "abs": "https://arxiv.org/abs/2602.20328", "authors": ["Romario Gualdr\u00f3n-Hurtado", "Roman Jacome", "Rafael S. Suarez", "Henry Arguello"], "title": "GSNR: Graph Smooth Null-Space Representation for Inverse Problems", "comment": "23 pages, 24 figures, Accepted to The IEEE/CVF Conference on Computer Vision and Pattern Recognition 2026", "summary": "Inverse problems in imaging are ill-posed, leading to infinitely many solutions consistent with the measurements due to the non-trivial null-space of the sensing matrix. Common image priors promote solutions on the general image manifold, such as sparsity, smoothness, or score function. However, as these priors do not constrain the null-space component, they can bias the reconstruction. Thus, we aim to incorporate meaningful null-space information in the reconstruction framework. Inspired by smooth image representation on graphs, we propose Graph-Smooth Null-Space Representation (GSNR), a mechanism that imposes structure only into the invisible component. Particularly, given a graph Laplacian, we construct a null-restricted Laplacian that encodes similarity between neighboring pixels in the null-space signal, and we design a low-dimensional projection matrix from the $p$-smoothest spectral graph modes (lowest graph frequencies). This approach has strong theoretical and practical implications: i) improved convergence via a null-only graph regularizer, ii) better coverage, how much null-space variance is captured by $p$ modes, and iii) high predictability, how well these modes can be inferred from the measurements. GSNR is incorporated into well-known inverse problem solvers, e.g., PnP, DIP, and diffusion solvers, in four scenarios: image deblurring, compressed sensing, demosaicing, and image super-resolution, providing consistent improvement of up to 4.3 dB over baseline formulations and up to 1 dB compared with end-to-end learned models in terms of PSNR.", "AI": {"tldr": "\u63d0\u51faGraph-Smooth Null-Space Representation (GSNR)\u65b9\u6cd5\uff0c\u901a\u8fc7\u56fe\u62c9\u666e\u62c9\u65af\u5728\u9006\u95ee\u9898\u7684\u96f6\u7a7a\u95f4\u5206\u91cf\u4e2d\u5f15\u5165\u7ed3\u6784\u7ea6\u675f\uff0c\u63d0\u9ad8\u56fe\u50cf\u91cd\u5efa\u8d28\u91cf", "motivation": "\u4f20\u7edf\u56fe\u50cf\u5148\u9a8c\uff08\u5982\u7a00\u758f\u6027\u3001\u5e73\u6ed1\u6027\uff09\u65e0\u6cd5\u7ea6\u675f\u9006\u95ee\u9898\u4e2d\u7684\u96f6\u7a7a\u95f4\u5206\u91cf\uff0c\u53ef\u80fd\u5bfc\u81f4\u91cd\u5efa\u7ed3\u679c\u6709\u504f\u5dee\u3002\u9700\u8981\u5c06\u6709\u610f\u4e49\u7684\u7ed3\u6784\u4fe1\u606f\u5f15\u5165\u96f6\u7a7a\u95f4\u8868\u793a\u4e2d", "method": "\u57fa\u4e8e\u56fe\u5e73\u6ed1\u8868\u793a\uff0c\u6784\u5efa\u96f6\u7a7a\u95f4\u53d7\u9650\u7684\u62c9\u666e\u62c9\u65af\u77e9\u9635\uff0c\u7f16\u7801\u96f6\u7a7a\u95f4\u4fe1\u53f7\u4e2d\u76f8\u90bb\u50cf\u7d20\u7684\u76f8\u4f3c\u6027\u3002\u8bbe\u8ba1\u4ece\u6700\u5e73\u6ed1\u7684\u8c31\u56fe\u6a21\u5f0f\uff08\u6700\u4f4e\u56fe\u9891\u7387\uff09\u7684\u4f4e\u7ef4\u6295\u5f71\u77e9\u9635", "result": "\u5728\u56fe\u50cf\u53bb\u6a21\u7cca\u3001\u538b\u7f29\u611f\u77e5\u3001\u53bb\u9a6c\u8d5b\u514b\u548c\u8d85\u5206\u8fa8\u7387\u56db\u79cd\u573a\u666f\u4e2d\uff0cGSNR\u65b9\u6cd5\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u63d0\u5347\u8fbe4.3dB\uff0c\u76f8\u6bd4\u7aef\u5230\u7aef\u5b66\u4e60\u6a21\u578b\u63d0\u5347\u8fbe1dB\uff08PSNR\u6307\u6807\uff09", "conclusion": "GSNR\u901a\u8fc7\u5c06\u7ed3\u6784\u7ea6\u675f\u5f15\u5165\u9006\u95ee\u9898\u7684\u96f6\u7a7a\u95f4\u5206\u91cf\uff0c\u663e\u8457\u6539\u5584\u4e86\u56fe\u50cf\u91cd\u5efa\u8d28\u91cf\uff0c\u5177\u6709\u7406\u8bba\u610f\u4e49\u548c\u5b9e\u9645\u5e94\u7528\u4ef7\u503c"}}
{"id": "2602.20595", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.20595", "abs": "https://arxiv.org/abs/2602.20595", "authors": ["Longxiang Wang", "Xiang Zheng", "Xuhao Zhang", "Yao Zhang", "Ye Wu", "Cong Wang"], "title": "OptiLeak: Efficient Prompt Reconstruction via Reinforcement Learning in Multi-tenant LLM Services", "comment": null, "summary": "Multi-tenant LLM serving frameworks widely adopt shared Key-Value caches to enhance efficiency. However, this creates side-channel vulnerabilities enabling prompt leakage attacks. Prior studies identified these attack surfaces yet focused on expanding attack vectors rather than optimizing attack performance, reporting impractically high attack costs that underestimate the true privacy risk. We propose OptiLeak, a reinforcement learning-enhanced framework that maximizes prompt reconstruction efficiency through two-stage fine-tuning. Our key insight is that domain-specific ``hard tokens'' -- terms difficult to predict yet carrying sensitive information -- can be automatically identified via likelihood ranking and used to construct preference pairs for Direct Preference Optimization, eliminating manual annotation. This enables effective preference alignment while avoiding the overfitting issues of extended supervised fine-tuning. Evaluated on three benchmarks spanning medical and financial domains, OptiLeak achieves up to $12.48\\times$ reduction in average requests per token compared to baseline approaches, with consistent improvements across model scales from 3B to 14B parameters. Our findings demonstrate that cache-based prompt leakage poses a more severe threat than previously reported, underscoring the need for robust cache isolation in production deployments.", "AI": {"tldr": "OptiLeak\uff1a\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u4f18\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u5fae\u8c03\u6700\u5927\u5316\u63d0\u793a\u91cd\u5efa\u6548\u7387\uff0c\u663e\u8457\u964d\u4f4e\u7f13\u5b58\u4fa7\u4fe1\u9053\u653b\u51fb\u6210\u672c\uff0c\u63ed\u793aLLM\u5171\u4eab\u7f13\u5b58\u5b58\u5728\u66f4\u4e25\u91cd\u7684\u9690\u79c1\u98ce\u9669", "motivation": "\u591a\u79df\u6237LLM\u670d\u52a1\u6846\u67b6\u5e7f\u6cdb\u91c7\u7528\u5171\u4eab\u952e\u503c\u7f13\u5b58\u63d0\u9ad8\u6548\u7387\uff0c\u4f46\u8fd9\u5e26\u6765\u4e86\u4fa7\u4fe1\u9053\u6f0f\u6d1e\uff0c\u53ef\u80fd\u5bfc\u81f4\u63d0\u793a\u6cc4\u9732\u653b\u51fb\u3002\u5148\u524d\u7814\u7a76\u867d\u7136\u8bc6\u522b\u4e86\u653b\u51fb\u9762\uff0c\u4f46\u4e3b\u8981\u5173\u6ce8\u6269\u5c55\u653b\u51fb\u5411\u91cf\u800c\u975e\u4f18\u5316\u653b\u51fb\u6027\u80fd\uff0c\u62a5\u544a\u7684\u653b\u51fb\u6210\u672c\u8fc7\u9ad8\uff0c\u4f4e\u4f30\u4e86\u5b9e\u9645\u7684\u9690\u79c1\u98ce\u9669", "method": "\u63d0\u51faOptiLeak\u6846\u67b6\uff0c\u91c7\u7528\u5f3a\u5316\u5b66\u4e60\u589e\u5f3a\u7684\u4e24\u9636\u6bb5\u5fae\u8c03\u65b9\u6cd5\u3002\u6838\u5fc3\u6d1e\u5bdf\u662f\u901a\u8fc7\u4f3c\u7136\u6392\u5e8f\u81ea\u52a8\u8bc6\u522b\u9886\u57df\u7279\u5b9a\u7684\"\u786c\u6807\u8bb0\"\uff08\u96be\u4ee5\u9884\u6d4b\u4f46\u643a\u5e26\u654f\u611f\u4fe1\u606f\u7684\u672f\u8bed\uff09\uff0c\u5e76\u5229\u7528\u8fd9\u4e9b\u6807\u8bb0\u6784\u5efa\u504f\u597d\u5bf9\u8fdb\u884c\u76f4\u63a5\u504f\u597d\u4f18\u5316\uff0c\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\u3002\u8fd9\u5b9e\u73b0\u4e86\u6709\u6548\u7684\u504f\u597d\u5bf9\u9f50\uff0c\u540c\u65f6\u907f\u514d\u4e86\u6269\u5c55\u76d1\u7763\u5fae\u8c03\u7684\u8fc7\u62df\u5408\u95ee\u9898", "result": "\u5728\u533b\u7597\u548c\u91d1\u878d\u9886\u57df\u7684\u4e09\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cOptiLeak\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u5b9e\u73b0\u4e86\u9ad8\u8fbe12.48\u500d\u7684\u6bcf\u4e2a\u6807\u8bb0\u5e73\u5747\u8bf7\u6c42\u6570\u51cf\u5c11\uff0c\u57283B\u523014B\u53c2\u6570\u89c4\u6a21\u7684\u5404\u79cd\u6a21\u578b\u4e0a\u5747\u8868\u73b0\u51fa\u4e00\u81f4\u7684\u6539\u8fdb", "conclusion": "\u57fa\u4e8e\u7f13\u5b58\u7684\u63d0\u793a\u6cc4\u9732\u653b\u51fb\u6bd4\u5148\u524d\u62a5\u544a\u7684\u8981\u4e25\u91cd\u5f97\u591a\uff0c\u7a81\u663e\u4e86\u5728\u751f\u4ea7\u90e8\u7f72\u4e2d\u9700\u8981\u5f3a\u5927\u7684\u7f13\u5b58\u9694\u79bb\u673a\u5236\u3002OptiLeak\u6846\u67b6\u663e\u8457\u63d0\u9ad8\u4e86\u653b\u51fb\u6548\u7387\uff0c\u63ed\u793a\u4e86\u5171\u4eab\u7f13\u5b58\u67b6\u6784\u7684\u5b9e\u9645\u9690\u79c1\u98ce\u9669"}}
{"id": "2602.20598", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.20598", "abs": "https://arxiv.org/abs/2602.20598", "authors": ["Shoma Ansai", "Masaki Waga"], "title": "A Case Study on Runtime Verification of a Continuous Deployment Process", "comment": "Presented at the Runtime Verification Case-Studies Workshop 2025 (RVCase'25), with no formal proceedings", "summary": "We report our experience in applying runtime monitoring to a FluxCD-based continuous deployment (CD) process. Our target system consists of GitHub Actions, GitHub Container Registry (GHCR), FluxCD, and an application running on Kubernetes. We monitored its logs using SyMon. In our setting, we regard a deployment update as detected when FluxCD's polling log resolves the latest image tag. Through the case study, we found that FluxCD did not always detect a new image within five minutes after it was pushed to GHCR, whereas it always did so within ten minutes in the collected logs. Moreover, our results show that SyMon is fast enough for near-real-time monitoring in our setting.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5e94\u7528\u8fd0\u884c\u65f6\u76d1\u63a7\u5230\u57fa\u4e8eFluxCD\u7684\u6301\u7eed\u90e8\u7f72\u6d41\u7a0b\uff0c\u53d1\u73b0FluxCD\u5728\u955c\u50cf\u63a8\u9001\u5230GHCR\u540e5\u5206\u949f\u5185\u4e0d\u4e00\u5b9a\u80fd\u68c0\u6d4b\u5230\u65b0\u955c\u50cf\uff0c\u4f46\u572810\u5206\u949f\u5185\u603b\u80fd\u68c0\u6d4b\u5230\uff0c\u4e14SyMon\u76d1\u63a7\u5de5\u5177\u8db3\u591f\u5feb\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u63a2\u7d22\u5728\u57fa\u4e8eFluxCD\u7684\u6301\u7eed\u90e8\u7f72\u6d41\u7a0b\u4e2d\u5e94\u7528\u8fd0\u884c\u65f6\u76d1\u63a7\u7684\u5b9e\u9645\u6548\u679c\uff0c\u4e86\u89e3\u90e8\u7f72\u66f4\u65b0\u7684\u68c0\u6d4b\u5ef6\u8fdf\u60c5\u51b5\uff0c\u5e76\u8bc4\u4f30\u76d1\u63a7\u5de5\u5177\u7684\u6027\u80fd\u3002", "method": "\u7814\u7a76\u65b9\u6cd5\u5305\u62ec\uff1a1\uff09\u6784\u5efa\u7531GitHub Actions\u3001GitHub Container Registry\u3001FluxCD\u548cKubernetes\u5e94\u7528\u7ec4\u6210\u7684\u7cfb\u7edf\uff1b2\uff09\u4f7f\u7528SyMon\u76d1\u63a7\u7cfb\u7edf\u65e5\u5fd7\uff1b3\uff09\u5b9a\u4e49\u90e8\u7f72\u66f4\u65b0\u68c0\u6d4b\u6807\u51c6\u4e3aFluxCD\u8f6e\u8be2\u65e5\u5fd7\u89e3\u6790\u5230\u6700\u65b0\u955c\u50cf\u6807\u7b7e\uff1b4\uff09\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u6536\u96c6\u548c\u5206\u6790\u6570\u636e\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u663e\u793a\uff1a1\uff09FluxCD\u5728\u955c\u50cf\u63a8\u9001\u5230GHCR\u540e5\u5206\u949f\u5185\u4e0d\u4e00\u5b9a\u80fd\u68c0\u6d4b\u5230\u65b0\u955c\u50cf\uff1b2\uff09\u4f46\u5728\u6536\u96c6\u7684\u65e5\u5fd7\u4e2d\uff0cFluxCD\u603b\u80fd\u572810\u5206\u949f\u5185\u68c0\u6d4b\u5230\u65b0\u955c\u50cf\uff1b3\uff09SyMon\u76d1\u63a7\u5de5\u5177\u5728\u5b9e\u9a8c\u8bbe\u7f6e\u4e2d\u8db3\u591f\u5feb\uff0c\u80fd\u591f\u652f\u6301\u8fd1\u5b9e\u65f6\u76d1\u63a7\u3002", "conclusion": "\u7ed3\u8bba\u8868\u660e\uff1a\u5728\u57fa\u4e8eFluxCD\u7684\u6301\u7eed\u90e8\u7f72\u73af\u5883\u4e2d\uff0c\u90e8\u7f72\u66f4\u65b0\u7684\u68c0\u6d4b\u5b58\u5728\u4e00\u5b9a\u5ef6\u8fdf\uff085-10\u5206\u949f\uff09\uff0c\u4f46SyMon\u80fd\u591f\u6709\u6548\u652f\u6301\u8fd1\u5b9e\u65f6\u76d1\u63a7\uff0c\u4e3a\u7c7b\u4f3c\u7cfb\u7edf\u7684\u76d1\u63a7\u5b9e\u8df5\u63d0\u4f9b\u4e86\u5b9e\u8bc1\u53c2\u8003\u3002"}}
{"id": "2602.20332", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.20332", "abs": "https://arxiv.org/abs/2602.20332", "authors": ["Nicole Cho", "William Watson", "Alec Koppel", "Sumitra Ganesh", "Manuela Veloso"], "title": "No One Size Fits All: QueryBandits for Hallucination Mitigation", "comment": null, "summary": "Advanced reasoning capabilities in Large Language Models (LLMs) have led to more frequent hallucinations; yet most mitigation work focuses on open-source models for post-hoc detection and parameter editing. The dearth of studies focusing on hallucinations in closed-source models is especially concerning, as they constitute the vast majority of models in institutional deployments. We introduce QueryBandits, a model-agnostic contextual bandit framework that adaptively learns online to select the optimal query-rewrite strategy by leveraging an empirically validated and calibrated reward function. Across 16 QA scenarios, our top QueryBandit (Thompson Sampling) achieves an 87.5% win rate over a No-Rewrite baseline and outperforms zero-shot static policies (e.g., Paraphrase or Expand) by 42.6% and 60.3%, respectively. Moreover, all contextual bandits outperform vanilla bandits across all datasets, with higher feature variance coinciding with greater variance in arm selection. This substantiates our finding that there is no single rewrite policy optimal for all queries. We also discover that certain static policies incur higher cumulative regret than No-Rewrite, indicating that an inflexible query-rewriting policy can worsen hallucinations. Thus, learning an online policy over semantic features with QueryBandits can shift model behavior purely through forward-pass mechanisms, enabling its use with closed-source models and bypassing the need for retraining or gradient-based adaptation.", "AI": {"tldr": "QueryBandits\u662f\u4e00\u4e2a\u6a21\u578b\u65e0\u5173\u7684\u4e0a\u4e0b\u6587\u8d4c\u535a\u673a\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u7ebf\u5b66\u4e60\u9009\u62e9\u6700\u4f18\u67e5\u8be2\u91cd\u5199\u7b56\u7565\u6765\u51cf\u5c11\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u7279\u522b\u9488\u5bf9\u95ed\u6e90\u6a21\u578b\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u867d\u7136\u5177\u5907\u5148\u8fdb\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u5e7b\u89c9\u95ee\u9898\u65e5\u76ca\u4e25\u91cd\u3002\u73b0\u6709\u7f13\u89e3\u5de5\u4f5c\u4e3b\u8981\u5173\u6ce8\u5f00\u6e90\u6a21\u578b\u7684\u540e\u68c0\u6d4b\u548c\u53c2\u6570\u7f16\u8f91\uff0c\u800c\u95ed\u6e90\u6a21\u578b\u5728\u5b9e\u9645\u673a\u6784\u90e8\u7f72\u4e2d\u5360\u7edd\u5927\u591a\u6570\uff0c\u5374\u7f3a\u4e4f\u76f8\u5173\u7814\u7a76\u3002", "method": "\u63d0\u51faQueryBandits\u6846\u67b6\uff0c\u8fd9\u662f\u4e00\u4e2a\u6a21\u578b\u65e0\u5173\u7684\u4e0a\u4e0b\u6587\u8d4c\u535a\u673a\u7cfb\u7edf\u3002\u5b83\u901a\u8fc7\u5728\u7ebf\u5b66\u4e60\u81ea\u9002\u5e94\u5730\u9009\u62e9\u6700\u4f18\u67e5\u8be2\u91cd\u5199\u7b56\u7565\uff0c\u5229\u7528\u7ecf\u9a8c\u9a8c\u8bc1\u548c\u6821\u51c6\u7684\u5956\u52b1\u51fd\u6570\uff0c\u4ec5\u901a\u8fc7\u524d\u5411\u4f20\u64ad\u673a\u5236\u6539\u53d8\u6a21\u578b\u884c\u4e3a\u3002", "result": "\u572816\u4e2aQA\u573a\u666f\u4e2d\uff0c\u6700\u4f73QueryBandits\uff08Thompson Sampling\uff09\u76f8\u6bd4\u65e0\u91cd\u5199\u57fa\u7ebf\u83b7\u5f9787.5%\u7684\u80dc\u7387\uff0c\u5206\u522b\u6bd4\u96f6\u6837\u672c\u9759\u6001\u7b56\u7565\uff08\u5982Paraphrase\u6216Expand\uff09\u9ad8\u51fa42.6%\u548c60.3%\u3002\u6240\u6709\u4e0a\u4e0b\u6587\u8d4c\u535a\u673a\u5728\u6240\u6709\u6570\u636e\u96c6\u4e0a\u90fd\u4f18\u4e8e\u666e\u901a\u8d4c\u535a\u673a\u3002", "conclusion": "\u6ca1\u6709\u5355\u4e00\u7684\u91cd\u5199\u7b56\u7565\u9002\u7528\u4e8e\u6240\u6709\u67e5\u8be2\uff0c\u4e0d\u7075\u6d3b\u7684\u67e5\u8be2\u91cd\u5199\u7b56\u7565\u53ef\u80fd\u52a0\u5267\u5e7b\u89c9\u3002QueryBandits\u901a\u8fc7\u5728\u7ebf\u5b66\u4e60\u8bed\u4e49\u7279\u5f81\u7b56\u7565\uff0c\u53ef\u4ee5\u4ec5\u901a\u8fc7\u524d\u5411\u4f20\u64ad\u673a\u5236\u6539\u53d8\u6a21\u578b\u884c\u4e3a\uff0c\u9002\u7528\u4e8e\u95ed\u6e90\u6a21\u578b\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u6216\u57fa\u4e8e\u68af\u5ea6\u7684\u9002\u5e94\u3002"}}
{"id": "2602.20426", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.20426", "abs": "https://arxiv.org/abs/2602.20426", "authors": ["Ruocheng Guo", "Kaiwen Dong", "Xiang Gao", "Kamalika Das"], "title": "Learning to Rewrite Tool Descriptions for Reliable LLM-Agent Tool Use", "comment": "Preprint", "summary": "The performance of LLM-based agents depends not only on the agent itself but also on the quality of the tool interfaces it consumes. While prior work has focused heavily on agent fine-tuning, tool interfaces-including natural language descriptions and parameter schemas-remain largely human-oriented and often become a bottleneck, especially when agents must select from large candidate tool sets. Existing approaches to improving tool interfaces rely on execution traces, which are frequently unavailable in cold-start or privacy-constrained settings, and typically optimize each tool independently, limiting scalability and generalization to unseen tools. We propose Trace-Free+, a curriculum learning framework that progressively transfers supervision from trace-rich settings to trace-free deployment, encouraging the model to abstract reusable interface-usage patterns and tool usage outcomes. To support this approach, we construct a large-scale dataset of high-quality tool interfaces using a structured workflow over a diverse collection of tools. Experiments on StableToolBench and RestBench show consistent gains on unseen tools, strong cross-domain generalization, and robustness as the number of candidate tools scales to over 100, demonstrating that tool interface optimization is a practical and deployable complement to agent fine-tuning.", "AI": {"tldr": "Trace-Free+\uff1a\u4e00\u79cd\u65e0\u9700\u6267\u884c\u8f68\u8ff9\u7684\u8bfe\u7a0b\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u6e10\u8fdb\u5f0f\u76d1\u7763\u8f6c\u79fb\u4f18\u5316\u5de5\u5177\u63a5\u53e3\uff0c\u63d0\u5347LLM\u667a\u80fd\u4f53\u5728\u672a\u89c1\u5de5\u5177\u4e0a\u7684\u6027\u80fd\u8868\u73b0", "motivation": "LLM\u667a\u80fd\u4f53\u7684\u6027\u80fd\u4e0d\u4ec5\u53d6\u51b3\u4e8e\u667a\u80fd\u4f53\u672c\u8eab\uff0c\u8fd8\u4f9d\u8d56\u4e8e\u6240\u4f7f\u7528\u5de5\u5177\u63a5\u53e3\u7684\u8d28\u91cf\u3002\u73b0\u6709\u5de5\u5177\u63a5\u53e3\u4e3b\u8981\u9762\u5411\u4eba\u7c7b\u8bbe\u8ba1\uff0c\u5728\u5927\u89c4\u6a21\u5019\u9009\u5de5\u5177\u96c6\u4e2d\u6210\u4e3a\u74f6\u9888\u3002\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u6267\u884c\u8f68\u8ff9\uff0c\u4f46\u5728\u51b7\u542f\u52a8\u6216\u9690\u79c1\u53d7\u9650\u573a\u666f\u4e2d\u96be\u4ee5\u83b7\u53d6\uff0c\u4e14\u901a\u5e38\u72ec\u7acb\u4f18\u5316\u6bcf\u4e2a\u5de5\u5177\uff0c\u9650\u5236\u4e86\u53ef\u6269\u5c55\u6027\u548c\u5bf9\u672a\u89c1\u5de5\u5177\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u63d0\u51faTrace-Free+\u8bfe\u7a0b\u5b66\u4e60\u6846\u67b6\uff0c\u6e10\u8fdb\u5f0f\u5730\u5c06\u76d1\u7763\u4ece\u8f68\u8ff9\u4e30\u5bcc\u7684\u8bbe\u7f6e\u8f6c\u79fb\u5230\u65e0\u8f68\u8ff9\u90e8\u7f72\u73af\u5883\uff0c\u9f13\u52b1\u6a21\u578b\u62bd\u8c61\u53ef\u91cd\u7528\u7684\u63a5\u53e3\u4f7f\u7528\u6a21\u5f0f\u548c\u5de5\u5177\u4f7f\u7528\u7ed3\u679c\u3002\u901a\u8fc7\u7ed3\u6784\u5316\u5de5\u4f5c\u6d41\u7a0b\u6784\u5efa\u5927\u89c4\u6a21\u9ad8\u8d28\u91cf\u5de5\u5177\u63a5\u53e3\u6570\u636e\u96c6\u6765\u652f\u6301\u8be5\u65b9\u6cd5\u3002", "result": "\u5728StableToolBench\u548cRestBench\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5728\u672a\u89c1\u5de5\u5177\u4e0a\u83b7\u5f97\u4e00\u81f4\u6027\u80fd\u63d0\u5347\uff0c\u5177\u6709\u5f3a\u5927\u7684\u8de8\u9886\u57df\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u4e14\u5728\u5019\u9009\u5de5\u5177\u6570\u91cf\u8d85\u8fc7100\u65f6\u4ecd\u4fdd\u6301\u9c81\u68d2\u6027\uff0c\u8bc1\u660e\u5de5\u5177\u63a5\u53e3\u4f18\u5316\u662f\u667a\u80fd\u4f53\u5fae\u8c03\u7684\u5b9e\u9645\u53ef\u884c\u8865\u5145\u65b9\u6848\u3002", "conclusion": "\u5de5\u5177\u63a5\u53e3\u4f18\u5316\u662f\u63d0\u5347LLM\u667a\u80fd\u4f53\u6027\u80fd\u7684\u91cd\u8981\u8865\u5145\u65b9\u5411\u3002Trace-Free+\u6846\u67b6\u901a\u8fc7\u8bfe\u7a0b\u5b66\u4e60\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u5bf9\u6267\u884c\u8f68\u8ff9\u7684\u4f9d\u8d56\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u5bf9\u672a\u89c1\u5de5\u5177\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4e3a\u5927\u89c4\u6a21\u5de5\u5177\u96c6\u573a\u666f\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u90e8\u7f72\u65b9\u6848\u3002"}}
{"id": "2602.20657", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2602.20657", "abs": "https://arxiv.org/abs/2602.20657", "authors": ["Shahzad Ahmad", "Stefan Rass", "Zahra Seyedi"], "title": "Post-Quantum Sanitizable Signatures from McEliece-Based Chameleon Hashing", "comment": "19 pages", "summary": "We introduce a novel post-quantum sanitizable signature scheme constructed upon a chameleon hash function derived from the McEliece cryptosystem. In this design, the designated sanitizer possesses the inherent trapdoor of a Goppa code, which facilitates controlled collision-finding via Patterson decoding. This mechanism enables authorized modification of specific message blocks while ensuring all other content remains immutably bound. We provide formal security definitions and rigorous proofs of existential unforgeability and immutability, grounded in the hardness of syndrome decoding in the random-oracle model, where a robust random oracle thwarts trivial linear hash collisions. A key innovation lies in our precise characterization of the transparency property: by imposing a specific weight constraint on the randomizers generated by the signer, we achieve perfect transparency, rendering sanitized signatures indistinguishable from freshly signed ones. This work establishes the first transparent, code-based, post-quantum sanitizable signature scheme, offering strong theoretical guarantees and a pathway for practical deployment in long-term secure applications.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u9996\u4e2a\u57fa\u4e8e\u7f16\u7801\u7684\u3001\u540e\u91cf\u5b50\u5b89\u5168\u7684\u53ef\u51c0\u5316\u7b7e\u540d\u65b9\u6848\uff0c\u4f7f\u7528\u57fa\u4e8eMcEliece\u5bc6\u7801\u7cfb\u7edf\u7684\u53d8\u8272\u9f99\u54c8\u5e0c\u51fd\u6570\uff0c\u5b9e\u73b0\u4e86\u5b8c\u7f8e\u7684\u900f\u660e\u6027\uff0c\u4f7f\u51c0\u5316\u540e\u7684\u7b7e\u540d\u4e0e\u539f\u59cb\u7b7e\u540d\u65e0\u6cd5\u533a\u5206\u3002", "motivation": "\u968f\u7740\u91cf\u5b50\u8ba1\u7b97\u7684\u53d1\u5c55\uff0c\u4f20\u7edf\u7b7e\u540d\u65b9\u6848\u9762\u4e34\u5b89\u5168\u5a01\u80c1\uff0c\u9700\u8981\u8bbe\u8ba1\u540e\u91cf\u5b50\u5b89\u5168\u7684\u53ef\u51c0\u5316\u7b7e\u540d\u65b9\u6848\u3002\u73b0\u6709\u65b9\u6848\u7f3a\u4e4f\u57fa\u4e8e\u7f16\u7801\u7684\u900f\u660e\u53ef\u51c0\u5316\u7b7e\u540d\u65b9\u6848\uff0c\u8fd9\u9650\u5236\u4e86\u5728\u957f\u671f\u5b89\u5168\u5e94\u7528\u4e2d\u7684\u90e8\u7f72\u3002", "method": "\u65b9\u6848\u57fa\u4e8eMcEliece\u5bc6\u7801\u7cfb\u7edf\u6784\u5efa\u53d8\u8272\u9f99\u54c8\u5e0c\u51fd\u6570\uff0c\u6307\u5b9a\u51c0\u5316\u8005\u62e5\u6709Goppa\u7801\u7684\u9677\u95e8\uff0c\u901a\u8fc7Patterson\u89e3\u7801\u5b9e\u73b0\u53ef\u63a7\u78b0\u649e\u67e5\u627e\u3002\u901a\u8fc7\u65bd\u52a0\u7279\u5b9a\u7684\u968f\u673a\u5316\u6743\u91cd\u7ea6\u675f\uff0c\u5b9e\u73b0\u5b8c\u7f8e\u7684\u900f\u660e\u6027\u3002", "result": "\u63d0\u4f9b\u4e86\u5f62\u5f0f\u5316\u7684\u5b89\u5168\u5b9a\u4e49\u548c\u4e25\u683c\u7684\u5b58\u5728\u4e0d\u53ef\u4f2a\u9020\u6027\u548c\u4e0d\u53ef\u53d8\u6027\u8bc1\u660e\uff0c\u57fa\u4e8e\u968f\u673a\u9884\u8a00\u6a21\u578b\u4e2d\u7efc\u5408\u5f81\u89e3\u7801\u7684\u56f0\u96be\u6027\u3002\u5b9e\u73b0\u4e86\u9996\u4e2a\u900f\u660e\u7684\u3001\u57fa\u4e8e\u7f16\u7801\u7684\u3001\u540e\u91cf\u5b50\u5b89\u5168\u7684\u53ef\u51c0\u5316\u7b7e\u540d\u65b9\u6848\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u5efa\u7acb\u4e86\u9996\u4e2a\u900f\u660e\u7684\u3001\u57fa\u4e8e\u7f16\u7801\u7684\u3001\u540e\u91cf\u5b50\u5b89\u5168\u7684\u53ef\u51c0\u5316\u7b7e\u540d\u65b9\u6848\uff0c\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u7406\u8bba\u4fdd\u8bc1\uff0c\u5e76\u4e3a\u957f\u671f\u5b89\u5168\u5e94\u7528\u7684\u5b9e\u9645\u90e8\u7f72\u5f00\u8f9f\u4e86\u9014\u5f84\u3002"}}
{"id": "2602.20610", "categories": ["cs.SE", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.20610", "abs": "https://arxiv.org/abs/2602.20610", "authors": ["Cuong Chi Le", "Minh V. T Pham", "Tung Vu Duy", "Cuong Duc Van", "Huy N. Phan", "Hoang N. Phan", "Tien N. Nguyen"], "title": "SpecMind: Cognitively Inspired, Interactive Multi-Turn Framework for Postcondition Inference", "comment": null, "summary": "Specifications are vital for ensuring program correctness, yet writing them manually remains challenging and time-intensive. Recent large language model (LLM)-based methods have shown successes in generating specifications such as postconditions, but existing single-pass prompting often yields inaccurate results. In this paper, we present SpecMind, a novel framework for postcondition generation that treats LLMs as interactive and exploratory reasoners rather than one-shot generators. SpecMind employs feedback-driven multi-turn prompting approaches, enabling the model to iteratively refine candidate postconditions by incorporating implicit and explicit correctness feedback, while autonomously deciding when to stop. This process fosters deeper code comprehension and improves alignment with true program behavior via exploratory attempts. Our empirical evaluation shows that SpecMind significantly outperforms state-of-the-art approaches in both accuracy and completeness of generated postconditions.", "AI": {"tldr": "SpecMind\uff1a\u4e00\u4e2a\u57fa\u4e8e\u53cd\u9988\u9a71\u52a8\u591a\u8f6e\u63d0\u793a\u7684LLM\u6846\u67b6\uff0c\u7528\u4e8e\u8fed\u4ee3\u751f\u6210\u548c\u4f18\u5316\u7a0b\u5e8f\u540e\u7f6e\u6761\u4ef6\uff0c\u663e\u8457\u63d0\u5347\u51c6\u786e\u6027\u548c\u5b8c\u6574\u6027", "motivation": "\u624b\u52a8\u7f16\u5199\u7a0b\u5e8f\u89c4\u8303\u8017\u65f6\u4e14\u56f0\u96be\uff0c\u73b0\u6709\u57fa\u4e8eLLM\u7684\u5355\u6b21\u63d0\u793a\u65b9\u6cd5\u751f\u6210\u7684\u540e\u7f6e\u6761\u4ef6\u5f80\u5f80\u4e0d\u51c6\u786e\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u81ea\u52a8\u5316\u89c4\u8303\u751f\u6210\u65b9\u6cd5", "method": "\u5c06LLM\u4f5c\u4e3a\u4ea4\u4e92\u5f0f\u63a2\u7d22\u63a8\u7406\u5668\u800c\u975e\u4e00\u6b21\u6027\u751f\u6210\u5668\uff0c\u91c7\u7528\u53cd\u9988\u9a71\u52a8\u7684\u591a\u8f6e\u63d0\u793a\u65b9\u6cd5\uff0c\u901a\u8fc7\u9690\u5f0f\u548c\u663e\u5f0f\u6b63\u786e\u6027\u53cd\u9988\u8fed\u4ee3\u4f18\u5316\u5019\u9009\u540e\u7f6e\u6761\u4ef6\uff0c\u5e76\u81ea\u4e3b\u51b3\u5b9a\u4f55\u65f6\u505c\u6b62", "result": "SpecMind\u5728\u751f\u6210\u540e\u7f6e\u6761\u4ef6\u7684\u51c6\u786e\u6027\u548c\u5b8c\u6574\u6027\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5", "conclusion": "\u901a\u8fc7\u5c06LLM\u4f5c\u4e3a\u4ea4\u4e92\u5f0f\u63a2\u7d22\u63a8\u7406\u5668\uff0c\u91c7\u7528\u53cd\u9988\u9a71\u52a8\u7684\u591a\u8f6e\u63d0\u793a\u65b9\u6cd5\uff0c\u80fd\u591f\u6709\u6548\u63d0\u5347\u7a0b\u5e8f\u540e\u7f6e\u6761\u4ef6\u751f\u6210\u7684\u51c6\u786e\u6027\u548c\u5b8c\u6574\u6027"}}
{"id": "2602.20647", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.20647", "abs": "https://arxiv.org/abs/2602.20647", "authors": ["W. Frederick Zimmerman"], "title": "Semantic Novelty at Scale: Narrative Shape Taxonomy and Readership Prediction in 28,606 Books", "comment": "six figures. dataset available at Hugging Face", "summary": "I introduce semantic novelty--cosine distance between each paragraph's sentence embedding and the running centroid of all preceding paragraphs--as an information-theoretic measure of narrative structure at corpus scale. Applying it to 28,606 books in PG19 (pre-1920 English literature), I compute paragraph-level novelty curves using 768-dimensional SBERT embeddings, then reduce each to a 16-segment Piecewise Aggregate Approximation (PAA). Ward-linkage clustering on PAA vectors reveals eight canonical narrative shape archetypes, from Steep Descent (rapid convergence) to Steep Ascent (escalating unpredictability). Volume--variance of the novelty trajectory--is the strongest length-independent predictor of readership (partial rho = 0.32), followed by speed (rho = 0.19) and Terminal/Initial ratio (rho = 0.19). Circuitousness shows strong raw correlation (rho = 0.41) but is 93 percent correlated with length; after control, partial rho drops to 0.11--demonstrating that naive correlations in corpus studies can be dominated by length confounds. Genre strongly constrains narrative shape (chi squared = 2121.6, p < 10 to the power negative 242), with fiction maintaining plateau profiles while nonfiction front-loads information. Historical analysis shows books became progressively more predictable between 1840 and 1910 (T/I ratio trend r = negative 0.74, p = 0.037). SAX analysis reveals 85 percent signature uniqueness, suggesting each book traces a nearly unique path through semantic space. These findings demonstrate that information-density dynamics, distinct from sentiment or topic, constitute a fundamental dimension of narrative structure with measurable consequences for reader engagement. Dataset: https://huggingface.co/datasets/wfzimmerman/pg19-semantic-novelty", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u8bed\u4e49\u65b0\u9896\u6027\u4f5c\u4e3a\u53d9\u4e8b\u7ed3\u6784\u7684\u5ea6\u91cf\u6307\u6807\uff0c\u5e94\u7528\u4e8e28,606\u672cPG19\u4e66\u7c4d\uff0c\u53d1\u73b08\u79cd\u53d9\u4e8b\u539f\u578b\uff0c\u5176\u4e2d\u65b0\u9896\u6027\u8f68\u8ff9\u7684\u65b9\u5dee\u6700\u80fd\u9884\u6d4b\u8bfb\u8005\u53c2\u4e0e\u5ea6\uff0c\u5e76\u63ed\u793a\u4e86\u4f53\u88c1\u5bf9\u53d9\u4e8b\u5f62\u72b6\u7684\u5f3a\u70c8\u7ea6\u675f\u548c\u5386\u53f2\u8d8b\u52bf\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u5f00\u53d1\u4e00\u79cd\u4fe1\u606f\u8bba\u65b9\u6cd5\u6765\u91cf\u5316\u5927\u89c4\u6a21\u8bed\u6599\u5e93\u4e2d\u7684\u53d9\u4e8b\u7ed3\u6784\uff0c\u8d85\u8d8a\u4f20\u7edf\u7684\u60c5\u611f\u6216\u4e3b\u9898\u5206\u6790\uff0c\u63a2\u7d22\u4fe1\u606f\u5bc6\u5ea6\u52a8\u6001\u5982\u4f55\u6784\u6210\u53d9\u4e8b\u7684\u57fa\u672c\u7ef4\u5ea6\u53ca\u5176\u5bf9\u8bfb\u8005\u53c2\u4e0e\u7684\u5f71\u54cd\u3002", "method": "\u4f7f\u7528SBERT\u5d4c\u5165\u8ba1\u7b97\u6bb5\u843d\u7ea7\u8bed\u4e49\u65b0\u9896\u6027\uff08\u4e0e\u524d\u9762\u6bb5\u843d\u8d28\u5fc3\u7684\u4f59\u5f26\u8ddd\u79bb\uff09\uff0c\u5c0628,606\u672c\u4e66\u7684\u65b0\u9896\u6027\u66f2\u7ebf\u901a\u8fc7\u5206\u6bb5\u805a\u5408\u8fd1\u4f3c\u964d\u7ef4\u4e3a16\u6bb5\uff0c\u7136\u540e\u4f7f\u7528Ward-linkage\u805a\u7c7b\u8bc6\u522b\u53d9\u4e8b\u539f\u578b\uff0c\u5e76\u5206\u6790\u5404\u79cd\u6307\u6807\u4e0e\u8bfb\u8005\u53c2\u4e0e\u5ea6\u7684\u5173\u7cfb\u3002", "result": "\u53d1\u73b08\u79cd\u5178\u578b\u53d9\u4e8b\u5f62\u72b6\u539f\u578b\uff08\u4ece\u9661\u5ced\u4e0b\u964d\u5230\u9661\u5ced\u4e0a\u5347\uff09\uff0c\u65b0\u9896\u6027\u8f68\u8ff9\u7684\u65b9\u5dee\uff08\u4f53\u79ef\uff09\u662f\u957f\u5ea6\u65e0\u5173\u7684\u6700\u5f3a\u8bfb\u8005\u53c2\u4e0e\u5ea6\u9884\u6d4b\u56e0\u5b50\uff08\u504frho=0.32\uff09\uff0c\u4f53\u88c1\u5f3a\u70c8\u7ea6\u675f\u53d9\u4e8b\u5f62\u72b6\uff08\u5361\u65b9=2121.6\uff09\uff0c\u5386\u53f2\u5206\u6790\u663e\u793a1840-1910\u5e74\u95f4\u4e66\u7c4d\u53d8\u5f97\u8d8a\u6765\u8d8a\u53ef\u9884\u6d4b\u3002", "conclusion": "\u4fe1\u606f\u5bc6\u5ea6\u52a8\u6001\u662f\u53d9\u4e8b\u7ed3\u6784\u7684\u57fa\u672c\u7ef4\u5ea6\uff0c\u5177\u6709\u53ef\u6d4b\u91cf\u7684\u8bfb\u8005\u53c2\u4e0e\u540e\u679c\uff0c\u7814\u7a76\u63ed\u793a\u4e86\u4f53\u88c1\u7ea6\u675f\u3001\u5386\u53f2\u8d8b\u52bf\u548c\u4e2a\u4f53\u72ec\u7279\u6027\uff0c\u540c\u65f6\u8b66\u544a\u8bed\u6599\u5e93\u7814\u7a76\u4e2d\u957f\u5ea6\u6df7\u6dc6\u5bf9\u76f8\u5173\u6027\u5206\u6790\u7684\u4e25\u91cd\u5f71\u54cd\u3002"}}
{"id": "2602.20476", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.20476", "abs": "https://arxiv.org/abs/2602.20476", "authors": ["Anindita Ghosh", "Vladislav Golyanik", "Taku Komura", "Philipp Slusallek", "Christian Theobalt", "Rishabh Dabral"], "title": "SceMoS: Scene-Aware 3D Human Motion Synthesis by Planning with Geometry-Grounded Tokens", "comment": "13 pages, 6 figures, 4 tables", "summary": "Synthesizing text-driven 3D human motion within realistic scenes requires learning both semantic intent (\"walk to the couch\") and physical feasibility (e.g., avoiding collisions). Current methods use generative frameworks that simultaneously learn high-level planning and low-level contact reasoning, and rely on computationally expensive 3D scene data such as point clouds or voxel occupancy grids. We propose SceMoS, a scene-aware motion synthesis framework that shows that structured 2D scene representations can serve as a powerful alternative to full 3D supervision in physically grounded motion synthesis. SceMoS disentangles global planning from local execution using lightweight 2D cues and relying on (1) a text-conditioned autoregressive global motion planner that operates on a bird's-eye-view (BEV) image rendered from an elevated corner of the scene, encoded with DINOv2 features, as the scene representation, and (2) a geometry-grounded motion tokenizer trained via a conditional VQ-VAE, that uses 2D local scene heightmap, thus embedding surface physics directly into a discrete vocabulary. This 2D factorization reaches an efficiency-fidelity trade-off: BEV semantics capture spatial layout and affordance for global reasoning, while local heightmaps enforce fine-grained physical adherence without full 3D volumetric reasoning. SceMoS achieves state-of-the-art motion realism and contact accuracy on the TRUMANS benchmark, reducing the number of trainable parameters for scene encoding by over 50%, showing that 2D scene cues can effectively ground 3D human-scene interaction.", "AI": {"tldr": "SceMoS\u4f7f\u75282D\u573a\u666f\u8868\u793a\u800c\u975e3D\u6570\u636e\u5408\u6210\u6587\u672c\u9a71\u52a8\u76843D\u4eba\u4f53\u8fd0\u52a8\uff0c\u901a\u8fc7BEV\u56fe\u50cf\u8fdb\u884c\u5168\u5c40\u89c4\u5212\uff0c\u5c40\u90e8\u9ad8\u5ea6\u56fe\u8fdb\u884c\u7269\u7406\u7ea6\u675f\uff0c\u5728\u6548\u7387\u548c\u4fdd\u771f\u5ea6\u95f4\u53d6\u5f97\u5e73\u8861\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u8ba1\u7b97\u6602\u8d35\u76843D\u573a\u666f\u6570\u636e\uff08\u5982\u70b9\u4e91\u3001\u4f53\u7d20\u7f51\u683c\uff09\uff0c\u540c\u65f6\u5b66\u4e60\u9ad8\u5c42\u89c4\u5212\u548c\u4f4e\u5c42\u63a5\u89e6\u63a8\u7406\u3002\u9700\u8981\u66f4\u9ad8\u6548\u4e14\u80fd\u4fdd\u6301\u7269\u7406\u771f\u5b9e\u6027\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "1) \u57fa\u4e8e\u6587\u672c\u6761\u4ef6\u7684\u81ea\u56de\u5f52\u5168\u5c40\u8fd0\u52a8\u89c4\u5212\u5668\uff0c\u4f7f\u7528DINOv2\u7279\u5f81\u7f16\u7801\u7684\u9e1f\u77b0\u56fe\u4f5c\u4e3a\u573a\u666f\u8868\u793a\uff1b2) \u57fa\u4e8e\u6761\u4ef6VQ-VAE\u8bad\u7ec3\u7684\u51e0\u4f55\u57fa\u7840\u8fd0\u52a8\u5206\u8bcd\u5668\uff0c\u4f7f\u75282D\u5c40\u90e8\u573a\u666f\u9ad8\u5ea6\u56fe\u5c06\u8868\u9762\u7269\u7406\u5d4c\u5165\u79bb\u6563\u8bcd\u6c47\u3002", "result": "\u5728TRUMANS\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u8fd0\u52a8\u771f\u5b9e\u6027\u548c\u63a5\u89e6\u51c6\u786e\u6027\uff0c\u573a\u666f\u7f16\u7801\u53ef\u8bad\u7ec3\u53c2\u6570\u51cf\u5c1150%\u4ee5\u4e0a\uff0c\u8bc1\u660e2D\u573a\u666f\u7ebf\u7d22\u80fd\u6709\u6548\u652f\u64913D\u4eba-\u573a\u666f\u4ea4\u4e92\u3002", "conclusion": "\u7ed3\u6784\u53162D\u573a\u666f\u8868\u793a\u53ef\u4f5c\u4e3a\u5b8c\u65743D\u76d1\u7763\u7684\u6709\u529b\u66ff\u4ee3\u65b9\u6848\uff0c\u5b9e\u73b0\u7269\u7406\u57fa\u7840\u8fd0\u52a8\u5408\u6210\uff0cBEV\u8bed\u4e49\u6355\u83b7\u7a7a\u95f4\u5e03\u5c40\uff0c\u5c40\u90e8\u9ad8\u5ea6\u56fe\u786e\u4fdd\u7269\u7406\u9075\u5faa\uff0c\u8fbe\u5230\u6548\u7387\u4e0e\u4fdd\u771f\u5ea6\u7684\u5e73\u8861\u3002"}}
{"id": "2602.21033", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.SE"], "pdf": "https://arxiv.org/pdf/2602.21033", "abs": "https://arxiv.org/abs/2602.21033", "authors": ["Tianhao Fu", "Yucheng Chen"], "title": "MIP Candy: A Modular PyTorch Framework for Medical Image Processing", "comment": null, "summary": "Medical image processing demands specialized software that handles high-dimensional volumetric data, heterogeneous file formats, and domain-specific training procedures. Existing frameworks either provide low-level components that require substantial integration effort or impose rigid, monolithic pipelines that resist modification. We present MIP Candy (MIPCandy), a freely available, PyTorch-based framework designed specifically for medical image processing. MIPCandy provides a complete, modular pipeline spanning data loading, training, inference, and evaluation, allowing researchers to obtain a fully functional process workflow by implementing a single method, $\\texttt{build_network}$, while retaining fine-grained control over every component. Central to the design is $\\texttt{LayerT}$, a deferred configuration mechanism that enables runtime substitution of convolution, normalization, and activation modules without subclassing. The framework further offers built-in $k$-fold cross-validation, dataset inspection with automatic region-of-interest detection, deep supervision, exponential moving average, multi-frontend experiment tracking (Weights & Biases, Notion, MLflow), training state recovery, and validation score prediction via quotient regression. An extensible bundle ecosystem provides pre-built model implementations that follow a consistent trainer--predictor pattern and integrate with the core framework without modification. MIPCandy is open-source under the Apache-2.0 license and requires Python~3.12 or later. Source code and documentation are available at https://github.com/ProjectNeura/MIPCandy.", "AI": {"tldr": "MIPCandy\u662f\u4e00\u4e2a\u57fa\u4e8ePyTorch\u7684\u533b\u7597\u56fe\u50cf\u5904\u7406\u6846\u67b6\uff0c\u63d0\u4f9b\u5b8c\u6574\u7684\u6a21\u5757\u5316\u6d41\u7a0b\uff0c\u901a\u8fc7\u5355\u4e00\u65b9\u6cd5\u5b9e\u73b0\u5168\u529f\u80fd\u5de5\u4f5c\u6d41\uff0c\u540c\u65f6\u4fdd\u6301\u5bf9\u6bcf\u4e2a\u7ec4\u4ef6\u7684\u7ec6\u7c92\u5ea6\u63a7\u5236\u3002", "motivation": "\u533b\u7597\u56fe\u50cf\u5904\u7406\u9700\u8981\u5904\u7406\u9ad8\u7ef4\u4f53\u79ef\u6570\u636e\u3001\u5f02\u6784\u6587\u4ef6\u683c\u5f0f\u548c\u9886\u57df\u7279\u5b9a\u8bad\u7ec3\u6d41\u7a0b\u7684\u4e13\u7528\u8f6f\u4ef6\u3002\u73b0\u6709\u6846\u67b6\u8981\u4e48\u63d0\u4f9b\u9700\u8981\u5927\u91cf\u96c6\u6210\u5de5\u4f5c\u7684\u4f4e\u7ea7\u7ec4\u4ef6\uff0c\u8981\u4e48\u91c7\u7528\u50f5\u5316\u7684\u5355\u4f53\u5f0f\u6d41\u7a0b\u96be\u4ee5\u4fee\u6539\u3002", "method": "\u57fa\u4e8ePyTorch\u7684\u6a21\u5757\u5316\u6846\u67b6\uff0c\u6838\u5fc3\u8bbe\u8ba1\u5305\u62ecLayerT\u5ef6\u8fdf\u914d\u7f6e\u673a\u5236\uff08\u652f\u6301\u8fd0\u884c\u65f6\u66ff\u6362\u5377\u79ef\u3001\u5f52\u4e00\u5316\u548c\u6fc0\u6d3b\u6a21\u5757\u800c\u65e0\u9700\u5b50\u7c7b\u5316\uff09\uff0c\u63d0\u4f9b\u6570\u636e\u52a0\u8f7d\u3001\u8bad\u7ec3\u3001\u63a8\u7406\u548c\u8bc4\u4f30\u7684\u5b8c\u6574\u6d41\u7a0b\u3002\u7528\u6237\u53ea\u9700\u5b9e\u73b0build_network\u65b9\u6cd5\u5373\u53ef\u83b7\u5f97\u5168\u529f\u80fd\u5de5\u4f5c\u6d41\u3002", "result": "\u6846\u67b6\u63d0\u4f9bk\u6298\u4ea4\u53c9\u9a8c\u8bc1\u3001\u81ea\u52a8ROI\u68c0\u6d4b\u7684\u6570\u636e\u96c6\u68c0\u67e5\u3001\u6df1\u5ea6\u76d1\u7763\u3001\u6307\u6570\u79fb\u52a8\u5e73\u5747\u3001\u591a\u524d\u7aef\u5b9e\u9a8c\u8ddf\u8e2a\uff08Weights & Biases\u3001Notion\u3001MLflow\uff09\u3001\u8bad\u7ec3\u72b6\u6001\u6062\u590d\u3001\u5546\u56de\u5f52\u9a8c\u8bc1\u5206\u6570\u9884\u6d4b\u7b49\u529f\u80fd\u3002\u6269\u5c55\u5305\u751f\u6001\u7cfb\u7edf\u63d0\u4f9b\u9884\u5efa\u6a21\u578b\u5b9e\u73b0\u3002", "conclusion": "MIPCandy\u662f\u4e00\u4e2a\u5f00\u6e90\u7684\u533b\u7597\u56fe\u50cf\u5904\u7406\u6846\u67b6\uff0c\u5728Apache-2.0\u8bb8\u53ef\u4e0b\u63d0\u4f9b\uff0c\u9700\u8981Python 3.12+\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6846\u67b6\u5728\u7075\u6d3b\u6027\u548c\u6613\u7528\u6027\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u4e3a\u7814\u7a76\u4eba\u5458\u63d0\u4f9b\u4e86\u65e2\u5b8c\u6574\u53c8\u53ef\u5b9a\u5236\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.20648", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.20648", "abs": "https://arxiv.org/abs/2602.20648", "authors": ["Anqi Li", "Chenxiao Wang", "Yu Lu", "Renjun Xu", "Lizhi Ma", "Zhenzhong Lan"], "title": "CARE: An Explainable Computational Framework for Assessing Client-Perceived Therapeutic Alliance Using Large Language Models", "comment": "14 pages, 4 figures", "summary": "Client perceptions of the therapeutic alliance are critical for counseling effectiveness. Accurately capturing these perceptions remains challenging, as traditional post-session questionnaires are burdensome and often delayed, while existing computational approaches produce coarse scores, lack interpretable rationales, and fail to model holistic session context. We present CARE, an LLM-based framework to automatically predict multi-dimensional alliance scores and generate interpretable rationales from counseling transcripts. Built on the CounselingWAI dataset and enriched with 9,516 expert-curated rationales, CARE is fine-tuned using rationale-augmented supervision with the LLaMA-3.1-8B-Instruct backbone. Experiments show that CARE outperforms leading LLMs and substantially reduces the gap between counselor evaluations and client-perceived alliance, achieving over 70% higher Pearson correlation with client ratings. Rationale-augmented supervision further improves predictive accuracy. CARE also produces high-quality, contextually grounded rationales, validated by both automatic and human evaluations. Applied to real-world Chinese online counseling sessions, CARE uncovers common alliance-building challenges, illustrates how interaction patterns shape alliance development, and provides actionable insights, demonstrating its potential as an AI-assisted tool for supporting mental health care.", "AI": {"tldr": "CARE\u662f\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u5fc3\u7406\u54a8\u8be2\u5bf9\u8bdd\u8bb0\u5f55\u4e2d\u81ea\u52a8\u9884\u6d4b\u591a\u7ef4\u5ea6\u7684\u6cbb\u7597\u8054\u76df\u8bc4\u5206\u5e76\u751f\u6210\u53ef\u89e3\u91ca\u7684\u7406\u7531\uff0c\u5728\u9884\u6d4b\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u5fc3\u7406\u54a8\u8be2\u540e\u95ee\u5377\u8d1f\u62c5\u91cd\u4e14\u5ef6\u8fdf\uff0c\u73b0\u6709\u8ba1\u7b97\u65b9\u6cd5\u8bc4\u5206\u7c97\u7cd9\u3001\u7f3a\u4e4f\u53ef\u89e3\u91ca\u7406\u7531\u4e14\u65e0\u6cd5\u5efa\u6a21\u5b8c\u6574\u4f1a\u8bdd\u4e0a\u4e0b\u6587\uff0c\u9700\u8981\u66f4\u51c6\u786e\u3001\u53ef\u89e3\u91ca\u7684\u81ea\u52a8\u8bc4\u4f30\u5de5\u5177\u3002", "method": "\u57fa\u4e8eCounselingWAI\u6570\u636e\u96c6\uff0c\u4f7f\u75289,516\u4e2a\u4e13\u5bb6\u6807\u6ce8\u7684\u7406\u7531\u8fdb\u884c\u589e\u5f3a\u76d1\u7763\u5fae\u8c03\uff0c\u91c7\u7528LLaMA-3.1-8B-Instruct\u4f5c\u4e3a\u57fa\u7840\u6a21\u578b\uff0c\u6784\u5efaCARE\u6846\u67b6\u3002", "result": "CARE\u5728\u9884\u6d4b\u5ba2\u6237\u611f\u77e5\u7684\u6cbb\u7597\u8054\u76df\u65b9\u9762\u4f18\u4e8e\u4e3b\u6d41LLM\uff0c\u4e0e\u5ba2\u6237\u8bc4\u5206\u7684\u76ae\u5c14\u900a\u76f8\u5173\u7cfb\u6570\u63d0\u9ad870%\u4ee5\u4e0a\uff0c\u51cf\u5c11\u54a8\u8be2\u5e08\u8bc4\u4f30\u4e0e\u5ba2\u6237\u611f\u77e5\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u4e0a\u4e0b\u6587\u76f8\u5173\u7684\u7406\u7531\u3002", "conclusion": "CARE\u4f5c\u4e3aAI\u8f85\u52a9\u5de5\u5177\u5728\u5fc3\u7406\u5065\u5eb7\u62a4\u7406\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u80fd\u591f\u8bc6\u522b\u8054\u76df\u5efa\u8bbe\u6311\u6218\u3001\u5206\u6790\u4e92\u52a8\u6a21\u5f0f\u5982\u4f55\u5f71\u54cd\u8054\u76df\u53d1\u5c55\uff0c\u5e76\u63d0\u4f9b\u53ef\u64cd\u4f5c\u7684\u89c1\u89e3\u3002"}}
{"id": "2602.20496", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.20496", "abs": "https://arxiv.org/abs/2602.20496", "authors": ["Jintu Zheng", "Qizhe Liu", "HuangXin Xu", "Zhuojie Chen"], "title": "Pip-Stereo: Progressive Iterations Pruner for Iterative Optimization based Stereo Matching", "comment": "Accepted to CVPR 2026 (3D vision track)", "summary": "While iterative stereo matching achieves high accuracy, its dependence on Recurrent Neural Networks (RNN) hinders edge deployment, a challenge underexplored in existing researches. We analyze iterative refinement and reveal that disparity updates are spatially sparse and temporally redundant. First, we introduce a progressive iteration pruning strategy that suppresses redundant update steps, effectively collapsing the recursive computation into a near-single-pass inference. Second, we propose a collaborative monocular prior transfer framework that implicitly embeds depth priors without requiring a dedicated monocular encoder, thereby eliminating its associated computational burden. Third, we develop FlashGRU, a hardware-aware RNN operator leveraging structured sparsity and I/O-conscious design, achieving a 7.28$\\times$ speedup, 76.6\\% memory peak reduction and 80.9\\% global memory requests reduction over natvie ConvGRUs under 2K resolution. Our PipStereo enables real-time, high-fidelity stereo matching on edge hardware: it processes 320$\\times$640 frames in just 75ms on an NVIDIA Jetson Orin NX (FP16) and 19ms on RTX 4090, matching the accuracy of large iterative based models, and our generalization ability and accuracy far exceeds that of existing real-time methods. Our embedded AI projects will be updated at: https://github.com/XPENG-Aridge-AI.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51faPipStereo\uff0c\u4e00\u79cd\u9488\u5bf9\u8fb9\u7f18\u8bbe\u5907\u4f18\u5316\u7684\u5b9e\u65f6\u7acb\u4f53\u5339\u914d\u65b9\u6cd5\uff0c\u901a\u8fc7\u6e10\u8fdb\u8fed\u4ee3\u526a\u679d\u3001\u534f\u4f5c\u5f0f\u5355\u76ee\u5148\u9a8c\u8fc1\u79fb\u548c\u786c\u4ef6\u611f\u77e5\u7684FlashGRU\u7b97\u5b50\uff0c\u5728\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u7684\u540c\u65f6\u5927\u5e45\u63d0\u5347\u63a8\u7406\u901f\u5ea6\u3002", "motivation": "\u73b0\u6709\u8fed\u4ee3\u5f0f\u7acb\u4f53\u5339\u914d\u65b9\u6cd5\u4f9d\u8d56RNN\uff0c\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u90e8\u7f72\u56f0\u96be\uff0c\u4e14\u8fed\u4ee3\u8fc7\u7a0b\u4e2d\u7684\u89c6\u5dee\u66f4\u65b0\u5b58\u5728\u7a7a\u95f4\u7a00\u758f\u6027\u548c\u65f6\u95f4\u5197\u4f59\u6027\uff0c\u8fd9\u4e9b\u95ee\u9898\u5728\u73b0\u6709\u7814\u7a76\u4e2d\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002", "method": "1. \u6e10\u8fdb\u8fed\u4ee3\u526a\u679d\u7b56\u7565\uff1a\u6291\u5236\u5197\u4f59\u66f4\u65b0\u6b65\u9aa4\uff0c\u5c06\u9012\u5f52\u8ba1\u7b97\u538b\u7f29\u4e3a\u8fd1\u4f3c\u5355\u6b21\u63a8\u7406\uff1b2. \u534f\u4f5c\u5f0f\u5355\u76ee\u5148\u9a8c\u8fc1\u79fb\u6846\u67b6\uff1a\u9690\u5f0f\u5d4c\u5165\u6df1\u5ea6\u5148\u9a8c\uff0c\u65e0\u9700\u4e13\u7528\u5355\u76ee\u7f16\u7801\u5668\uff1b3. FlashGRU\uff1a\u5229\u7528\u7ed3\u6784\u5316\u7a00\u758f\u6027\u548cI/O\u611f\u77e5\u8bbe\u8ba1\u7684\u786c\u4ef6\u611f\u77e5RNN\u7b97\u5b50\u3002", "result": "\u5728NVIDIA Jetson Orin NX\u4e0a\u5904\u7406320\u00d7640\u5e27\u4ec5\u970075ms\uff08FP16\uff09\uff0cRTX 4090\u4e0a\u4ec5\u970019ms\uff0c\u7cbe\u5ea6\u4e0e\u5927\u578b\u8fed\u4ee3\u6a21\u578b\u76f8\u5f53\uff0c\u6cdb\u5316\u80fd\u529b\u548c\u51c6\u786e\u6027\u8fdc\u8d85\u73b0\u6709\u5b9e\u65f6\u65b9\u6cd5\u3002FlashGRU\u57282K\u5206\u8fa8\u7387\u4e0b\u76f8\u6bd4\u539f\u751fConvGRU\u5b9e\u73b07.28\u500d\u52a0\u901f\u300176.6%\u5185\u5b58\u5cf0\u503c\u964d\u4f4e\u548c80.9%\u5168\u5c40\u5185\u5b58\u8bf7\u6c42\u51cf\u5c11\u3002", "conclusion": "PipStereo\u5b9e\u73b0\u4e86\u5728\u8fb9\u7f18\u786c\u4ef6\u4e0a\u7684\u5b9e\u65f6\u9ad8\u4fdd\u771f\u7acb\u4f53\u5339\u914d\uff0c\u901a\u8fc7\u6d88\u9664\u8ba1\u7b97\u5197\u4f59\u548c\u786c\u4ef6\u4f18\u5316\uff0c\u5728\u4fdd\u6301\u7cbe\u5ea6\u7684\u540c\u65f6\u5927\u5e45\u63d0\u5347\u6548\u7387\uff0c\u4e3a\u8fb9\u7f18AI\u5e94\u7528\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.20727", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.20727", "abs": "https://arxiv.org/abs/2602.20727", "authors": ["Xindian Ma", "Rundong Kong", "Peng Zhang", "Ruoxiang Huang", "Yongyu Jiang"], "title": "ID-LoRA: Efficient Low-Rank Adaptation Inspired by Matrix Interpolative Decomposition", "comment": null, "summary": "LoRA has become a universal Parameter-Efficient Fine-Tuning (PEFT) technique that equips Large Language Models (LLMs) to adapt quickly to new tasks. However, when these models are scaled up, even the latest LoRA variants still introduce considerable overhead in trainable parameters. Conversely, aggressively lowering the rank to curb this overhead markedly degrades performance in complex multi-task settings. We propose ID-LoRA, a novel PEFT framework that breaks the trade-off. Its core innovation lies in extracting and reusing clustered parameter groups from the pretrained weight matrix. These groups are then used to form multiple low-rank components, all of which share only a single initialized trainable low-rank matrix. This approach cuts the number of trainable parameters while keeping the model's capacity intact. We evaluate ID-LoRA on five diverse benchmarks: Mathematical Reasoning, Code Generation, MMLU, CommonsenseQA, and Safety Alignment. ID-LoRA outperforms both full fine-tuning and existing PEFT baselines (e.g., LoRA, DoRA, HydraLoRA) while using up to 46% fewer trainable parameters than the standard LoRA. In multi-task scenarios, it surpasses LoRA and its recent variants (e.g., DoRA and HydraLoRA) on both Code and MMLU tasks, yet requires only 54% of the trainable parameters demanded by the conventional LoRA.", "AI": {"tldr": "ID-LoRA\u662f\u4e00\u79cd\u65b0\u578b\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u6846\u67b6\uff0c\u901a\u8fc7\u63d0\u53d6\u548c\u91cd\u7528\u9884\u8bad\u7ec3\u6743\u91cd\u77e9\u9635\u4e2d\u7684\u805a\u7c7b\u53c2\u6570\u7ec4\uff0c\u5f62\u6210\u591a\u4e2a\u5171\u4eab\u5355\u4e00\u53ef\u8bad\u7ec3\u4f4e\u79e9\u77e9\u9635\u7684\u4f4e\u79e9\u7ec4\u4ef6\uff0c\u5728\u51cf\u5c11\u53ef\u8bad\u7ec3\u53c2\u6570\u7684\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u80fd\u529b\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u89c4\u6a21\u6269\u5927\uff0c\u73b0\u6709\u7684LoRA\u53d8\u4f53\u4ecd\u7136\u5f15\u5165\u5927\u91cf\u53ef\u8bad\u7ec3\u53c2\u6570\u5f00\u9500\uff0c\u800c\u8fc7\u5ea6\u964d\u4f4e\u79e9\u53c8\u4f1a\u5728\u590d\u6742\u591a\u4efb\u52a1\u573a\u666f\u4e2d\u663e\u8457\u964d\u4f4e\u6027\u80fd\uff0c\u9700\u8981\u6253\u7834\u8fd9\u79cd\u6743\u8861\u3002", "method": "ID-LoRA\u4ece\u9884\u8bad\u7ec3\u6743\u91cd\u77e9\u9635\u4e2d\u63d0\u53d6\u805a\u7c7b\u53c2\u6570\u7ec4\uff0c\u91cd\u7528\u8fd9\u4e9b\u7ec4\u5f62\u6210\u591a\u4e2a\u4f4e\u79e9\u7ec4\u4ef6\uff0c\u6240\u6709\u7ec4\u4ef6\u5171\u4eab\u5355\u4e00\u521d\u59cb\u5316\u7684\u53ef\u8bad\u7ec3\u4f4e\u79e9\u77e9\u9635\uff0c\u4ece\u800c\u51cf\u5c11\u53ef\u8bad\u7ec3\u53c2\u6570\u6570\u91cf\u3002", "result": "\u5728\u6570\u5b66\u63a8\u7406\u3001\u4ee3\u7801\u751f\u6210\u3001MMLU\u3001\u5e38\u8bc6\u95ee\u7b54\u548c\u5b89\u5168\u5bf9\u9f50\u4e94\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cID-LoRA\u4f18\u4e8e\u5168\u91cf\u5fae\u8c03\u548c\u73b0\u6709PEFT\u57fa\u7ebf\uff0c\u540c\u65f6\u6bd4\u6807\u51c6LoRA\u51cf\u5c11\u9ad8\u8fbe46%\u7684\u53ef\u8bad\u7ec3\u53c2\u6570\u3002\u5728\u591a\u4efb\u52a1\u573a\u666f\u4e2d\uff0c\u5728\u4ee3\u7801\u548cMMLU\u4efb\u52a1\u4e0a\u8d85\u8d8aLoRA\u53ca\u5176\u53d8\u4f53\uff0c\u4ec5\u9700\u4f20\u7edfLoRA 54%\u7684\u53c2\u6570\u3002", "conclusion": "ID-LoRA\u6210\u529f\u6253\u7834\u4e86\u53c2\u6570\u6548\u7387\u548c\u6027\u80fd\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u5728\u663e\u8457\u51cf\u5c11\u53ef\u8bad\u7ec3\u53c2\u6570\u7684\u540c\u65f6\u4fdd\u6301\u751a\u81f3\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u590d\u6742\u591a\u4efb\u52a1\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2602.20743", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.20743", "abs": "https://arxiv.org/abs/2602.20743", "authors": ["Gabriel Loiseau", "Damien Sileo", "Damien Riquet", "Maxime Meyer", "Marc Tommasi"], "title": "Adaptive Text Anonymization: Learning Privacy-Utility Trade-offs via Prompt Optimization", "comment": null, "summary": "Anonymizing textual documents is a highly context-sensitive problem: the appropriate balance between privacy protection and utility preservation varies with the data domain, privacy objectives, and downstream application. However, existing anonymization methods rely on static, manually designed strategies that lack the flexibility to adjust to diverse requirements and often fail to generalize across domains. We introduce adaptive text anonymization, a new task formulation in which anonymization strategies are automatically adapted to specific privacy-utility requirements. We propose a framework for task-specific prompt optimization that automatically constructs anonymization instructions for language models, enabling adaptation to different privacy goals, domains, and downstream usage patterns. To evaluate our approach, we present a benchmark spanning five datasets with diverse domains, privacy constraints, and utility objectives. Across all evaluated settings, our framework consistently achieves a better privacy-utility trade-off than existing baselines, while remaining computationally efficient and effective on open-source language models, with performance comparable to larger closed-source models. Additionally, we show that our method can discover novel anonymization strategies that explore different points along the privacy-utility trade-off frontier.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u81ea\u9002\u5e94\u6587\u672c\u533f\u540d\u5316\u4efb\u52a1\uff0c\u901a\u8fc7\u4efb\u52a1\u7279\u5b9a\u63d0\u793a\u4f18\u5316\u6846\u67b6\u81ea\u52a8\u6784\u5efa\u533f\u540d\u5316\u6307\u4ee4\uff0c\u9002\u5e94\u4e0d\u540c\u9690\u79c1\u76ee\u6807\u3001\u9886\u57df\u548c\u4e0b\u6e38\u4f7f\u7528\u6a21\u5f0f\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u6587\u672c\u533f\u540d\u5316\u662f\u9ad8\u5ea6\u4e0a\u4e0b\u6587\u654f\u611f\u7684\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u9759\u6001\u624b\u52a8\u8bbe\u8ba1\u7b56\u7565\uff0c\u7f3a\u4e4f\u9002\u5e94\u4e0d\u540c\u9700\u6c42\u7684\u7075\u6d3b\u6027\uff0c\u4e14\u96be\u4ee5\u8de8\u9886\u57df\u6cdb\u5316\u3002", "method": "\u63d0\u51fa\u81ea\u9002\u5e94\u6587\u672c\u533f\u540d\u5316\u4efb\u52a1\u6846\u67b6\uff0c\u901a\u8fc7\u4efb\u52a1\u7279\u5b9a\u63d0\u793a\u4f18\u5316\u81ea\u52a8\u4e3a\u8bed\u8a00\u6a21\u578b\u6784\u5efa\u533f\u540d\u5316\u6307\u4ee4\uff0c\u9002\u5e94\u4e0d\u540c\u9690\u79c1\u76ee\u6807\u3001\u9886\u57df\u548c\u4e0b\u6e38\u4f7f\u7528\u6a21\u5f0f\u3002", "result": "\u5728\u5305\u542b\u4e94\u4e2a\u4e0d\u540c\u9886\u57df\u3001\u9690\u79c1\u7ea6\u675f\u548c\u6548\u7528\u76ee\u6807\u7684\u6570\u636e\u96c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u6240\u6709\u8bbe\u7f6e\u4e0b\u90fd\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\uff0c\u5728\u9690\u79c1-\u6548\u7528\u6743\u8861\u4e0a\u8868\u73b0\u66f4\u597d\uff0c\u8ba1\u7b97\u6548\u7387\u9ad8\uff0c\u5728\u5f00\u6e90\u6a21\u578b\u4e0a\u6548\u679c\u4e0e\u5927\u578b\u95ed\u6e90\u6a21\u578b\u76f8\u5f53\u3002", "conclusion": "\u81ea\u9002\u5e94\u6587\u672c\u533f\u540d\u5316\u6846\u67b6\u80fd\u591f\u81ea\u52a8\u9002\u5e94\u4e0d\u540c\u9690\u79c1-\u6548\u7528\u9700\u6c42\uff0c\u53d1\u73b0\u65b0\u7684\u533f\u540d\u5316\u7b56\u7565\uff0c\u5728\u9690\u79c1\u4fdd\u62a4\u548c\u6548\u7528\u4fdd\u6301\u4e4b\u95f4\u5b9e\u73b0\u66f4\u597d\u7684\u6743\u8861\u3002"}}
{"id": "2602.20696", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.20696", "abs": "https://arxiv.org/abs/2602.20696", "authors": ["Baolong Bi", "Yuyao Ge", "Shenghua Liu", "Yuchen He", "Siqian Tong", "Lizhe Chen", "Lingrui Mei", "Zehao Li", "Yiwei Wang", "Yujun Cai", "Ming-Hsuan Yang", "Xueqi Cheng"], "title": "PromptCD: Test-Time Behavior Enhancement via Polarity-Prompt Contrastive Decoding", "comment": null, "summary": "Reliable AI systems require large language models (LLMs) to exhibit behaviors aligned with human preferences and values. However, most existing alignment approaches operate at training time and rely on additional high-quality data, incurring significant computational and annotation costs. While recent work has shown that contrastive decoding can leverage a model's internal distributions to improve specific capabilities, its applicability remains limited to narrow behavioral scopes and scenarios. In this work, we introduce Polarity-Prompt Contrastive Decoding (PromptCD), a test-time behavior control method that generalizes contrastive decoding to broader enhancement settings. PromptCD constructs paired positive and negative guiding prompts for a target behavior and contrasts model responses-specifically token-level probability distributions in LLMs and visual attention patterns in VLMs-to reinforce desirable outcomes. This formulation extends contrastive decoding to a wide range of enhancement objectives and is applicable to both LLMs and Vision-Language Models (VLMs) without additional training. For LLMs, experiments on the \"3H\" alignment objectives (helpfulness, honesty, and harmlessness) demonstrate consistent and substantial improvements, indicating that post-trained models can achieve meaningful self-enhancement purely at test time. For VLMs, we further analyze contrastive effects on visual attention, showing that PromptCD significantly improves VQA performance by reinforcing behavior-consistent visual grounding. Collectively, these results highlight PromptCD as a simple, general, and cost-efficient strategy for reliable behavior control across modalities.", "AI": {"tldr": "\u63d0\u51faPromptCD\u65b9\u6cd5\uff0c\u901a\u8fc7\u6784\u5efa\u6b63\u8d1f\u5f15\u5bfc\u63d0\u793a\u5bf9\u6bd4\u6a21\u578b\u54cd\u5e94\uff0c\u5728\u6d4b\u8bd5\u65f6\u63a7\u5236LLM\u548cVLM\u7684\u884c\u4e3a\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u5373\u53ef\u63d0\u5347\u5bf9\u9f50\u6027\u80fd", "motivation": "\u73b0\u6709\u5bf9\u9f50\u65b9\u6cd5\u591a\u5728\u8bad\u7ec3\u65f6\u8fdb\u884c\uff0c\u9700\u8981\u9ad8\u8d28\u91cf\u6570\u636e\u548c\u8ba1\u7b97\u6210\u672c\u9ad8\u3002\u5bf9\u6bd4\u89e3\u7801\u65b9\u6cd5\u9002\u7528\u8303\u56f4\u6709\u9650\uff0c\u9700\u8981\u66f4\u901a\u7528\u7684\u6d4b\u8bd5\u65f6\u884c\u4e3a\u63a7\u5236\u65b9\u6cd5", "method": "\u63d0\u51faPolarity-Prompt Contrastive Decoding (PromptCD)\uff1a\u4e3a\u76ee\u6807\u884c\u4e3a\u6784\u5efa\u6b63\u8d1f\u5f15\u5bfc\u63d0\u793a\u5bf9\uff0c\u5bf9\u6bd4\u6a21\u578b\u54cd\u5e94\uff08LLM\u4e2d\u7684token\u6982\u7387\u5206\u5e03\u548cVLM\u4e2d\u7684\u89c6\u89c9\u6ce8\u610f\u529b\u6a21\u5f0f\uff09\u6765\u5f3a\u5316\u671f\u671b\u7ed3\u679c", "result": "LLM\u5728\"3H\"\u5bf9\u9f50\u76ee\u6807\uff08helpfulness\u3001honesty\u3001harmlessness\uff09\u4e0a\u83b7\u5f97\u4e00\u81f4\u663e\u8457\u6539\u8fdb\uff1bVLM\u901a\u8fc7\u5f3a\u5316\u884c\u4e3a\u4e00\u81f4\u7684\u89c6\u89c9\u57fa\u7840\u663e\u8457\u63d0\u5347VQA\u6027\u80fd", "conclusion": "PromptCD\u662f\u4e00\u79cd\u7b80\u5355\u3001\u901a\u7528\u4e14\u6210\u672c\u9ad8\u6548\u7684\u8de8\u6a21\u6001\u53ef\u9760\u884c\u4e3a\u63a7\u5236\u7b56\u7565\uff0c\u4f7f\u540e\u8bad\u7ec3\u6a21\u578b\u80fd\u5728\u6d4b\u8bd5\u65f6\u5b9e\u73b0\u6709\u610f\u4e49\u7684\u81ea\u6211\u589e\u5f3a"}}
{"id": "2602.20759", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.20759", "abs": "https://arxiv.org/abs/2602.20759", "authors": ["Yu Fu", "Seongho Son", "Ilija Bogunovic"], "title": "Overton Pluralistic Reinforcement Learning for Large Language Models", "comment": "28 pages, 8 figures", "summary": "Existing alignment paradigms remain limited in capturing the pluralistic nature of human values. Overton Pluralism addresses this gap by generating responses with diverse perspectives from a single query. This paper introduces OP-GRPO (Overton Pluralistic Group Relative Policy Optimization), a reinforcement learning framework for implicit Overton Pluralism that enables a single large language model to produce pluralistic responses without explicit prompting or modular orchestration. Our workflow consists of two main steps. First, similarity estimator training fine-tunes a Sentence Transformer for Overton Pluralism tasks to provide more accurate coverage evaluation of generated responses. Second, OP-GRPO training incorporates this similarity estimator into a dual-reward system designed to ensure both broad coverage of genuine human perspectives and the uniqueness of each perspective, thereby promoting diversity. Empirical results demonstrate a \"small models, big perspective coverage\" effect. The trained Qwen2.5-3B-Instruct model surpasses a 20B GPT-OSS baseline with a 37.4 percent relative accuracy gain on a Natural Language Inference benchmark, and also outperforms a modular architecture baseline with a 19.1 percent relative improvement. Additional evaluations using GPT-4.1 as a large language model judge further confirm the robustness of the approach.", "AI": {"tldr": "OP-GRPO\u662f\u4e00\u4e2a\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u5956\u52b1\u7cfb\u7edf\u8ba9\u5355\u4e2a\u5927\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u751f\u6210\u591a\u5143\u89c6\u89d2\u7684\u56de\u5e94\uff0c\u65e0\u9700\u663e\u5f0f\u63d0\u793a\u6216\u6a21\u5757\u5316\u7f16\u6392\uff0c\u5b9e\u73b0\u4e86\"\u5c0f\u6a21\u578b\uff0c\u5927\u89c6\u89d2\u8986\u76d6\"\u7684\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u5bf9\u9f50\u8303\u5f0f\u5728\u6355\u6349\u4eba\u7c7b\u4ef7\u503c\u89c2\u7684\u591a\u5143\u6027\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u4ece\u5355\u4e00\u67e5\u8be2\u751f\u6210\u591a\u79cd\u89c6\u89d2\u56de\u5e94\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u63d0\u51faOP-GRPO\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u4e3b\u8981\u6b65\u9aa4\uff1a1) \u8bad\u7ec3\u76f8\u4f3c\u6027\u4f30\u8ba1\u5668\u7528\u4e8e\u66f4\u51c6\u786e\u8bc4\u4f30\u751f\u6210\u56de\u5e94\u7684\u8986\u76d6\u8303\u56f4\uff1b2) \u5c06\u76f8\u4f3c\u6027\u4f30\u8ba1\u5668\u96c6\u6210\u5230\u53cc\u5956\u52b1\u7cfb\u7edf\u4e2d\uff0c\u786e\u4fdd\u5e7f\u6cdb\u8986\u76d6\u771f\u5b9e\u4eba\u7c7b\u89c6\u89d2\u7684\u540c\u65f6\u4fdd\u6301\u6bcf\u4e2a\u89c6\u89d2\u7684\u72ec\u7279\u6027\u3002", "result": "Qwen2.5-3B-Instruct\u6a21\u578b\u5728\u81ea\u7136\u8bed\u8a00\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u76f8\u5bf920B GPT-OSS\u57fa\u7ebf\u63d0\u5347\u4e8637.4%\u7684\u51c6\u786e\u7387\uff0c\u76f8\u5bf9\u6a21\u5757\u5316\u67b6\u6784\u57fa\u7ebf\u63d0\u5347\u4e8619.1%\u7684\u76f8\u5bf9\u6539\u8fdb\u3002GPT-4.1\u4f5c\u4e3a\u5927\u8bed\u8a00\u6a21\u578b\u8bc4\u5224\u8005\u7684\u989d\u5916\u8bc4\u4f30\u8fdb\u4e00\u6b65\u8bc1\u5b9e\u4e86\u65b9\u6cd5\u7684\u7a33\u5065\u6027\u3002", "conclusion": "OP-GRPO\u6846\u67b6\u6210\u529f\u5b9e\u73b0\u4e86\u9690\u5f0f\u7684Overton\u591a\u5143\u4e3b\u4e49\uff0c\u4f7f\u5355\u4e2a\u5927\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u751f\u6210\u591a\u5143\u89c6\u89d2\u7684\u56de\u5e94\uff0c\u5c55\u793a\u4e86\"\u5c0f\u6a21\u578b\uff0c\u5927\u89c6\u89d2\u8986\u76d6\"\u7684\u6548\u679c\uff0c\u4e3a\u6355\u6349\u4eba\u7c7b\u4ef7\u503c\u89c2\u7684\u591a\u5143\u6027\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.20816", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.20816", "abs": "https://arxiv.org/abs/2602.20816", "authors": ["Sayantan Dasgupta", "Trevor Cohn", "Timothy Baldwin"], "title": "Don't Ignore the Tail: Decoupling top-K Probabilities for Efficient Language Model Distillation", "comment": null, "summary": "The core learning signal used in language model distillation is the standard Kullback-Leibler (KL) divergence between the student and teacher distributions. Traditional KL divergence tends to be dominated by the next tokens with the highest probabilities, i.e., the teacher's modes, thereby diminishing the influence of less probable yet potentially informative components of the output distribution. We propose a new tail-aware divergence that decouples the contribution of the teacher model's top-K predicted probabilities from that of lower-probability predictions, while maintaining the same computational profile as the KL Divergence. Our decoupled approach reduces the impact of the teacher modes and, consequently, increases the contribution of the tail of the distribution. Experimental results demonstrate that our modified distillation method yields competitive performance in both pre-training and supervised distillation of decoder models across various datasets. Furthermore, the distillation process is efficient and can be performed with a modest academic budget for large datasets, eliminating the need for industry-scale computing.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5c3e\u90e8\u5206\u5e03\u611f\u77e5\u7684KL\u6563\u5ea6\u65b9\u6cd5\uff0c\u7528\u4e8e\u8bed\u8a00\u6a21\u578b\u84b8\u998f\uff0c\u901a\u8fc7\u89e3\u8026\u6559\u5e08\u6a21\u578b\u524dK\u4e2a\u9ad8\u6982\u7387\u9884\u6d4b\u4e0e\u4f4e\u6982\u7387\u9884\u6d4b\u7684\u8d21\u732e\uff0c\u51cf\u5c11\u6559\u5e08\u6a21\u5f0f\u4e3b\u5bfc\u6548\u5e94\uff0c\u589e\u5f3a\u5206\u5e03\u5c3e\u90e8\u4fe1\u606f\u7684\u5f71\u54cd\u3002", "motivation": "\u4f20\u7edfKL\u6563\u5ea6\u5728\u8bed\u8a00\u6a21\u578b\u84b8\u998f\u4e2d\u4e3b\u8981\u53d7\u6559\u5e08\u6a21\u578b\u9ad8\u6982\u7387token\uff08\u6a21\u5f0f\uff09\u4e3b\u5bfc\uff0c\u800c\u5ffd\u7565\u4e86\u4f4e\u6982\u7387\u4f46\u53ef\u80fd\u4fe1\u606f\u4e30\u5bcc\u7684\u5206\u5e03\u5c3e\u90e8\u6210\u5206\uff0c\u8fd9\u9650\u5236\u4e86\u84b8\u998f\u6548\u679c\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5c3e\u90e8\u5206\u5e03\u611f\u77e5\u7684\u6563\u5ea6\u65b9\u6cd5\uff0c\u5c06\u6559\u5e08\u6a21\u578b\u7684\u524dK\u4e2a\u9ad8\u6982\u7387\u9884\u6d4b\u4e0e\u4f4e\u6982\u7387\u9884\u6d4b\u89e3\u8026\u5904\u7406\uff0c\u4fdd\u6301\u4e0eKL\u6563\u5ea6\u76f8\u540c\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u4f46\u51cf\u5c11\u6559\u5e08\u6a21\u5f0f\u7684\u5f71\u54cd\uff0c\u589e\u5f3a\u5206\u5e03\u5c3e\u90e8\u7684\u8d21\u732e\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6539\u8fdb\u7684\u84b8\u998f\u65b9\u6cd5\u5728\u5404\u79cd\u6570\u636e\u96c6\u7684\u89e3\u7801\u5668\u6a21\u578b\u9884\u8bad\u7ec3\u548c\u76d1\u7763\u84b8\u998f\u4e2d\u90fd\u53d6\u5f97\u4e86\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\uff0c\u4e14\u84b8\u998f\u8fc7\u7a0b\u9ad8\u6548\uff0c\u53ef\u4ee5\u5728\u5b66\u672f\u9884\u7b97\u4e0b\u5904\u7406\u5927\u578b\u6570\u636e\u96c6\uff0c\u65e0\u9700\u5de5\u4e1a\u7ea7\u8ba1\u7b97\u8d44\u6e90\u3002", "conclusion": "\u63d0\u51fa\u7684\u5c3e\u90e8\u5206\u5e03\u611f\u77e5\u6563\u5ea6\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edfKL\u6563\u5ea6\u4e2d\u6559\u5e08\u6a21\u5f0f\u4e3b\u5bfc\u7684\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u8bed\u8a00\u6a21\u578b\u84b8\u998f\u7684\u6548\u679c\u548c\u6548\u7387\uff0c\u4e3a\u8d44\u6e90\u6709\u9650\u7684\u7814\u7a76\u73af\u5883\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u84b8\u998f\u65b9\u6848\u3002"}}
{"id": "2602.20537", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.20537", "abs": "https://arxiv.org/abs/2602.20537", "authors": ["Xinyong Cai", "Changbin Sun", "Yong Wang", "Hongyu Yang", "Yuankai Wu"], "title": "PFGNet: A Fully Convolutional Frequency-Guided Peripheral Gating Network for Efficient Spatiotemporal Predictive Learning", "comment": "Accepted to CVPR 2026", "summary": "Spatiotemporal predictive learning (STPL) aims to forecast future frames from past observations and is essential across a wide range of applications. Compared with recurrent or hybrid architectures, pure convolutional models offer superior efficiency and full parallelism, yet their fixed receptive fields limit their ability to adaptively capture spatially varying motion patterns. Inspired by biological center-surround organization and frequency-selective signal processing, we propose PFGNet, a fully convolutional framework that dynamically modulates receptive fields through pixel-wise frequency-guided gating. The core Peripheral Frequency Gating (PFG) block extracts localized spectral cues and adaptively fuses multi-scale large-kernel peripheral responses with learnable center suppression, effectively forming spatially adaptive band-pass filters. To maintain efficiency, all large kernels are decomposed into separable 1D convolutions ($1 \\times k$ followed by $k \\times 1$), reducing per-channel computational cost from $O(k^2)$ to $O(2k)$. PFGNet enables structure-aware spatiotemporal modeling without recurrence or attention. Experiments on Moving MNIST, TaxiBJ, Human3.6M, and KTH show that PFGNet delivers SOTA or near-SOTA forecasting performance with substantially fewer parameters and FLOPs. Our code is available at https://github.com/fhjdqaq/PFGNet.", "AI": {"tldr": "PFGNet\uff1a\u4e00\u79cd\u57fa\u4e8e\u50cf\u7d20\u7ea7\u9891\u7387\u5f15\u5bfc\u95e8\u63a7\u7684\u5168\u5377\u79ef\u65f6\u7a7a\u9884\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u8c03\u5236\u611f\u53d7\u91ce\u6765\u6355\u6349\u7a7a\u95f4\u53d8\u5316\u7684\u8fd0\u52a8\u6a21\u5f0f\uff0c\u5728\u4fdd\u6301\u9ad8\u6548\u7387\u7684\u540c\u65f6\u5b9e\u73b0SOTA\u6216\u63a5\u8fd1SOTA\u7684\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u7eaf\u5377\u79ef\u6a21\u578b\u5728\u65f6\u7a7a\u9884\u6d4b\u5b66\u4e60\u4e2d\u5177\u6709\u9ad8\u6548\u6027\u548c\u5b8c\u5168\u5e76\u884c\u5316\u7684\u4f18\u52bf\uff0c\u4f46\u5176\u56fa\u5b9a\u611f\u53d7\u91ce\u9650\u5236\u4e86\u81ea\u9002\u5e94\u6355\u6349\u7a7a\u95f4\u53d8\u5316\u8fd0\u52a8\u6a21\u5f0f\u7684\u80fd\u529b\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u52a8\u6001\u8c03\u5236\u611f\u53d7\u91ce\u7684\u65b9\u6cd5\u6765\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "method": "\u63d0\u51faPFGNet\u6846\u67b6\uff0c\u6838\u5fc3\u662f\u5916\u56f4\u9891\u7387\u95e8\u63a7\uff08PFG\uff09\u6a21\u5757\uff0c\u901a\u8fc7\u50cf\u7d20\u7ea7\u9891\u7387\u5f15\u5bfc\u95e8\u63a7\u52a8\u6001\u8c03\u5236\u611f\u53d7\u91ce\u3002PFG\u6a21\u5757\u63d0\u53d6\u5c40\u90e8\u9891\u8c31\u7ebf\u7d22\uff0c\u81ea\u9002\u5e94\u878d\u5408\u591a\u5c3a\u5ea6\u5927\u6838\u5916\u56f4\u54cd\u5e94\u4e0e\u53ef\u5b66\u4e60\u7684\u4e2d\u5fc3\u6291\u5236\uff0c\u5f62\u6210\u7a7a\u95f4\u81ea\u9002\u5e94\u5e26\u901a\u6ee4\u6ce2\u5668\u3002\u4e3a\u63d0\u9ad8\u6548\u7387\uff0c\u6240\u6709\u5927\u6838\u90fd\u5206\u89e3\u4e3a\u53ef\u5206\u79bb\u76841D\u5377\u79ef\uff081\u00d7k\u540e\u63a5k\u00d71\uff09\uff0c\u5c06\u6bcf\u901a\u9053\u8ba1\u7b97\u6210\u672c\u4eceO(k\u00b2)\u964d\u4f4e\u5230O(2k)\u3002", "result": "\u5728Moving MNIST\u3001TaxiBJ\u3001Human3.6M\u548cKTH\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cPFGNet\u4ee5\u663e\u8457\u66f4\u5c11\u7684\u53c2\u6570\u548cFLOPs\u5b9e\u73b0\u4e86SOTA\u6216\u63a5\u8fd1SOTA\u7684\u9884\u6d4b\u6027\u80fd\u3002", "conclusion": "PFGNet\u901a\u8fc7\u9891\u7387\u5f15\u5bfc\u7684\u81ea\u9002\u5e94\u611f\u53d7\u91ce\u8c03\u5236\uff0c\u5b9e\u73b0\u4e86\u65e0\u9700\u5faa\u73af\u6216\u6ce8\u610f\u529b\u673a\u5236\u7684\u7ed3\u6784\u611f\u77e5\u65f6\u7a7a\u5efa\u6a21\uff0c\u5728\u4fdd\u6301\u9ad8\u6548\u7387\u7684\u540c\u65f6\u8fbe\u5230\u4e86\u4f18\u5f02\u7684\u9884\u6d4b\u6027\u80fd\u3002"}}
{"id": "2602.21054", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.21054", "abs": "https://arxiv.org/abs/2602.21054", "authors": ["Seongheon Park", "Changdae Oh", "Hyeong Kyu Choi", "Xuefeng Du", "Sharon Li"], "title": "VAUQ: Vision-Aware Uncertainty Quantification for LVLM Self-Evaluation", "comment": null, "summary": "Large Vision-Language Models (LVLMs) frequently hallucinate, limiting their safe deployment in real-world applications. Existing LLM self-evaluation methods rely on a model's ability to estimate the correctness of its own outputs, which can improve deployment reliability; however, they depend heavily on language priors and are therefore ill-suited for evaluating vision-conditioned predictions. We propose VAUQ, a vision-aware uncertainty quantification framework for LVLM self-evaluation that explicitly measures how strongly a model's output depends on visual evidence. VAUQ introduces the Image-Information Score (IS), which captures the reduction in predictive uncertainty attributable to visual input, and an unsupervised core-region masking strategy that amplifies the influence of salient regions. Combining predictive entropy with this core-masked IS yields a training-free scoring function that reliably reflects answer correctness. Comprehensive experiments show that VAUQ consistently outperforms existing self-evaluation methods across multiple datasets.", "AI": {"tldr": "VAUQ\u662f\u4e00\u79cd\u89c6\u89c9\u611f\u77e5\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u901a\u8fc7\u56fe\u50cf\u4fe1\u606f\u5206\u6570\u548c\u6838\u5fc3\u533a\u57df\u63a9\u7801\u7b56\u7565\u6765\u6d4b\u91cf\u6a21\u578b\u8f93\u51fa\u5bf9\u89c6\u89c9\u8bc1\u636e\u7684\u4f9d\u8d56\u7a0b\u5ea6\u3002", "motivation": "\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7ecf\u5e38\u4ea7\u751f\u5e7b\u89c9\uff0c\u9650\u5236\u4e86\u5176\u5728\u73b0\u5b9e\u5e94\u7528\u4e2d\u7684\u5b89\u5168\u90e8\u7f72\u3002\u73b0\u6709\u7684LLM\u81ea\u8bc4\u4f30\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u8bed\u8a00\u5148\u9a8c\uff0c\u4e0d\u9002\u5408\u8bc4\u4f30\u89c6\u89c9\u6761\u4ef6\u9884\u6d4b\u3002", "method": "\u63d0\u51faVAUQ\u6846\u67b6\uff0c\u5f15\u5165\u56fe\u50cf\u4fe1\u606f\u5206\u6570\u6765\u6355\u6349\u89c6\u89c9\u8f93\u5165\u5bf9\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u7684\u51cf\u5c11\u7a0b\u5ea6\uff0c\u91c7\u7528\u65e0\u76d1\u7763\u6838\u5fc3\u533a\u57df\u63a9\u7801\u7b56\u7565\u6765\u589e\u5f3a\u663e\u8457\u533a\u57df\u7684\u5f71\u54cd\uff0c\u7ed3\u5408\u9884\u6d4b\u71b5\u548c\u6838\u5fc3\u63a9\u7801IS\u6784\u5efa\u65e0\u9700\u8bad\u7ec3\u7684\u6253\u5206\u51fd\u6570\u3002", "result": "\u7efc\u5408\u5b9e\u9a8c\u8868\u660e\uff0cVAUQ\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u6301\u7eed\u4f18\u4e8e\u73b0\u6709\u7684\u81ea\u8bc4\u4f30\u65b9\u6cd5\u3002", "conclusion": "VAUQ\u901a\u8fc7\u663e\u5f0f\u6d4b\u91cf\u6a21\u578b\u8f93\u51fa\u5bf9\u89c6\u89c9\u8bc1\u636e\u7684\u4f9d\u8d56\u7a0b\u5ea6\uff0c\u4e3a\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u81ea\u8bc4\u4f30\u6846\u67b6\uff0c\u80fd\u591f\u53ef\u9760\u53cd\u6620\u7b54\u6848\u6b63\u786e\u6027\u3002"}}
{"id": "2602.21154", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.21154", "abs": "https://arxiv.org/abs/2602.21154", "authors": ["Ziwei Niu", "Hao Sun", "Shujun Bian", "Xihong Yang", "Lanfen Lin", "Yuxin Liu", "Yueming Jin"], "title": "CG-DMER: Hybrid Contrastive-Generative Framework for Disentangled Multimodal ECG Representation Learning", "comment": "Accepted by ICASSP 2026", "summary": "Accurate interpretation of electrocardiogram (ECG) signals is crucial for diagnosing cardiovascular diseases. Recent multimodal approaches that integrate ECGs with accompanying clinical reports show strong potential, but they still face two main concerns from a modality perspective: (1) intra-modality: existing models process ECGs in a lead-agnostic manner, overlooking spatial-temporal dependencies across leads, which restricts their effectiveness in modeling fine-grained diagnostic patterns; (2) inter-modality: existing methods directly align ECG signals with clinical reports, introducing modality-specific biases due to the free-text nature of the reports. In light of these two issues, we propose CG-DMER, a contrastive-generative framework for disentangled multimodal ECG representation learning, powered by two key designs: (1) Spatial-temporal masked modeling is designed to better capture fine-grained temporal dynamics and inter-lead spatial dependencies by applying masking across both spatial and temporal dimensions and reconstructing the missing information. (2) A representation disentanglement and alignment strategy is designed to mitigate unnecessary noise and modality-specific biases by introducing modality-specific and modality-shared encoders, ensuring a clearer separation between modality-invariant and modality-specific representations. Experiments on three public datasets demonstrate that CG-DMER achieves state-of-the-art performance across diverse downstream tasks.", "AI": {"tldr": "CG-DMER\u662f\u4e00\u4e2a\u7528\u4e8e\u5fc3\u7535\u56fe\u591a\u6a21\u6001\u8868\u793a\u5b66\u4e60\u7684\u5bf9\u6bd4\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u65f6\u7a7a\u63a9\u7801\u5efa\u6a21\u548c\u8868\u793a\u89e3\u8026\u5bf9\u9f50\u7b56\u7565\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u6a21\u6001\u5185\u548c\u6a21\u6001\u95f4\u7684\u95ee\u9898\uff0c\u5728\u4e09\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u5fc3\u7535\u56fe\u5206\u6790\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a1) \u6a21\u6001\u5185\u95ee\u9898\uff1a\u73b0\u6709\u6a21\u578b\u4ee5\u5bfc\u8054\u65e0\u5173\u7684\u65b9\u5f0f\u5904\u7406\u5fc3\u7535\u56fe\uff0c\u5ffd\u7565\u4e86\u5bfc\u8054\u95f4\u7684\u65f6\u7a7a\u4f9d\u8d56\u6027\uff0c\u9650\u5236\u4e86\u5176\u5bf9\u7ec6\u7c92\u5ea6\u8bca\u65ad\u6a21\u5f0f\u7684\u5efa\u6a21\u80fd\u529b\uff1b2) \u6a21\u6001\u95f4\u95ee\u9898\uff1a\u73b0\u6709\u65b9\u6cd5\u76f4\u63a5\u5c06\u5fc3\u7535\u56fe\u4fe1\u53f7\u4e0e\u4e34\u5e8a\u62a5\u544a\u5bf9\u9f50\uff0c\u7531\u4e8e\u62a5\u544a\u7684\u81ea\u7531\u6587\u672c\u7279\u6027\u5f15\u5165\u4e86\u6a21\u6001\u7279\u5b9a\u504f\u5dee\u3002", "method": "\u63d0\u51fa\u4e86CG-DMER\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u5173\u952e\u8bbe\u8ba1\uff1a1) \u65f6\u7a7a\u63a9\u7801\u5efa\u6a21\uff1a\u901a\u8fc7\u5728\u65f6\u7a7a\u7ef4\u5ea6\u4e0a\u5e94\u7528\u63a9\u7801\u5e76\u91cd\u5efa\u7f3a\u5931\u4fe1\u606f\uff0c\u66f4\u597d\u5730\u6355\u6349\u7ec6\u7c92\u5ea6\u65f6\u95f4\u52a8\u6001\u548c\u5bfc\u8054\u95f4\u7a7a\u95f4\u4f9d\u8d56\u6027\uff1b2) \u8868\u793a\u89e3\u8026\u4e0e\u5bf9\u9f50\u7b56\u7565\uff1a\u901a\u8fc7\u5f15\u5165\u6a21\u6001\u7279\u5b9a\u548c\u6a21\u6001\u5171\u4eab\u7f16\u7801\u5668\uff0c\u5206\u79bb\u6a21\u6001\u4e0d\u53d8\u548c\u6a21\u6001\u7279\u5b9a\u8868\u793a\uff0c\u51cf\u5c11\u4e0d\u5fc5\u8981\u566a\u58f0\u548c\u6a21\u6001\u7279\u5b9a\u504f\u5dee\u3002", "result": "\u5728\u4e09\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cCG-DMER\u5728\u591a\u79cd\u4e0b\u6e38\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "CG-DMER\u901a\u8fc7\u65f6\u7a7a\u63a9\u7801\u5efa\u6a21\u548c\u8868\u793a\u89e3\u8026\u5bf9\u9f50\u7b56\u7565\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5fc3\u7535\u56fe\u591a\u6a21\u6001\u5206\u6790\u4e2d\u7684\u6a21\u6001\u5185\u548c\u6a21\u6001\u95f4\u95ee\u9898\uff0c\u4e3a\u5fc3\u8840\u7ba1\u75be\u75c5\u8bca\u65ad\u63d0\u4f9b\u4e86\u66f4\u51c6\u786e\u7684\u5fc3\u7535\u56fe\u8868\u793a\u5b66\u4e60\u65b9\u6cd5\u3002"}}
{"id": "2602.20658", "categories": ["cs.CV", "cs.AI", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.20658", "abs": "https://arxiv.org/abs/2602.20658", "authors": ["Mohammad Sadra Rajabi", "Aanuoluwapo Ojelade", "Sunwook Kim", "Maury A. Nussbaum"], "title": "Vision-Language Models for Ergonomic Assessment of Manual Lifting Tasks: Estimating Horizontal and Vertical Hand Distances from RGB Video", "comment": null, "summary": "Manual lifting tasks are a major contributor to work-related musculoskeletal disorders, and effective ergonomic risk assessment is essential for quantifying physical exposure and informing ergonomic interventions. The Revised NIOSH Lifting Equation (RNLE) is a widely used ergonomic risk assessment tool for lifting tasks that relies on six task variables, including horizontal (H) and vertical (V) hand distances; such distances are typically obtained through manual measurement or specialized sensing systems and are difficult to use in real-world environments. We evaluated the feasibility of using innovative vision-language models (VLMs) to non-invasively estimate H and V from RGB video streams. Two multi-stage VLM-based pipelines were developed: a text-guided detection-only pipeline and a detection-plus-segmentation pipeline. Both pipelines used text-guided localization of task-relevant regions of interest, visual feature extraction from those regions, and transformer-based temporal regression to estimate H and V at the start and end of a lift. For a range of lifting tasks, estimation performance was evaluated using leave-one-subject-out validation across the two pipelines and seven camera view conditions. Results varied significantly across pipelines and camera view conditions, with the segmentation-based, multi-view pipeline consistently yielding the smallest errors, achieving mean absolute errors of approximately 6-8 cm when estimating H and 5-8 cm when estimating V. Across pipelines and camera view configurations, pixel-level segmentation reduced estimation error by approximately 20-30% for H and 35-40% for V relative to the detection-only pipeline. These findings support the feasibility of VLM-based pipelines for video-based estimation of RNLE distance parameters.", "AI": {"tldr": "\u7814\u7a76\u8bc4\u4f30\u4e86\u4f7f\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4eceRGB\u89c6\u9891\u6d41\u4e2d\u975e\u4fb5\u5165\u5f0f\u4f30\u8ba1NIOSH\u63d0\u5347\u65b9\u7a0b\u4e2d\u6c34\u5e73\u548c\u5782\u76f4\u624b\u90e8\u8ddd\u79bb\u7684\u53ef\u884c\u6027\uff0c\u5f00\u53d1\u4e86\u4e24\u79cd\u591a\u9636\u6bb5VLM\u7ba1\u9053\uff0c\u5176\u4e2d\u57fa\u4e8e\u5206\u5272\u7684\u591a\u89c6\u56fe\u7ba1\u9053\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u624b\u52a8\u63d0\u5347\u4efb\u52a1\u662f\u5de5\u4f5c\u76f8\u5173\u808c\u8089\u9aa8\u9abc\u75be\u75c5\u7684\u4e3b\u8981\u539f\u56e0\uff0c\u6709\u6548\u7684\u5de5\u6548\u5b66\u98ce\u9669\u8bc4\u4f30\u81f3\u5173\u91cd\u8981\u3002\u4fee\u8ba2\u7248NIOSH\u63d0\u5347\u65b9\u7a0b\u662f\u5e7f\u6cdb\u4f7f\u7528\u7684\u8bc4\u4f30\u5de5\u5177\uff0c\u4f46\u5176\u6240\u9700\u7684\u6c34\u5e73\u548c\u5782\u76f4\u624b\u90e8\u8ddd\u79bb\u53c2\u6570\u901a\u5e38\u9700\u8981\u624b\u52a8\u6d4b\u91cf\u6216\u4e13\u7528\u4f20\u611f\u7cfb\u7edf\uff0c\u96be\u4ee5\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u4f7f\u7528\u3002", "method": "\u5f00\u53d1\u4e86\u4e24\u79cd\u591a\u9636\u6bb5\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7ba1\u9053\uff1a\u6587\u672c\u5f15\u5bfc\u7684\u4ec5\u68c0\u6d4b\u7ba1\u9053\u548c\u68c0\u6d4b\u52a0\u5206\u5272\u7ba1\u9053\u3002\u4e24\u79cd\u7ba1\u9053\u90fd\u4f7f\u7528\u6587\u672c\u5f15\u5bfc\u5b9a\u4f4d\u4efb\u52a1\u76f8\u5173\u611f\u5174\u8da3\u533a\u57df\uff0c\u4ece\u8fd9\u4e9b\u533a\u57df\u63d0\u53d6\u89c6\u89c9\u7279\u5f81\uff0c\u5e76\u4f7f\u7528\u57fa\u4e8e\u53d8\u6362\u5668\u7684\u65f6\u95f4\u56de\u5f52\u6765\u4f30\u8ba1\u63d0\u5347\u5f00\u59cb\u548c\u7ed3\u675f\u65f6\u7684\u6c34\u5e73\u548c\u5782\u76f4\u8ddd\u79bb\u3002\u5728\u591a\u79cd\u63d0\u5347\u4efb\u52a1\u548c\u4e03\u4e2a\u76f8\u673a\u89c6\u89d2\u6761\u4ef6\u4e0b\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u7ed3\u679c\u56e0\u7ba1\u9053\u548c\u76f8\u673a\u89c6\u89d2\u6761\u4ef6\u800c\u5f02\uff0c\u57fa\u4e8e\u5206\u5272\u7684\u591a\u89c6\u56fe\u7ba1\u9053\u8868\u73b0\u6700\u7a33\u5b9a\uff0c\u4f30\u8ba1\u6c34\u5e73\u8ddd\u79bb\u7684\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\u7ea6\u4e3a6-8\u5398\u7c73\uff0c\u5782\u76f4\u8ddd\u79bb\u7ea6\u4e3a5-8\u5398\u7c73\u3002\u4e0e\u4ec5\u68c0\u6d4b\u7ba1\u9053\u76f8\u6bd4\uff0c\u50cf\u7d20\u7ea7\u5206\u5272\u5c06\u6c34\u5e73\u8ddd\u79bb\u4f30\u8ba1\u8bef\u5dee\u964d\u4f4e\u4e8620-30%\uff0c\u5782\u76f4\u8ddd\u79bb\u8bef\u5dee\u964d\u4f4e\u4e8635-40%\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u652f\u6301\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u7ba1\u9053\u7528\u4e8e\u89c6\u9891\u4f30\u8ba1NIOSH\u63d0\u5347\u65b9\u7a0b\u8ddd\u79bb\u53c2\u6570\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u5728\u771f\u5b9e\u5de5\u4f5c\u73af\u5883\u4e2d\u8fdb\u884c\u975e\u4fb5\u5165\u5f0f\u5de5\u6548\u5b66\u98ce\u9669\u8bc4\u4f30\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u65b9\u6cd5\u3002"}}
{"id": "2602.20664", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.20664", "abs": "https://arxiv.org/abs/2602.20664", "authors": ["Hailong Yan", "Shice Liu", "Tao Wang", "Xiangtao Zhang", "Yijie Zhong", "Jinwei Chen", "Le Zhang", "Bo Li"], "title": "AnimeAgent: Is the Multi-Agent via Image-to-Video models a Good Disney Storytelling Artist?", "comment": "Tech Report", "summary": "Custom Storyboard Generation (CSG) aims to produce high-quality, multi-character consistent storytelling. Current approaches based on static diffusion models, whether used in a one-shot manner or within multi-agent frameworks, face three key limitations: (1) Static models lack dynamic expressiveness and often resort to \"copy-paste\" pattern. (2) One-shot inference cannot iteratively correct missing attributes or poor prompt adherence. (3) Multi-agents rely on non-robust evaluators, ill-suited for assessing stylized, non-realistic animation. To address these, we propose AnimeAgent, the first Image-to-Video (I2V)-based multi-agent framework for CSG. Inspired by Disney's \"Combination of Straight Ahead and Pose to Pose\" workflow, AnimeAgent leverages I2V's implicit motion prior to enhance consistency and expressiveness, while a mixed subjective-objective reviewer enables reliable iterative refinement. We also collect a human-annotated CSG benchmark with ground-truth. Experiments show AnimeAgent achieves SOTA performance in consistency, prompt fidelity, and stylization.", "AI": {"tldr": "\u63d0\u51faAnimeAgent\u6846\u67b6\uff0c\u9996\u4e2a\u57fa\u4e8e\u56fe\u50cf\u5230\u89c6\u9891\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u7528\u4e8e\u5b9a\u5236\u6545\u4e8b\u677f\u751f\u6210\uff0c\u89e3\u51b3\u73b0\u6709\u9759\u6001\u6269\u6563\u6a21\u578b\u5728\u4e00\u81f4\u6027\u3001\u8868\u8fbe\u529b\u548c\u8fed\u4ee3\u4f18\u5316\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u9759\u6001\u6269\u6563\u6a21\u578b\u7684\u6545\u4e8b\u677f\u751f\u6210\u65b9\u6cd5\u5b58\u5728\u4e09\u4e2a\u5173\u952e\u95ee\u9898\uff1a1) \u9759\u6001\u6a21\u578b\u7f3a\u4e4f\u52a8\u6001\u8868\u73b0\u529b\uff0c\u5e38\u91c7\u7528\"\u590d\u5236\u7c98\u8d34\"\u6a21\u5f0f\uff1b2) \u5355\u6b21\u63a8\u7406\u65e0\u6cd5\u8fed\u4ee3\u4fee\u6b63\u7f3a\u5931\u5c5e\u6027\u6216\u63d0\u793a\u8bcd\u9075\u5faa\u4e0d\u8db3\uff1b3) \u591a\u667a\u80fd\u4f53\u65b9\u6cd5\u4f9d\u8d56\u4e0d\u53ef\u9760\u7684\u8bc4\u4f30\u5668\uff0c\u4e0d\u9002\u5408\u8bc4\u4f30\u98ce\u683c\u5316\u975e\u5199\u5b9e\u52a8\u753b\u3002", "method": "\u63d0\u51faAnimeAgent\u6846\u67b6\uff0c\u53d7\u8fea\u58eb\u5c3c\"\u76f4\u8fdb\u4e0e\u59ff\u52bf\u5230\u59ff\u52bf\u7ed3\u5408\"\u5de5\u4f5c\u6d41\u7a0b\u542f\u53d1\uff0c\u5229\u7528\u56fe\u50cf\u5230\u89c6\u9891\u6a21\u578b\u7684\u9690\u5f0f\u8fd0\u52a8\u5148\u9a8c\u589e\u5f3a\u4e00\u81f4\u6027\u548c\u8868\u73b0\u529b\uff0c\u91c7\u7528\u6df7\u5408\u4e3b\u5ba2\u89c2\u8bc4\u5ba1\u673a\u5236\u5b9e\u73b0\u53ef\u9760\u7684\u8fed\u4ee3\u4f18\u5316\u3002", "result": "\u5b9e\u9a8c\u8868\u660eAnimeAgent\u5728\u4e00\u81f4\u6027\u3001\u63d0\u793a\u8bcd\u9075\u5faa\u5ea6\u548c\u98ce\u683c\u5316\u65b9\u9762\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5e76\u6536\u96c6\u4e86\u5e26\u4eba\u5de5\u6807\u6ce8\u7684\u771f\u5b9e\u57fa\u51c6\u6570\u636e\u96c6\u3002", "conclusion": "AnimeAgent\u901a\u8fc7\u56fe\u50cf\u5230\u89c6\u9891\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u5b9a\u5236\u6545\u4e8b\u677f\u751f\u6210\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u5728\u591a\u4e2a\u8bc4\u4f30\u7ef4\u5ea6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2602.20673", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.20673", "abs": "https://arxiv.org/abs/2602.20673", "authors": ["Hao Zhang", "Lue Fan", "Qitai Wang", "Wenbo Li", "Zehuan Wu", "Lewei Lu", "Zhaoxiang Zhang", "Hongsheng Li"], "title": "GA-Drive: Geometry-Appearance Decoupled Modeling for Free-viewpoint Driving Scene Generatio", "comment": null, "summary": "A free-viewpoint, editable, and high-fidelity driving simulator is crucial for training and evaluating end-to-end autonomous driving systems. In this paper, we present GA-Drive, a novel simulation framework capable of generating camera views along user-specified novel trajectories through Geometry-Appearance Decoupling and Diffusion-Based Generation. Given a set of images captured along a recorded trajectory and the corresponding scene geometry, GA-Drive synthesizes novel pseudo-views using geometry information. These pseudo-views are then transformed into photorealistic views using a trained video diffusion model. In this way, we decouple the geometry and appearance of scenes. An advantage of such decoupling is its support for appearance editing via state-of-the-art video-to-video editing techniques, while preserving the underlying geometry, enabling consistent edits across both original and novel trajectories. Extensive experiments demonstrate that GA-Drive substantially outperforms existing methods in terms of NTA-IoU, NTL-IoU, and FID scores.", "AI": {"tldr": "GA-Drive\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u9a7e\u9a76\u6a21\u62df\u6846\u67b6\uff0c\u901a\u8fc7\u51e0\u4f55-\u5916\u89c2\u89e3\u8026\u548c\u6269\u6563\u6a21\u578b\u751f\u6210\uff0c\u80fd\u591f\u6cbf\u7528\u6237\u6307\u5b9a\u7684\u65b0\u8f68\u8ff9\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u53ef\u7f16\u8f91\u7684\u76f8\u673a\u89c6\u56fe\u3002", "motivation": "\u9700\u8981\u6784\u5efa\u4e00\u4e2a\u81ea\u7531\u89c6\u89d2\u3001\u53ef\u7f16\u8f91\u4e14\u9ad8\u4fdd\u771f\u7684\u9a7e\u9a76\u6a21\u62df\u5668\uff0c\u7528\u4e8e\u8bad\u7ec3\u548c\u8bc4\u4f30\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u3002", "method": "\u91c7\u7528\u51e0\u4f55-\u5916\u89c2\u89e3\u8026\u65b9\u6cd5\uff1a\u9996\u5148\u5229\u7528\u51e0\u4f55\u4fe1\u606f\u5408\u6210\u65b0\u7684\u4f2a\u89c6\u56fe\uff0c\u7136\u540e\u4f7f\u7528\u8bad\u7ec3\u597d\u7684\u89c6\u9891\u6269\u6563\u6a21\u578b\u5c06\u5176\u8f6c\u6362\u4e3a\u903c\u771f\u7684\u89c6\u56fe\u3002\u652f\u6301\u901a\u8fc7\u5148\u8fdb\u7684\u89c6\u9891\u5230\u89c6\u9891\u7f16\u8f91\u6280\u672f\u8fdb\u884c\u5916\u89c2\u7f16\u8f91\uff0c\u540c\u65f6\u4fdd\u6301\u5e95\u5c42\u51e0\u4f55\u7ed3\u6784\u7684\u4e00\u81f4\u6027\u3002", "result": "\u5728NTA-IoU\u3001NTL-IoU\u548cFID\u5206\u6570\u65b9\u9762\uff0cGA-Drive\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "GA-Drive\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u6548\u7684\u9a7e\u9a76\u6a21\u62df\u6846\u67b6\uff0c\u901a\u8fc7\u51e0\u4f55-\u5916\u89c2\u89e3\u8026\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u89c6\u56fe\u751f\u6210\u548c\u5916\u89c2\u7f16\u8f91\u529f\u80fd\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u8bad\u7ec3\u548c\u8bc4\u4f30\u63d0\u4f9b\u4e86\u6709\u529b\u5de5\u5177\u3002"}}
{"id": "2602.20689", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.20689", "abs": "https://arxiv.org/abs/2602.20689", "authors": ["Bedrettin Cetinkaya", "Sinan Kalkan", "Emre Akbas"], "title": "MatchED: Crisp Edge Detection Using End-to-End, Matching-based Supervision", "comment": "Accepted to CVPR 2026", "summary": "Generating crisp, i.e., one-pixel-wide, edge maps remains one of the fundamental challenges in edge detection, affecting both traditional and learning-based methods. To obtain crisp edges, most existing approaches rely on two hand-crafted post-processing algorithms, Non-Maximum Suppression (NMS) and skeleton-based thinning, which are non-differentiable and hinder end-to-end optimization. Moreover, all existing crisp edge detection methods still depend on such post-processing to achieve satisfactory results. To address this limitation, we propose \\MethodLPP, a lightweight, only $\\sim$21K additional parameters, and plug-and-play matching-based supervision module that can be appended to any edge detection model for joint end-to-end learning of crisp edges. At each training iteration, \\MethodLPP performs one-to-one matching between predicted and ground-truth edges based on spatial distance and confidence, ensuring consistency between training and testing protocols. Extensive experiments on four popular datasets demonstrate that integrating \\MethodLPP substantially improves the performance of existing edge detection models. In particular, \\MethodLPP increases the Average Crispness (AC) metric by up to 2--4$\\times$ compared to baseline models. Under the crispness-emphasized evaluation (CEval), \\MethodLPP further boosts baseline performance by up to 20--35\\% in ODS and achieves similar gains in OIS and AP, achieving SOTA performance that matches or surpasses standard post-processing for the first time. Code is available at https://cvpr26-matched.github.io.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u3001\u5373\u63d2\u5373\u7528\u7684\u5339\u914d\u76d1\u7763\u6a21\u5757LPP\uff0c\u7528\u4e8e\u7aef\u5230\u7aef\u5b66\u4e60\u5355\u50cf\u7d20\u5bbd\u5ea6\u7684\u6e05\u6670\u8fb9\u7f18\uff0c\u65e0\u9700\u4f20\u7edf\u540e\u5904\u7406\uff0c\u663e\u8457\u63d0\u5347\u8fb9\u7f18\u68c0\u6d4b\u6a21\u578b\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u8fb9\u7f18\u68c0\u6d4b\u65b9\u6cd5\u751f\u6210\u6e05\u6670\uff08\u5355\u50cf\u7d20\u5bbd\u5ea6\uff09\u8fb9\u7f18\u56fe\u65f6\uff0c\u4e25\u91cd\u4f9d\u8d56\u975e\u6700\u5927\u6291\u5236\uff08NMS\uff09\u548c\u9aa8\u67b6\u7ec6\u5316\u7b49\u624b\u5de5\u540e\u5904\u7406\u7b97\u6cd5\uff0c\u8fd9\u4e9b\u7b97\u6cd5\u4e0d\u53ef\u5fae\u5206\uff0c\u963b\u788d\u4e86\u7aef\u5230\u7aef\u4f18\u5316\u3002\u6240\u6709\u73b0\u6709\u6e05\u6670\u8fb9\u7f18\u68c0\u6d4b\u65b9\u6cd5\u4ecd\u9700\u6b64\u7c7b\u540e\u5904\u7406\u624d\u80fd\u83b7\u5f97\u6ee1\u610f\u7ed3\u679c\u3002", "method": "\u63d0\u51faLPP\u6a21\u5757\uff0c\u4ec5\u9700\u7ea621K\u989d\u5916\u53c2\u6570\uff0c\u53ef\u9644\u52a0\u5230\u4efb\u4f55\u8fb9\u7f18\u68c0\u6d4b\u6a21\u578b\u4e0a\uff0c\u8fdb\u884c\u7aef\u5230\u7aef\u8054\u5408\u5b66\u4e60\u3002\u5728\u6bcf\u4e2a\u8bad\u7ec3\u8fed\u4ee3\u4e2d\uff0c\u57fa\u4e8e\u7a7a\u95f4\u8ddd\u79bb\u548c\u7f6e\u4fe1\u5ea6\u5728\u9884\u6d4b\u8fb9\u7f18\u548c\u771f\u5b9e\u8fb9\u7f18\u4e4b\u95f4\u6267\u884c\u4e00\u5bf9\u4e00\u5339\u914d\uff0c\u786e\u4fdd\u8bad\u7ec3\u548c\u6d4b\u8bd5\u534f\u8bae\u7684\u4e00\u81f4\u6027\u3002", "result": "\u5728\u56db\u4e2a\u6d41\u884c\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u96c6\u6210LPP\u663e\u8457\u63d0\u5347\u4e86\u73b0\u6709\u8fb9\u7f18\u68c0\u6d4b\u6a21\u578b\u7684\u6027\u80fd\u3002LPP\u5c06\u5e73\u5747\u6e05\u6670\u5ea6\uff08AC\uff09\u6307\u6807\u63d0\u9ad8\u4e862-4\u500d\u3002\u5728\u5f3a\u8c03\u6e05\u6670\u5ea6\u7684\u8bc4\u4f30\uff08CEval\uff09\u4e0b\uff0cLPP\u5c06\u57fa\u7ebf\u6027\u80fd\u5728ODS\u4e0a\u63d0\u5347\u4e8620-35%\uff0c\u5728OIS\u548cAP\u4e0a\u4e5f\u6709\u7c7b\u4f3c\u63d0\u5347\uff0c\u9996\u6b21\u8fbe\u5230\u6216\u8d85\u8d8a\u4e86\u6807\u51c6\u540e\u5904\u7406\u7684SOTA\u6027\u80fd\u3002", "conclusion": "LPP\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u3001\u5373\u63d2\u5373\u7528\u7684\u5339\u914d\u76d1\u7763\u6a21\u5757\uff0c\u80fd\u591f\u5b9e\u73b0\u6e05\u6670\u8fb9\u7f18\u7684\u7aef\u5230\u7aef\u5b66\u4e60\uff0c\u65e0\u9700\u4f20\u7edf\u540e\u5904\u7406\uff0c\u663e\u8457\u63d0\u5347\u8fb9\u7f18\u68c0\u6d4b\u6027\u80fd\uff0c\u9996\u6b21\u8fbe\u5230\u6216\u8d85\u8d8a\u6807\u51c6\u540e\u5904\u7406\u65b9\u6cd5\u7684\u6027\u80fd\u3002"}}
{"id": "2602.20700", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.20700", "abs": "https://arxiv.org/abs/2602.20700", "authors": ["Anna Badalyan", "Pratheba Selvaraju", "Giorgio Becherini", "Omid Taheri", "Victoria Fernandez Abrevaya", "Michael Black"], "title": "NGL-Prompter: Training-Free Sewing Pattern Estimation from a Single Image", "comment": "10 pages, 7 figures", "summary": "Estimating sewing patterns from images is a practical approach for creating high-quality 3D garments. Due to the lack of real-world pattern-image paired data, prior approaches fine-tune large vision language models (VLMs) on synthetic garment datasets generated by randomly sampling from a parametric garment model GarmentCode. However, these methods often struggle to generalize to in-the-wild images, fail to capture real-world correlations between garment parts, and are typically restricted to single-layer outfits. In contrast, we observe that VLMs are effective at describing garments in natural language, yet perform poorly when asked to directly regress GarmentCode parameters from images. To bridge this gap, we propose NGL (Natural Garment Language), a novel intermediate language that restructures GarmentCode into a representation more understandable to language models. Leveraging this language, we introduce NGL-Prompter, a training-free pipeline that queries large VLMs to extract structured garment parameters, which are then deterministically mapped to valid GarmentCode. We evaluate our method on the Dress4D, CloSe and a newly collected dataset of approximately 5,000 in-the-wild fashion images. Our approach achieves state-of-the-art performance on standard geometry metrics and is strongly preferred in both human and GPT-based perceptual evaluations compared to existing baselines. Furthermore, NGL-prompter can recover multi-layer outfits whereas competing methods focus mostly on single-layer garments, highlighting its strong generalization to real-world images even with occluded parts. These results demonstrate that accurate sewing pattern reconstruction is possible without costly model training. Our code and data will be released for research use.", "AI": {"tldr": "\u63d0\u51faNGL\uff08\u81ea\u7136\u670d\u88c5\u8bed\u8a00\uff09\u4f5c\u4e3a\u4e2d\u95f4\u8868\u793a\uff0c\u901a\u8fc7NGL-Prompter\u65e0\u9700\u8bad\u7ec3\u5373\u53ef\u4ece\u56fe\u50cf\u4e2d\u63d0\u53d6\u7ed3\u6784\u5316\u670d\u88c5\u53c2\u6570\uff0c\u5b9e\u73b0\u9ad8\u8d28\u91cf3D\u670d\u88c5\u91cd\u5efa", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u5408\u6210\u6570\u636e\u5fae\u8c03\u5927\u6a21\u578b\uff0c\u96be\u4ee5\u6cdb\u5316\u5230\u771f\u5b9e\u56fe\u50cf\uff0c\u65e0\u6cd5\u6355\u6349\u771f\u5b9e\u670d\u88c5\u90e8\u4ef6\u76f8\u5173\u6027\uff0c\u4e14\u901a\u5e38\u4ec5\u9650\u4e8e\u5355\u5c42\u670d\u88c5\u3002\u800cVLMs\u64c5\u957f\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u670d\u88c5\uff0c\u4f46\u5728\u76f4\u63a5\u4ece\u56fe\u50cf\u56de\u5f52GarmentCode\u53c2\u6570\u65f6\u8868\u73b0\u4e0d\u4f73", "method": "\u63d0\u51faNGL\u5c06GarmentCode\u91cd\u6784\u4e3a\u8bed\u8a00\u6a21\u578b\u66f4\u6613\u7406\u89e3\u7684\u8868\u793a\uff0c\u5f00\u53d1NGL-Prompter\u8bad\u7ec3\u514d\u8d39\u6d41\u7a0b\uff0c\u67e5\u8be2\u5927VLMs\u63d0\u53d6\u7ed3\u6784\u5316\u670d\u88c5\u53c2\u6570\uff0c\u7136\u540e\u786e\u5b9a\u6027\u5730\u6620\u5c04\u5230\u6709\u6548GarmentCode", "result": "\u5728Dress4D\u3001CloSe\u548c\u7ea65000\u5f20\u771f\u5b9e\u65f6\u5c1a\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0c\u5728\u6807\u51c6\u51e0\u4f55\u6307\u6807\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5728\u4eba\u7c7b\u548cGPT\u611f\u77e5\u8bc4\u4f30\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\uff0c\u80fd\u6062\u590d\u591a\u5c42\u670d\u88c5\u800c\u7ade\u4e89\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u5355\u5c42", "conclusion": "\u65e0\u9700\u6602\u8d35\u6a21\u578b\u8bad\u7ec3\u5373\u53ef\u5b9e\u73b0\u51c6\u786e\u7684\u7f1d\u7eab\u56fe\u6848\u91cd\u5efa\uff0cNGL-Prompter\u5bf9\u771f\u5b9e\u56fe\u50cf\uff08\u5373\u4f7f\u6709\u906e\u6321\u90e8\u5206\uff09\u5177\u6709\u5f3a\u6cdb\u5316\u80fd\u529b\uff0c\u53ef\u5904\u7406\u591a\u5c42\u670d\u88c5"}}
{"id": "2602.20718", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.20718", "abs": "https://arxiv.org/abs/2602.20718", "authors": ["Yangsen Chen", "Hao Wang"], "title": "Monocular Endoscopic Tissue 3D Reconstruction with Multi-Level Geometry Regularization", "comment": "ijcnn 2025", "summary": "Reconstructing deformable endoscopic tissues is crucial for achieving robot-assisted surgery. However, 3D Gaussian Splatting-based approaches encounter challenges in achieving consistent tissue surface reconstruction, while existing NeRF-based methods lack real-time rendering capabilities. In pursuit of both smooth deformable surfaces and real-time rendering, we introduce a novel approach based on 3D Gaussian Splatting. Specifically, we introduce surface-aware reconstruction, initially employing a Sign Distance Field-based method to construct a mesh, subsequently utilizing this mesh to constrain the Gaussian Splatting reconstruction process. Furthermore, to ensure the generation of physically plausible deformations, we incorporate local rigidity and global non-rigidity restrictions to guide Gaussian deformation, tailored for the highly deformable nature of soft endoscopic tissue. Based on 3D Gaussian Splatting, our proposed method delivers a fast rendering process and smooth surface appearances. Quantitative and qualitative analysis against alternative methodologies shows that our approach achieves solid reconstruction quality in both textures and geometries.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e3D\u9ad8\u65af\u6cfc\u6e85\u7684\u53d8\u5f62\u5185\u7aa5\u955c\u7ec4\u7ec7\u91cd\u5efa\u65b9\u6cd5\uff0c\u901a\u8fc7\u8868\u9762\u611f\u77e5\u91cd\u5efa\u548c\u7269\u7406\u7ea6\u675f\u5b9e\u73b0\u5e73\u6ed1\u53d8\u5f62\u8868\u9762\u548c\u5b9e\u65f6\u6e32\u67d3\u3002", "motivation": "\u673a\u5668\u4eba\u8f85\u52a9\u624b\u672f\u9700\u8981\u91cd\u5efa\u53d8\u5f62\u5185\u7aa5\u955c\u7ec4\u7ec7\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff1a3D\u9ad8\u65af\u6cfc\u6e85\u65b9\u6cd5\u96be\u4ee5\u5b9e\u73b0\u4e00\u81f4\u7684\u8868\u9762\u91cd\u5efa\uff0c\u800cNeRF\u65b9\u6cd5\u7f3a\u4e4f\u5b9e\u65f6\u6e32\u67d3\u80fd\u529b\u3002\u56e0\u6b64\u9700\u8981\u540c\u65f6\u5b9e\u73b0\u5e73\u6ed1\u53d8\u5f62\u8868\u9762\u548c\u5b9e\u65f6\u6e32\u67d3\u7684\u65b0\u65b9\u6cd5\u3002", "method": "1. \u63d0\u51fa\u8868\u9762\u611f\u77e5\u91cd\u5efa\uff1a\u9996\u5148\u4f7f\u7528\u7b26\u53f7\u8ddd\u79bb\u573a\u65b9\u6cd5\u6784\u5efa\u7f51\u683c\uff0c\u7136\u540e\u7528\u8be5\u7f51\u683c\u7ea6\u675f\u9ad8\u65af\u6cfc\u6e85\u91cd\u5efa\u8fc7\u7a0b\u30022. \u5f15\u5165\u7269\u7406\u7ea6\u675f\uff1a\u7ed3\u5408\u5c40\u90e8\u521a\u6027\u548c\u5168\u5c40\u975e\u521a\u6027\u7ea6\u675f\u6765\u6307\u5bfc\u9ad8\u65af\u53d8\u5f62\uff0c\u9002\u5e94\u8f6f\u5185\u7aa5\u955c\u7ec4\u7ec7\u7684\u9ad8\u5ea6\u53d8\u5f62\u7279\u6027\u3002", "result": "\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u5feb\u901f\u6e32\u67d3\u8fc7\u7a0b\u548c\u5e73\u6ed1\u8868\u9762\u5916\u89c2\u3002\u4e0e\u66ff\u4ee3\u65b9\u6cd5\u7684\u5b9a\u91cf\u548c\u5b9a\u6027\u5206\u6790\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u7eb9\u7406\u548c\u51e0\u4f55\u65b9\u9762\u90fd\u5b9e\u73b0\u4e86\u575a\u5b9e\u7684\u91cd\u5efa\u8d28\u91cf\u3002", "conclusion": "\u57fa\u4e8e3D\u9ad8\u65af\u6cfc\u6e85\u7684\u63d0\u51fa\u7684\u65b9\u6cd5\u6210\u529f\u89e3\u51b3\u4e86\u53d8\u5f62\u5185\u7aa5\u955c\u7ec4\u7ec7\u91cd\u5efa\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u5e73\u6ed1\u53d8\u5f62\u8868\u9762\u548c\u5b9e\u65f6\u6e32\u67d3\u7684\u5e73\u8861\uff0c\u4e3a\u673a\u5668\u4eba\u8f85\u52a9\u624b\u672f\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.20725", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.20725", "abs": "https://arxiv.org/abs/2602.20725", "authors": ["Junwei Shu", "Wenjie Liu", "Changgu Chen", "Hantang Liu", "Yang Li", "Changbo Wang"], "title": "Bridging Physically Based Rendering and Diffusion Models with Stochastic Differential Equation", "comment": "preprint", "summary": "Diffusion-based image generators excel at producing realistic content from text or image conditions, but they offer only limited explicit control over low-level, physically grounded shading and material properties. In contrast, physically based rendering (PBR) offers fine-grained physical control but lacks prompt-driven flexibility. Although these two paradigms originate from distinct communities, both share a common evolution -- from noisy observations to clean images. In this paper, we propose a unified stochastic formulation that bridges Monte Carlo rendering and diffusion-based generative modeling. First, a general stochastic differential equation (SDE) formulation for Monte Carlo integration under the Central Limit Theorem is modeled. Through instantiation via physically based path tracing, we convert it into a physically grounded SDE representation. Moreover, we provide a systematic analysis of how the physical characteristics of path tracing can be extended to existing diffusion models from the perspective of noise variance. Extensive experiments across multiple tasks show that our method can exert physically grounded control over diffusion-generated results, covering tasks such as rendering and material editing.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u968f\u673a\u516c\u5f0f\uff0c\u5c06\u8499\u7279\u5361\u6d1b\u6e32\u67d3\u4e0e\u57fa\u4e8e\u6269\u6563\u7684\u751f\u6210\u5efa\u6a21\u76f8\u7ed3\u5408\uff0c\u5b9e\u73b0\u5bf9\u6269\u6563\u751f\u6210\u7ed3\u679c\u7684\u7269\u7406\u57fa\u7840\u63a7\u5236\u3002", "motivation": "\u57fa\u4e8e\u6269\u6563\u7684\u56fe\u50cf\u751f\u6210\u5668\u5728\u4ece\u6587\u672c\u6216\u56fe\u50cf\u6761\u4ef6\u751f\u6210\u903c\u771f\u5185\u5bb9\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u4f4e\u5c42\u6b21\u3001\u7269\u7406\u57fa\u7840\u7684\u7740\u8272\u548c\u6750\u8d28\u5c5e\u6027\u7684\u663e\u5f0f\u63a7\u5236\u3002\u800c\u57fa\u4e8e\u7269\u7406\u7684\u6e32\u67d3\uff08PBR\uff09\u63d0\u4f9b\u7ec6\u7c92\u5ea6\u7684\u7269\u7406\u63a7\u5236\uff0c\u4f46\u7f3a\u4e4f\u63d0\u793a\u9a71\u52a8\u7684\u7075\u6d3b\u6027\u3002\u8fd9\u4e24\u79cd\u8303\u5f0f\u90fd\u9075\u5faa\u4ece\u566a\u58f0\u89c2\u5bdf\u5230\u5e72\u51c0\u56fe\u50cf\u7684\u5171\u540c\u6f14\u5316\u8def\u5f84\u3002", "method": "\u63d0\u51fa\u7edf\u4e00\u7684\u968f\u673a\u516c\u5f0f\uff0c\u9996\u5148\u5728\u4e2d\u5fc3\u6781\u9650\u5b9a\u7406\u4e0b\u4e3a\u8499\u7279\u5361\u6d1b\u79ef\u5206\u5efa\u7acb\u4e00\u822c\u968f\u673a\u5fae\u5206\u65b9\u7a0b\uff08SDE\uff09\u516c\u5f0f\uff0c\u901a\u8fc7\u57fa\u4e8e\u7269\u7406\u7684\u8def\u5f84\u8ffd\u8e2a\u8fdb\u884c\u5b9e\u4f8b\u5316\uff0c\u5c06\u5176\u8f6c\u6362\u4e3a\u7269\u7406\u57fa\u7840\u7684SDE\u8868\u793a\u3002\u4ece\u566a\u58f0\u65b9\u5dee\u89d2\u5ea6\u5206\u6790\u8def\u5f84\u8ffd\u8e2a\u7684\u7269\u7406\u7279\u6027\u5982\u4f55\u6269\u5c55\u5230\u73b0\u6709\u6269\u6563\u6a21\u578b\u3002", "result": "\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u5bf9\u6269\u6563\u751f\u6210\u7ed3\u679c\u65bd\u52a0\u7269\u7406\u57fa\u7840\u7684\u63a7\u5236\uff0c\u6db5\u76d6\u6e32\u67d3\u548c\u6750\u8d28\u7f16\u8f91\u7b49\u4efb\u52a1\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u5730\u5c06\u8499\u7279\u5361\u6d1b\u6e32\u67d3\u4e0e\u6269\u6563\u751f\u6210\u5efa\u6a21\u7edf\u4e00\u8d77\u6765\uff0c\u5b9e\u73b0\u4e86\u5bf9\u6269\u6563\u751f\u6210\u5185\u5bb9\u7684\u7269\u7406\u57fa\u7840\u63a7\u5236\uff0c\u4e3a\u7ed3\u5408\u4e24\u79cd\u8303\u5f0f\u7684\u4f18\u52bf\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2602.20790", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.20790", "abs": "https://arxiv.org/abs/2602.20790", "authors": ["Sheng Zhong", "Zhongyang Ren", "Xiya Zhu", "Dehao Yuan", "Cornelia Fermuller", "Yi Zhou"], "title": "Real-time Motion Segmentation with Event-based Normal Flow", "comment": null, "summary": "Event-based cameras are bio-inspired sensors with pixels that independently and asynchronously respond to brightness changes at microsecond resolution, offering the potential to handle visual tasks in challenging scenarios. However, due to the sparse information content in individual events, directly processing the raw event data to solve vision tasks is highly inefficient, which severely limits the applicability of state-of-the-art methods in real-time tasks, such as motion segmentation, a fundamental task for dynamic scene understanding. Incorporating normal flow as an intermediate representation to compress motion information from event clusters within a localized region provides a more effective solution. In this work, we propose a normal flow-based motion segmentation framework for event-based vision. Leveraging the dense normal flow directly learned from event neighborhoods as input, we formulate the motion segmentation task as an energy minimization problem solved via graph cuts, and optimize it iteratively with normal flow clustering and motion model fitting. By using a normal flow-based motion model initialization and fitting method, the proposed system is able to efficiently estimate the motion models of independently moving objects with only a limited number of candidate models, which significantly reduces the computational complexity and ensures real-time performance, achieving nearly a 800x speedup in comparison to the open-source state-of-the-art method. Extensive evaluations on multiple public datasets fully demonstrate the accuracy and efficiency of our framework.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u6cd5\u5411\u6d41\u7684\u4e8b\u4ef6\u76f8\u673a\u8fd0\u52a8\u5206\u5272\u6846\u67b6\uff0c\u901a\u8fc7\u6cd5\u5411\u6d41\u538b\u7f29\u4e8b\u4ef6\u4fe1\u606f\uff0c\u4f7f\u7528\u56fe\u5272\u80fd\u91cf\u6700\u5c0f\u5316\u5b9e\u73b0\u5b9e\u65f6\u8fd0\u52a8\u5206\u5272\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u5b9e\u73b0\u8fd1800\u500d\u52a0\u901f", "motivation": "\u4e8b\u4ef6\u76f8\u673a\u5177\u6709\u5fae\u79d2\u7ea7\u5206\u8fa8\u7387\uff0c\u4f46\u5728\u6311\u6218\u6027\u573a\u666f\u4e2d\u5904\u7406\u539f\u59cb\u4e8b\u4ef6\u6570\u636e\u6548\u7387\u4f4e\u4e0b\uff0c\u9650\u5236\u4e86\u5b9e\u65f6\u5e94\u7528\u3002\u6cd5\u5411\u6d41\u4f5c\u4e3a\u4e2d\u95f4\u8868\u793a\u53ef\u4ee5\u538b\u7f29\u5c40\u90e8\u533a\u57df\u7684\u4e8b\u4ef6\u96c6\u7fa4\u8fd0\u52a8\u4fe1\u606f\uff0c\u63d0\u4f9b\u66f4\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u6cd5\u5411\u6d41\u7684\u8fd0\u52a8\u5206\u5272\u6846\u67b6\uff1a1) \u4ece\u4e8b\u4ef6\u90bb\u57df\u76f4\u63a5\u5b66\u4e60\u5bc6\u96c6\u6cd5\u5411\u6d41\u4f5c\u4e3a\u8f93\u5165\uff1b2) \u5c06\u8fd0\u52a8\u5206\u5272\u5efa\u6a21\u4e3a\u80fd\u91cf\u6700\u5c0f\u5316\u95ee\u9898\uff0c\u901a\u8fc7\u56fe\u5272\u6c42\u89e3\uff1b3) \u901a\u8fc7\u6cd5\u5411\u6d41\u805a\u7c7b\u548c\u8fd0\u52a8\u6a21\u578b\u62df\u5408\u8fdb\u884c\u8fed\u4ee3\u4f18\u5316\uff1b4) \u4f7f\u7528\u6cd5\u5411\u6d41\u521d\u59cb\u5316\u8fd0\u52a8\u6a21\u578b\uff0c\u51cf\u5c11\u5019\u9009\u6a21\u578b\u6570\u91cf\u3002", "result": "\u76f8\u6bd4\u5f00\u6e90\u6700\u5148\u8fdb\u65b9\u6cd5\u5b9e\u73b0\u8fd1800\u500d\u52a0\u901f\uff0c\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u786e\u4fdd\u5b9e\u65f6\u6027\u80fd\u3002\u5728\u591a\u4e2a\u516c\u5171\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u8bc4\u4f30\u5145\u5206\u8bc1\u660e\u4e86\u6846\u67b6\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "conclusion": "\u57fa\u4e8e\u6cd5\u5411\u6d41\u7684\u4e8b\u4ef6\u76f8\u673a\u8fd0\u52a8\u5206\u5272\u6846\u67b6\u901a\u8fc7\u538b\u7f29\u8fd0\u52a8\u4fe1\u606f\u3001\u51cf\u5c11\u5019\u9009\u6a21\u578b\u6570\u91cf\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u51c6\u786e\u7684\u5b9e\u65f6\u8fd0\u52a8\u5206\u5272\uff0c\u4e3a\u52a8\u6001\u573a\u666f\u7406\u89e3\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.20792", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.20792", "abs": "https://arxiv.org/abs/2602.20792", "authors": ["Muhammad Saif Ullah Khan", "Didier Stricker"], "title": "SIMSPINE: A Biomechanics-Aware Simulation Framework for 3D Spine Motion Annotation and Benchmarking", "comment": "Accepted at CVPR 2026", "summary": "Modeling spinal motion is fundamental to understanding human biomechanics, yet remains underexplored in computer vision due to the spine's complex multi-joint kinematics and the lack of large-scale 3D annotations. We present a biomechanics-aware keypoint simulation framework that augments existing human pose datasets with anatomically consistent 3D spinal keypoints derived from musculoskeletal modeling. Using this framework, we create the first open dataset, named SIMSPINE, which provides sparse vertebra-level 3D spinal annotations for natural full-body motions in indoor multi-camera capture without external restraints. With 2.14 million frames, this enables data-driven learning of vertebral kinematics from subtle posture variations and bridges the gap between musculoskeletal simulation and computer vision. In addition, we release pretrained baselines covering fine-tuned 2D detectors, monocular 3D pose lifting models, and multi-view reconstruction pipelines, establishing a unified benchmark for biomechanically valid spine motion estimation. Specifically, our 2D spine baselines improve the state-of-the-art from 0.63 to 0.80 AUC in controlled environments, and from 0.91 to 0.93 AP for in-the-wild spine tracking. Together, the simulation framework and SIMSPINE dataset advance research in vision-based biomechanics, motion analysis, and digital human modeling by enabling reproducible, anatomically grounded 3D spine estimation under natural conditions.", "AI": {"tldr": "\u63d0\u51faSIMSPINE\u6570\u636e\u96c6\u548c\u751f\u7269\u529b\u5b66\u611f\u77e5\u5173\u952e\u70b9\u6a21\u62df\u6846\u67b6\uff0c\u4e3a\u81ea\u7136\u5168\u8eab\u8fd0\u52a8\u63d0\u4f9b\u9996\u4e2a\u5f00\u653e\u7684\u5927\u89c4\u6a213D\u810a\u67f1\u5173\u952e\u70b9\u6807\u6ce8\uff0c\u63d0\u5347\u810a\u67f1\u8fd0\u52a8\u4f30\u8ba1\u6027\u80fd", "motivation": "\u810a\u67f1\u8fd0\u52a8\u5efa\u6a21\u5bf9\u4eba\u7c7b\u751f\u7269\u529b\u5b66\u7406\u89e3\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7531\u4e8e\u810a\u67f1\u590d\u6742\u7684\u591a\u5173\u8282\u8fd0\u52a8\u5b66\u548c\u7f3a\u4e4f\u5927\u89c4\u6a213D\u6807\u6ce8\uff0c\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u9886\u57df\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22", "method": "\u5f00\u53d1\u751f\u7269\u529b\u5b66\u611f\u77e5\u5173\u952e\u70b9\u6a21\u62df\u6846\u67b6\uff0c\u901a\u8fc7\u808c\u8089\u9aa8\u9abc\u5efa\u6a21\u4ece\u73b0\u6709\u4eba\u4f53\u59ff\u6001\u6570\u636e\u96c6\u4e2d\u751f\u6210\u89e3\u5256\u5b66\u4e00\u81f4\u76843D\u810a\u67f1\u5173\u952e\u70b9\uff0c\u521b\u5efaSIMSPINE\u6570\u636e\u96c6\uff08214\u4e07\u5e27\uff09\uff0c\u5e76\u63d0\u4f9b\u9884\u8bad\u7ec3\u57fa\u7ebf\u6a21\u578b", "result": "2D\u810a\u67f1\u57fa\u7ebf\u5728\u53d7\u63a7\u73af\u5883\u4e2d\u5c06AUC\u4ece0.63\u63d0\u5347\u81f30.80\uff0c\u5728\u91ce\u5916\u810a\u67f1\u8ddf\u8e2a\u4e2d\u5c06AP\u4ece0.91\u63d0\u5347\u81f30.93\uff0c\u5efa\u7acb\u4e86\u7edf\u4e00\u7684\u751f\u7269\u529b\u5b66\u6709\u6548\u810a\u67f1\u8fd0\u52a8\u4f30\u8ba1\u57fa\u51c6", "conclusion": "\u6a21\u62df\u6846\u67b6\u548cSIMSPINE\u6570\u636e\u96c6\u901a\u8fc7\u5b9e\u73b0\u53ef\u91cd\u590d\u3001\u89e3\u5256\u5b66\u57fa\u7840\u76843D\u810a\u67f1\u4f30\u8ba1\uff0c\u63a8\u52a8\u4e86\u57fa\u4e8e\u89c6\u89c9\u7684\u751f\u7269\u529b\u5b66\u3001\u8fd0\u52a8\u5206\u6790\u548c\u6570\u5b57\u4eba\u4f53\u5efa\u6a21\u7814\u7a76"}}
{"id": "2602.20794", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.20794", "abs": "https://arxiv.org/abs/2602.20794", "authors": ["Jie Wang", "Guang Li", "Zhijian Huang", "Chenxu Dang", "Hangjun Ye", "Yahong Han", "Long Chen"], "title": "VGGDrive: Empowering Vision-Language Models with Cross-View Geometric Grounding for Autonomous Driving", "comment": "CVPR 2026", "summary": "The significance of cross-view 3D geometric modeling capabilities for autonomous driving is self-evident, yet existing Vision-Language Models (VLMs) inherently lack this capability, resulting in their mediocre performance. While some promising approaches attempt to mitigate this by constructing Q&A data for auxiliary training, they still fail to fundamentally equip VLMs with the ability to comprehensively handle diverse evaluation protocols. We thus chart a new course, advocating for the infusion of VLMs with the cross-view geometric grounding of mature 3D foundation models, closing this critical capability gap in autonomous driving. In this spirit, we propose a novel architecture, VGGDrive, which empowers Vision-language models with cross-view Geometric Grounding for autonomous Driving. Concretely, to bridge the cross-view 3D geometric features from the frozen visual 3D model with the VLM's 2D visual features, we introduce a plug-and-play Cross-View 3D Geometric Enabler (CVGE). The CVGE decouples the base VLM architecture and effectively empowers the VLM with 3D features through a hierarchical adaptive injection mechanism. Extensive experiments show that VGGDrive enhances base VLM performance across five autonomous driving benchmarks, including tasks like cross-view risk perception, motion prediction, and trajectory planning. It's our belief that mature 3D foundation models can empower autonomous driving tasks through effective integration, and we hope our initial exploration demonstrates the potential of this paradigm to the autonomous driving community.", "AI": {"tldr": "VGGDrive\uff1a\u901a\u8fc7\u8de8\u89c6\u56fe\u51e0\u4f55\u611f\u77e5\u589e\u5f3a\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u81ea\u52a8\u9a7e\u9a76\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\uff0c\u5c06\u6210\u719f\u76843D\u57fa\u7840\u6a21\u578b\u7684\u51e0\u4f55\u7406\u89e3\u80fd\u529b\u6ce8\u5165\u5230VLM\u4e2d", "motivation": "\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7f3a\u4e4f\u8de8\u89c6\u56fe3D\u51e0\u4f55\u5efa\u6a21\u80fd\u529b\uff0c\u5bfc\u81f4\u5728\u81ea\u52a8\u9a7e\u9a76\u4efb\u52a1\u4e2d\u8868\u73b0\u5e73\u5eb8\u3002\u867d\u7136\u6709\u4e9b\u65b9\u6cd5\u5c1d\u8bd5\u901a\u8fc7\u6784\u5efa\u95ee\u7b54\u6570\u636e\u8fdb\u884c\u8f85\u52a9\u8bad\u7ec3\uff0c\u4f46\u4ecd\u65e0\u6cd5\u4ece\u6839\u672c\u4e0a\u8ba9VLM\u5168\u9762\u5904\u7406\u591a\u6837\u5316\u7684\u8bc4\u4f30\u534f\u8bae", "method": "\u63d0\u51faVGGDrive\u67b6\u6784\uff0c\u901a\u8fc7\u53ef\u63d2\u62d4\u7684\u8de8\u89c6\u56fe3D\u51e0\u4f55\u4f7f\u80fd\u5668\uff08CVGE\uff09\u5c06\u51bb\u7ed3\u76843D\u89c6\u89c9\u6a21\u578b\u7684\u8de8\u89c6\u56fe\u51e0\u4f55\u7279\u5f81\u4e0eVLM\u76842D\u89c6\u89c9\u7279\u5f81\u76f8\u878d\u5408\u3002CVGE\u91c7\u7528\u5206\u5c42\u81ea\u9002\u5e94\u6ce8\u5165\u673a\u5236\uff0c\u6709\u6548\u589e\u5f3aVLM\u76843D\u7279\u5f81\u80fd\u529b", "result": "\u5728\u4e94\u4e2a\u81ea\u52a8\u9a7e\u9a76\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u57fa\u7840VLM\u7684\u6027\u80fd\uff0c\u5305\u62ec\u8de8\u89c6\u56fe\u98ce\u9669\u611f\u77e5\u3001\u8fd0\u52a8\u9884\u6d4b\u548c\u8f68\u8ff9\u89c4\u5212\u7b49\u4efb\u52a1", "conclusion": "\u6210\u719f\u76843D\u57fa\u7840\u6a21\u578b\u53ef\u4ee5\u901a\u8fc7\u6709\u6548\u96c6\u6210\u6765\u8d4b\u80fd\u81ea\u52a8\u9a7e\u9a76\u4efb\u52a1\uff0c\u8fd9\u9879\u521d\u6b65\u63a2\u7d22\u5c55\u793a\u4e86\u8be5\u8303\u5f0f\u5728\u81ea\u52a8\u9a7e\u9a76\u9886\u57df\u7684\u6f5c\u529b"}}
{"id": "2602.20807", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.20807", "abs": "https://arxiv.org/abs/2602.20807", "authors": ["Yangfan Zhao", "Hanwei Zhang", "Ke Huang", "Qiufeng Wang", "Zhenzhou Shao", "Dengyu Wu"], "title": "RU4D-SLAM: Reweighting Uncertainty in Gaussian Splatting SLAM for 4D Scene Reconstruction", "comment": null, "summary": "Combining 3D Gaussian splatting with Simultaneous Localization and Mapping (SLAM) has gained popularity as it enables continuous 3D environment reconstruction during motion. However, existing methods struggle in dynamic environments, particularly moving objects complicate 3D reconstruction and, in turn, hinder reliable tracking. The emergence of 4D reconstruction, especially 4D Gaussian splatting, offers a promising direction for addressing these challenges, yet its potential for 4D-aware SLAM remains largely underexplored. Along this direction, we propose a robust and efficient framework, namely Reweighting Uncertainty in Gaussian Splatting SLAM (RU4D-SLAM) for 4D scene reconstruction, that introduces temporal factors into spatial 3D representation while incorporating uncertainty-aware perception of scene changes, blurred image synthesis, and dynamic scene reconstruction. We enhance dynamic scene representation by integrating motion blur rendering, and improve uncertainty-aware tracking by extending per-pixel uncertainty modeling, which is originally designed for static scenarios, to handle blurred images. Furthermore, we propose a semantic-guided reweighting mechanism for per-pixel uncertainty estimation in dynamic scenes, and introduce a learnable opacity weight to support adaptive 4D mapping. Extensive experiments on standard benchmarks demonstrate that our method substantially outperforms state-of-the-art approaches in both trajectory accuracy and 4D scene reconstruction, particularly in dynamic environments with moving objects and low-quality inputs. Code available: https://ru4d-slam.github.io", "AI": {"tldr": "RU4D-SLAM\u662f\u4e00\u4e2a\u7ed3\u54084D\u9ad8\u65af\u6cfc\u6e85\u4e0eSLAM\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5165\u65f6\u95f4\u56e0\u7d20\u3001\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u548c\u8bed\u4e49\u5f15\u5bfc\u91cd\u52a0\u6743\u673a\u5236\uff0c\u5728\u52a8\u6001\u73af\u5883\u4e2d\u5b9e\u73b0\u9c81\u68d2\u76844D\u573a\u666f\u91cd\u5efa\u548c\u8f68\u8ff9\u8ddf\u8e2a\u3002", "motivation": "\u73b0\u67093D\u9ad8\u65af\u6cfc\u6e85\u4e0eSLAM\u7ed3\u5408\u7684\u65b9\u6cd5\u5728\u52a8\u6001\u73af\u5883\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u79fb\u52a8\u7269\u4f53\u4f7f3D\u91cd\u5efa\u590d\u6742\u5316\u5e76\u5f71\u54cd\u8ddf\u8e2a\u53ef\u9760\u6027\u30024D\u91cd\u5efa\uff08\u7279\u522b\u662f4D\u9ad8\u65af\u6cfc\u6e85\uff09\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u65b9\u5411\uff0c\u4f46\u5176\u57284D\u611f\u77e5SLAM\u4e2d\u7684\u5e94\u7528\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "\u63d0\u51faRU4D-SLAM\u6846\u67b6\uff1a1\uff09\u5c06\u65f6\u95f4\u56e0\u7d20\u5f15\u5165\u7a7a\u95f43D\u8868\u793a\uff1b2\uff09\u96c6\u6210\u8fd0\u52a8\u6a21\u7cca\u6e32\u67d3\u589e\u5f3a\u52a8\u6001\u573a\u666f\u8868\u793a\uff1b3\uff09\u6269\u5c55\u6bcf\u50cf\u7d20\u4e0d\u786e\u5b9a\u6027\u5efa\u6a21\u4ee5\u5904\u7406\u6a21\u7cca\u56fe\u50cf\uff1b4\uff09\u63d0\u51fa\u8bed\u4e49\u5f15\u5bfc\u7684\u91cd\u52a0\u6743\u673a\u5236\u7528\u4e8e\u52a8\u6001\u573a\u666f\u7684\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\uff1b5\uff09\u5f15\u5165\u53ef\u5b66\u4e60\u7684\u900f\u660e\u5ea6\u6743\u91cd\u652f\u6301\u81ea\u9002\u5e944D\u6620\u5c04\u3002", "result": "\u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u8f68\u8ff9\u7cbe\u5ea6\u548c4D\u573a\u666f\u91cd\u5efa\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5728\u5305\u542b\u79fb\u52a8\u7269\u4f53\u548c\u4f4e\u8d28\u91cf\u8f93\u5165\u7684\u52a8\u6001\u73af\u5883\u4e2d\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "RU4D-SLAM\u901a\u8fc7\u7ed3\u54084D\u9ad8\u65af\u6cfc\u6e85\u4e0e\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u673a\u5236\uff0c\u4e3a\u52a8\u6001\u73af\u5883\u4e2d\u7684\u9c81\u68d2SLAM\u548c4D\u573a\u666f\u91cd\u5efa\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u8f68\u8ff9\u8ddf\u8e2a\u548c\u91cd\u5efa\u8d28\u91cf\u65b9\u9762\u5747\u53d6\u5f97\u663e\u8457\u63d0\u5347\u3002"}}
{"id": "2602.20818", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.20818", "abs": "https://arxiv.org/abs/2602.20818", "authors": ["Yingying Guo", "Ke Zhang", "Zirong Zeng"], "title": "GatedCLIP: Gated Multimodal Fusion for Hateful Memes Detection", "comment": "Preprint", "summary": "Detecting hateful content in multimodal memes presents unique challenges, as harmful messages often emerge from the complex interplay between benign images and text. We propose GatedCLIP, a Vision-Language model that enhances CLIP's multimodal capabilities with specialized architectural improvements for hateful memes detection. Our approach introduces learned projection heads that map CLIP embeddings to a task-optimized semantic space, a dynamic gated fusion mechanism that adaptively weights visual and textual features, and a contrastive learning objective that maintains cross-modal semantic alignment. Experiments on the Hateful Memes dataset demonstrate that GatedCLIP achieves an AUROC of 0.66, substantially outperforming the CLIP baseline (AUROC 0.49) while maintaining computational efficiency with only 350K trainable parameters.", "AI": {"tldr": "GatedCLIP\u6a21\u578b\u901a\u8fc7\u6539\u8fdbCLIP\u67b6\u6784\uff0c\u7ed3\u5408\u5b66\u4e60\u6295\u5f71\u5934\u3001\u52a8\u6001\u95e8\u63a7\u878d\u5408\u673a\u5236\u548c\u5bf9\u6bd4\u5b66\u4e60\u76ee\u6807\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u6a21\u6001\u4ec7\u6068\u8868\u60c5\u5305\u68c0\u6d4b\u6027\u80fd\uff0c\u5728Hateful Memes\u6570\u636e\u96c6\u4e0aAUROC\u8fbe\u52300.66\uff0c\u8fdc\u8d85CLIP\u57fa\u7ebf\u76840.49\u3002", "motivation": "\u591a\u6a21\u6001\u8868\u60c5\u5305\u4e2d\u7684\u4ec7\u6068\u5185\u5bb9\u68c0\u6d4b\u9762\u4e34\u72ec\u7279\u6311\u6218\uff0c\u56e0\u4e3a\u6709\u5bb3\u4fe1\u606f\u5f80\u5f80\u6765\u81ea\u826f\u6027\u56fe\u50cf\u548c\u6587\u672c\u7684\u590d\u6742\u4ea4\u4e92\u3002\u73b0\u6709\u6a21\u578b\u5982CLIP\u5728\u5904\u7406\u8fd9\u79cd\u5fae\u5999\u7684\u591a\u6a21\u6001\u4ec7\u6068\u8868\u8fbe\u65f6\u6548\u679c\u6709\u9650\u3002", "method": "\u63d0\u51faGatedCLIP\u6a21\u578b\uff0c\u5728CLIP\u57fa\u7840\u4e0a\u5f15\u5165\u4e09\u4e2a\u5173\u952e\u6539\u8fdb\uff1a1\uff09\u5b66\u4e60\u6295\u5f71\u5934\u5c06CLIP\u5d4c\u5165\u6620\u5c04\u5230\u4efb\u52a1\u4f18\u5316\u7684\u8bed\u4e49\u7a7a\u95f4\uff1b2\uff09\u52a8\u6001\u95e8\u63a7\u878d\u5408\u673a\u5236\u81ea\u9002\u5e94\u52a0\u6743\u89c6\u89c9\u548c\u6587\u672c\u7279\u5f81\uff1b3\uff09\u5bf9\u6bd4\u5b66\u4e60\u76ee\u6807\u4fdd\u6301\u8de8\u6a21\u6001\u8bed\u4e49\u5bf9\u9f50\u3002", "result": "\u5728Hateful Memes\u6570\u636e\u96c6\u4e0a\uff0cGatedCLIP\u8fbe\u5230AUROC 0.66\uff0c\u663e\u8457\u4f18\u4e8eCLIP\u57fa\u7ebf\u76840.49\u3002\u6a21\u578b\u4ec5\u970035\u4e07\u53ef\u8bad\u7ec3\u53c2\u6570\uff0c\u4fdd\u6301\u4e86\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "GatedCLIP\u901a\u8fc7\u4e13\u95e8\u7684\u67b6\u6784\u6539\u8fdb\u6709\u6548\u589e\u5f3a\u4e86CLIP\u7684\u591a\u6a21\u6001\u80fd\u529b\uff0c\u4e3a\u4ec7\u6068\u8868\u60c5\u5305\u68c0\u6d4b\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u8bc1\u660e\u4e86\u4efb\u52a1\u7279\u5b9a\u4f18\u5316\u5728\u591a\u6a21\u6001\u4ec7\u6068\u5185\u5bb9\u68c0\u6d4b\u4e2d\u7684\u91cd\u8981\u6027\u3002"}}
