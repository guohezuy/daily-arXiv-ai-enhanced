{"id": "2511.11549", "categories": ["cs.CR", "cs.DB", "cs.IT", "eess.SP"], "pdf": "https://arxiv.org/pdf/2511.11549", "abs": "https://arxiv.org/abs/2511.11549", "authors": ["Shreya Meel", "Sennur Ulukus"], "title": "HetDAPAC: Leveraging Attribute Heterogeneity in Distributed Attribute-Based Private Access Control", "comment": null, "summary": "Verifying user attributes to provide fine-grained access control to databases is fundamental to attribute-based authentication. Either a single (central) authority verifies all the attributes, or multiple independent authorities verify the attributes distributedly. In the central setup, the authority verifies all user attributes, and the user downloads only the authorized record. While this is communication efficient, it reveals all user attributes to the authority. A distributed setup prevents this privacy breach by letting each authority verify and learn only one attribute. Motivated by this, Jafarpisheh~et~al. introduced an information-theoretic formulation, called distributed attribute-based private access control (DAPAC). With $N$ non-colluding authorities (servers), $N$ attributes and $K$ possible values for each attribute, the DAPAC system lets each server learn only the single attribute value that it verifies, and is oblivious to the remaining $N-1$. The user retrieves its designated record, without learning anything about the remaining database records. The goal is to maximize the rate, i.e., the ratio of desired message size to total download size. However, not all attributes are sensitive, and DAPAC's privacy constraints can be too restrictive, negatively affecting the rate. To leverage the heterogeneous privacy requirements of user attributes, we propose heterogeneous (Het)DAPAC, a framework which off-loads verification of $N-D$ of the $N$ attributes to a central server, and retains DAPAC's architecture for the $D$ sensitive attributes. We first present a HetDAPAC scheme, which improves the rate from $\\frac{1}{2K}$ to $\\frac{1}{K+1}$, while sacrificing the privacy of a few non-sensitive attributes. Unlike DAPAC, our scheme entails a download imbalance across servers; we propose a second scheme achieving a balanced per-server download and a rate of $\\frac{D+1}{2KD}$.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86HetDAPAC\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u975e\u654f\u611f\u5c5e\u6027\u9a8c\u8bc1\u96c6\u4e2d\u5316\u5904\u7406\uff0c\u5728\u4fdd\u6301\u654f\u611f\u5c5e\u6027\u9690\u79c1\u7684\u540c\u65f6\u63d0\u9ad8\u7cfb\u7edf\u6548\u7387\uff0c\u89e3\u51b3\u4e86DAPAC\u6846\u67b6\u4e2d\u6240\u6709\u5c5e\u6027\u4e25\u683c\u9690\u79c1\u4fdd\u62a4\u5bfc\u81f4\u7684\u6548\u7387\u95ee\u9898\u3002", "motivation": "\u4f20\u7edfDAPAC\u7cfb\u7edf\u5bf9\u6240\u6709\u7528\u6237\u5c5e\u6027\u91c7\u7528\u76f8\u540c\u7684\u9690\u79c1\u4fdd\u62a4\u6807\u51c6\uff0c\u5bfc\u81f4\u7cfb\u7edf\u6548\u7387\u8f83\u4f4e\u3002\u5b9e\u9645\u4e0a\u5e76\u975e\u6240\u6709\u5c5e\u6027\u90fd\u662f\u654f\u611f\u7684\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u80fd\u591f\u6839\u636e\u5c5e\u6027\u654f\u611f\u5ea6\u5dee\u5f02\u63d0\u4f9b\u4e0d\u540c\u9690\u79c1\u4fdd\u62a4\u7ea7\u522b\u7684\u6846\u67b6\u3002", "method": "\u63d0\u51faHetDAPAC\u6846\u67b6\uff0c\u5c06N\u4e2a\u5c5e\u6027\u4e2d\u7684N-D\u4e2a\u975e\u654f\u611f\u5c5e\u6027\u9a8c\u8bc1\u96c6\u4e2d\u5230\u4e2d\u592e\u670d\u52a1\u5668\u5904\u7406\uff0c\u4ec5\u5bf9D\u4e2a\u654f\u611f\u5c5e\u6027\u4fdd\u6301DAPAC\u7684\u5206\u5e03\u5f0f\u9a8c\u8bc1\u67b6\u6784\u3002\u63d0\u51fa\u4e86\u4e24\u79cd\u65b9\u6848\uff1a\u4e00\u79cd\u63d0\u9ad8\u7cfb\u7edf\u901f\u7387\uff0c\u53e6\u4e00\u79cd\u5b9e\u73b0\u670d\u52a1\u5668\u95f4\u4e0b\u8f7d\u8d1f\u8f7d\u5747\u8861\u3002", "result": "\u7b2c\u4e00\u79cd\u65b9\u6848\u5c06\u901f\u7387\u4ece1/2K\u63d0\u9ad8\u52301/(K+1)\uff0c\u4f46\u5b58\u5728\u670d\u52a1\u5668\u95f4\u4e0b\u8f7d\u4e0d\u5e73\u8861\u95ee\u9898\uff1b\u7b2c\u4e8c\u79cd\u65b9\u6848\u5b9e\u73b0\u6bcf\u670d\u52a1\u5668\u4e0b\u8f7d\u5747\u8861\uff0c\u901f\u7387\u4e3a(D+1)/(2KD)\u3002", "conclusion": "HetDAPAC\u6846\u67b6\u901a\u8fc7\u533a\u5206\u654f\u611f\u548c\u975e\u654f\u611f\u5c5e\u6027\uff0c\u5728\u4fdd\u6301\u654f\u611f\u5c5e\u6027\u9690\u79c1\u7684\u540c\u65f6\u663e\u8457\u63d0\u9ad8\u4e86\u7cfb\u7edf\u6548\u7387\uff0c\u4e3a\u5c5e\u6027\u9a8c\u8bc1\u7cfb\u7edf\u63d0\u4f9b\u4e86\u66f4\u5b9e\u7528\u7684\u9690\u79c1-\u6548\u7387\u6743\u8861\u65b9\u6848\u3002"}}
{"id": "2511.11552", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.11552", "abs": "https://arxiv.org/abs/2511.11552", "authors": ["Dawei Zhu", "Rui Meng", "Jiefeng Chen", "Sujian Li", "Tomas Pfister", "Jinsung Yoon"], "title": "DocLens : A Tool-Augmented Multi-Agent Framework for Long Visual Document Understanding", "comment": null, "summary": "Comprehending long visual documents, where information is distributed across extensive pages of text and visual elements, is a critical but challenging task for modern Vision-Language Models (VLMs). Existing approaches falter on a fundamental challenge: evidence localization. They struggle to retrieve relevant pages and overlook fine-grained details within visual elements, leading to limited performance and model hallucination. To address this, we propose DocLens, a tool-augmented multi-agent framework that effectively ``zooms in'' on evidence like a lens. It first navigates from the full document to specific visual elements on relevant pages, then employs a sampling-adjudication mechanism to generate a single, reliable answer. Paired with Gemini-2.5-Pro, DocLens achieves state-of-the-art performance on MMLongBench-Doc and FinRAGBench-V, surpassing even human experts. The framework's superiority is particularly evident on vision-centric and unanswerable queries, demonstrating the power of its enhanced localization capabilities.", "AI": {"tldr": "DocLens\u662f\u4e00\u4e2a\u5de5\u5177\u589e\u5f3a\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\"\u653e\u5927\"\u8bc1\u636e\u6765\u6709\u6548\u7406\u89e3\u957f\u89c6\u89c9\u6587\u6863\uff0c\u5728MMLongBench-Doc\u548cFinRAGBench-V\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u751a\u81f3\u8d85\u8d8a\u4eba\u7c7b\u4e13\u5bb6\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u957f\u89c6\u89c9\u6587\u6863\u65f6\u5b58\u5728\u8bc1\u636e\u5b9a\u4f4d\u7684\u6839\u672c\u6311\u6218\uff0c\u65e0\u6cd5\u6709\u6548\u68c0\u7d22\u76f8\u5173\u9875\u9762\u548c\u6355\u6349\u89c6\u89c9\u5143\u7d20\u4e2d\u7684\u7ec6\u7c92\u5ea6\u7ec6\u8282\uff0c\u5bfc\u81f4\u6027\u80fd\u6709\u9650\u548c\u6a21\u578b\u5e7b\u89c9\u3002", "method": "\u63d0\u51faDocLens\u6846\u67b6\uff0c\u9996\u5148\u4ece\u5b8c\u6574\u6587\u6863\u5bfc\u822a\u5230\u76f8\u5173\u9875\u9762\u7684\u7279\u5b9a\u89c6\u89c9\u5143\u7d20\uff0c\u7136\u540e\u91c7\u7528\u91c7\u6837-\u88c1\u51b3\u673a\u5236\u751f\u6210\u5355\u4e00\u53ef\u9760\u7b54\u6848\u3002", "result": "\u4e0eGemini-2.5-Pro\u914d\u5bf9\uff0cDocLens\u5728MMLongBench-Doc\u548cFinRAGBench-V\u4e0a\u5b9e\u73b0\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u89c6\u89c9\u4e2d\u5fc3\u548c\u4e0d\u53ef\u56de\u7b54\u67e5\u8be2\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "DocLens\u901a\u8fc7\u589e\u5f3a\u7684\u5b9a\u4f4d\u80fd\u529b\u5c55\u793a\u4e86\u5176\u4f18\u52bf\uff0c\u8bc1\u660e\u4e86\u8be5\u6846\u67b6\u5728\u5904\u7406\u957f\u89c6\u89c9\u6587\u6863\u7406\u89e3\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
