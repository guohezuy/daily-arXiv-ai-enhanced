<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 20]
- [cs.CL](#cs.CL) [Total: 10]
- [cs.AI](#cs.AI) [Total: 5]
- [cs.CR](#cs.CR) [Total: 2]
- [cs.SE](#cs.SE) [Total: 3]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [De-rendering, Reasoning, and Repairing Charts with Vision-Language Models](https://arxiv.org/abs/2602.20291)
*Valentin Bonas,Martin Sinnona,Viviana Siless,Emmanuel Iarussi*

Main category: cs.CV

TL;DR: 本文提出一个结合图表反渲染、自动分析和迭代改进的框架，为可视化设计提供可操作、可解释的反馈，通过LLM驱动的推荐系统生成基于可视化原则的结构化设计建议。


<details>
  <summary>Details</summary>
Motivation: 数据可视化在科学传播、新闻和日常决策中至关重要，但常存在错误导致误解。基于规则的检查器缺乏上下文且无法提供有意义的改进建议，而通用LLM直接查询可视化质量不可靠，因为它们缺乏遵循可视化设计原则的训练。

Method: 开发了一个结合图表反渲染、自动分析和迭代改进的框架：1) 从图像重建图表结构；2) 使用视觉语言推理识别设计缺陷；3) 基于可视化研究原则提出具体修改建议；4) 用户可选择应用改进并重新渲染，形成反馈循环。

Result: 在Chart2Code基准的1,000个图表上评估，系统生成了10,452个设计建议，聚类为10个连贯类别（如轴格式化、颜色可访问性、图例一致性）。这些结果展示了LLM驱动推荐系统在提供结构化、基于原则的可视化设计反馈方面的潜力。

Conclusion: 该框架为可视化设计提供了可操作、可解释的反馈，促进了更高质量的可视化和可视化素养的发展，为更智能、更易用的创作工具打开了大门。

Abstract: Data visualizations are central to scientific communication, journalism, and everyday decision-making, yet they are frequently prone to errors that can distort interpretation or mislead audiences. Rule-based visualization linters can flag violations, but they miss context and do not suggest meaningful design changes. Directly querying general-purpose LLMs about visualization quality is unreliable: lacking training to follow visualization design principles, they often produce inconsistent or incorrect feedback. In this work, we introduce a framework that combines chart de-rendering, automated analysis, and iterative improvement to deliver actionable, interpretable feedback on visualization design. Our system reconstructs the structure of a chart from an image, identifies design flaws using vision-language reasoning, and proposes concrete modifications supported by established principles in visualization research. Users can selectively apply these improvements and re-render updated figures, creating a feedback loop that promotes both higher-quality visualizations and the development of visualization literacy. In our evaluation on 1,000 charts from the Chart2Code benchmark, the system generated 10,452 design recommendations, which clustered into 10 coherent categories (e.g., axis formatting, color accessibility, legend consistency). These results highlight the promise of LLM-driven recommendation systems for delivering structured, principle-based feedback on visualization design, opening the door to more intelligent and accessible authoring tools.

</details>


### [2] [N4MC: Neural 4D Mesh Compression](https://arxiv.org/abs/2602.20312)
*Guodong Chen,Huanshuo Dong,Mallesham Dasari*

Main category: cs.CV

TL;DR: N4MC是首个4D神经压缩框架，通过利用时间冗余来高效压缩时变网格序列，采用类似2D视频编解码器的帧间压缩思想，在长网格序列中学习运动补偿。


<details>
  <summary>Details</summary>
Motivation: 现有神经网格压缩方法独立处理每个网格帧，忽略了时间冗余。受2D视频编解码器中帧间压缩的启发，需要开发能够利用网格序列时间相关性的压缩方法。

Method: 1) 将连续的不规则网格帧转换为规则的4D张量；2) 使用自动解码器压缩张量，捕获时空相关性；3) 引入基于Transformer的插值模型，通过跟踪体积中心的潜在嵌入预测中间网格帧，消除运动模糊。

Result: N4MC在率失真性能上优于现有最先进方法，同时能够实时解码4D网格序列。代码已开源。

Conclusion: N4MC成功展示了利用时间冗余进行4D网格序列压缩的有效性，为时变网格数据的高效压缩提供了新思路。

Abstract: We present N4MC, the first 4D neural compression framework to efficiently compress time-varying mesh sequences by exploiting their temporal redundancy. Unlike prior neural mesh compression methods that treat each mesh frame independently, N4MC takes inspiration from inter-frame compression in 2D video codecs, and learns motion compensation in long mesh sequences. Specifically, N4MC converts consecutive irregular mesh frames into regular 4D tensors to provide a uniform and compact representation. These tensors are then condensed using an auto-decoder, which captures both spatial and temporal correlations for redundancy removal. To enhance temporal coherence, we introduce a transformer-based interpolation model that predicts intermediate mesh frames conditioned on latent embeddings derived from tracked volume centers, eliminating motion ambiguities. Extensive evaluations show that N4MC outperforms state-of-the-art in rate-distortion performance, while enabling real-time decoding of 4D mesh sequences. The implementation of our method is available at: https://github.com/frozzzen3/N4MC.

</details>


### [3] [GSNR: Graph Smooth Null-Space Representation for Inverse Problems](https://arxiv.org/abs/2602.20328)
*Romario Gualdrón-Hurtado,Roman Jacome,Rafael S. Suarez,Henry Arguello*

Main category: cs.CV

TL;DR: 提出Graph-Smooth Null-Space Representation (GSNR)方法，通过图拉普拉斯在逆问题的零空间分量中引入结构约束，提高图像重建质量


<details>
  <summary>Details</summary>
Motivation: 传统图像先验（如稀疏性、平滑性）无法约束逆问题中的零空间分量，可能导致重建结果有偏差。需要将有意义的结构信息引入零空间表示中

Method: 基于图平滑表示，构建零空间受限的拉普拉斯矩阵，编码零空间信号中相邻像素的相似性。设计从最平滑的谱图模式（最低图频率）的低维投影矩阵

Result: 在图像去模糊、压缩感知、去马赛克和超分辨率四种场景中，GSNR方法相比基线方法提升达4.3dB，相比端到端学习模型提升达1dB（PSNR指标）

Conclusion: GSNR通过将结构约束引入逆问题的零空间分量，显著改善了图像重建质量，具有理论意义和实际应用价值

Abstract: Inverse problems in imaging are ill-posed, leading to infinitely many solutions consistent with the measurements due to the non-trivial null-space of the sensing matrix. Common image priors promote solutions on the general image manifold, such as sparsity, smoothness, or score function. However, as these priors do not constrain the null-space component, they can bias the reconstruction. Thus, we aim to incorporate meaningful null-space information in the reconstruction framework. Inspired by smooth image representation on graphs, we propose Graph-Smooth Null-Space Representation (GSNR), a mechanism that imposes structure only into the invisible component. Particularly, given a graph Laplacian, we construct a null-restricted Laplacian that encodes similarity between neighboring pixels in the null-space signal, and we design a low-dimensional projection matrix from the $p$-smoothest spectral graph modes (lowest graph frequencies). This approach has strong theoretical and practical implications: i) improved convergence via a null-only graph regularizer, ii) better coverage, how much null-space variance is captured by $p$ modes, and iii) high predictability, how well these modes can be inferred from the measurements. GSNR is incorporated into well-known inverse problem solvers, e.g., PnP, DIP, and diffusion solvers, in four scenarios: image deblurring, compressed sensing, demosaicing, and image super-resolution, providing consistent improvement of up to 4.3 dB over baseline formulations and up to 1 dB compared with end-to-end learned models in terms of PSNR.

</details>


### [4] [SceMoS: Scene-Aware 3D Human Motion Synthesis by Planning with Geometry-Grounded Tokens](https://arxiv.org/abs/2602.20476)
*Anindita Ghosh,Vladislav Golyanik,Taku Komura,Philipp Slusallek,Christian Theobalt,Rishabh Dabral*

Main category: cs.CV

TL;DR: SceMoS使用2D场景表示而非3D数据合成文本驱动的3D人体运动，通过BEV图像进行全局规划，局部高度图进行物理约束，在效率和保真度间取得平衡。


<details>
  <summary>Details</summary>
Motivation: 现有方法需要计算昂贵的3D场景数据（如点云、体素网格），同时学习高层规划和低层接触推理。需要更高效且能保持物理真实性的解决方案。

Method: 1) 基于文本条件的自回归全局运动规划器，使用DINOv2特征编码的鸟瞰图作为场景表示；2) 基于条件VQ-VAE训练的几何基础运动分词器，使用2D局部场景高度图将表面物理嵌入离散词汇。

Result: 在TRUMANS基准测试中达到最先进的运动真实性和接触准确性，场景编码可训练参数减少50%以上，证明2D场景线索能有效支撑3D人-场景交互。

Conclusion: 结构化2D场景表示可作为完整3D监督的有力替代方案，实现物理基础运动合成，BEV语义捕获空间布局，局部高度图确保物理遵循，达到效率与保真度的平衡。

Abstract: Synthesizing text-driven 3D human motion within realistic scenes requires learning both semantic intent ("walk to the couch") and physical feasibility (e.g., avoiding collisions). Current methods use generative frameworks that simultaneously learn high-level planning and low-level contact reasoning, and rely on computationally expensive 3D scene data such as point clouds or voxel occupancy grids. We propose SceMoS, a scene-aware motion synthesis framework that shows that structured 2D scene representations can serve as a powerful alternative to full 3D supervision in physically grounded motion synthesis. SceMoS disentangles global planning from local execution using lightweight 2D cues and relying on (1) a text-conditioned autoregressive global motion planner that operates on a bird's-eye-view (BEV) image rendered from an elevated corner of the scene, encoded with DINOv2 features, as the scene representation, and (2) a geometry-grounded motion tokenizer trained via a conditional VQ-VAE, that uses 2D local scene heightmap, thus embedding surface physics directly into a discrete vocabulary. This 2D factorization reaches an efficiency-fidelity trade-off: BEV semantics capture spatial layout and affordance for global reasoning, while local heightmaps enforce fine-grained physical adherence without full 3D volumetric reasoning. SceMoS achieves state-of-the-art motion realism and contact accuracy on the TRUMANS benchmark, reducing the number of trainable parameters for scene encoding by over 50%, showing that 2D scene cues can effectively ground 3D human-scene interaction.

</details>


### [5] [MIP Candy: A Modular PyTorch Framework for Medical Image Processing](https://arxiv.org/abs/2602.21033)
*Tianhao Fu,Yucheng Chen*

Main category: cs.CV

TL;DR: MIPCandy是一个基于PyTorch的医疗图像处理框架，提供完整的模块化流程，通过单一方法实现全功能工作流，同时保持对每个组件的细粒度控制。


<details>
  <summary>Details</summary>
Motivation: 医疗图像处理需要处理高维体积数据、异构文件格式和领域特定训练流程的专用软件。现有框架要么提供需要大量集成工作的低级组件，要么采用僵化的单体式流程难以修改。

Method: 基于PyTorch的模块化框架，核心设计包括LayerT延迟配置机制（支持运行时替换卷积、归一化和激活模块而无需子类化），提供数据加载、训练、推理和评估的完整流程。用户只需实现build_network方法即可获得全功能工作流。

Result: 框架提供k折交叉验证、自动ROI检测的数据集检查、深度监督、指数移动平均、多前端实验跟踪（Weights & Biases、Notion、MLflow）、训练状态恢复、商回归验证分数预测等功能。扩展包生态系统提供预建模型实现。

Conclusion: MIPCandy是一个开源的医疗图像处理框架，在Apache-2.0许可下提供，需要Python 3.12+，解决了现有框架在灵活性和易用性方面的不足，为研究人员提供了既完整又可定制的解决方案。

Abstract: Medical image processing demands specialized software that handles high-dimensional volumetric data, heterogeneous file formats, and domain-specific training procedures. Existing frameworks either provide low-level components that require substantial integration effort or impose rigid, monolithic pipelines that resist modification. We present MIP Candy (MIPCandy), a freely available, PyTorch-based framework designed specifically for medical image processing. MIPCandy provides a complete, modular pipeline spanning data loading, training, inference, and evaluation, allowing researchers to obtain a fully functional process workflow by implementing a single method, $\texttt{build_network}$, while retaining fine-grained control over every component. Central to the design is $\texttt{LayerT}$, a deferred configuration mechanism that enables runtime substitution of convolution, normalization, and activation modules without subclassing. The framework further offers built-in $k$-fold cross-validation, dataset inspection with automatic region-of-interest detection, deep supervision, exponential moving average, multi-frontend experiment tracking (Weights & Biases, Notion, MLflow), training state recovery, and validation score prediction via quotient regression. An extensible bundle ecosystem provides pre-built model implementations that follow a consistent trainer--predictor pattern and integrate with the core framework without modification. MIPCandy is open-source under the Apache-2.0 license and requires Python~3.12 or later. Source code and documentation are available at https://github.com/ProjectNeura/MIPCandy.

</details>


### [6] [Pip-Stereo: Progressive Iterations Pruner for Iterative Optimization based Stereo Matching](https://arxiv.org/abs/2602.20496)
*Jintu Zheng,Qizhe Liu,HuangXin Xu,Zhuojie Chen*

Main category: cs.CV

TL;DR: 该论文提出PipStereo，一种针对边缘设备优化的实时立体匹配方法，通过渐进迭代剪枝、协作式单目先验迁移和硬件感知的FlashGRU算子，在保持高精度的同时大幅提升推理速度。


<details>
  <summary>Details</summary>
Motivation: 现有迭代式立体匹配方法依赖RNN，在边缘设备上部署困难，且迭代过程中的视差更新存在空间稀疏性和时间冗余性，这些问题在现有研究中未得到充分探索。

Method: 1. 渐进迭代剪枝策略：抑制冗余更新步骤，将递归计算压缩为近似单次推理；2. 协作式单目先验迁移框架：隐式嵌入深度先验，无需专用单目编码器；3. FlashGRU：利用结构化稀疏性和I/O感知设计的硬件感知RNN算子。

Result: 在NVIDIA Jetson Orin NX上处理320×640帧仅需75ms（FP16），RTX 4090上仅需19ms，精度与大型迭代模型相当，泛化能力和准确性远超现有实时方法。FlashGRU在2K分辨率下相比原生ConvGRU实现7.28倍加速、76.6%内存峰值降低和80.9%全局内存请求减少。

Conclusion: PipStereo实现了在边缘硬件上的实时高保真立体匹配，通过消除计算冗余和硬件优化，在保持精度的同时大幅提升效率，为边缘AI应用提供了实用解决方案。

Abstract: While iterative stereo matching achieves high accuracy, its dependence on Recurrent Neural Networks (RNN) hinders edge deployment, a challenge underexplored in existing researches. We analyze iterative refinement and reveal that disparity updates are spatially sparse and temporally redundant. First, we introduce a progressive iteration pruning strategy that suppresses redundant update steps, effectively collapsing the recursive computation into a near-single-pass inference. Second, we propose a collaborative monocular prior transfer framework that implicitly embeds depth priors without requiring a dedicated monocular encoder, thereby eliminating its associated computational burden. Third, we develop FlashGRU, a hardware-aware RNN operator leveraging structured sparsity and I/O-conscious design, achieving a 7.28$\times$ speedup, 76.6\% memory peak reduction and 80.9\% global memory requests reduction over natvie ConvGRUs under 2K resolution. Our PipStereo enables real-time, high-fidelity stereo matching on edge hardware: it processes 320$\times$640 frames in just 75ms on an NVIDIA Jetson Orin NX (FP16) and 19ms on RTX 4090, matching the accuracy of large iterative based models, and our generalization ability and accuracy far exceeds that of existing real-time methods. Our embedded AI projects will be updated at: https://github.com/XPENG-Aridge-AI.

</details>


### [7] [PFGNet: A Fully Convolutional Frequency-Guided Peripheral Gating Network for Efficient Spatiotemporal Predictive Learning](https://arxiv.org/abs/2602.20537)
*Xinyong Cai,Changbin Sun,Yong Wang,Hongyu Yang,Yuankai Wu*

Main category: cs.CV

TL;DR: PFGNet：一种基于像素级频率引导门控的全卷积时空预测框架，通过自适应调制感受野来捕捉空间变化的运动模式，在保持高效率的同时实现SOTA或接近SOTA的预测性能。


<details>
  <summary>Details</summary>
Motivation: 纯卷积模型在时空预测学习中具有高效性和完全并行化的优势，但其固定感受野限制了自适应捕捉空间变化运动模式的能力。需要一种能够动态调制感受野的方法来提升模型性能。

Method: 提出PFGNet框架，核心是外围频率门控（PFG）模块，通过像素级频率引导门控动态调制感受野。PFG模块提取局部频谱线索，自适应融合多尺度大核外围响应与可学习的中心抑制，形成空间自适应带通滤波器。为提高效率，所有大核都分解为可分离的1D卷积（1×k后接k×1），将每通道计算成本从O(k²)降低到O(2k)。

Result: 在Moving MNIST、TaxiBJ、Human3.6M和KTH数据集上的实验表明，PFGNet以显著更少的参数和FLOPs实现了SOTA或接近SOTA的预测性能。

Conclusion: PFGNet通过频率引导的自适应感受野调制，实现了无需循环或注意力机制的结构感知时空建模，在保持高效率的同时达到了优异的预测性能。

Abstract: Spatiotemporal predictive learning (STPL) aims to forecast future frames from past observations and is essential across a wide range of applications. Compared with recurrent or hybrid architectures, pure convolutional models offer superior efficiency and full parallelism, yet their fixed receptive fields limit their ability to adaptively capture spatially varying motion patterns. Inspired by biological center-surround organization and frequency-selective signal processing, we propose PFGNet, a fully convolutional framework that dynamically modulates receptive fields through pixel-wise frequency-guided gating. The core Peripheral Frequency Gating (PFG) block extracts localized spectral cues and adaptively fuses multi-scale large-kernel peripheral responses with learnable center suppression, effectively forming spatially adaptive band-pass filters. To maintain efficiency, all large kernels are decomposed into separable 1D convolutions ($1 \times k$ followed by $k \times 1$), reducing per-channel computational cost from $O(k^2)$ to $O(2k)$. PFGNet enables structure-aware spatiotemporal modeling without recurrence or attention. Experiments on Moving MNIST, TaxiBJ, Human3.6M, and KTH show that PFGNet delivers SOTA or near-SOTA forecasting performance with substantially fewer parameters and FLOPs. Our code is available at https://github.com/fhjdqaq/PFGNet.

</details>


### [8] [VAUQ: Vision-Aware Uncertainty Quantification for LVLM Self-Evaluation](https://arxiv.org/abs/2602.21054)
*Seongheon Park,Changdae Oh,Hyeong Kyu Choi,Xuefeng Du,Sharon Li*

Main category: cs.CV

TL;DR: VAUQ是一种视觉感知的不确定性量化框架，用于评估大型视觉语言模型的幻觉问题，通过图像信息分数和核心区域掩码策略来测量模型输出对视觉证据的依赖程度。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型经常产生幻觉，限制了其在现实应用中的安全部署。现有的LLM自评估方法主要依赖语言先验，不适合评估视觉条件预测。

Method: 提出VAUQ框架，引入图像信息分数来捕捉视觉输入对预测不确定性的减少程度，采用无监督核心区域掩码策略来增强显著区域的影响，结合预测熵和核心掩码IS构建无需训练的打分函数。

Result: 综合实验表明，VAUQ在多个数据集上持续优于现有的自评估方法。

Conclusion: VAUQ通过显式测量模型输出对视觉证据的依赖程度，为大型视觉语言模型提供了一种有效的自评估框架，能够可靠反映答案正确性。

Abstract: Large Vision-Language Models (LVLMs) frequently hallucinate, limiting their safe deployment in real-world applications. Existing LLM self-evaluation methods rely on a model's ability to estimate the correctness of its own outputs, which can improve deployment reliability; however, they depend heavily on language priors and are therefore ill-suited for evaluating vision-conditioned predictions. We propose VAUQ, a vision-aware uncertainty quantification framework for LVLM self-evaluation that explicitly measures how strongly a model's output depends on visual evidence. VAUQ introduces the Image-Information Score (IS), which captures the reduction in predictive uncertainty attributable to visual input, and an unsupervised core-region masking strategy that amplifies the influence of salient regions. Combining predictive entropy with this core-masked IS yields a training-free scoring function that reliably reflects answer correctness. Comprehensive experiments show that VAUQ consistently outperforms existing self-evaluation methods across multiple datasets.

</details>


### [9] [Vision-Language Models for Ergonomic Assessment of Manual Lifting Tasks: Estimating Horizontal and Vertical Hand Distances from RGB Video](https://arxiv.org/abs/2602.20658)
*Mohammad Sadra Rajabi,Aanuoluwapo Ojelade,Sunwook Kim,Maury A. Nussbaum*

Main category: cs.CV

TL;DR: 研究评估了使用视觉语言模型从RGB视频流中非侵入式估计NIOSH提升方程中水平和垂直手部距离的可行性，开发了两种多阶段VLM管道，其中基于分割的多视图管道表现最佳。


<details>
  <summary>Details</summary>
Motivation: 手动提升任务是工作相关肌肉骨骼疾病的主要原因，有效的工效学风险评估至关重要。修订版NIOSH提升方程是广泛使用的评估工具，但其所需的水平和垂直手部距离参数通常需要手动测量或专用传感系统，难以在真实环境中使用。

Method: 开发了两种多阶段视觉语言模型管道：文本引导的仅检测管道和检测加分割管道。两种管道都使用文本引导定位任务相关感兴趣区域，从这些区域提取视觉特征，并使用基于变换器的时间回归来估计提升开始和结束时的水平和垂直距离。在多种提升任务和七个相机视角条件下进行评估。

Result: 结果因管道和相机视角条件而异，基于分割的多视图管道表现最稳定，估计水平距离的平均绝对误差约为6-8厘米，垂直距离约为5-8厘米。与仅检测管道相比，像素级分割将水平距离估计误差降低了20-30%，垂直距离误差降低了35-40%。

Conclusion: 研究结果支持基于视觉语言模型的管道用于视频估计NIOSH提升方程距离参数的可行性，为在真实工作环境中进行非侵入式工效学风险评估提供了有前景的方法。

Abstract: Manual lifting tasks are a major contributor to work-related musculoskeletal disorders, and effective ergonomic risk assessment is essential for quantifying physical exposure and informing ergonomic interventions. The Revised NIOSH Lifting Equation (RNLE) is a widely used ergonomic risk assessment tool for lifting tasks that relies on six task variables, including horizontal (H) and vertical (V) hand distances; such distances are typically obtained through manual measurement or specialized sensing systems and are difficult to use in real-world environments. We evaluated the feasibility of using innovative vision-language models (VLMs) to non-invasively estimate H and V from RGB video streams. Two multi-stage VLM-based pipelines were developed: a text-guided detection-only pipeline and a detection-plus-segmentation pipeline. Both pipelines used text-guided localization of task-relevant regions of interest, visual feature extraction from those regions, and transformer-based temporal regression to estimate H and V at the start and end of a lift. For a range of lifting tasks, estimation performance was evaluated using leave-one-subject-out validation across the two pipelines and seven camera view conditions. Results varied significantly across pipelines and camera view conditions, with the segmentation-based, multi-view pipeline consistently yielding the smallest errors, achieving mean absolute errors of approximately 6-8 cm when estimating H and 5-8 cm when estimating V. Across pipelines and camera view configurations, pixel-level segmentation reduced estimation error by approximately 20-30% for H and 35-40% for V relative to the detection-only pipeline. These findings support the feasibility of VLM-based pipelines for video-based estimation of RNLE distance parameters.

</details>


### [10] [AnimeAgent: Is the Multi-Agent via Image-to-Video models a Good Disney Storytelling Artist?](https://arxiv.org/abs/2602.20664)
*Hailong Yan,Shice Liu,Tao Wang,Xiangtao Zhang,Yijie Zhong,Jinwei Chen,Le Zhang,Bo Li*

Main category: cs.CV

TL;DR: 提出AnimeAgent框架，首个基于图像到视频的多智能体系统，用于定制故事板生成，解决现有静态扩散模型在一致性、表达力和迭代优化方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前基于静态扩散模型的故事板生成方法存在三个关键问题：1) 静态模型缺乏动态表现力，常采用"复制粘贴"模式；2) 单次推理无法迭代修正缺失属性或提示词遵循不足；3) 多智能体方法依赖不可靠的评估器，不适合评估风格化非写实动画。

Method: 提出AnimeAgent框架，受迪士尼"直进与姿势到姿势结合"工作流程启发，利用图像到视频模型的隐式运动先验增强一致性和表现力，采用混合主客观评审机制实现可靠的迭代优化。

Result: 实验表明AnimeAgent在一致性、提示词遵循度和风格化方面达到最先进性能，并收集了带人工标注的真实基准数据集。

Conclusion: AnimeAgent通过图像到视频的多智能体框架有效解决了定制故事板生成中的关键挑战，在多个评估维度上优于现有方法。

Abstract: Custom Storyboard Generation (CSG) aims to produce high-quality, multi-character consistent storytelling. Current approaches based on static diffusion models, whether used in a one-shot manner or within multi-agent frameworks, face three key limitations: (1) Static models lack dynamic expressiveness and often resort to "copy-paste" pattern. (2) One-shot inference cannot iteratively correct missing attributes or poor prompt adherence. (3) Multi-agents rely on non-robust evaluators, ill-suited for assessing stylized, non-realistic animation. To address these, we propose AnimeAgent, the first Image-to-Video (I2V)-based multi-agent framework for CSG. Inspired by Disney's "Combination of Straight Ahead and Pose to Pose" workflow, AnimeAgent leverages I2V's implicit motion prior to enhance consistency and expressiveness, while a mixed subjective-objective reviewer enables reliable iterative refinement. We also collect a human-annotated CSG benchmark with ground-truth. Experiments show AnimeAgent achieves SOTA performance in consistency, prompt fidelity, and stylization.

</details>


### [11] [GA-Drive: Geometry-Appearance Decoupled Modeling for Free-viewpoint Driving Scene Generatio](https://arxiv.org/abs/2602.20673)
*Hao Zhang,Lue Fan,Qitai Wang,Wenbo Li,Zehuan Wu,Lewei Lu,Zhaoxiang Zhang,Hongsheng Li*

Main category: cs.CV

TL;DR: GA-Drive是一个新颖的驾驶模拟框架，通过几何-外观解耦和扩散模型生成，能够沿用户指定的新轨迹生成高质量、可编辑的相机视图。


<details>
  <summary>Details</summary>
Motivation: 需要构建一个自由视角、可编辑且高保真的驾驶模拟器，用于训练和评估端到端自动驾驶系统。

Method: 采用几何-外观解耦方法：首先利用几何信息合成新的伪视图，然后使用训练好的视频扩散模型将其转换为逼真的视图。支持通过先进的视频到视频编辑技术进行外观编辑，同时保持底层几何结构的一致性。

Result: 在NTA-IoU、NTL-IoU和FID分数方面，GA-Drive显著优于现有方法。

Conclusion: GA-Drive提供了一个有效的驾驶模拟框架，通过几何-外观解耦实现了高质量视图生成和外观编辑功能，为自动驾驶系统的训练和评估提供了有力工具。

Abstract: A free-viewpoint, editable, and high-fidelity driving simulator is crucial for training and evaluating end-to-end autonomous driving systems. In this paper, we present GA-Drive, a novel simulation framework capable of generating camera views along user-specified novel trajectories through Geometry-Appearance Decoupling and Diffusion-Based Generation. Given a set of images captured along a recorded trajectory and the corresponding scene geometry, GA-Drive synthesizes novel pseudo-views using geometry information. These pseudo-views are then transformed into photorealistic views using a trained video diffusion model. In this way, we decouple the geometry and appearance of scenes. An advantage of such decoupling is its support for appearance editing via state-of-the-art video-to-video editing techniques, while preserving the underlying geometry, enabling consistent edits across both original and novel trajectories. Extensive experiments demonstrate that GA-Drive substantially outperforms existing methods in terms of NTA-IoU, NTL-IoU, and FID scores.

</details>


### [12] [MatchED: Crisp Edge Detection Using End-to-End, Matching-based Supervision](https://arxiv.org/abs/2602.20689)
*Bedrettin Cetinkaya,Sinan Kalkan,Emre Akbas*

Main category: cs.CV

TL;DR: 提出了一种轻量级、即插即用的匹配监督模块LPP，用于端到端学习单像素宽度的清晰边缘，无需传统后处理，显著提升边缘检测模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有边缘检测方法生成清晰（单像素宽度）边缘图时，严重依赖非最大抑制（NMS）和骨架细化等手工后处理算法，这些算法不可微分，阻碍了端到端优化。所有现有清晰边缘检测方法仍需此类后处理才能获得满意结果。

Method: 提出LPP模块，仅需约21K额外参数，可附加到任何边缘检测模型上，进行端到端联合学习。在每个训练迭代中，基于空间距离和置信度在预测边缘和真实边缘之间执行一对一匹配，确保训练和测试协议的一致性。

Result: 在四个流行数据集上的实验表明，集成LPP显著提升了现有边缘检测模型的性能。LPP将平均清晰度（AC）指标提高了2-4倍。在强调清晰度的评估（CEval）下，LPP将基线性能在ODS上提升了20-35%，在OIS和AP上也有类似提升，首次达到或超越了标准后处理的SOTA性能。

Conclusion: LPP是一个轻量级、即插即用的匹配监督模块，能够实现清晰边缘的端到端学习，无需传统后处理，显著提升边缘检测性能，首次达到或超越标准后处理方法的性能。

Abstract: Generating crisp, i.e., one-pixel-wide, edge maps remains one of the fundamental challenges in edge detection, affecting both traditional and learning-based methods. To obtain crisp edges, most existing approaches rely on two hand-crafted post-processing algorithms, Non-Maximum Suppression (NMS) and skeleton-based thinning, which are non-differentiable and hinder end-to-end optimization. Moreover, all existing crisp edge detection methods still depend on such post-processing to achieve satisfactory results. To address this limitation, we propose \MethodLPP, a lightweight, only $\sim$21K additional parameters, and plug-and-play matching-based supervision module that can be appended to any edge detection model for joint end-to-end learning of crisp edges. At each training iteration, \MethodLPP performs one-to-one matching between predicted and ground-truth edges based on spatial distance and confidence, ensuring consistency between training and testing protocols. Extensive experiments on four popular datasets demonstrate that integrating \MethodLPP substantially improves the performance of existing edge detection models. In particular, \MethodLPP increases the Average Crispness (AC) metric by up to 2--4$\times$ compared to baseline models. Under the crispness-emphasized evaluation (CEval), \MethodLPP further boosts baseline performance by up to 20--35\% in ODS and achieves similar gains in OIS and AP, achieving SOTA performance that matches or surpasses standard post-processing for the first time. Code is available at https://cvpr26-matched.github.io.

</details>


### [13] [NGL-Prompter: Training-Free Sewing Pattern Estimation from a Single Image](https://arxiv.org/abs/2602.20700)
*Anna Badalyan,Pratheba Selvaraju,Giorgio Becherini,Omid Taheri,Victoria Fernandez Abrevaya,Michael Black*

Main category: cs.CV

TL;DR: 提出NGL（自然服装语言）作为中间表示，通过NGL-Prompter无需训练即可从图像中提取结构化服装参数，实现高质量3D服装重建


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖合成数据微调大模型，难以泛化到真实图像，无法捕捉真实服装部件相关性，且通常仅限于单层服装。而VLMs擅长自然语言描述服装，但在直接从图像回归GarmentCode参数时表现不佳

Method: 提出NGL将GarmentCode重构为语言模型更易理解的表示，开发NGL-Prompter训练免费流程，查询大VLMs提取结构化服装参数，然后确定性地映射到有效GarmentCode

Result: 在Dress4D、CloSe和约5000张真实时尚图像数据集上评估，在标准几何指标上达到最先进性能，在人类和GPT感知评估中显著优于现有基线，能恢复多层服装而竞争方法主要关注单层

Conclusion: 无需昂贵模型训练即可实现准确的缝纫图案重建，NGL-Prompter对真实图像（即使有遮挡部分）具有强泛化能力，可处理多层服装

Abstract: Estimating sewing patterns from images is a practical approach for creating high-quality 3D garments. Due to the lack of real-world pattern-image paired data, prior approaches fine-tune large vision language models (VLMs) on synthetic garment datasets generated by randomly sampling from a parametric garment model GarmentCode. However, these methods often struggle to generalize to in-the-wild images, fail to capture real-world correlations between garment parts, and are typically restricted to single-layer outfits. In contrast, we observe that VLMs are effective at describing garments in natural language, yet perform poorly when asked to directly regress GarmentCode parameters from images. To bridge this gap, we propose NGL (Natural Garment Language), a novel intermediate language that restructures GarmentCode into a representation more understandable to language models. Leveraging this language, we introduce NGL-Prompter, a training-free pipeline that queries large VLMs to extract structured garment parameters, which are then deterministically mapped to valid GarmentCode. We evaluate our method on the Dress4D, CloSe and a newly collected dataset of approximately 5,000 in-the-wild fashion images. Our approach achieves state-of-the-art performance on standard geometry metrics and is strongly preferred in both human and GPT-based perceptual evaluations compared to existing baselines. Furthermore, NGL-prompter can recover multi-layer outfits whereas competing methods focus mostly on single-layer garments, highlighting its strong generalization to real-world images even with occluded parts. These results demonstrate that accurate sewing pattern reconstruction is possible without costly model training. Our code and data will be released for research use.

</details>


### [14] [Monocular Endoscopic Tissue 3D Reconstruction with Multi-Level Geometry Regularization](https://arxiv.org/abs/2602.20718)
*Yangsen Chen,Hao Wang*

Main category: cs.CV

TL;DR: 该论文提出了一种基于3D高斯泼溅的变形内窥镜组织重建方法，通过表面感知重建和物理约束实现平滑变形表面和实时渲染。


<details>
  <summary>Details</summary>
Motivation: 机器人辅助手术需要重建变形内窥镜组织，但现有方法存在局限性：3D高斯泼溅方法难以实现一致的表面重建，而NeRF方法缺乏实时渲染能力。因此需要同时实现平滑变形表面和实时渲染的新方法。

Method: 1. 提出表面感知重建：首先使用符号距离场方法构建网格，然后用该网格约束高斯泼溅重建过程。2. 引入物理约束：结合局部刚性和全局非刚性约束来指导高斯变形，适应软内窥镜组织的高度变形特性。

Result: 该方法实现了快速渲染过程和平滑表面外观。与替代方法的定量和定性分析表明，该方法在纹理和几何方面都实现了坚实的重建质量。

Conclusion: 基于3D高斯泼溅的提出的方法成功解决了变形内窥镜组织重建中的关键挑战，实现了平滑变形表面和实时渲染的平衡，为机器人辅助手术提供了有效的解决方案。

Abstract: Reconstructing deformable endoscopic tissues is crucial for achieving robot-assisted surgery. However, 3D Gaussian Splatting-based approaches encounter challenges in achieving consistent tissue surface reconstruction, while existing NeRF-based methods lack real-time rendering capabilities. In pursuit of both smooth deformable surfaces and real-time rendering, we introduce a novel approach based on 3D Gaussian Splatting. Specifically, we introduce surface-aware reconstruction, initially employing a Sign Distance Field-based method to construct a mesh, subsequently utilizing this mesh to constrain the Gaussian Splatting reconstruction process. Furthermore, to ensure the generation of physically plausible deformations, we incorporate local rigidity and global non-rigidity restrictions to guide Gaussian deformation, tailored for the highly deformable nature of soft endoscopic tissue. Based on 3D Gaussian Splatting, our proposed method delivers a fast rendering process and smooth surface appearances. Quantitative and qualitative analysis against alternative methodologies shows that our approach achieves solid reconstruction quality in both textures and geometries.

</details>


### [15] [Bridging Physically Based Rendering and Diffusion Models with Stochastic Differential Equation](https://arxiv.org/abs/2602.20725)
*Junwei Shu,Wenjie Liu,Changgu Chen,Hantang Liu,Yang Li,Changbo Wang*

Main category: cs.CV

TL;DR: 该论文提出了一种统一随机公式，将蒙特卡洛渲染与基于扩散的生成建模相结合，实现对扩散生成结果的物理基础控制。


<details>
  <summary>Details</summary>
Motivation: 基于扩散的图像生成器在从文本或图像条件生成逼真内容方面表现出色，但缺乏对低层次、物理基础的着色和材质属性的显式控制。而基于物理的渲染（PBR）提供细粒度的物理控制，但缺乏提示驱动的灵活性。这两种范式都遵循从噪声观察到干净图像的共同演化路径。

Method: 提出统一的随机公式，首先在中心极限定理下为蒙特卡洛积分建立一般随机微分方程（SDE）公式，通过基于物理的路径追踪进行实例化，将其转换为物理基础的SDE表示。从噪声方差角度分析路径追踪的物理特性如何扩展到现有扩散模型。

Result: 在多个任务上的广泛实验表明，该方法能够对扩散生成结果施加物理基础的控制，涵盖渲染和材质编辑等任务。

Conclusion: 该方法成功地将蒙特卡洛渲染与扩散生成建模统一起来，实现了对扩散生成内容的物理基础控制，为结合两种范式的优势提供了新途径。

Abstract: Diffusion-based image generators excel at producing realistic content from text or image conditions, but they offer only limited explicit control over low-level, physically grounded shading and material properties. In contrast, physically based rendering (PBR) offers fine-grained physical control but lacks prompt-driven flexibility. Although these two paradigms originate from distinct communities, both share a common evolution -- from noisy observations to clean images. In this paper, we propose a unified stochastic formulation that bridges Monte Carlo rendering and diffusion-based generative modeling. First, a general stochastic differential equation (SDE) formulation for Monte Carlo integration under the Central Limit Theorem is modeled. Through instantiation via physically based path tracing, we convert it into a physically grounded SDE representation. Moreover, we provide a systematic analysis of how the physical characteristics of path tracing can be extended to existing diffusion models from the perspective of noise variance. Extensive experiments across multiple tasks show that our method can exert physically grounded control over diffusion-generated results, covering tasks such as rendering and material editing.

</details>


### [16] [Real-time Motion Segmentation with Event-based Normal Flow](https://arxiv.org/abs/2602.20790)
*Sheng Zhong,Zhongyang Ren,Xiya Zhu,Dehao Yuan,Cornelia Fermuller,Yi Zhou*

Main category: cs.CV

TL;DR: 提出基于法向流的事件相机运动分割框架，通过法向流压缩事件信息，使用图割能量最小化实现实时运动分割，相比现有方法实现近800倍加速


<details>
  <summary>Details</summary>
Motivation: 事件相机具有微秒级分辨率，但在挑战性场景中处理原始事件数据效率低下，限制了实时应用。法向流作为中间表示可以压缩局部区域的事件集群运动信息，提供更有效的解决方案。

Method: 提出基于法向流的运动分割框架：1) 从事件邻域直接学习密集法向流作为输入；2) 将运动分割建模为能量最小化问题，通过图割求解；3) 通过法向流聚类和运动模型拟合进行迭代优化；4) 使用法向流初始化运动模型，减少候选模型数量。

Result: 相比开源最先进方法实现近800倍加速，显著降低计算复杂度，确保实时性能。在多个公共数据集上的广泛评估充分证明了框架的准确性和效率。

Conclusion: 基于法向流的事件相机运动分割框架通过压缩运动信息、减少候选模型数量，实现了高效准确的实时运动分割，为动态场景理解提供了实用解决方案。

Abstract: Event-based cameras are bio-inspired sensors with pixels that independently and asynchronously respond to brightness changes at microsecond resolution, offering the potential to handle visual tasks in challenging scenarios. However, due to the sparse information content in individual events, directly processing the raw event data to solve vision tasks is highly inefficient, which severely limits the applicability of state-of-the-art methods in real-time tasks, such as motion segmentation, a fundamental task for dynamic scene understanding. Incorporating normal flow as an intermediate representation to compress motion information from event clusters within a localized region provides a more effective solution. In this work, we propose a normal flow-based motion segmentation framework for event-based vision. Leveraging the dense normal flow directly learned from event neighborhoods as input, we formulate the motion segmentation task as an energy minimization problem solved via graph cuts, and optimize it iteratively with normal flow clustering and motion model fitting. By using a normal flow-based motion model initialization and fitting method, the proposed system is able to efficiently estimate the motion models of independently moving objects with only a limited number of candidate models, which significantly reduces the computational complexity and ensures real-time performance, achieving nearly a 800x speedup in comparison to the open-source state-of-the-art method. Extensive evaluations on multiple public datasets fully demonstrate the accuracy and efficiency of our framework.

</details>


### [17] [SIMSPINE: A Biomechanics-Aware Simulation Framework for 3D Spine Motion Annotation and Benchmarking](https://arxiv.org/abs/2602.20792)
*Muhammad Saif Ullah Khan,Didier Stricker*

Main category: cs.CV

TL;DR: 提出SIMSPINE数据集和生物力学感知关键点模拟框架，为自然全身运动提供首个开放的大规模3D脊柱关键点标注，提升脊柱运动估计性能


<details>
  <summary>Details</summary>
Motivation: 脊柱运动建模对人类生物力学理解至关重要，但由于脊柱复杂的多关节运动学和缺乏大规模3D标注，在计算机视觉领域尚未得到充分探索

Method: 开发生物力学感知关键点模拟框架，通过肌肉骨骼建模从现有人体姿态数据集中生成解剖学一致的3D脊柱关键点，创建SIMSPINE数据集（214万帧），并提供预训练基线模型

Result: 2D脊柱基线在受控环境中将AUC从0.63提升至0.80，在野外脊柱跟踪中将AP从0.91提升至0.93，建立了统一的生物力学有效脊柱运动估计基准

Conclusion: 模拟框架和SIMSPINE数据集通过实现可重复、解剖学基础的3D脊柱估计，推动了基于视觉的生物力学、运动分析和数字人体建模研究

Abstract: Modeling spinal motion is fundamental to understanding human biomechanics, yet remains underexplored in computer vision due to the spine's complex multi-joint kinematics and the lack of large-scale 3D annotations. We present a biomechanics-aware keypoint simulation framework that augments existing human pose datasets with anatomically consistent 3D spinal keypoints derived from musculoskeletal modeling. Using this framework, we create the first open dataset, named SIMSPINE, which provides sparse vertebra-level 3D spinal annotations for natural full-body motions in indoor multi-camera capture without external restraints. With 2.14 million frames, this enables data-driven learning of vertebral kinematics from subtle posture variations and bridges the gap between musculoskeletal simulation and computer vision. In addition, we release pretrained baselines covering fine-tuned 2D detectors, monocular 3D pose lifting models, and multi-view reconstruction pipelines, establishing a unified benchmark for biomechanically valid spine motion estimation. Specifically, our 2D spine baselines improve the state-of-the-art from 0.63 to 0.80 AUC in controlled environments, and from 0.91 to 0.93 AP for in-the-wild spine tracking. Together, the simulation framework and SIMSPINE dataset advance research in vision-based biomechanics, motion analysis, and digital human modeling by enabling reproducible, anatomically grounded 3D spine estimation under natural conditions.

</details>


### [18] [VGGDrive: Empowering Vision-Language Models with Cross-View Geometric Grounding for Autonomous Driving](https://arxiv.org/abs/2602.20794)
*Jie Wang,Guang Li,Zhijian Huang,Chenxu Dang,Hangjun Ye,Yahong Han,Long Chen*

Main category: cs.CV

TL;DR: VGGDrive：通过跨视图几何感知增强视觉语言模型在自动驾驶任务中的性能，将成熟的3D基础模型的几何理解能力注入到VLM中


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型缺乏跨视图3D几何建模能力，导致在自动驾驶任务中表现平庸。虽然有些方法尝试通过构建问答数据进行辅助训练，但仍无法从根本上让VLM全面处理多样化的评估协议

Method: 提出VGGDrive架构，通过可插拔的跨视图3D几何使能器（CVGE）将冻结的3D视觉模型的跨视图几何特征与VLM的2D视觉特征相融合。CVGE采用分层自适应注入机制，有效增强VLM的3D特征能力

Result: 在五个自动驾驶基准测试中显著提升了基础VLM的性能，包括跨视图风险感知、运动预测和轨迹规划等任务

Conclusion: 成熟的3D基础模型可以通过有效集成来赋能自动驾驶任务，这项初步探索展示了该范式在自动驾驶领域的潜力

Abstract: The significance of cross-view 3D geometric modeling capabilities for autonomous driving is self-evident, yet existing Vision-Language Models (VLMs) inherently lack this capability, resulting in their mediocre performance. While some promising approaches attempt to mitigate this by constructing Q&A data for auxiliary training, they still fail to fundamentally equip VLMs with the ability to comprehensively handle diverse evaluation protocols. We thus chart a new course, advocating for the infusion of VLMs with the cross-view geometric grounding of mature 3D foundation models, closing this critical capability gap in autonomous driving. In this spirit, we propose a novel architecture, VGGDrive, which empowers Vision-language models with cross-view Geometric Grounding for autonomous Driving. Concretely, to bridge the cross-view 3D geometric features from the frozen visual 3D model with the VLM's 2D visual features, we introduce a plug-and-play Cross-View 3D Geometric Enabler (CVGE). The CVGE decouples the base VLM architecture and effectively empowers the VLM with 3D features through a hierarchical adaptive injection mechanism. Extensive experiments show that VGGDrive enhances base VLM performance across five autonomous driving benchmarks, including tasks like cross-view risk perception, motion prediction, and trajectory planning. It's our belief that mature 3D foundation models can empower autonomous driving tasks through effective integration, and we hope our initial exploration demonstrates the potential of this paradigm to the autonomous driving community.

</details>


### [19] [RU4D-SLAM: Reweighting Uncertainty in Gaussian Splatting SLAM for 4D Scene Reconstruction](https://arxiv.org/abs/2602.20807)
*Yangfan Zhao,Hanwei Zhang,Ke Huang,Qiufeng Wang,Zhenzhou Shao,Dengyu Wu*

Main category: cs.CV

TL;DR: RU4D-SLAM是一个结合4D高斯泼溅与SLAM的框架，通过引入时间因素、不确定性感知和语义引导重加权机制，在动态环境中实现鲁棒的4D场景重建和轨迹跟踪。


<details>
  <summary>Details</summary>
Motivation: 现有3D高斯泼溅与SLAM结合的方法在动态环境中表现不佳，移动物体使3D重建复杂化并影响跟踪可靠性。4D重建（特别是4D高斯泼溅）为解决这些挑战提供了有前景的方向，但其在4D感知SLAM中的应用尚未充分探索。

Method: 提出RU4D-SLAM框架：1）将时间因素引入空间3D表示；2）集成运动模糊渲染增强动态场景表示；3）扩展每像素不确定性建模以处理模糊图像；4）提出语义引导的重加权机制用于动态场景的不确定性估计；5）引入可学习的透明度权重支持自适应4D映射。

Result: 在标准基准测试上的大量实验表明，该方法在轨迹精度和4D场景重建方面显著优于最先进方法，特别是在包含移动物体和低质量输入的动态环境中表现突出。

Conclusion: RU4D-SLAM通过结合4D高斯泼溅与不确定性感知机制，为动态环境中的鲁棒SLAM和4D场景重建提供了有效解决方案，在轨迹跟踪和重建质量方面均取得显著提升。

Abstract: Combining 3D Gaussian splatting with Simultaneous Localization and Mapping (SLAM) has gained popularity as it enables continuous 3D environment reconstruction during motion. However, existing methods struggle in dynamic environments, particularly moving objects complicate 3D reconstruction and, in turn, hinder reliable tracking. The emergence of 4D reconstruction, especially 4D Gaussian splatting, offers a promising direction for addressing these challenges, yet its potential for 4D-aware SLAM remains largely underexplored. Along this direction, we propose a robust and efficient framework, namely Reweighting Uncertainty in Gaussian Splatting SLAM (RU4D-SLAM) for 4D scene reconstruction, that introduces temporal factors into spatial 3D representation while incorporating uncertainty-aware perception of scene changes, blurred image synthesis, and dynamic scene reconstruction. We enhance dynamic scene representation by integrating motion blur rendering, and improve uncertainty-aware tracking by extending per-pixel uncertainty modeling, which is originally designed for static scenarios, to handle blurred images. Furthermore, we propose a semantic-guided reweighting mechanism for per-pixel uncertainty estimation in dynamic scenes, and introduce a learnable opacity weight to support adaptive 4D mapping. Extensive experiments on standard benchmarks demonstrate that our method substantially outperforms state-of-the-art approaches in both trajectory accuracy and 4D scene reconstruction, particularly in dynamic environments with moving objects and low-quality inputs. Code available: https://ru4d-slam.github.io

</details>


### [20] [GatedCLIP: Gated Multimodal Fusion for Hateful Memes Detection](https://arxiv.org/abs/2602.20818)
*Yingying Guo,Ke Zhang,Zirong Zeng*

Main category: cs.CV

TL;DR: GatedCLIP模型通过改进CLIP架构，结合学习投影头、动态门控融合机制和对比学习目标，显著提升了多模态仇恨表情包检测性能，在Hateful Memes数据集上AUROC达到0.66，远超CLIP基线的0.49。


<details>
  <summary>Details</summary>
Motivation: 多模态表情包中的仇恨内容检测面临独特挑战，因为有害信息往往来自良性图像和文本的复杂交互。现有模型如CLIP在处理这种微妙的多模态仇恨表达时效果有限。

Method: 提出GatedCLIP模型，在CLIP基础上引入三个关键改进：1）学习投影头将CLIP嵌入映射到任务优化的语义空间；2）动态门控融合机制自适应加权视觉和文本特征；3）对比学习目标保持跨模态语义对齐。

Result: 在Hateful Memes数据集上，GatedCLIP达到AUROC 0.66，显著优于CLIP基线的0.49。模型仅需35万可训练参数，保持了计算效率。

Conclusion: GatedCLIP通过专门的架构改进有效增强了CLIP的多模态能力，为仇恨表情包检测提供了高效解决方案，证明了任务特定优化在多模态仇恨内容检测中的重要性。

Abstract: Detecting hateful content in multimodal memes presents unique challenges, as harmful messages often emerge from the complex interplay between benign images and text. We propose GatedCLIP, a Vision-Language model that enhances CLIP's multimodal capabilities with specialized architectural improvements for hateful memes detection. Our approach introduces learned projection heads that map CLIP embeddings to a task-optimized semantic space, a dynamic gated fusion mechanism that adaptively weights visual and textual features, and a contrastive learning objective that maintains cross-modal semantic alignment. Experiments on the Hateful Memes dataset demonstrate that GatedCLIP achieves an AUROC of 0.66, substantially outperforming the CLIP baseline (AUROC 0.49) while maintaining computational efficiency with only 350K trainable parameters.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [21] [Multimodal Multi-Agent Empowered Legal Judgment Prediction](https://arxiv.org/abs/2601.12815)
*Zhaolu Kang,Junhao Gong,Qingxi Chen,Hao Zhang,Jiaxin Liu,Rong Fu,Zhiyuan Feng,Yuan Wang,Simon Fong,Kaiyue Zhou*

Main category: cs.CL

TL;DR: 提出JurisMMA框架用于法律判决预测，通过任务分解和流程标准化处理多指控、多样化证据问题，并构建包含10万+中文司法记录的JurisMM数据集进行验证。


<details>
  <summary>Details</summary>
Motivation: 传统法律判决预测方法在处理多指控、多样化证据时面临挑战且缺乏适应性，需要更有效的框架来推进法律系统发展。

Method: 提出JurisMMA框架，通过分解审判任务、标准化流程并组织为不同阶段，同时构建包含文本和多模态视频-文本数据的JurisMM大规模数据集。

Result: 在JurisMM和LawBench基准测试上的实验验证了框架的有效性，表明该框架不仅适用于法律判决预测，还可扩展到更广泛的法律应用。

Conclusion: JurisMMA框架为法律判决预测提供了有效解决方案，同时构建的大规模数据集为未来法律方法和数据集发展提供了新视角。

Abstract: Legal Judgment Prediction (LJP) aims to predict the outcomes of legal cases based on factual descriptions, serving as a fundamental task to advance the development of legal systems. Traditional methods often rely on statistical analyses or role-based simulations but face challenges with multiple allegations, diverse evidence, and lack adaptability. In this paper, we introduce JurisMMA, a novel framework for LJP that effectively decomposes trial tasks, standardizes processes, and organizes them into distinct stages. Furthermore, we build JurisMM, a large dataset with over 100,000 recent Chinese judicial records, including both text and multimodal video-text data, enabling comprehensive evaluation. Experiments on JurisMM and the benchmark LawBench validate our framework's effectiveness. These results indicate that our framework is effective not only for LJP but also for a broader range of legal applications, offering new perspectives for the development of future legal methods and datasets.

</details>


### [22] [Benchmarking Distilled Language Models: Performance and Efficiency in Resource-Constrained Settings](https://arxiv.org/abs/2602.20164)
*Sachin Gopal Wani,Eric Page,Ajay Dholakia,David Ellison*

Main category: cs.CL

TL;DR: 知识蒸馏能创建比传统训练更高效的小语言模型，8B蒸馏模型的计算效率比普通训练高2000倍以上，推理能力可与10倍大小的标准模型媲美甚至超越


<details>
  <summary>Details</summary>
Motivation: 为资源受限环境开发强大而高效的小语言模型，通过知识蒸馏技术实现模型压缩和性能提升，验证蒸馏作为构建先进可访问AI的主要策略

Method: 对蒸馏模型与普通模型及专有模型进行性能与计算成本基准测试，提供效率的定量分析，比较不同规模模型的计算效率和推理能力

Result: 蒸馏创造了更优的性能-计算曲线，8B蒸馏模型的计算效率比普通训练高2000倍以上，推理能力与10倍大小的标准模型相当甚至超越

Conclusion: 知识蒸馏不仅是压缩技术，更是构建先进、可访问AI的主要策略，能够显著提升小语言模型的效率和性能

Abstract: Knowledge distillation offers a transformative pathway to developing powerful, yet efficient, small language models (SLMs) suitable for resource-constrained environments. In this paper, we benchmark the performance and computational cost of distilled models against their vanilla and proprietary counterparts, providing a quantitative analysis of their efficiency. Our results demonstrate that distillation creates a superior performance-tocompute curve. We find that creating a distilled 8B model is over 2,000 times more compute-efficient than training its vanilla counterpart, while achieving reasoning capabilities on par with, or even exceeding, standard models ten times its size. These findings validate distillation not just as a compression technique, but as a primary strategy for building state-of-the-art, accessible AI

</details>


### [23] [What Makes a Good Query? Measuring the Impact of Human-Confusing Linguistic Features on LLM Performance](https://arxiv.org/abs/2602.20300)
*William Watson,Nicole Cho,Sumitra Ganesh,Manuela Veloso*

Main category: cs.CL

TL;DR: 研究发现查询语句的特定特征（如从句嵌套深度、指代不明确等）与LLM幻觉风险相关，而意图明确和可回答性则降低幻觉概率


<details>
  <summary>Details</summary>
Motivation: 传统上将LLM幻觉视为模型或解码策略的缺陷，但本文从语言学角度探讨查询语句的形式特征如何影响模型响应和幻觉概率

Method: 构建22维查询特征向量，涵盖从句复杂度、词汇稀有度、指代、否定、可回答性和意图基础等语言学特征，使用369,837个真实世界查询进行大规模分析

Result: 发现一致的"风险图谱"：某些特征（如深层从句嵌套和指代不明确）与更高的幻觉倾向相关，而清晰的意图基础和可回答性则与较低的幻觉率相关；其他特征（如领域特异性）则呈现混合、数据集和模型依赖效应

Conclusion: 建立了与幻觉风险相关的可观察查询特征表示，为引导性查询重写和未来干预研究铺平了道路

Abstract: Large Language Model (LLM) hallucinations are usually treated as defects of the model or its decoding strategy. Drawing on classical linguistics, we argue that a query's form can also shape a listener's (and model's) response. We operationalize this insight by constructing a 22-dimension query feature vector covering clause complexity, lexical rarity, and anaphora, negation, answerability, and intention grounding, all known to affect human comprehension. Using 369,837 real-world queries, we ask: Are there certain types of queries that make hallucination more likely? A large-scale analysis reveals a consistent "risk landscape": certain features such as deep clause nesting and underspecification align with higher hallucination propensity. In contrast, clear intention grounding and answerability align with lower hallucination rates. Others, including domain specificity, show mixed, dataset- and model-dependent effects. Thus, these findings establish an empirically observable query-feature representation correlated with hallucination risk, paving the way for guided query rewriting and future intervention studies.

</details>


### [24] [No One Size Fits All: QueryBandits for Hallucination Mitigation](https://arxiv.org/abs/2602.20332)
*Nicole Cho,William Watson,Alec Koppel,Sumitra Ganesh,Manuela Veloso*

Main category: cs.CL

TL;DR: QueryBandits是一个模型无关的上下文赌博机框架，通过在线学习选择最优查询重写策略来减少大语言模型中的幻觉问题，特别针对闭源模型。


<details>
  <summary>Details</summary>
Motivation: 大语言模型虽然具备先进的推理能力，但幻觉问题日益严重。现有缓解工作主要关注开源模型的后检测和参数编辑，而闭源模型在实际机构部署中占绝大多数，却缺乏相关研究。

Method: 提出QueryBandits框架，这是一个模型无关的上下文赌博机系统。它通过在线学习自适应地选择最优查询重写策略，利用经验验证和校准的奖励函数，仅通过前向传播机制改变模型行为。

Result: 在16个QA场景中，最佳QueryBandits（Thompson Sampling）相比无重写基线获得87.5%的胜率，分别比零样本静态策略（如Paraphrase或Expand）高出42.6%和60.3%。所有上下文赌博机在所有数据集上都优于普通赌博机。

Conclusion: 没有单一的重写策略适用于所有查询，不灵活的查询重写策略可能加剧幻觉。QueryBandits通过在线学习语义特征策略，可以仅通过前向传播机制改变模型行为，适用于闭源模型，无需重新训练或基于梯度的适应。

Abstract: Advanced reasoning capabilities in Large Language Models (LLMs) have led to more frequent hallucinations; yet most mitigation work focuses on open-source models for post-hoc detection and parameter editing. The dearth of studies focusing on hallucinations in closed-source models is especially concerning, as they constitute the vast majority of models in institutional deployments. We introduce QueryBandits, a model-agnostic contextual bandit framework that adaptively learns online to select the optimal query-rewrite strategy by leveraging an empirically validated and calibrated reward function. Across 16 QA scenarios, our top QueryBandit (Thompson Sampling) achieves an 87.5% win rate over a No-Rewrite baseline and outperforms zero-shot static policies (e.g., Paraphrase or Expand) by 42.6% and 60.3%, respectively. Moreover, all contextual bandits outperform vanilla bandits across all datasets, with higher feature variance coinciding with greater variance in arm selection. This substantiates our finding that there is no single rewrite policy optimal for all queries. We also discover that certain static policies incur higher cumulative regret than No-Rewrite, indicating that an inflexible query-rewriting policy can worsen hallucinations. Thus, learning an online policy over semantic features with QueryBandits can shift model behavior purely through forward-pass mechanisms, enabling its use with closed-source models and bypassing the need for retraining or gradient-based adaptation.

</details>


### [25] [Semantic Novelty at Scale: Narrative Shape Taxonomy and Readership Prediction in 28,606 Books](https://arxiv.org/abs/2602.20647)
*W. Frederick Zimmerman*

Main category: cs.CL

TL;DR: 该研究提出语义新颖性作为叙事结构的度量指标，应用于28,606本PG19书籍，发现8种叙事原型，其中新颖性轨迹的方差最能预测读者参与度，并揭示了体裁对叙事形状的强烈约束和历史趋势。


<details>
  <summary>Details</summary>
Motivation: 研究动机是开发一种信息论方法来量化大规模语料库中的叙事结构，超越传统的情感或主题分析，探索信息密度动态如何构成叙事的基本维度及其对读者参与的影响。

Method: 使用SBERT嵌入计算段落级语义新颖性（与前面段落质心的余弦距离），将28,606本书的新颖性曲线通过分段聚合近似降维为16段，然后使用Ward-linkage聚类识别叙事原型，并分析各种指标与读者参与度的关系。

Result: 发现8种典型叙事形状原型（从陡峭下降到陡峭上升），新颖性轨迹的方差（体积）是长度无关的最强读者参与度预测因子（偏rho=0.32），体裁强烈约束叙事形状（卡方=2121.6），历史分析显示1840-1910年间书籍变得越来越可预测。

Conclusion: 信息密度动态是叙事结构的基本维度，具有可测量的读者参与后果，研究揭示了体裁约束、历史趋势和个体独特性，同时警告语料库研究中长度混淆对相关性分析的严重影响。

Abstract: I introduce semantic novelty--cosine distance between each paragraph's sentence embedding and the running centroid of all preceding paragraphs--as an information-theoretic measure of narrative structure at corpus scale. Applying it to 28,606 books in PG19 (pre-1920 English literature), I compute paragraph-level novelty curves using 768-dimensional SBERT embeddings, then reduce each to a 16-segment Piecewise Aggregate Approximation (PAA). Ward-linkage clustering on PAA vectors reveals eight canonical narrative shape archetypes, from Steep Descent (rapid convergence) to Steep Ascent (escalating unpredictability). Volume--variance of the novelty trajectory--is the strongest length-independent predictor of readership (partial rho = 0.32), followed by speed (rho = 0.19) and Terminal/Initial ratio (rho = 0.19). Circuitousness shows strong raw correlation (rho = 0.41) but is 93 percent correlated with length; after control, partial rho drops to 0.11--demonstrating that naive correlations in corpus studies can be dominated by length confounds. Genre strongly constrains narrative shape (chi squared = 2121.6, p < 10 to the power negative 242), with fiction maintaining plateau profiles while nonfiction front-loads information. Historical analysis shows books became progressively more predictable between 1840 and 1910 (T/I ratio trend r = negative 0.74, p = 0.037). SAX analysis reveals 85 percent signature uniqueness, suggesting each book traces a nearly unique path through semantic space. These findings demonstrate that information-density dynamics, distinct from sentiment or topic, constitute a fundamental dimension of narrative structure with measurable consequences for reader engagement. Dataset: https://huggingface.co/datasets/wfzimmerman/pg19-semantic-novelty

</details>


### [26] [CARE: An Explainable Computational Framework for Assessing Client-Perceived Therapeutic Alliance Using Large Language Models](https://arxiv.org/abs/2602.20648)
*Anqi Li,Chenxiao Wang,Yu Lu,Renjun Xu,Lizhi Ma,Zhenzhong Lan*

Main category: cs.CL

TL;DR: CARE是一个基于LLM的框架，用于从心理咨询对话记录中自动预测多维度的治疗联盟评分并生成可解释的理由，在预测准确性和可解释性方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统心理咨询后问卷负担重且延迟，现有计算方法评分粗糙、缺乏可解释理由且无法建模完整会话上下文，需要更准确、可解释的自动评估工具。

Method: 基于CounselingWAI数据集，使用9,516个专家标注的理由进行增强监督微调，采用LLaMA-3.1-8B-Instruct作为基础模型，构建CARE框架。

Result: CARE在预测客户感知的治疗联盟方面优于主流LLM，与客户评分的皮尔逊相关系数提高70%以上，减少咨询师评估与客户感知之间的差距，生成高质量、上下文相关的理由。

Conclusion: CARE作为AI辅助工具在心理健康护理中具有潜力，能够识别联盟建设挑战、分析互动模式如何影响联盟发展，并提供可操作的见解。

Abstract: Client perceptions of the therapeutic alliance are critical for counseling effectiveness. Accurately capturing these perceptions remains challenging, as traditional post-session questionnaires are burdensome and often delayed, while existing computational approaches produce coarse scores, lack interpretable rationales, and fail to model holistic session context. We present CARE, an LLM-based framework to automatically predict multi-dimensional alliance scores and generate interpretable rationales from counseling transcripts. Built on the CounselingWAI dataset and enriched with 9,516 expert-curated rationales, CARE is fine-tuned using rationale-augmented supervision with the LLaMA-3.1-8B-Instruct backbone. Experiments show that CARE outperforms leading LLMs and substantially reduces the gap between counselor evaluations and client-perceived alliance, achieving over 70% higher Pearson correlation with client ratings. Rationale-augmented supervision further improves predictive accuracy. CARE also produces high-quality, contextually grounded rationales, validated by both automatic and human evaluations. Applied to real-world Chinese online counseling sessions, CARE uncovers common alliance-building challenges, illustrates how interaction patterns shape alliance development, and provides actionable insights, demonstrating its potential as an AI-assisted tool for supporting mental health care.

</details>


### [27] [ID-LoRA: Efficient Low-Rank Adaptation Inspired by Matrix Interpolative Decomposition](https://arxiv.org/abs/2602.20727)
*Xindian Ma,Rundong Kong,Peng Zhang,Ruoxiang Huang,Yongyu Jiang*

Main category: cs.CL

TL;DR: ID-LoRA是一种新型参数高效微调框架，通过提取和重用预训练权重矩阵中的聚类参数组，形成多个共享单一可训练低秩矩阵的低秩组件，在减少可训练参数的同时保持模型能力。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型规模扩大，现有的LoRA变体仍然引入大量可训练参数开销，而过度降低秩又会在复杂多任务场景中显著降低性能，需要打破这种权衡。

Method: ID-LoRA从预训练权重矩阵中提取聚类参数组，重用这些组形成多个低秩组件，所有组件共享单一初始化的可训练低秩矩阵，从而减少可训练参数数量。

Result: 在数学推理、代码生成、MMLU、常识问答和安全对齐五个基准测试中，ID-LoRA优于全量微调和现有PEFT基线，同时比标准LoRA减少高达46%的可训练参数。在多任务场景中，在代码和MMLU任务上超越LoRA及其变体，仅需传统LoRA 54%的参数。

Conclusion: ID-LoRA成功打破了参数效率和性能之间的权衡，在显著减少可训练参数的同时保持甚至提升了模型性能，特别是在复杂多任务场景中表现出色。

Abstract: LoRA has become a universal Parameter-Efficient Fine-Tuning (PEFT) technique that equips Large Language Models (LLMs) to adapt quickly to new tasks. However, when these models are scaled up, even the latest LoRA variants still introduce considerable overhead in trainable parameters. Conversely, aggressively lowering the rank to curb this overhead markedly degrades performance in complex multi-task settings. We propose ID-LoRA, a novel PEFT framework that breaks the trade-off. Its core innovation lies in extracting and reusing clustered parameter groups from the pretrained weight matrix. These groups are then used to form multiple low-rank components, all of which share only a single initialized trainable low-rank matrix. This approach cuts the number of trainable parameters while keeping the model's capacity intact. We evaluate ID-LoRA on five diverse benchmarks: Mathematical Reasoning, Code Generation, MMLU, CommonsenseQA, and Safety Alignment. ID-LoRA outperforms both full fine-tuning and existing PEFT baselines (e.g., LoRA, DoRA, HydraLoRA) while using up to 46% fewer trainable parameters than the standard LoRA. In multi-task scenarios, it surpasses LoRA and its recent variants (e.g., DoRA and HydraLoRA) on both Code and MMLU tasks, yet requires only 54% of the trainable parameters demanded by the conventional LoRA.

</details>


### [28] [Adaptive Text Anonymization: Learning Privacy-Utility Trade-offs via Prompt Optimization](https://arxiv.org/abs/2602.20743)
*Gabriel Loiseau,Damien Sileo,Damien Riquet,Maxime Meyer,Marc Tommasi*

Main category: cs.CL

TL;DR: 本文提出自适应文本匿名化任务，通过任务特定提示优化框架自动构建匿名化指令，适应不同隐私目标、领域和下游使用模式，在多个数据集上优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 文本匿名化是高度上下文敏感的问题，现有方法依赖静态手动设计策略，缺乏适应不同需求的灵活性，且难以跨领域泛化。

Method: 提出自适应文本匿名化任务框架，通过任务特定提示优化自动为语言模型构建匿名化指令，适应不同隐私目标、领域和下游使用模式。

Result: 在包含五个不同领域、隐私约束和效用目标的数据集基准测试中，该方法在所有设置下都优于现有基线，在隐私-效用权衡上表现更好，计算效率高，在开源模型上效果与大型闭源模型相当。

Conclusion: 自适应文本匿名化框架能够自动适应不同隐私-效用需求，发现新的匿名化策略，在隐私保护和效用保持之间实现更好的权衡。

Abstract: Anonymizing textual documents is a highly context-sensitive problem: the appropriate balance between privacy protection and utility preservation varies with the data domain, privacy objectives, and downstream application. However, existing anonymization methods rely on static, manually designed strategies that lack the flexibility to adjust to diverse requirements and often fail to generalize across domains. We introduce adaptive text anonymization, a new task formulation in which anonymization strategies are automatically adapted to specific privacy-utility requirements. We propose a framework for task-specific prompt optimization that automatically constructs anonymization instructions for language models, enabling adaptation to different privacy goals, domains, and downstream usage patterns. To evaluate our approach, we present a benchmark spanning five datasets with diverse domains, privacy constraints, and utility objectives. Across all evaluated settings, our framework consistently achieves a better privacy-utility trade-off than existing baselines, while remaining computationally efficient and effective on open-source language models, with performance comparable to larger closed-source models. Additionally, we show that our method can discover novel anonymization strategies that explore different points along the privacy-utility trade-off frontier.

</details>


### [29] [Overton Pluralistic Reinforcement Learning for Large Language Models](https://arxiv.org/abs/2602.20759)
*Yu Fu,Seongho Son,Ilija Bogunovic*

Main category: cs.CL

TL;DR: OP-GRPO是一个强化学习框架，通过双奖励系统让单个大语言模型能够生成多元视角的回应，无需显式提示或模块化编排，实现了"小模型，大视角覆盖"的效果。


<details>
  <summary>Details</summary>
Motivation: 现有对齐范式在捕捉人类价值观的多元性方面存在局限，需要一种能够从单一查询生成多种视角回应的新方法。

Method: 提出OP-GRPO框架，包含两个主要步骤：1) 训练相似性估计器用于更准确评估生成回应的覆盖范围；2) 将相似性估计器集成到双奖励系统中，确保广泛覆盖真实人类视角的同时保持每个视角的独特性。

Result: Qwen2.5-3B-Instruct模型在自然语言推理基准测试中相对20B GPT-OSS基线提升了37.4%的准确率，相对模块化架构基线提升了19.1%的相对改进。GPT-4.1作为大语言模型评判者的额外评估进一步证实了方法的稳健性。

Conclusion: OP-GRPO框架成功实现了隐式的Overton多元主义，使单个大语言模型能够生成多元视角的回应，展示了"小模型，大视角覆盖"的效果，为捕捉人类价值观的多元性提供了有效解决方案。

Abstract: Existing alignment paradigms remain limited in capturing the pluralistic nature of human values. Overton Pluralism addresses this gap by generating responses with diverse perspectives from a single query. This paper introduces OP-GRPO (Overton Pluralistic Group Relative Policy Optimization), a reinforcement learning framework for implicit Overton Pluralism that enables a single large language model to produce pluralistic responses without explicit prompting or modular orchestration. Our workflow consists of two main steps. First, similarity estimator training fine-tunes a Sentence Transformer for Overton Pluralism tasks to provide more accurate coverage evaluation of generated responses. Second, OP-GRPO training incorporates this similarity estimator into a dual-reward system designed to ensure both broad coverage of genuine human perspectives and the uniqueness of each perspective, thereby promoting diversity. Empirical results demonstrate a "small models, big perspective coverage" effect. The trained Qwen2.5-3B-Instruct model surpasses a 20B GPT-OSS baseline with a 37.4 percent relative accuracy gain on a Natural Language Inference benchmark, and also outperforms a modular architecture baseline with a 19.1 percent relative improvement. Additional evaluations using GPT-4.1 as a large language model judge further confirm the robustness of the approach.

</details>


### [30] [Don't Ignore the Tail: Decoupling top-K Probabilities for Efficient Language Model Distillation](https://arxiv.org/abs/2602.20816)
*Sayantan Dasgupta,Trevor Cohn,Timothy Baldwin*

Main category: cs.CL

TL;DR: 提出了一种新的尾部分布感知的KL散度方法，用于语言模型蒸馏，通过解耦教师模型前K个高概率预测与低概率预测的贡献，减少教师模式主导效应，增强分布尾部信息的影响。


<details>
  <summary>Details</summary>
Motivation: 传统KL散度在语言模型蒸馏中主要受教师模型高概率token（模式）主导，而忽略了低概率但可能信息丰富的分布尾部成分，这限制了蒸馏效果。

Method: 提出了一种尾部分布感知的散度方法，将教师模型的前K个高概率预测与低概率预测解耦处理，保持与KL散度相同的计算复杂度，但减少教师模式的影响，增强分布尾部的贡献。

Result: 实验结果表明，改进的蒸馏方法在各种数据集的解码器模型预训练和监督蒸馏中都取得了有竞争力的性能，且蒸馏过程高效，可以在学术预算下处理大型数据集，无需工业级计算资源。

Conclusion: 提出的尾部分布感知散度方法有效解决了传统KL散度中教师模式主导的问题，提高了语言模型蒸馏的效果和效率，为资源有限的研究环境提供了可行的蒸馏方案。

Abstract: The core learning signal used in language model distillation is the standard Kullback-Leibler (KL) divergence between the student and teacher distributions. Traditional KL divergence tends to be dominated by the next tokens with the highest probabilities, i.e., the teacher's modes, thereby diminishing the influence of less probable yet potentially informative components of the output distribution. We propose a new tail-aware divergence that decouples the contribution of the teacher model's top-K predicted probabilities from that of lower-probability predictions, while maintaining the same computational profile as the KL Divergence. Our decoupled approach reduces the impact of the teacher modes and, consequently, increases the contribution of the tail of the distribution. Experimental results demonstrate that our modified distillation method yields competitive performance in both pre-training and supervised distillation of decoder models across various datasets. Furthermore, the distillation process is efficient and can be performed with a modest academic budget for large datasets, eliminating the need for industry-scale computing.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [31] [Multilevel Determinants of Overweight and Obesity Among U.S. Children Aged 10-17: Comparative Evaluation of Statistical and Machine Learning Approaches Using the 2021 National Survey of Children's Health](https://arxiv.org/abs/2602.20303)
*Joyanta Jyoti Mondal*

Main category: cs.AI

TL;DR: 研究比较了统计、机器学习和深度学习模型在预测美国青少年超重/肥胖方面的表现，发现逻辑回归、梯度提升和多层感知器表现最稳定，模型复杂性增加带来的改进有限，不同种族和贫困群体间的性能差异持续存在。


<details>
  <summary>Details</summary>
Motivation: 儿童和青少年超重/肥胖是美国主要的公共卫生问题，受行为、家庭和社区因素共同影响。目前对这些多层次预测因素在人群层面的联合预测结构了解不足，需要比较不同预测模型的性能、校准和亚组公平性。

Method: 分析2021年全国儿童健康调查中18,792名10-17岁儿童数据。使用BMI定义超重/肥胖。预测因素包括饮食、体育活动、睡眠、父母压力、社会经济条件、不良经历和社区特征。比较了逻辑回归、随机森林、梯度提升、XGBoost、LightGBM、多层感知器和TabNet等模型。使用AUC、准确率、精确率、召回率、F1分数和Brier分数评估性能。

Result: 模型区分度在0.66到0.79之间。逻辑回归、梯度提升和多层感知器在区分度和校准方面表现最稳定。提升方法和深度学习在召回率和F1分数上有适度改进。没有模型在所有方面都表现最优。不同种族和贫困群体间的性能差异在所有算法中持续存在。

Conclusion: 增加模型复杂性对逻辑回归的改进有限。预测因素持续涵盖行为、家庭和社区领域。持续的亚组差异表明需要改进数据质量和关注公平性的监测，而不是增加算法复杂性。

Abstract: Background: Childhood and adolescent overweight and obesity remain major public health concerns in the United States and are shaped by behavioral, household, and community factors. Their joint predictive structure at the population level remains incompletely characterized. Objectives: The study aims to identify multilevel predictors of overweight and obesity among U.S. adolescents and compare the predictive performance, calibration, and subgroup equity of statistical, machine-learning, and deep-learning models. Data and Methods: We analyze 18,792 children aged 10-17 years from the 2021 National Survey of Children's Health. Overweight/obesity is defined using BMI categories. Predictors included diet, physical activity, sleep, parental stress, socioeconomic conditions, adverse experiences, and neighborhood characteristics. Models include logistic regression, random forest, gradient boosting, XGBoost, LightGBM, multilayer perceptron, and TabNet. Performance is evaluated using AUC, accuracy, precision, recall, F1 score, and Brier score. Results: Discrimination range from 0.66 to 0.79. Logistic regression, gradient boosting, and MLP showed the most stable balance of discrimination and calibration. Boosting and deep learning modestly improve recall and F1 score. No model was uniformly superior. Performance disparities across race and poverty groups persist across algorithms. Conclusion: Increased model complexity yields limited gains over logistic regression. Predictors consistently span behavioral, household, and neighborhood domains. Persistent subgroup disparities indicate the need for improved data quality and equity-focused surveillance rather than greater algorithmic complexity.

</details>


### [32] [Diffusion Modulation via Environment Mechanism Modeling for Planning](https://arxiv.org/abs/2602.20422)
*Hanping Zhang,Yuhong Guo*

Main category: cs.AI

TL;DR: DMEMM是一种新的基于扩散模型的规划方法，通过建模环境机制（转移动态和奖励函数）来调制扩散模型训练，解决了传统扩散规划方法在离线强化学习中轨迹生成不一致的问题。


<details>
  <summary>Details</summary>
Motivation: 传统基于扩散模型的规划方法在离线强化学习中生成轨迹时，往往忽略了轨迹中转移之间的一致性要求，导致生成的轨迹与真实环境机制存在显著差异。

Method: 提出DMEMM方法，通过建模环境的关键机制（特别是转移动态和奖励函数）来调制扩散模型的训练过程，确保生成的轨迹在真实环境中具有一致性。

Result: 实验结果表明，DMEMM在离线强化学习的规划任务中达到了最先进的性能。

Conclusion: 通过将环境机制建模融入扩散模型训练，DMEMM能够生成更符合真实环境一致性的轨迹，从而提升离线强化学习中规划的效果。

Abstract: Diffusion models have shown promising capabilities in trajectory generation for planning in offline reinforcement learning (RL). However, conventional diffusion-based planning methods often fail to account for the fact that generating trajectories in RL requires unique consistency between transitions to ensure coherence in real environments. This oversight can result in considerable discrepancies between the generated trajectories and the underlying mechanisms of a real environment. To address this problem, we propose a novel diffusion-based planning method, termed as Diffusion Modulation via Environment Mechanism Modeling (DMEMM). DMEMM modulates diffusion model training by incorporating key RL environment mechanisms, particularly transition dynamics and reward functions. Experimental results demonstrate that DMEMM achieves state-of-the-art performance for planning with offline reinforcement learning.

</details>


### [33] [Learning to Rewrite Tool Descriptions for Reliable LLM-Agent Tool Use](https://arxiv.org/abs/2602.20426)
*Ruocheng Guo,Kaiwen Dong,Xiang Gao,Kamalika Das*

Main category: cs.AI

TL;DR: Trace-Free+：一种无需执行轨迹的课程学习框架，通过渐进式监督转移优化工具接口，提升LLM智能体在未见工具上的性能表现


<details>
  <summary>Details</summary>
Motivation: LLM智能体的性能不仅取决于智能体本身，还依赖于所使用工具接口的质量。现有工具接口主要面向人类设计，在大规模候选工具集中成为瓶颈。传统方法依赖执行轨迹，但在冷启动或隐私受限场景中难以获取，且通常独立优化每个工具，限制了可扩展性和对未见工具的泛化能力。

Method: 提出Trace-Free+课程学习框架，渐进式地将监督从轨迹丰富的设置转移到无轨迹部署环境，鼓励模型抽象可重用的接口使用模式和工具使用结果。通过结构化工作流程构建大规模高质量工具接口数据集来支持该方法。

Result: 在StableToolBench和RestBench上的实验显示，该方法在未见工具上获得一致性能提升，具有强大的跨领域泛化能力，并且在候选工具数量超过100时仍保持鲁棒性，证明工具接口优化是智能体微调的实际可行补充方案。

Conclusion: 工具接口优化是提升LLM智能体性能的重要补充方向。Trace-Free+框架通过课程学习有效解决了传统方法对执行轨迹的依赖问题，实现了对未见工具的泛化能力，为大规模工具集场景提供了实用的部署方案。

Abstract: The performance of LLM-based agents depends not only on the agent itself but also on the quality of the tool interfaces it consumes. While prior work has focused heavily on agent fine-tuning, tool interfaces-including natural language descriptions and parameter schemas-remain largely human-oriented and often become a bottleneck, especially when agents must select from large candidate tool sets. Existing approaches to improving tool interfaces rely on execution traces, which are frequently unavailable in cold-start or privacy-constrained settings, and typically optimize each tool independently, limiting scalability and generalization to unseen tools. We propose Trace-Free+, a curriculum learning framework that progressively transfers supervision from trace-rich settings to trace-free deployment, encouraging the model to abstract reusable interface-usage patterns and tool usage outcomes. To support this approach, we construct a large-scale dataset of high-quality tool interfaces using a structured workflow over a diverse collection of tools. Experiments on StableToolBench and RestBench show consistent gains on unseen tools, strong cross-domain generalization, and robustness as the number of candidate tools scales to over 100, demonstrating that tool interface optimization is a practical and deployable complement to agent fine-tuning.

</details>


### [34] [PromptCD: Test-Time Behavior Enhancement via Polarity-Prompt Contrastive Decoding](https://arxiv.org/abs/2602.20696)
*Baolong Bi,Yuyao Ge,Shenghua Liu,Yuchen He,Siqian Tong,Lizhe Chen,Lingrui Mei,Zehao Li,Yiwei Wang,Yujun Cai,Ming-Hsuan Yang,Xueqi Cheng*

Main category: cs.AI

TL;DR: 提出PromptCD方法，通过构建正负引导提示对比模型响应，在测试时控制LLM和VLM的行为，无需额外训练即可提升对齐性能


<details>
  <summary>Details</summary>
Motivation: 现有对齐方法多在训练时进行，需要高质量数据和计算成本高。对比解码方法适用范围有限，需要更通用的测试时行为控制方法

Method: 提出Polarity-Prompt Contrastive Decoding (PromptCD)：为目标行为构建正负引导提示对，对比模型响应（LLM中的token概率分布和VLM中的视觉注意力模式）来强化期望结果

Result: LLM在"3H"对齐目标（helpfulness、honesty、harmlessness）上获得一致显著改进；VLM通过强化行为一致的视觉基础显著提升VQA性能

Conclusion: PromptCD是一种简单、通用且成本高效的跨模态可靠行为控制策略，使后训练模型能在测试时实现有意义的自我增强

Abstract: Reliable AI systems require large language models (LLMs) to exhibit behaviors aligned with human preferences and values. However, most existing alignment approaches operate at training time and rely on additional high-quality data, incurring significant computational and annotation costs. While recent work has shown that contrastive decoding can leverage a model's internal distributions to improve specific capabilities, its applicability remains limited to narrow behavioral scopes and scenarios. In this work, we introduce Polarity-Prompt Contrastive Decoding (PromptCD), a test-time behavior control method that generalizes contrastive decoding to broader enhancement settings. PromptCD constructs paired positive and negative guiding prompts for a target behavior and contrasts model responses-specifically token-level probability distributions in LLMs and visual attention patterns in VLMs-to reinforce desirable outcomes. This formulation extends contrastive decoding to a wide range of enhancement objectives and is applicable to both LLMs and Vision-Language Models (VLMs) without additional training. For LLMs, experiments on the "3H" alignment objectives (helpfulness, honesty, and harmlessness) demonstrate consistent and substantial improvements, indicating that post-trained models can achieve meaningful self-enhancement purely at test time. For VLMs, we further analyze contrastive effects on visual attention, showing that PromptCD significantly improves VQA performance by reinforcing behavior-consistent visual grounding. Collectively, these results highlight PromptCD as a simple, general, and cost-efficient strategy for reliable behavior control across modalities.

</details>


### [35] [CG-DMER: Hybrid Contrastive-Generative Framework for Disentangled Multimodal ECG Representation Learning](https://arxiv.org/abs/2602.21154)
*Ziwei Niu,Hao Sun,Shujun Bian,Xihong Yang,Lanfen Lin,Yuxin Liu,Yueming Jin*

Main category: cs.AI

TL;DR: CG-DMER是一个用于心电图多模态表示学习的对比生成框架，通过时空掩码建模和表示解耦对齐策略，解决了现有方法在模态内和模态间的问题，在三个公开数据集上取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 当前多模态心电图分析方法存在两个主要问题：1) 模态内问题：现有模型以导联无关的方式处理心电图，忽略了导联间的时空依赖性，限制了其对细粒度诊断模式的建模能力；2) 模态间问题：现有方法直接将心电图信号与临床报告对齐，由于报告的自由文本特性引入了模态特定偏差。

Method: 提出了CG-DMER框架，包含两个关键设计：1) 时空掩码建模：通过在时空维度上应用掩码并重建缺失信息，更好地捕捉细粒度时间动态和导联间空间依赖性；2) 表示解耦与对齐策略：通过引入模态特定和模态共享编码器，分离模态不变和模态特定表示，减少不必要噪声和模态特定偏差。

Result: 在三个公开数据集上的实验表明，CG-DMER在多种下游任务中实现了最先进的性能。

Conclusion: CG-DMER通过时空掩码建模和表示解耦对齐策略，有效解决了心电图多模态分析中的模态内和模态间问题，为心血管疾病诊断提供了更准确的心电图表示学习方法。

Abstract: Accurate interpretation of electrocardiogram (ECG) signals is crucial for diagnosing cardiovascular diseases. Recent multimodal approaches that integrate ECGs with accompanying clinical reports show strong potential, but they still face two main concerns from a modality perspective: (1) intra-modality: existing models process ECGs in a lead-agnostic manner, overlooking spatial-temporal dependencies across leads, which restricts their effectiveness in modeling fine-grained diagnostic patterns; (2) inter-modality: existing methods directly align ECG signals with clinical reports, introducing modality-specific biases due to the free-text nature of the reports. In light of these two issues, we propose CG-DMER, a contrastive-generative framework for disentangled multimodal ECG representation learning, powered by two key designs: (1) Spatial-temporal masked modeling is designed to better capture fine-grained temporal dynamics and inter-lead spatial dependencies by applying masking across both spatial and temporal dimensions and reconstructing the missing information. (2) A representation disentanglement and alignment strategy is designed to mitigate unnecessary noise and modality-specific biases by introducing modality-specific and modality-shared encoders, ensuring a clearer separation between modality-invariant and modality-specific representations. Experiments on three public datasets demonstrate that CG-DMER achieves state-of-the-art performance across diverse downstream tasks.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [36] [OptiLeak: Efficient Prompt Reconstruction via Reinforcement Learning in Multi-tenant LLM Services](https://arxiv.org/abs/2602.20595)
*Longxiang Wang,Xiang Zheng,Xuhao Zhang,Yao Zhang,Ye Wu,Cong Wang*

Main category: cs.CR

TL;DR: OptiLeak：基于强化学习的优化框架，通过两阶段微调最大化提示重建效率，显著降低缓存侧信道攻击成本，揭示LLM共享缓存存在更严重的隐私风险


<details>
  <summary>Details</summary>
Motivation: 多租户LLM服务框架广泛采用共享键值缓存提高效率，但这带来了侧信道漏洞，可能导致提示泄露攻击。先前研究虽然识别了攻击面，但主要关注扩展攻击向量而非优化攻击性能，报告的攻击成本过高，低估了实际的隐私风险

Method: 提出OptiLeak框架，采用强化学习增强的两阶段微调方法。核心洞察是通过似然排序自动识别领域特定的"硬标记"（难以预测但携带敏感信息的术语），并利用这些标记构建偏好对进行直接偏好优化，无需人工标注。这实现了有效的偏好对齐，同时避免了扩展监督微调的过拟合问题

Result: 在医疗和金融领域的三个基准测试中，OptiLeak相比基线方法实现了高达12.48倍的每个标记平均请求数减少，在3B到14B参数规模的各种模型上均表现出一致的改进

Conclusion: 基于缓存的提示泄露攻击比先前报告的要严重得多，突显了在生产部署中需要强大的缓存隔离机制。OptiLeak框架显著提高了攻击效率，揭示了共享缓存架构的实际隐私风险

Abstract: Multi-tenant LLM serving frameworks widely adopt shared Key-Value caches to enhance efficiency. However, this creates side-channel vulnerabilities enabling prompt leakage attacks. Prior studies identified these attack surfaces yet focused on expanding attack vectors rather than optimizing attack performance, reporting impractically high attack costs that underestimate the true privacy risk. We propose OptiLeak, a reinforcement learning-enhanced framework that maximizes prompt reconstruction efficiency through two-stage fine-tuning. Our key insight is that domain-specific ``hard tokens'' -- terms difficult to predict yet carrying sensitive information -- can be automatically identified via likelihood ranking and used to construct preference pairs for Direct Preference Optimization, eliminating manual annotation. This enables effective preference alignment while avoiding the overfitting issues of extended supervised fine-tuning. Evaluated on three benchmarks spanning medical and financial domains, OptiLeak achieves up to $12.48\times$ reduction in average requests per token compared to baseline approaches, with consistent improvements across model scales from 3B to 14B parameters. Our findings demonstrate that cache-based prompt leakage poses a more severe threat than previously reported, underscoring the need for robust cache isolation in production deployments.

</details>


### [37] [Post-Quantum Sanitizable Signatures from McEliece-Based Chameleon Hashing](https://arxiv.org/abs/2602.20657)
*Shahzad Ahmad,Stefan Rass,Zahra Seyedi*

Main category: cs.CR

TL;DR: 本文提出了首个基于编码的、后量子安全的可净化签名方案，使用基于McEliece密码系统的变色龙哈希函数，实现了完美的透明性，使净化后的签名与原始签名无法区分。


<details>
  <summary>Details</summary>
Motivation: 随着量子计算的发展，传统签名方案面临安全威胁，需要设计后量子安全的可净化签名方案。现有方案缺乏基于编码的透明可净化签名方案，这限制了在长期安全应用中的部署。

Method: 方案基于McEliece密码系统构建变色龙哈希函数，指定净化者拥有Goppa码的陷门，通过Patterson解码实现可控碰撞查找。通过施加特定的随机化权重约束，实现完美的透明性。

Result: 提供了形式化的安全定义和严格的存在不可伪造性和不可变性证明，基于随机预言模型中综合征解码的困难性。实现了首个透明的、基于编码的、后量子安全的可净化签名方案。

Conclusion: 该工作建立了首个透明的、基于编码的、后量子安全的可净化签名方案，提供了强大的理论保证，并为长期安全应用的实际部署开辟了途径。

Abstract: We introduce a novel post-quantum sanitizable signature scheme constructed upon a chameleon hash function derived from the McEliece cryptosystem. In this design, the designated sanitizer possesses the inherent trapdoor of a Goppa code, which facilitates controlled collision-finding via Patterson decoding. This mechanism enables authorized modification of specific message blocks while ensuring all other content remains immutably bound. We provide formal security definitions and rigorous proofs of existential unforgeability and immutability, grounded in the hardness of syndrome decoding in the random-oracle model, where a robust random oracle thwarts trivial linear hash collisions. A key innovation lies in our precise characterization of the transparency property: by imposing a specific weight constraint on the randomizers generated by the signer, we achieve perfect transparency, rendering sanitized signatures indistinguishable from freshly signed ones. This work establishes the first transparent, code-based, post-quantum sanitizable signature scheme, offering strong theoretical guarantees and a pathway for practical deployment in long-term secure applications.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [38] [UAMTERS: Uncertainty-Aware Mutation Analysis for DL-enabled Robotic Software](https://arxiv.org/abs/2602.20334)
*Chengjie Lu,Jiahui Wu,Shaukat Ali,Malaika Din Hashmi,Sebastian Mathias Thomle Mason,Francois Picard,Mikkel Labori Olsen,Thomas Peyrucain*

Main category: cs.SE

TL;DR: UAMTERS是一个不确定性感知的变异分析框架，专门针对深度学习机器人软件，通过注入随机不确定性来评估测试套件在动态环境中的有效性。


<details>
  <summary>Details</summary>
Motivation: 深度学习组件为自适应机器人提供了感知、决策和控制功能，但其固有的不确定性使得在动态环境中确保软件可靠性变得困难。现有的变异分析技术缺乏针对深度学习软件不确定性的评估方法。

Method: 提出UAMTERS框架，包含不确定性感知的变异算子来显式注入随机不确定性到深度学习机器人软件中，模拟行为不确定性，并提出变异分数指标来量化测试套件在不同不确定性水平下检测故障的能力。

Result: 在三个机器人案例研究中评估UAMTERS，证明其能更有效地区分测试套件质量，并捕获深度学习软件中由不确定性引起的故障。

Conclusion: UAMTERS框架为深度学习机器人软件的可靠性评估提供了有效的工具，能够更好地评估测试套件在不确定性环境中的有效性。

Abstract: Self-adaptive robots adjust their behaviors in response to unpredictable environmental changes. These robots often incorporate deep learning (DL) components into their software to support functionality such as perception, decision-making, and control, enhancing autonomy and self-adaptability. However, the inherent uncertainty of DL-enabled software makes it challenging to ensure its dependability in dynamic environments. Consequently, test generation techniques have been developed to test robot software, and classical mutation analysis injects faults into the software to assess the test suite's effectiveness in detecting the resulting failures. However, there is a lack of mutation analysis techniques to assess the effectiveness under the uncertainty inherent to DL-enabled software. To this end, we propose UAMTERS, an uncertainty-aware mutation analysis framework that introduces uncertainty-aware mutation operators to explicitly inject stochastic uncertainty into DL-enabled robotic software, simulating uncertainty in its behavior. We further propose mutation score metrics to quantify a test suite's ability to detect failures under varying levels of uncertainty. We evaluate UAMTERS across three robotic case studies, demonstrating that UAMTERS more effectively distinguishes test suite quality and captures uncertainty-induced failures in DL-enabled software.

</details>


### [39] [A Case Study on Runtime Verification of a Continuous Deployment Process](https://arxiv.org/abs/2602.20598)
*Shoma Ansai,Masaki Waga*

Main category: cs.SE

TL;DR: 该研究应用运行时监控到基于FluxCD的持续部署流程，发现FluxCD在镜像推送到GHCR后5分钟内不一定能检测到新镜像，但在10分钟内总能检测到，且SyMon监控工具足够快。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索在基于FluxCD的持续部署流程中应用运行时监控的实际效果，了解部署更新的检测延迟情况，并评估监控工具的性能。

Method: 研究方法包括：1）构建由GitHub Actions、GitHub Container Registry、FluxCD和Kubernetes应用组成的系统；2）使用SyMon监控系统日志；3）定义部署更新检测标准为FluxCD轮询日志解析到最新镜像标签；4）通过案例研究收集和分析数据。

Result: 研究结果显示：1）FluxCD在镜像推送到GHCR后5分钟内不一定能检测到新镜像；2）但在收集的日志中，FluxCD总能在10分钟内检测到新镜像；3）SyMon监控工具在实验设置中足够快，能够支持近实时监控。

Conclusion: 结论表明：在基于FluxCD的持续部署环境中，部署更新的检测存在一定延迟（5-10分钟），但SyMon能够有效支持近实时监控，为类似系统的监控实践提供了实证参考。

Abstract: We report our experience in applying runtime monitoring to a FluxCD-based continuous deployment (CD) process. Our target system consists of GitHub Actions, GitHub Container Registry (GHCR), FluxCD, and an application running on Kubernetes. We monitored its logs using SyMon. In our setting, we regard a deployment update as detected when FluxCD's polling log resolves the latest image tag. Through the case study, we found that FluxCD did not always detect a new image within five minutes after it was pushed to GHCR, whereas it always did so within ten minutes in the collected logs. Moreover, our results show that SyMon is fast enough for near-real-time monitoring in our setting.

</details>


### [40] [SpecMind: Cognitively Inspired, Interactive Multi-Turn Framework for Postcondition Inference](https://arxiv.org/abs/2602.20610)
*Cuong Chi Le,Minh V. T Pham,Tung Vu Duy,Cuong Duc Van,Huy N. Phan,Hoang N. Phan,Tien N. Nguyen*

Main category: cs.SE

TL;DR: SpecMind：一个基于反馈驱动多轮提示的LLM框架，用于迭代生成和优化程序后置条件，显著提升准确性和完整性


<details>
  <summary>Details</summary>
Motivation: 手动编写程序规范耗时且困难，现有基于LLM的单次提示方法生成的后置条件往往不准确，需要更有效的自动化规范生成方法

Method: 将LLM作为交互式探索推理器而非一次性生成器，采用反馈驱动的多轮提示方法，通过隐式和显式正确性反馈迭代优化候选后置条件，并自主决定何时停止

Result: SpecMind在生成后置条件的准确性和完整性方面显著优于现有最先进方法

Conclusion: 通过将LLM作为交互式探索推理器，采用反馈驱动的多轮提示方法，能够有效提升程序后置条件生成的准确性和完整性

Abstract: Specifications are vital for ensuring program correctness, yet writing them manually remains challenging and time-intensive. Recent large language model (LLM)-based methods have shown successes in generating specifications such as postconditions, but existing single-pass prompting often yields inaccurate results. In this paper, we present SpecMind, a novel framework for postcondition generation that treats LLMs as interactive and exploratory reasoners rather than one-shot generators. SpecMind employs feedback-driven multi-turn prompting approaches, enabling the model to iteratively refine candidate postconditions by incorporating implicit and explicit correctness feedback, while autonomously deciding when to stop. This process fosters deeper code comprehension and improves alignment with true program behavior via exploratory attempts. Our empirical evaluation shows that SpecMind significantly outperforms state-of-the-art approaches in both accuracy and completeness of generated postconditions.

</details>
